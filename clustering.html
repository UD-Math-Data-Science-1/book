<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Science 1 - 6&nbsp; Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./networks.html" rel="next">
<link href="./regression.html" rel="prev">
<link href="./_media/logo_small.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script> 
MathJax = {
  chtml: {
    scale: 0.92,
  }
}
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science 1</a> 
        <div class="sidebar-tools-main">
    <a href="./Data-Science-1.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">Resources</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Representation of data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./networks.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#clustering" id="toc-clustering" class="nav-link active" data-scroll-target="#clustering"><span class="toc-section-number">7</span>  Clustering</a>
  <ul class="collapse">
  <li><a href="#similarity-and-distance" id="toc-similarity-and-distance" class="nav-link" data-scroll-target="#similarity-and-distance"><span class="toc-section-number">7.1</span>  Similarity and distance</a>
  <ul class="collapse">
  <li><a href="#distance-metrics" id="toc-distance-metrics" class="nav-link" data-scroll-target="#distance-metrics"><span class="toc-section-number">7.1.1</span>  Distance metrics</a></li>
  <li><a href="#probability-distributions" id="toc-probability-distributions" class="nav-link" data-scroll-target="#probability-distributions"><span class="toc-section-number">7.1.2</span>  Probability distributions</a></li>
  <li><a href="#distance-matrix" id="toc-distance-matrix" class="nav-link" data-scroll-target="#distance-matrix"><span class="toc-section-number">7.1.3</span>  Distance matrix</a></li>
  <li><a href="#distance-in-high-dimensions" id="toc-distance-in-high-dimensions" class="nav-link" data-scroll-target="#distance-in-high-dimensions"><span class="toc-section-number">7.1.4</span>  Distance in high dimensions</a></li>
  </ul></li>
  <li><a href="#performance-measures" id="toc-performance-measures" class="nav-link" data-scroll-target="#performance-measures"><span class="toc-section-number">7.2</span>  Performance measures</a>
  <ul class="collapse">
  <li><a href="#rand-index-and-ari" id="toc-rand-index-and-ari" class="nav-link" data-scroll-target="#rand-index-and-ari"><span class="toc-section-number">7.2.1</span>  Rand index and ARI</a></li>
  <li><a href="#silhouettes" id="toc-silhouettes" class="nav-link" data-scroll-target="#silhouettes"><span class="toc-section-number">7.2.2</span>  Silhouettes</a></li>
  </ul></li>
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means"><span class="toc-section-number">7.3</span>  k-means</a>
  <ul class="collapse">
  <li><a href="#lloyds-algorithm" id="toc-lloyds-algorithm" class="nav-link" data-scroll-target="#lloyds-algorithm"><span class="toc-section-number">7.3.1</span>  Lloyd’s algorithm</a></li>
  <li><a href="#practical-issues" id="toc-practical-issues" class="nav-link" data-scroll-target="#practical-issues"><span class="toc-section-number">7.3.2</span>  Practical issues</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="toc-section-number">7.4</span>  Hierarchical clustering</a>
  <ul class="collapse">
  <li><a href="#case-study-penguins" id="toc-case-study-penguins" class="nav-link" data-scroll-target="#case-study-penguins"><span class="toc-section-number">7.4.1</span>  Case study: Penguins</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div>
<div class="hidden">
<p><span class="math display">\[
    \newcommand{\float}{\mathbb{F}}
    \newcommand{\real}{\mathbb{R}}
    \newcommand{\complex}{\mathbb{C}}
    \newcommand{\nat}{\mathbb{N}}
    \newcommand{\integer}{\mathbb{Z}}
    \newcommand{\bfa}{\mathbf{a}}
    \newcommand{\bfe}{\mathbf{e}}
    \newcommand{\bfh}{\mathbf{h}}
    \newcommand{\bfp}{\mathbf{p}}
    \newcommand{\bfq}{\mathbf{q}}
    \newcommand{\bfu}{\mathbf{u}}
    \newcommand{\bfv}{\mathbf{v}}
    \newcommand{\bfw}{\mathbf{w}}
    \newcommand{\bfx}{\mathbf{x}}
    \newcommand{\bfy}{\mathbf{y}}
    \newcommand{\bfz}{\mathbf{z}}
    \newcommand{\bfA}{\mathbf{A}}
    \newcommand{\bfW}{\mathbf{W}}
    \newcommand{\bfX}{\mathbf{X}}
    \newcommand{\bfzero}{\boldsymbol{0}}
    \newcommand{\bfmu}{\boldsymbol{\mu}}
    \newcommand{\TP}{\text{TP}}
    \newcommand{\TN}{\text{TN}}
    \newcommand{\FP}{\text{FP}}
    \newcommand{\FN}{\text{FN}}
    \newcommand{\rmn}[2]{\mathbb{R}^{#1 \times #2}}
    \newcommand{\dd}[2]{\frac{d #1}{d #2}}
    \newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
    \newcommand{\norm}[1]{\left\lVert \mathstrut #1 \right\rVert}
    \newcommand{\abs}[1]{\left\lvert \mathstrut #1 \right\rvert}
    \newcommand{\twonorm}[1]{\norm{#1}_2}
    \newcommand{\onenorm}[1]{\norm{#1}_1}
    \newcommand{\infnorm}[1]{\norm{#1}_\infty}
    \newcommand{\innerprod}[2]{\langle #1,#2 \rangle}
    \newcommand{\pr}[1]{^{(#1)}}
    \newcommand{\diag}{\operatorname{diag}}
    \newcommand{\sign}{\operatorname{sign}}
    \newcommand{\dist}{\operatorname{dist}}
    \newcommand{\simil}{\operatorname{sim}}
    \newcommand{\ee}{\times 10^}
    \newcommand{\floor}[1]{\lfloor#1\rfloor}
    \newcommand{\argmin}{\operatorname{argmin}}
    \newcommand{\E}[1]{\operatorname{\mathbb{E}}\left[\mathstrut #1\right]}
    \newcommand{\Cov}{\operatorname{Cov}}
    \newcommand{\logit}{\operatorname{logit}}
\]</span></p>
</div>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> default_rng</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> shuffle</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, f1_score, balanced_accuracy_score</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, StratifiedKFold</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_validate, validation_curve</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<section id="clustering" class="level1 page-columns page-full" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Clustering</h1>
<p>In supervised learning, the data samples are supplied with labels, and the goal of the learner is to generalize the examples to new values. In unsupervised learning, there are no labels. Instead, the goal is to discover structure that is intrinsic to the feature matrix. Common problem types in unsupervised learning are</p>
<ul>
<li><strong>Clustering</strong>. Determine whether the samples roughly divide into a small number of classes.</li>
<li><strong>Dimension reduction</strong>. Find a reduced set of features, or create a small set of new features, that describe the data well.</li>
<li><strong>Outlier detection</strong>. Find anomalous values in the data set, and remove them or impute replacements.</li>
</ul>
<p>In this chapter we will look at clustering. In order to get a feeling for the algorithms, we will apply them to three illustrative datasets:</p>
<ul>
<li><dl>
<dt>blobs</dt>
<dd>
This dataset has one distinct blob, plus two that kind of overlap a bit:
</dd>
</dl></li>
</ul>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blobs_data():</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> make_blobs(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        n_samples<span class="op">=</span>[<span class="dv">60</span>, <span class="dv">50</span>, <span class="dv">40</span>],</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        centers<span class="op">=</span>[[<span class="op">-</span><span class="dv">2</span>,<span class="dv">3</span>], [<span class="dv">3</span>,<span class="fl">1.5</span>], [<span class="dv">1</span>,<span class="op">-</span><span class="dv">3</span>]],</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        cluster_std<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">1.2</span>],</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        random_state <span class="op">=</span> <span class="dv">19716</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">"x1"</span>:X[:,<span class="dv">0</span>], <span class="st">"x2"</span>:X[:,<span class="dv">1</span>], <span class="st">"class"</span>:y})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>blobs <span class="op">=</span> blobs_data()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(blobs, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"class"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-4-output-1.png" width="526" height="468"></p>
</div>
</div>
<ul>
<li><dl>
<dt>stripes</dt>
<dd>
In this dataset, there is clear separation along one axis and a continuous blob along the other:
</dd>
</dl></li>
</ul>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stripes_data():</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> default_rng(<span class="dv">9</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    x1,x2,cls <span class="op">=</span> [],[],[]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        x1.extend( rng.uniform(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, size<span class="op">=</span><span class="dv">200</span>) )</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        x2.extend( <span class="fl">2.5</span><span class="op">*</span>i<span class="op">+</span>rng.uniform(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span><span class="dv">200</span>) )</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        cls.extend( [i]<span class="op">*</span><span class="dv">200</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">"x1"</span>: x1, <span class="st">"x2"</span>: x2, <span class="st">"class"</span>: cls})</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>stripes <span class="op">=</span> stripes_data()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(stripes, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span><span class="st">"class"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-6-output-1.png" width="528" height="468"></p>
</div>
</div>
<ul>
<li><dl>
<dt>bullseye</dt>
<dd>
This is the most challenging dataset, because the clusters can’t be separated by anything like straight lines:
</dd>
</dl></li>
</ul>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bullseye_data():</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> default_rng(<span class="dv">6</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    inner <span class="op">=</span> <span class="fl">0.8</span><span class="op">*</span>rng.normal(size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">2</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> rng.uniform(<span class="dv">3</span>, <span class="dv">4</span>, size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    middle <span class="op">=</span> np.vstack((r<span class="op">*</span>np.cos(theta), r<span class="op">*</span>np.sin(theta))).T</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> rng.uniform(<span class="dv">5</span>,<span class="dv">6</span>,size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    outer <span class="op">=</span> np.vstack((r<span class="op">*</span>np.cos(theta), r<span class="op">*</span>np.sin(theta))).T</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    cls <span class="op">=</span> np.hstack( ([<span class="dv">1</span>]<span class="op">*</span><span class="dv">100</span>, [<span class="dv">2</span>]<span class="op">*</span><span class="dv">200</span>, [<span class="dv">3</span>]<span class="op">*</span><span class="dv">200</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    bullseye <span class="op">=</span> pd.DataFrame( np.vstack((inner,middle,outer)), columns<span class="op">=</span>[<span class="st">"x1"</span>, <span class="st">"x2"</span>] )</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    bullseye[<span class="st">"class"</span>] <span class="op">=</span> cls </span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> bullseye</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>bullseye <span class="op">=</span> bullseye_data()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sns.relplot(bullseye,</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"class"</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>p.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-8-output-1.png" width="524" height="468"></p>
</div>
</div>
<section id="similarity-and-distance" class="level2" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="similarity-and-distance"><span class="header-section-number">7.1</span> Similarity and distance</h2>
<p>Given an <span class="math inline">\(n\times d\)</span> feature matrix, we want to define disjoint subsets of the <span class="math inline">\(\bfx_i\)</span> such that the samples within a subset, or <strong>cluster</strong>, are more similar to one another than they are to members of other clusters.</p>
<p>The first decision we have to make is how to measure <em>similarity</em>. When a distance metric is available, we consider similarity to be inversely related to distance. For example, if we have defined a distance function between pairs of vectors as <span class="math inline">\(\dist(\bfx,\bfy)\)</span>, then we could define similarity as</p>
<p><span class="math display">\[
\simil(\bfx,\bfy) = \exp \left[ - \frac{\dist(\bfx,\bfy)^2}{2\sigma^2}  \right].
\]</span></p>
<p>Thus a distance of zero implies a similarity of 1, while the similarity tends to zero as distance increases. The scaling parameter <span class="math inline">\(\sigma\)</span> controls the rate of decrease; for instance, when the distance is <span class="math inline">\(\sigma\)</span>, the similarity is <span class="math inline">\(e^{-1/2}\approx 0.6\)</span>.</p>
<p>There are ways to define similarity without making use of a distance, but we won’t be using them.</p>
<section id="distance-metrics" class="level3" data-number="7.1.1">
<h3 data-number="7.1.1" class="anchored" data-anchor-id="distance-metrics"><span class="header-section-number">7.1.1</span> Distance metrics</h3>
<div id="def-cluster-metric" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.1 </strong></span>A <strong>distance metric</strong> is a function dist on pairs of vectors that satisfies the following properties for all vectors:</p>
<ol type="1">
<li><span class="math inline">\(\dist(\bfx,\bfy)=0\)</span> if and only if <span class="math inline">\(\bfx=\bfy\)</span>,</li>
<li><span class="math inline">\(\dist(\bfx,\bfy) = \dist(\bfy,\bfx)\)</span>, and</li>
<li><span class="math inline">\(\dist(\bfx,\bfy) \le \dist(\bfx,\bfz) + \dist(\bfz,\bfy)\)</span>, known as the triangle inequality.</li>
</ol>
</div>
<p>These are considered the essential axioms of a distance metric. From them, you can also deduce that the distance function is always nonnegative.</p>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>The term <em>distance metric</em> isn’t always used carefully to mean a function satisfying the three axioms, however, and some applications use a metric that does not satisfy the triangle inequality.</p>
</div>
</div>
<p>We already have the distance metric defined by</p>
<p><span class="math display">\[
\dist(\bfx,\bfy) = \norm{\bfx-\bfy}
\]</span></p>
<p>for any vector norm.</p>
<p>Another proper distance metric is <strong>angular distance</strong>. Generalizing from 2D and 3D vector geometry, we define the angle <span class="math inline">\(\theta\)</span> between vectors <span class="math inline">\(\bfu\)</span> and <span class="math inline">\(\bfv\)</span> in <span class="math inline">\(\real^d\)</span> by</p>
<p><span id="eq-clustering-angle"><span class="math display">\[
\cos(\theta) = \frac{\mathbf{u}^T\mathbf{v}}{\twonorm{\mathbf{u}} \, \twonorm{\mathbf{v}}}.
\tag{7.1}\]</span></span></p>
<p>Then the quantity <span class="math inline">\(\theta/\pi\)</span> is a distance metric. Because arccos is a relatively expensive computational operation, though, it’s common to use <strong>cosine similarity</strong>, defined as <span class="math inline">\(\cos(\theta)\)</span>, and the related <strong>cosine distance</strong> <span class="math inline">\(\tfrac{1}{2}[1-\cos(\theta)]\)</span>, even though the latter does not satisfy the triangle inequality.</p>
<p>Categorical variables can be included in distance metrics. An ordinal variable is easily converted to equally spaced numerical values, which then may get a standard treatment. Nominal features are often compared using <strong>Hamming distance</strong>, which is just the total number of features that have different values in the two vectors.</p>
</section>
<section id="probability-distributions" class="level3" data-number="7.1.2">
<h3 data-number="7.1.2" class="anchored" data-anchor-id="probability-distributions"><span class="header-section-number">7.1.2</span> Probability distributions</h3>
<div id="def-cluster-probdist" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.2 </strong></span>A <strong>discrete probability distribution</strong> is a vector <span class="math inline">\(\bfx\)</span> whose components are nonnegative and satisfying <span class="math inline">\(\onenorm{\bfx}=1\)</span>.</p>
</div>
<p>Such a vector can be interpreted as frequencies or probabilities of observing different classes, for example. We already encountered one way to measure the dissimilarity of two probability distributions, the <em>cross-entropy</em>:</p>
<p><span class="math display">\[
\operatorname{CE}(\bfx,\bfy) = -\sum_{i=1}^d x_i \log(y_i).
\]</span></p>
<p>A related measure is the <strong>Kullback–Leibler (KL) divergence</strong> or <em>relative entropy</em>,</p>
<p><span class="math display">\[
\operatorname{KL}(\bfx,\bfy) = \sum_{i=1}^d x_i \log\left( \frac{x_i}{y_i} \right).
\]</span></p>
<p>Whenever <span class="math inline">\(0\cdot \log(0)\)</span> is encountered in the CE or KL definitions, it equals zero, in accordance with its limiting value from calculus.</p>
<p>Neither cross-entropy nor KL divergence are symmetric in their arguments. But there is a related value called <strong>information radius</strong>, defined as</p>
<p><span class="math display">\[
\operatorname{IR}(\bfu,\bfv) = \frac{1}{2} \left[ \operatorname{KL}(\bfu,\bfz) + \operatorname{KL}(\bfv,\bfz) \right]
\]</span></p>
<p>where <span class="math inline">\(\bfz=(\bfu+\bfv)/2\)</span>. Typically one uses a base-2 logarithm, in which case IR ranges between 0 and 1. The square root of IR is a distance metric.</p>
<div id="exm-cluster-IR" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.1 </strong></span>Let <span class="math inline">\(\bfu=\frac{1}{4}[1,3]\)</span> and <span class="math inline">\(\bfv=\frac{1}{4}[3,1]\)</span>. Then <span class="math inline">\(\bfz=(\bfu+\bfv)/2=[\tfrac{1}{2},\tfrac{1}{2}]\)</span>, and</p>
<p><span class="math display">\[
\begin{split}
    \operatorname{KL}(\bfu,\bfz)  &amp;= \tfrac{1}{4} \cdot \log \left( \frac{1/4}{1/2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3/4}{1/2} \right) \\ &amp;= \tfrac{1}{4} \cdot \log \left( \frac{1}{2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3}{2} \right) = -\tfrac{1}{4} + \tfrac{3}{4} (\log(3)-1),\\
    \operatorname{KL}(\bfv,\bfz)  &amp;= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4},\\
    \operatorname{IR}(\bfu,\bfu)  &amp;= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4} \approx 0.1887.
\end{split}
\]</span></p>
</div>
</section>
<section id="distance-matrix" class="level3" data-number="7.1.3">
<h3 data-number="7.1.3" class="anchored" data-anchor-id="distance-matrix"><span class="header-section-number">7.1.3</span> Distance matrix</h3>
<div id="def-cluster-distmat" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.3 </strong></span>Given the feature vectors <span class="math inline">\(\bfx_1,\ldots,\bfx_n\)</span>, the pairwise distances between them are collected in the <span class="math inline">\(n\times n\)</span> <strong>distance matrix</strong></p>
<p><span class="math display">\[
D_{ij} = \text{dist}(\bfx_i,\bfx_j).
\]</span></p>
</div>
<p>Note that <span class="math inline">\(D_{ii}=0\)</span> and <span class="math inline">\(D_{ji}=D_{ij}\)</span>. Many clustering algorithms allow supplying <span class="math inline">\(\mathbf{D}\)</span> in lieu of the feature vectors.</p>
<p>One can analogously define a <em>similarity matrix</em> using the Gaussian kernel. An advantage of similarity is that small values can be rounded down to zero. This has little effect on the results, but can create big gains in execution time and memory usage.</p>
<div id="exm-cluster-distmat" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.2 </strong></span>The distance matrix of our bullseye dataset has some interesting structure:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> pairwise_distances</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> bullseye_data()[[<span class="st">"x1"</span>, <span class="st">"x2"</span>]]</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>D2 <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"euclidean"</span>)   <span class="co"># use 2-norm metric</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.heatmap(D2)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-9-output-1.png" width="494" height="430"></p>
</div>
</div>
<p>Because we set up three geometrically distinct groups of points, the distances of pairs within and between groups are fairly homogeneous. The lower-right corner, for example, shows that points in the outermost ring tend to be separated by the greatest distance.</p>
<p>In the 1-norm, the stripes dataset is also a little interesting:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> stripes_data()[[<span class="st">"x1"</span>, <span class="st">"x2"</span>]]</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>D1 <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"manhattan"</span>)   <span class="co"># use 1-norm metric</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.heatmap(D1)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-10-output-1.png" width="494" height="430"></p>
</div>
</div>
<p>Points in different stripes are always separated by at least the inter-stripe distance, while points within the same stripe have a range of possible distances.</p>
</div>
</section>
<section id="distance-in-high-dimensions" class="level3" data-number="7.1.4">
<h3 data-number="7.1.4" class="anchored" data-anchor-id="distance-in-high-dimensions"><span class="header-section-number">7.1.4</span> Distance in high dimensions</h3>
<p>High-dimensional space <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">does not conform to some intuitions</a> formed by our experiences in 2D and 3D.</p>
<p>For example, consider the unit hyperball <span class="math inline">\(\twonorm{\bfx}\le 1\)</span> in <span class="math inline">\(d\)</span> dimensions. We’ll take it as given that scaling a <span class="math inline">\(d\)</span>-dimensional object by a number <span class="math inline">\(r\)</span> will scale the volume by <span class="math inline">\(r^d\)</span>. Then for any <span class="math inline">\(r&lt;1\)</span>, the fraction of the unit hyperball’s volume lying <em>outside</em> the smaller hyperball of fixed radius <span class="math inline">\(r\)</span> is <span class="math inline">\(1-r^d\)</span>, which approaches <span class="math inline">\(1\)</span> as <span class="math inline">\(d\to \infty\)</span>. That is, <em>if we choose points randomly within a hyperball, almost all of them will be near the outer boundary</em>.</p>
<p>The volume of the unit hyperball also vanishes as <span class="math inline">\(d\to \infty\)</span>. This is because the inequality</p>
<p><span class="math display">\[
x_1^2 + x_2^2 + \cdots + x_d^2 \le 1,
\]</span></p>
<p>where each <span class="math inline">\(x_i\)</span> is chosen randomly in <span class="math inline">\([-1,1]\)</span>, becomes ever harder to satisfy as the number of terms in the sum grows, and the relative occurrence of such points is increasingly rare.</p>
<p>There are other, similar mathematical results demonstrating the unexpectedness of distances in high-dimensional space. These go under the colorful name <em>curse of dimensionality</em>, and the advice given in response to them is sometimes stated flatly as, “Don’t use distance metrics in high-dimensional space.”</p>
<p>But that advice is easy to overstate. The curse is essentially about <em>randomly</em> chosen points, and it is correct that dimensions of noisy or irrelevant features will make many learning algorithms less effective. But if features carry useful information, adding them usually makes matters better, not worse.</p>
</section>
</section>
<section id="performance-measures" class="level2" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="performance-measures"><span class="header-section-number">7.2</span> Performance measures</h2>
<p>Before we start generating clusterings, we need to decide how we will evaluate them. Recall that a clustering is simply a partitioning of the sample points into disjoint subsets. If a classification of the samples is available, then it automatically implies a clustering: divide the samples into subsets determined by class membership.</p>
<p>We will use some nonstandard terminology that makes the definitions a bit easier to state and read.</p>
<div id="def-cluster-buddies" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 7.4 </strong></span>We say that two sample points in a clustering are <strong>buddies</strong> if they are in the same cluster, and <strong>strangers</strong> otherwise.</p>
</div>
<section id="rand-index-and-ari" class="level3" data-number="7.2.1">
<h3 data-number="7.2.1" class="anchored" data-anchor-id="rand-index-and-ari"><span class="header-section-number">7.2.1</span> Rand index and ARI</h3>
<p>If a trusted or reference clustering is available, then we can compare it to any other clustering result. This allows us to use classification datasets as proving grounds for clustering, although the problems of classification and clustering have different goals (separation versus similarity).</p>
<p>Let <span class="math inline">\(b\)</span> be the number of pairs that are buddies in both clusterings, and let <span class="math inline">\(s\)</span> be the number of pairs that are strangers in both clusterings. Noting that there are <span class="math inline">\(\binom{n}{2}\)</span> distinct pairs of <span class="math inline">\(n\)</span> sample points, we define the <strong>Rand index</strong> by</p>
<p><span class="math display">\[
\text{RI} = \frac{b+s}{\binom{n}{2}}.
\]</span></p>
<p>One way to interpret the Rand index is through binary classification: if we define a positive result on a pair of samples to mean “in the same cluster” and a negative result to mean “in different clusters”, then the Rand index is the accuracy of the classifier over all pairs of samples.</p>
<div id="exm-cluster-rand" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.3 </strong></span>Suppose that samples <span class="math inline">\(x_1,x_2,x_4\)</span> are classified as blue, and <span class="math inline">\(x_3,x_5\)</span> are classified as red. Let’s compute the Rand index relative to the reference classification for the clustering <span class="math inline">\(A=\{x_1,x_2\}\)</span> and <span class="math inline">\(B=\{x_3,x_4,x_5\}\)</span>.</p>
<p>Here is a table showing which pairs of samples are buddies in both clusterings (indicated as TP), strangers in both (TN), or neither (F).</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(x_1\)</span></th>
<th><span class="math inline">\(x_2\)</span></th>
<th><span class="math inline">\(x_3\)</span></th>
<th><span class="math inline">\(x_4\)</span></th>
<th><span class="math inline">\(x_5\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(x_1\)</span></td>
<td></td>
<td>TP</td>
<td>TN</td>
<td>F</td>
<td>TN</td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_2\)</span></td>
<td></td>
<td></td>
<td>TN</td>
<td>F</td>
<td>TN</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x_3\)</span></td>
<td></td>
<td></td>
<td></td>
<td>F</td>
<td>TP</td>
</tr>
<tr class="even">
<td><span class="math inline">\(x_4\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>F</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(x_5\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Hence the Rand index is 6/10 = 0.6.</p>
</div>
<p>The Rand index has some attractive features:</p>
<ul>
<li>It is symmetric in the two clusterings; it doesn’t matter which is considered the reference.</li>
<li>There is no need to find a correspondence between the clusters in the two clusterings. In fact, the clusterings need not even have the same number of clusters.</li>
<li>The value is between 0 (complete disagreement) and 1 (complete agreement).</li>
</ul>
<p>A weakness of the Rand index is that it can be fairly close to 1 even for a random clustering. The <strong>adjusted Rand index</strong> is</p>
<p><span class="math display">\[
\text{ARI} = \frac{\text{RI} - E[\text{RI}]}{\text{max}(\text{RI})-E[\text{RI}]},
\]</span></p>
<p>where the mean and max operations are taken over all possible clusterings. (These values can be worked out exactly by combinatorics.) The value can be negative. An ARI of 0 indicates no better agreement than a random clustering, and an ARI of 1 is complete agreement.</p>
</section>
<section id="silhouettes" class="level3" data-number="7.2.2">
<h3 data-number="7.2.2" class="anchored" data-anchor-id="silhouettes"><span class="header-section-number">7.2.2</span> Silhouettes</h3>
<p>If no reference clustering is available, then we must use an intrinsic measurement to assess quality. Suppose <span class="math inline">\(\bfx_i\)</span> is a sample point. Let <span class="math inline">\(\bar{b}_i\)</span> be the mean distance between <span class="math inline">\(\bfx_i\)</span> and its buddies, and let <span class="math inline">\(\bar{r}_i\)</span> be the mean distance between <span class="math inline">\(\bfx_i\)</span> and the members of the nearest cluster of strangers. Then the <strong>silhouette value</strong> of <span class="math inline">\(\bfx_i\)</span> is</p>
<p><span class="math display">\[
s_i = \frac{\bar{r}_i-\bar{b}_i}{\max\{\bar{r}_i,\bar{b}_i\}}.
\]</span></p>
<p>This value is between <span class="math inline">\(-1\)</span> (bad) and <span class="math inline">\(1\)</span> (good) for every sample point. A <strong>silhouette score</strong> is derived by taking a mean of the silhouette values, either per cluster or overall depending on the usage.</p>
<div id="exm-cluster-silhouette" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.4 </strong></span>Suppose that two clusters in one dimension are defined as <span class="math inline">\(A=\{-4,-1,1\}\)</span> and <span class="math inline">\(B=\{2,6\}\)</span>. Find the silhouette values of all the samples, the silhouette scores of the clusters, and the overall silhouette score.</p>
<table class="table">
<colgroup>
<col style="width: 32%">
<col style="width: 23%">
<col style="width: 26%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(x_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\bar{b}_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\bar{r}_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(s_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(-4\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3+5}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{6+10}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{8-4}{8}=\frac{1}{2}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3+2}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3+7}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{5-2.5}{5}=\frac{1}{2}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{5+2}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1+5}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3-3.5}{3.5}=-\frac{1}{7}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{4}{1}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{6+3+1}{3}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{(10/3)-4}{4}=-\frac{1}{6}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(6\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{4}{1}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{10+7+5}{3}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{(22/3)-4}{(22/3)}=\frac{5}{11}\)</span></td>
</tr>
</tbody>
</table>
<p>The silhouette score of cluster <span class="math inline">\(A\)</span> is</p>
<p><span class="math display">\[
\frac{1}{3}\left( \frac{1}{2} + \frac{1}{2} - \frac{1}{7} \right) \approx 0.286,
\]</span></p>
<p>and of cluster <span class="math inline">\(B\)</span> is</p>
<p><span class="math display">\[
\frac{1}{2}\left( \frac{5}{11} - \frac{1}{6} \right) \approx 0.144.
\]</span></p>
<p>The overall score is the mean of the five values in the last column, which is about <span class="math inline">\(0.229\)</span>.</p>
</div>
<p>The silhouette score is fairly easy to understand and use. However, it relies on distances and tends to favor convex, compact clusters.</p>
<div id="exm-cluster-perform-blobs" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.5 </strong></span>Let’s use the predefined cluster assignments in our blobs dataset. We will add a column to the data frame that records the silhouette score for each point:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_samples</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>blobs <span class="op">=</span> blobs_data()</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> blobs.drop(<span class="st">"class"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>blobs[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X, blobs[<span class="st">"class"</span>])</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>sns.relplot(blobs,</span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"class"</span>,size<span class="op">=</span><span class="st">"sil"</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-11-output-1.png" width="547" height="468"></p>
</div>
</div>
<p>In the plot above, the size of each dot shows its silhouette coefficient. Those points which don’t belong comfortably with their cluster have negative scores and the smallest dots. We can find the average score in each cluster through a grouped mean:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>blobs.groupby(<span class="st">"class"</span>)[<span class="st">"sil"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>class
0    0.815086
1    0.641591
2    0.582871
Name: sil, dtype: float64</code></pre>
</div>
</div>
<p>These values are ordered as we would expect. Now let’s create another clustering based on the quadrants of the plane:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quad(x,y):</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> y <span class="op">&gt;</span> <span class="dv">0</span>: <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: <span class="cf">return</span> <span class="dv">4</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> y <span class="op">&gt;</span> <span class="dv">0</span>: <span class="cf">return</span> <span class="dv">2</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: <span class="cf">return</span> <span class="dv">3</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>blobs[<span class="st">"quadrant"</span>] <span class="op">=</span> [quad(x,y) <span class="cf">for</span> (x,y) <span class="kw">in</span> <span class="bu">zip</span>(blobs.x1, blobs.x2)]</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>blobs[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X, blobs[<span class="st">"quadrant"</span>])</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>sns.relplot(blobs, </span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"quadrant"</span>,size<span class="op">=</span><span class="st">"sil"</span></span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-13-output-1.png" width="554" height="468"></p>
</div>
</div>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>blobs.groupby(<span class="st">"quadrant"</span>)[<span class="st">"sil"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<pre><code>quadrant
1    0.654031
2    0.816362
3    0.357247
4    0.095618
Name: sil, dtype: float64</code></pre>
</div>
</div>
<p>Even though the original clustering had three classes, and there are four quadrants, we can still compare them by adjusted Rand index:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>adjusted_rand_score(blobs[<span class="st">"class"</span>], blobs[<span class="st">"quadrant"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>0.904092765401111</code></pre>
</div>
</div>
<p>Not surprisingly, they are seen as fairly similar.</p>
</div>
<div id="exm-cluster-perform-digits" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.6 </strong></span><code>sklearn</code> has a well-known dataset that contains labeled handwritten digits. Let’s extract the examples for just the numerals 4, 5, and 6:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>keep <span class="op">=</span> digits[<span class="st">"target"</span>].isin([<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>])</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> digits[keep]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.drop(<span class="st">"target"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>y.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<pre><code>5    182
4    181
6    181
Name: target, dtype: int64</code></pre>
</div>
</div>
<p>We can visualize the raw data. Here are some of the 6s:</p>
<div class="cell" data-execution_count="17">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_digits(X):</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    fig, axes <span class="op">=</span> plt.subplots(<span class="dv">4</span>,<span class="dv">4</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">4</span>):</span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>            row <span class="op">=</span> j <span class="op">+</span> <span class="dv">4</span><span class="op">*</span>i</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>            A <span class="op">=</span> np.reshape(np.array(X.iloc[row,:]),(<span class="dv">8</span>,<span class="dv">8</span>))</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>            sns.heatmap(A,ax<span class="op">=</span>axes[i,j],square<span class="op">=</span><span class="va">True</span>,cmap<span class="op">=</span><span class="st">"gray"</span>,cbar<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>            axes[i,j].axis(<span class="va">False</span>)</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>plot_digits(X[y<span class="op">==</span><span class="dv">6</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-17-output-1.png" width="507" height="389"></p>
</div>
</div>
<p>A clustering method won’t be able to learn from the ground truth labels. In order to set expectations, we should see how well the originally labels cluster the samples. Here are the mean silhouette scores for the clusters.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>digits[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X,y)</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>digits.groupby(<span class="st">"target"</span>)[<span class="st">"sil"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>target
4    0.194477
5    0.225677
6    0.327088
Name: sil, dtype: float64</code></pre>
</div>
</div>
<p>As usual, means can tell us only so much. A look at the distributions of the values reveals more details:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>sns.catplot(digits,</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"target"</span>, y<span class="op">=</span><span class="st">"sil"</span>,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"violin"</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-19-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>The values are mostly positive, which indicates nearly all of the samples for a digit are at least somewhat closer to each other than to the other samples. The 6s are the most distinct. The existence of values close to and below zero suggest that a clustering algorithm might reconstruct the classification to some extent, but the ground truth may represent something more than geometric distances in feature space.</p>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The universe doesn’t owe you a clustering. Not all phenomena are amenable to clustering in whatever features you happen to choose.</p>
</div>
</div>
<p>While classification just requires us to separate different classes of examples, clustering is more specific and more demanding: examples in a cluster need to be more like each other, or the “average” cluster member, than they are like members of other clusters. We should expect that edge cases, even within the training data, will look ambiguous.</p>
</section>
</section>
<section id="k-means" class="level2" data-number="7.3">
<h2 data-number="7.3" class="anchored" data-anchor-id="k-means"><span class="header-section-number">7.3</span> k-means</h2>
<p>The <strong><span class="math inline">\(k\)</span>-means algorithm</strong> is one of the best-known and most widely used clustering methods, although it has some serious limitations and drawbacks.</p>
<p>Given a sample matrix <span class="math inline">\(\bfX\)</span> with <span class="math inline">\(n\)</span> rows <span class="math inline">\(\bfx_i\)</span>, the algorithm divides the sample points into disjoint sets <span class="math inline">\(C_1,\ldots,C_k\)</span>, where <span class="math inline">\(k\)</span> is a preselected hyperparameter. Cluster <span class="math inline">\(j\)</span> has a <strong>centroid</strong> <span class="math inline">\(\bfmu_j\)</span>, which is the mean of the points in <span class="math inline">\(C_j\)</span>. Define the <strong>inertia</strong> of <span class="math inline">\(C_j\)</span> as</p>
<p><span class="math display">\[
I_j = \sum_{\bfx\in C_j} \norm{ \bfx - \bfmu_j }_2^2.
\]</span></p>
<p>The goal of the algorithm is to choose the clusters in order to minimize the total inertia,</p>
<p><span class="math display">\[
I = \sum_{j=1}^k I_j.
\]</span></p>
<!-- For any cluster, its centroid is the point that minimizes the inertia of the cluster. Suppose that $C_j$ is split into two parts $A$ and $B$ that have centroids $\bfmu_A$ and $\bfmu_B$. Those centroids minimize the inertias of the subclusters. Hence, 

$$
\sum_{\bfx\in A} \norm{ \bfx - \bfmu_A }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_B }^2 
\le  \sum_{\bfx\in A} \norm{ \bfx - \bfmu_j }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_j}^2  = I_j. 
$$

We conclude that splitting a cluster will make the total inertia decrease. In fact, if each sample point is put into its own cluster, the inertia is 0.  -->
<div id="exm-cluster-inertia" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.7 </strong></span>Let <span class="math inline">\(k=2\)</span>. Given the values <span class="math inline">\(-3,-2,-1,2,5,7\)</span>, we might cluster <span class="math inline">\(\{-3,-2,-1\}\)</span> and <span class="math inline">\(\{2,5,7\}\)</span>. The total inertia is then</p>
<p><span class="math display">\[
\left[  (-3+2)^2 + (-2+2)^2 + (-1+2)^2   \right]  + \left[  \bigl(2-\tfrac{14}{3}\bigr)^2 + \bigl(5-\tfrac{14}{3}\bigr)^2 + \bigl(7-\tfrac{14}{3}\bigr)^2   \right] = 2 + \frac{124}{9} = 15.78.
\]</span></p>
<p>If we instead cluster as <span class="math inline">\(\{-3,-2,-1,2\}\)</span> and <span class="math inline">\(\{5,7\}\)</span>, then the total inertia is</p>
<p><span class="math display">\[
\left[  (-3+1)^2 + (-2+1)^2 + (-1+1)^2  + (2+1)^2 \right]  + \left[   (5-6)^2 + (7-6)^2   \right] = 14 + 2 = 16.
\]</span></p>
</div>
<p>Finding the minimum inertia among all possible <span class="math inline">\(k\)</span>-clusterings is an infeasible problem to solve exactly at any practical size. Instead, the approach is to iteratively improve from a starting clustering.</p>
<section id="lloyds-algorithm" class="level3" data-number="7.3.1">
<h3 data-number="7.3.1" class="anchored" data-anchor-id="lloyds-algorithm"><span class="header-section-number">7.3.1</span> Lloyd’s algorithm</h3>
<p>The standard method is known as <strong>Lloyd’s algorithm</strong>. Starting with values for the <span class="math inline">\(k\)</span> centroids, there is an iteration consisting of two steps:</p>
<ul>
<li><strong>Assignment</strong> Each sample point is assigned to the cluster whose centroid is the nearest. (Ties are broken randomly.)</li>
<li><strong>Update</strong> Recalculate the centroids based on the cluster assignments:</li>
</ul>
<p><span class="math display">\[
\bfmu_j^+ = \frac{1}{|C_j|} \sum_{\bfx\in C_j} \bfx.
\]</span></p>
<p>The algorithm stops when the assignment step does not change any of the clusters. In practice, this almost always happens quickly. Here is a demonstration:</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="480" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="_media/kmeans_demo.mp4"></video></div>
<p>While Lloyd’s algorithm will find a local minimum of total inertia, in the sense that small changes cannot decrease it, there is no guarantee of converging to the global minimum.</p>
</section>
<section id="practical-issues" class="level3" data-number="7.3.2">
<h3 data-number="7.3.2" class="anchored" data-anchor-id="practical-issues"><span class="header-section-number">7.3.2</span> Practical issues</h3>
<ul>
<li><strong>Initialization</strong>. The performance of <span class="math inline">\(k\)</span>-means depends a great deal on the initial set of centroids. Traditionally, the centroids were chosen as random members of the sample set, but better/more reliable heuristics, such as <em><span class="math inline">\(k\)</span>-means++</em>, have since become more dominant.</li>
<li><strong>Multiple runs</strong>. All the initialization methods include an element of randomness, and since the Lloyd algorithm usually converges quickly, it is often run with multiple instances of the initialization, and the run with the lowest inertia is kept.</li>
<li><strong>Selection of <span class="math inline">\(k\)</span></strong>. The algorithm treats <span class="math inline">\(k\)</span> as a hyperparameter. Occam’s Razor dictates preferring smaller values to large ones. There are many suggestions on how to find the choice that gives the most “bang for the buck.”</li>
<li><strong>Distance metric</strong>. The Lloyd algorithm often fails to converge for norms other than the 2-norm, and must be modified if another norm is preferred.</li>
<li><strong>Shape effects</strong>. Because of the dependence on the norm, the inertia criterion disfavors long, skinny clusters and clusters of unequal dispersion. Basically, it wants to find spherical blobs (as defined by the metric) of roughly equal size.</li>
</ul>
<div id="exm-cluster-kmeans-blobs" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.8 </strong></span>Let’s generate some test blobs:</p>
<p>We start <span class="math inline">\(k\)</span>-means with <span class="math inline">\(k=2\)</span> clusters, not presupposing prior knowledge of how the samples were created.</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> blobs_data()[[<span class="st">"x1"</span>, <span class="st">"x2"</span>]]</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>km2 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>, n_init<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>km2.fit(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="49">
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KMeans(n_clusters=2, n_init='auto')</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked=""><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">KMeans</label><div class="sk-toggleable__content"><pre>KMeans(n_clusters=2, n_init='auto')</pre></div></div></div></div></div>
</div>
</div>
<p>The fitted clustering object can tell how many iterations were required, and what the final inertia and cluster centroids are:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"k=2 took"</span>,km2.n_iter_,<span class="st">"iterations"</span>)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"final inertia: </span><span class="sc">{</span>km2<span class="sc">.</span>inertia_<span class="sc">:.5g}</span><span class="ss">"</span>)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"cluster centroids:"</span>)</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(km2.cluster_centers_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>k=2 took 6 iterations
final inertia: 710.08
cluster centroids:
[[ 2.01902057 -0.69722874]
 [-2.01977462  2.86247767]]</code></pre>
</div>
</div>
<p>There is a <code>predict</code> method that can make cluster assignments for arbitrary points in feature space. In k-means, this just tells you which centroid is closest, i.e., the cluster membership:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>km2.predict([ [<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>,<span class="dv">2</span>] ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/base.py:409: UserWarning: X does not have valid feature names, but KMeans was fitted with feature names
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>array([1, 0], dtype=int32)</code></pre>
</div>
</div>
<p>For the training samples we don’t need to call <code>predict</code>. Every fitted clustering object has a <code>labels_</code> property that lists the of cluster index values. We can use those labels to compute silhouette scores:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report(clustering):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    blobs[<span class="st">"cluster"</span>] <span class="op">=</span> clustering.labels_</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>    blobs[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X, blobs[<span class="st">"cluster"</span>])</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"inertia: </span><span class="sc">{</span>clustering<span class="sc">.</span>inertia_<span class="sc">:.5g}</span><span class="ss">"</span>)</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"overall silhouette score: </span><span class="sc">{</span>blobs[<span class="st">'sil'</span>]<span class="sc">.</span>mean()<span class="sc">:.5g}</span><span class="ss">"</span>)</span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    sns.catplot(blobs,</span>
<span id="cb30-7"><a href="#cb30-7" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"cluster"</span>, y<span class="op">=</span><span class="st">"sil"</span>,</span>
<span id="cb30-8"><a href="#cb30-8" aria-hidden="true" tabindex="-1"></a>        kind<span class="op">=</span><span class="st">"violin"</span>, height<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb30-9"><a href="#cb30-9" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span>
<span id="cb30-10"><a href="#cb30-10" aria-hidden="true" tabindex="-1"></a>    sns.relplot(blobs,</span>
<span id="cb30-11"><a href="#cb30-11" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb30-12"><a href="#cb30-12" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, size<span class="op">=</span>blobs[<span class="st">"sil"</span>], height<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb30-13"><a href="#cb30-13" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span>
<span id="cb30-14"><a href="#cb30-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> blobs </span>
<span id="cb30-15"><a href="#cb30-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-16"><a href="#cb30-16" aria-hidden="true" tabindex="-1"></a>report(km2)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 710.08
overall silhouette score: 0.58767</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="52">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>class</th>
      <th>sil</th>
      <th>quadrant</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.936817</td>
      <td>1.681397</td>
      <td>1</td>
      <td>0.436436</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.693103</td>
      <td>1.379978</td>
      <td>1</td>
      <td>0.427938</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.400972</td>
      <td>-2.840545</td>
      <td>2</td>
      <td>0.533493</td>
      <td>4</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-2.147649</td>
      <td>2.716864</td>
      <td>0</td>
      <td>0.874506</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.438409</td>
      <td>-3.074790</td>
      <td>2</td>
      <td>0.463698</td>
      <td>4</td>
      <td>0</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>-2.644165</td>
      <td>3.143908</td>
      <td>0</td>
      <td>0.857694</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>146</th>
      <td>-0.141668</td>
      <td>-3.649230</td>
      <td>2</td>
      <td>0.406065</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>147</th>
      <td>-2.784788</td>
      <td>2.994060</td>
      <td>0</td>
      <td>0.848923</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>148</th>
      <td>-2.026725</td>
      <td>3.090605</td>
      <td>0</td>
      <td>0.874289</td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <th>149</th>
      <td>1.596019</td>
      <td>-2.509213</td>
      <td>2</td>
      <td>0.543888</td>
      <td>4</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 6 columns</p>
</div>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-23-output-3.png" width="325" height="324"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-23-output-4.png" width="396" height="324"></p>
</div>
</div>
<p>It’s clear in both plots that cluster 0 is more tightly packed than cluster 1. Let’s repeat the computation for <span class="math inline">\(k=3\)</span> clusters:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>km3 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, n_init<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>km3.fit(X)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>report(km3)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 203.3
overall silhouette score: 0.69987</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="53">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>class</th>
      <th>sil</th>
      <th>quadrant</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.936817</td>
      <td>1.681397</td>
      <td>1</td>
      <td>0.722226</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.693103</td>
      <td>1.379978</td>
      <td>1</td>
      <td>0.764179</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.400972</td>
      <td>-2.840545</td>
      <td>2</td>
      <td>0.682846</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-2.147649</td>
      <td>2.716864</td>
      <td>0</td>
      <td>0.866484</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.438409</td>
      <td>-3.074790</td>
      <td>2</td>
      <td>0.713818</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>-2.644165</td>
      <td>3.143908</td>
      <td>0</td>
      <td>0.849474</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>146</th>
      <td>-0.141668</td>
      <td>-3.649230</td>
      <td>2</td>
      <td>0.696419</td>
      <td>3</td>
      <td>1</td>
    </tr>
    <tr>
      <th>147</th>
      <td>-2.784788</td>
      <td>2.994060</td>
      <td>0</td>
      <td>0.841548</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>148</th>
      <td>-2.026725</td>
      <td>3.090605</td>
      <td>0</td>
      <td>0.863157</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>149</th>
      <td>1.596019</td>
      <td>-2.509213</td>
      <td>2</td>
      <td>0.621158</td>
      <td>4</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 6 columns</p>
</div>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-24-output-3.png" width="325" height="324"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-24-output-4.png" width="396" height="324"></p>
</div>
</div>
<p>This result shows a modest reduction in silhouette scores for original good cluster, but improvement for the problematic one.</p>
<p>Moving on to <span class="math inline">\(k=4\)</span> clusters shows clear degradation of the silhouette scores:</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>km4 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, n_init<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>km4.fit(X)</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>report(km4)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 166.02
overall silhouette score: 0.60839</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="54">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>class</th>
      <th>sil</th>
      <th>quadrant</th>
      <th>cluster</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.936817</td>
      <td>1.681397</td>
      <td>1</td>
      <td>0.677092</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2.693103</td>
      <td>1.379978</td>
      <td>1</td>
      <td>0.712691</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.400972</td>
      <td>-2.840545</td>
      <td>2</td>
      <td>0.223120</td>
      <td>4</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-2.147649</td>
      <td>2.716864</td>
      <td>0</td>
      <td>0.867032</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.438409</td>
      <td>-3.074790</td>
      <td>2</td>
      <td>0.202601</td>
      <td>4</td>
      <td>3</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>145</th>
      <td>-2.644165</td>
      <td>3.143908</td>
      <td>0</td>
      <td>0.849939</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>146</th>
      <td>-0.141668</td>
      <td>-3.649230</td>
      <td>2</td>
      <td>0.467246</td>
      <td>3</td>
      <td>3</td>
    </tr>
    <tr>
      <th>147</th>
      <td>-2.784788</td>
      <td>2.994060</td>
      <td>0</td>
      <td>0.842149</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>148</th>
      <td>-2.026725</td>
      <td>3.090605</td>
      <td>0</td>
      <td>0.863466</td>
      <td>2</td>
      <td>0</td>
    </tr>
    <tr>
      <th>149</th>
      <td>1.596019</td>
      <td>-2.509213</td>
      <td>2</td>
      <td>0.404255</td>
      <td>4</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>150 rows × 6 columns</p>
</div>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-25-output-3.png" width="325" height="324"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-25-output-4.png" width="396" height="324"></p>
</div>
</div>
<p>Based on silhouette scores, then, we would probably stop at <span class="math inline">\(k=3\)</span> clusters.</p>
</div>
<div id="exm-cluster-kmeans-stripes" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.9 </strong></span>K-means is expecting to find roughly spherical clusters. When the data do not conform to that model, it tends to perform poorly:</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>stripes <span class="op">=</span> stripes_data()</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> stripes[[<span class="st">"x1"</span>, <span class="st">"x2"</span>]]</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>]:</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, n_init<span class="op">=</span><span class="st">"auto"</span>)</span>
<span id="cb36-6"><a href="#cb36-6" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb36-7"><a href="#cb36-7" aria-hidden="true" tabindex="-1"></a>    stripes[<span class="st">"cluster"</span>] <span class="op">=</span> km.labels_</span>
<span id="cb36-8"><a href="#cb36-8" aria-hidden="true" tabindex="-1"></a>    stripes[<span class="st">"k"</span>] <span class="op">=</span> k</span>
<span id="cb36-9"><a href="#cb36-9" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.concat( (results, stripes) )</span>
<span id="cb36-10"><a href="#cb36-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb36-11"><a href="#cb36-11" aria-hidden="true" tabindex="-1"></a>sns.relplot(results,</span>
<span id="cb36-12"><a href="#cb36-12" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb36-13"><a href="#cb36-13" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"k"</span>, height<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb36-14"><a href="#cb36-14" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-26-output-1.png" width="1059" height="323"></p>
</div>
</div>
<p>It’s not a bad idea to standardize the data. But that’s no panacea:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>]:</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> make_pipeline(StandardScaler(), KMeans(n_clusters<span class="op">=</span>k, n_init<span class="op">=</span><span class="st">"auto"</span>))</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    stripes[<span class="st">"cluster"</span>] <span class="op">=</span> km[<span class="dv">1</span>].labels_</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>    stripes[<span class="st">"k"</span>] <span class="op">=</span> k</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.concat( (results, stripes) )</span>
<span id="cb37-8"><a href="#cb37-8" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb37-9"><a href="#cb37-9" aria-hidden="true" tabindex="-1"></a>sns.relplot(results,</span>
<span id="cb37-10"><a href="#cb37-10" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb37-11"><a href="#cb37-11" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"k"</span>, height<span class="op">=</span><span class="fl">3.5</span></span>
<span id="cb37-12"><a href="#cb37-12" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-27-output-1.png" width="1059" height="323"></p>
</div>
</div>
<p>Clustering is hard!</p>
</div>
<div id="exm-cluster-kmeans-digits" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.10 </strong></span>We return to the handwriting recognition dataset. Again we keep only the samples labeled 4, 5, or 6:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>keep <span class="op">=</span> digits[<span class="st">"target"</span>].isin([<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>])</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> digits[keep]</span>
<span id="cb38-4"><a href="#cb38-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb38-5"><a href="#cb38-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.drop(<span class="st">"target"</span>,axis<span class="op">=</span><span class="st">"columns"</span>)</span>
<span id="cb38-6"><a href="#cb38-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits[<span class="st">"target"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We fit 3 clusters to the feature matrix:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>km.fit(X)</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>digits[<span class="st">"kmeans3"</span>] <span class="op">=</span> km.labels_</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>digits[[<span class="st">"target"</span>, <span class="st">"kmeans3"</span>]].head(<span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="58">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>target</th>
      <th>kmeans3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>2</td>
    </tr>
    <tr>
      <th>5</th>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>6</th>
      <td>6</td>
      <td>0</td>
    </tr>
    <tr>
      <th>14</th>
      <td>4</td>
      <td>2</td>
    </tr>
    <tr>
      <th>15</th>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>16</th>
      <td>6</td>
      <td>0</td>
    </tr>
    <tr>
      <th>24</th>
      <td>4</td>
      <td>2</td>
    </tr>
    <tr>
      <th>25</th>
      <td>5</td>
      <td>1</td>
    </tr>
    <tr>
      <th>26</th>
      <td>6</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>The adjusted Rand index suggests that we have reproduced the classification very well:</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(y, digits[<span class="st">"kmeans3"</span>])</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ARI: 0.9618</code></pre>
</div>
</div>
<p>However, that conclusion benefits from our prior knowledge. What if we did not know how many clusters to look for? Let’s look over a range of <span class="math inline">\(k\)</span> values, recording the final total inertia and the mean silhouette score for each</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">8</span>):</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>    sil <span class="op">=</span> silhouette_score(X, km.labels_)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>    results.append( [k, sil] )</span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(results, columns<span class="op">=</span>[<span class="st">"k"</span>, <span class="st">"mean silhouette"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(
/Users/driscoll/mambaforge/envs/datasci/lib/python3.11/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  warnings.warn(</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="60">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>k</th>
      <th>mean silhouette</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>0.226800</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3</td>
      <td>0.251904</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4</td>
      <td>0.245855</td>
    </tr>
    <tr>
      <th>3</th>
      <td>5</td>
      <td>0.235199</td>
    </tr>
    <tr>
      <th>4</th>
      <td>6</td>
      <td>0.188324</td>
    </tr>
    <tr>
      <th>5</th>
      <td>7</td>
      <td>0.170556</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>The silhouette score is maximized at <span class="math inline">\(k=3\)</span>, which could be considered a reason to choose 3 clusters. While the score for 4 clusters is fairly close, we should prefer the less complex model.</p>
</div>
</section>
</section>
<section id="hierarchical-clustering" class="level2 page-columns page-full" data-number="7.4">
<h2 data-number="7.4" class="anchored" data-anchor-id="hierarchical-clustering"><span class="header-section-number">7.4</span> Hierarchical clustering</h2>
<p>The idea behind hierarchical clustering is to organize all the sample points into a tree structure called a <strong>dendrogram</strong>. At the root of the tree is the entire sample set, while each leaf of the tree is a single sample vector. Groups of similar samples are connected as nearby relatives in the tree, with less-similar groups located as more distant relatives.</p>
<p>Dendrograms can be found by starting with the root and recursively splitting, or by starting at the leaves and recursively merging. We will describe the latter approach, known as <strong>agglomerative clustering</strong>.</p>
<p>The algorithm begins with <span class="math inline">\(n\)</span> singleton clusters, i.e., <span class="math inline">\(C_i=\{\bfx_i\}\)</span>. Then, the similarity or distance between each pair of clusters is determined. The pair with the minimum distance is merged, and the process repeats.</p>
<p>Common ways to define the distance between two clusters <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span> are:</p>
<ul>
<li><dl>
<dt>single linkage</dt>
<dd>
(also called <em>minimum linkage</em>) <span id="eq-cluster-linkage-single"><span class="math display">\[
\displaystyle \min_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy } \}
\tag{7.2}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>complete linkage</dt>
<dd>
(also called <em>maximum linkage</em>) <span id="eq-cluster-linkage-complete"><span class="math display">\[
\displaystyle \max_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy} \}
\tag{7.3}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>average linkage</dt>
<dd>
<span id="eq-cluster-linkage-average"><span class="math display">\[
\displaystyle \frac{1}{|C_i|\,|C_j|} \sum_{\bfx\in C_i,\,\bfy\in C_j} \norm{ \bfx-\bfy }
\tag{7.4}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>Ward linkage</dt>
<dd>
The increase in inertia resulting from merging <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>, equal to
</dd>
</dl></li>
</ul>
<p><span id="eq-cluster-linkage-ward"><span class="math display">\[
\frac{ |C_i|\,|C_j| }{|C_i| + |C_j|} \norm{\bfmu_i - \bfmu_j}_2^2,
\tag{7.5}\]</span></span></p>
<p>where <span class="math inline">\(\bfmu_i\)</span> and <span class="math inline">\(\bfmu_j\)</span> are the centroids of <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>.</p>
<p>Agglomerative clustering with Ward linkage amounts to trying to minimize the increase of inertia with each merger. In that sense, it has the same objective as <span class="math inline">\(k\)</span>-means, but it is usually not as successful at minimizing inertia.</p>
<p>Single linkage only pays attention to the gaps between clusters, not the size or spread of them. Complete linkage, on the other hand, wants to keep clusters packed tightly together. Average linkage is a compromise between these extremes. All three of these options can work with a distance matrix in lieu of the original feature matrix.</p>
<div id="exm-cluster-linkage-tiny" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.11 </strong></span>Given clusters <span class="math inline">\(C_1=\{-3,-2,-1\}\)</span> and <span class="math inline">\(C_2=\{3,4,5\}\)</span>, we find the different linkages between them:</p>
<p><strong>Ward.</strong> The centroids of the clusters are <span class="math inline">\(-2\)</span> and <span class="math inline">\(4\)</span>. So the linkage is</p>
<p><span class="math display">\[
\frac{3\cdot 3}{3+3} \, 6^2 = 54.
\]</span></p>
<p><strong>Single.</strong> The pairwise distances between members of <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> form a <span class="math inline">\(3\times 3\)</span> matrix:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>-3</th>
<th>-2</th>
<th>-1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>6</td>
<td>5</td>
<td>4</td>
</tr>
<tr class="even">
<td>4</td>
<td>7</td>
<td>6</td>
<td>5</td>
</tr>
<tr class="odd">
<td>5</td>
<td>8</td>
<td>7</td>
<td>6</td>
</tr>
</tbody>
</table>
<p>The single linkage is therefore 4.</p>
<p><strong>Complete.</strong> The maximum of the matrix above is 8.</p>
<p><strong>Average.</strong> The average value of the matrix entries is <span class="math inline">\(54/9\)</span>, which is 6.</p>
</div>
<div id="exm-cluster-hier-toy" class="theorem example">
<p><span class="theorem-title"><strong>Example 7.12 </strong></span>Let’s use 5 sample points in the plane, and agglomerate them by single linkage. The <code>pairwise_distances</code> function converts sample points into a distance matrix:</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array( [[<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>] ,[<span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>], [<span class="dv">1</span>,<span class="fl">0.5</span>], [<span class="dv">0</span>,<span class="dv">2</span>], [<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>]] )</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"euclidean"</span>)</span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>D</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>array([[0.        , 4.12310563, 3.35410197, 3.60555128, 2.23606798],
       [4.12310563, 0.        , 2.6925824 , 4.47213595, 4.24264069],
       [3.35410197, 2.6925824 , 0.        , 1.80277564, 2.06155281],
       [3.60555128, 4.47213595, 1.80277564, 0.        , 1.41421356],
       [2.23606798, 4.24264069, 2.06155281, 1.41421356, 0.        ]])</code></pre>
</div>
</div>
<p>The minimum value in the upper triangle of the distance matrix is in row 3, column 4 (starting index at 0). So our first merge results in the cluster <span class="math inline">\(C_1=\{\bfx_3,\bfx_4\}\)</span>. The next-smallest entry in the upper triangle is at position <span class="math inline">\((2,3)\)</span>, so we want to merge those samples together next, resulting in</p>
<p><span class="math display">\[
C_1=\{\bfx_2,\bfx_3,\bfx_4\},\, C_2 = \{\bfx_0\},\, C_3=\{\bfx_1\}.
\]</span></p>
<p>The next-smallest element in the matrix is at <span class="math inline">\((2,4)\)</span>, but those points are already merged, so we move on to position <span class="math inline">\((0,4)\)</span>. Now we have</p>
<p><span class="math display">\[
C_1=\{\bfx_0,\bfx_2,\bfx_3,\bfx_4\},\, C_2 = \{\bfx_1\}.
\]</span></p>
<p>The final merge is to combine these.</p>
<p>The entire dendrogram can be visualized with seaborn:</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>sns.clustermap(X, </span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    col_cluster<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    dendrogram_ratio<span class="op">=</span>(<span class="fl">.75</span>,<span class="fl">.15</span>)</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-33-output-1.png" width="949" height="950"></p>
</div>
</div>
<p>The horizontal position in the dendrogram above indicates the linkage strength. Note on the right that the ordering of the samples has been changed (so that the lines won’t cross each other). The two colored columns show a heatmap of the two features of the sample points. Working from right to left, we see the merger of samples 3 and 4, which are then merged with sample 2, etc.</p>
<p>In effect, we get an entire family of clusterings by stopping at any linkage value we want. If we chose to stop at value 2.5, for instance, we would have two clusters of size 4 and 1. Or, if we predetermine that we want <span class="math inline">\(k\)</span> clusters, we can stop after <span class="math inline">\(n-k\)</span> merge steps.</p>
</div>
<div id="exm-cluster-linkage" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 7.13 </strong></span>We define a function that allows us to run all three linkages for a dataset:</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_experiment(data):</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> linkage <span class="kw">in</span> [<span class="st">"single"</span>, <span class="st">"complete"</span>, <span class="st">"ward"</span>]:</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>        agg <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span>linkage)</span>
<span id="cb48-7"><a href="#cb48-7" aria-hidden="true" tabindex="-1"></a>        agg.fit( data[[<span class="st">"x1"</span>, <span class="st">"x2"</span>]] )</span>
<span id="cb48-8"><a href="#cb48-8" aria-hidden="true" tabindex="-1"></a>        data[<span class="st">"cluster"</span>] <span class="op">=</span> agg.labels_</span>
<span id="cb48-9"><a href="#cb48-9" aria-hidden="true" tabindex="-1"></a>        data[<span class="st">"linkage"</span>] <span class="op">=</span> linkage</span>
<span id="cb48-10"><a href="#cb48-10" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> pd.concat( (results, data) )</span>
<span id="cb48-11"><a href="#cb48-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first try the blobs seen previously:</p>
<div class="cell" data-execution_count="35">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> make_blobs(</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span>[<span class="dv">60</span>, <span class="dv">50</span>, <span class="dv">40</span>],</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    centers<span class="op">=</span>[ [<span class="op">-</span><span class="dv">2</span>,<span class="dv">3</span>], [<span class="fl">3.5</span>,<span class="fl">1.5</span>], [<span class="dv">1</span>,<span class="op">-</span><span class="dv">3</span>] ],</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    cluster_std<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">1.2</span>],</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">19716</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>blobs <span class="op">=</span> pd.DataFrame( {<span class="st">"x1"</span>: X[:,<span class="dv">0</span>], <span class="st">"x2"</span>: X[:,<span class="dv">1</span>], <span class="st">"class"</span>: y} )</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>blobs.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="64">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>x1</th>
      <th>x2</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>4.436817</td>
      <td>1.681397</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>3.193103</td>
      <td>1.379978</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.400972</td>
      <td>-2.840545</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>-2.147649</td>
      <td>2.716864</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.438409</td>
      <td>-3.074790</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(blobs)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(results,</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, height<span class="op">=</span><span class="dv">4</span></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display column-body-outset" data-execution_count="65">
<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x15d25bcd0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display column-body-outset">
<p><img src="clustering_files/figure-html/cell-36-output-2.png" width="1203" height="371"></p>
</div>
</div>
<p>As you can see, the simple linkage was confused by the two blobs that nearly run together.</p>
<p>Next, we try data points lying in three distinct stripes:</p>
<div class="cell page-columns page-full" data-execution_count="37">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>stripes <span class="op">=</span> stripes_data()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(stripes)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>sns.relplot(results,</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, height<span class="op">=</span><span class="dv">4</span></span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>        )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display column-body-outset" data-execution_count="66">
<pre><code>&lt;seaborn.axisgrid.FacetGrid at 0x15d5ab690&gt;</code></pre>
</div>
<div class="cell-output cell-output-display column-body-outset">
<p><img src="clustering_files/figure-html/cell-37-output-2.png" width="1203" height="371"></p>
</div>
</div>
<p>Both the complete and Ward linkages are committed to finding compact, roughly spherical clusters. They group together points across stripes rather than clusters extending lengthwise. The single linkage has more flexibility.</p>
<p>Finally, we try the most demanding test, points that are arranged in rings:</p>
<div class="cell page-columns page-full" data-execution_count="38">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>bullseye <span class="op">=</span> bullseye_data()</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(bullseye)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sns.relplot(results,</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, height<span class="op">=</span><span class="dv">4</span></span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>p.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display column-body-outset">
<p><img src="clustering_files/figure-html/cell-38-output-1.png" width="1174" height="371"></p>
</div>
</div>
<p>The single linkage is the only one with enough geometric flexibility to cluster the rings properly. However, it’s a delicate situation, and it can be sensitive to individual samples. Here, we add just one point to the bullseye picture and get a big change:</p>
<div class="cell page-columns page-full" data-execution_count="39">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>bullseye <span class="op">=</span> pd.concat( ( bullseye, pd.DataFrame({<span class="st">"x1"</span>: [<span class="dv">0</span>], <span class="st">"x2"</span>: [<span class="fl">2.25</span>]}) ) )</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(bullseye)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sns.relplot(results,</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, height<span class="op">=</span><span class="dv">4</span></span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>p.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display column-body-outset">
<p><img src="clustering_files/figure-html/cell-39-output-1.png" width="1174" height="371"></p>
</div>
</div>
</div>
<section id="case-study-penguins" class="level3" data-number="7.4.1">
<h3 data-number="7.4.1" class="anchored" data-anchor-id="case-study-penguins"><span class="header-section-number">7.4.1</span> Case study: Penguins</h3>
<p>Let’s try agglomerative clustering to discover the species of the penguins. First, let’s recall how many of each species we have.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>).dropna()</span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"bill_length_mm"</span>, <span class="st">"bill_depth_mm"</span>, <span class="st">"flipper_length_mm"</span>, <span class="st">"body_mass_g"</span>]</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[features]</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"species"</span>].value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>Adelie       146
Gentoo       119
Chinstrap     68
Name: species, dtype: int64</code></pre>
</div>
</div>
<p>Our first attempt is single linkage. Because 2-norm distances are involved, we will use standardization in a pipeline with the clustering method. After fitting, the <code>labels_</code> property of the cluster object is a vector of cluster assignments.</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>single <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span><span class="st">"single"</span>)</span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(),single)</span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>pipe.fit(X)</span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"single"</span>] <span class="op">=</span> single.labels_       <span class="co"># cluster assignments</span></span>
<span id="cb58-7"><a href="#cb58-7" aria-hidden="true" tabindex="-1"></a>penguins.loc[::<span class="dv">24</span>,[<span class="st">"species"</span>, <span class="st">"single"</span>]]   <span class="co"># print out some rows</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>species</th>
      <th>single</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>0</td>
    </tr>
    <tr>
      <th>29</th>
      <td>Adelie</td>
      <td>0</td>
    </tr>
    <tr>
      <th>54</th>
      <td>Adelie</td>
      <td>0</td>
    </tr>
    <tr>
      <th>78</th>
      <td>Adelie</td>
      <td>0</td>
    </tr>
    <tr>
      <th>102</th>
      <td>Adelie</td>
      <td>0</td>
    </tr>
    <tr>
      <th>126</th>
      <td>Adelie</td>
      <td>0</td>
    </tr>
    <tr>
      <th>150</th>
      <td>Adelie</td>
      <td>0</td>
    </tr>
    <tr>
      <th>174</th>
      <td>Chinstrap</td>
      <td>0</td>
    </tr>
    <tr>
      <th>198</th>
      <td>Chinstrap</td>
      <td>0</td>
    </tr>
    <tr>
      <th>222</th>
      <td>Gentoo</td>
      <td>2</td>
    </tr>
    <tr>
      <th>247</th>
      <td>Gentoo</td>
      <td>2</td>
    </tr>
    <tr>
      <th>271</th>
      <td>Gentoo</td>
      <td>2</td>
    </tr>
    <tr>
      <th>296</th>
      <td>Gentoo</td>
      <td>2</td>
    </tr>
    <tr>
      <th>320</th>
      <td>Gentoo</td>
      <td>2</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>It seems that Gentoo is associated with cluster number 2, but the situation with the other species is less clear. Here are the value counts:</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"single linkage results:"</span>)</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"single"</span>].value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>single linkage results:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>0    213
2    119
1      1
Name: single, dtype: int64</code></pre>
</div>
</div>
<p>As we saw with the toy datasets in <a href="#exm-cluster-linkage">Example&nbsp;<span>7.13</span></a>, the single linkage is susceptible to declaring one isolated point to be a cluster, while grouping together other points we would like to separate. Here is the ARI for this clustering, compared to the true classification:</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(penguins[<span class="st">"species"</span>],penguins[<span class="st">"single"</span>])</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"single linkage ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>single linkage ARI: 0.6506</code></pre>
</div>
</div>
<p>Now let’s try it with Ward linkage (the default):</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>ward <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">2</span>, linkage<span class="op">=</span><span class="st">"ward"</span>)</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), ward)</span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>pipe.fit(X)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"ward"</span>] <span class="op">=</span> ward.labels_</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ward linkage results:"</span>)</span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(penguins[<span class="st">"ward"</span>].value_counts())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ward linkage results:
0    214
1    119
Name: ward, dtype: int64</code></pre>
</div>
</div>
<p>This result looks more promising. The ARI confirms that hunch:</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(penguins[<span class="st">"species"</span>], penguins[<span class="st">"ward"</span>])</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ward linkage ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ward linkage ARI: 0.6486</code></pre>
</div>
</div>
<p>If we guess at the likely correspondence between the cluster numbers and the different species, then we can find the confusion matrix:</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix,ConfusionMatrixDisplay</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> penguins[<span class="st">"species"</span>]</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert cluster numbers into labels:</span></span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> penguins[<span class="st">"ward"</span>].replace({<span class="dv">1</span>:<span class="st">"Adelie"</span>,<span class="dv">0</span>:<span class="st">"Gentoo"</span>,<span class="dv">2</span>:<span class="st">"Chinstrap"</span>}) </span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb68-6"><a href="#cb68-6" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay(confusion_matrix(y,y_hat),display_labels<span class="op">=</span>y.unique()).plot()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-46-output-1.png" width="560" height="429"></p>
</div>
</div>
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-cluster-positive" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.1 </strong></span>Using only the three axioms of a distance metric, prove that <span class="math inline">\(\dist(\bfx,\bfy) \ge 0\)</span> for all vectors <span class="math inline">\(\bfx\)</span> and <span class="math inline">\(\bfy\)</span>. (Hint: apply the triangle inequality to go from <span class="math inline">\(\bfx\)</span> to <span class="math inline">\(\bfy\)</span> and back again.)</p>
</div>
<div id="exr-cluster-angular" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.2 </strong></span>Prove that the angular distance between any nonzero vector and itself is zero.</p>
</div>
<div id="exr-cluster-cosine" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.3 </strong></span>Find a counterexample showing that cosine distance does not satisfy the triangle inequality. (Hint: it’s enough to consider some simple vectors in two dimensions.)</p>
</div>
<div id="exr-cluster-inertia" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.4 </strong></span>Let <span class="math inline">\(c\)</span> be a positive number, and consider the 12 sample points <span class="math inline">\(\{(\pm c,\pm j): j=1,2,3\}\)</span>. One way to cluster the sample points, which we designate as clustering <span class="math inline">\(\alpha\)</span>, is to split according to the sign of <span class="math inline">\(x_1\)</span>. Another way, which we designate as clustering <span class="math inline">\(\beta\)</span>, is to split according to the sign of <span class="math inline">\(x_2\)</span>. Compute the inertia of both clusterings. For which values of <span class="math inline">\(c\)</span>, if any, does clustering <span class="math inline">\(\alpha\)</span> have less inertia than clustering <span class="math inline">\(\beta\)</span>?</p>
</div>
<div id="exr-cluster-linkage" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.5 </strong></span>Here is a distance matrix for points <span class="math inline">\(\bfx_1,\ldots,\bfx_5\)</span>.</p>
<p><span class="math display">\[
\left[
\begin{array}{ccccc}
0 &amp; 2 &amp; 4 &amp; 5 &amp; 6 \\
2 &amp; 0 &amp; 2 &amp; 3 &amp; 4 \\
4 &amp; 2 &amp; 0 &amp; 1 &amp; 2 \\
5 &amp; 3 &amp; 1 &amp; 0 &amp; 1 \\
6 &amp; 4 &amp; 2 &amp; 1 &amp; 0 \\
\end{array}
\right]
\]</span></p>
<p>Compute the average linkage between the clusters with index sets <span class="math inline">\(C_1=\{1,3\}\)</span> and <span class="math inline">\(C_2=\{2,4,5\}\)</span>.</p>
</div>
<div id="exr-cluster-agglom" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 7.6 </strong></span>Perform by hand an agglomerative clustering for the values <span class="math inline">\(2,4,5,8,12\)</span> using single linkage. This means finding the four merge steps needed to convert five singleton clusters into one global cluster.</p>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./networks.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>videojs(video_shortcode_videojs_video1);</script>



</body></html>