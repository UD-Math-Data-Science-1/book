<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Science 1 - 6&nbsp; Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./networks.html" rel="next">
<link href="./regression.html" rel="prev">
<link href="./_media/logo_small.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script> 
MathJax = {
  chtml: {
    scale: 0.92,
  }
}
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./clustering.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></a></li></ol></nav>
        <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="quarto-sidebar-header"><div class="sidebar-header-item">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="_media/logo_small.png" class="quarto-figure quarto-figure-center figure-img" height="120"></p>
</figure>
</div>
</div></div>
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science 1</a> 
        <div class="sidebar-tools-main">
    <a href="./Data-Science-1.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">List of examples</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Resources</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./starting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./genAI.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Generative AI</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Representation of data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Review questions by section</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>Copyright (c) 2024 by <a href="https://tobydriscoll.net">Toby Driscoll</a></p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#similarity" id="toc-similarity" class="nav-link active" data-scroll-target="#similarity"><span class="header-section-number">6.1</span> Similarity</a>
  <ul class="collapse">
  <li><a href="#distance-matrix" id="toc-distance-matrix" class="nav-link" data-scroll-target="#distance-matrix"><span class="header-section-number">6.1.1</span> Distance matrix</a></li>
  <li><a href="#angular-distance" id="toc-angular-distance" class="nav-link" data-scroll-target="#angular-distance"><span class="header-section-number">6.1.2</span> Angular distance</a></li>
  <li><a href="#distance-in-high-dimensions" id="toc-distance-in-high-dimensions" class="nav-link" data-scroll-target="#distance-in-high-dimensions"><span class="header-section-number">6.1.3</span> Distance in high dimensions</a></li>
  </ul></li>
  <li><a href="#performance-measures" id="toc-performance-measures" class="nav-link" data-scroll-target="#performance-measures"><span class="header-section-number">6.2</span> Performance measures</a>
  <ul class="collapse">
  <li><a href="#rand-index-and-ari" id="toc-rand-index-and-ari" class="nav-link" data-scroll-target="#rand-index-and-ari"><span class="header-section-number">6.2.1</span> Rand index and ARI</a></li>
  <li><a href="#silhouettes" id="toc-silhouettes" class="nav-link" data-scroll-target="#silhouettes"><span class="header-section-number">6.2.2</span> Silhouettes</a></li>
  </ul></li>
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means"><span class="header-section-number">6.3</span> k-means</a>
  <ul class="collapse">
  <li><a href="#lloyds-algorithm" id="toc-lloyds-algorithm" class="nav-link" data-scroll-target="#lloyds-algorithm"><span class="header-section-number">6.3.1</span> Lloyd’s algorithm</a></li>
  <li><a href="#practical-issues" id="toc-practical-issues" class="nav-link" data-scroll-target="#practical-issues"><span class="header-section-number">6.3.2</span> Practical issues</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="header-section-number">6.4</span> Hierarchical clustering</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">
<p><span class="math display">\[
    \newcommand{\float}{\mathbb{F}}
    \newcommand{\real}{\mathbb{R}}
    \newcommand{\complex}{\mathbb{C}}
    \newcommand{\nat}{\mathbb{N}}
    \newcommand{\integer}{\mathbb{Z}}
    \newcommand{\bfa}{\mathbf{a}}
    \newcommand{\bfe}{\mathbf{e}}
    \newcommand{\bfh}{\mathbf{h}}
    \newcommand{\bfp}{\mathbf{p}}
    \newcommand{\bfq}{\mathbf{q}}
    \newcommand{\bfu}{\mathbf{u}}
    \newcommand{\bfv}{\mathbf{v}}
    \newcommand{\bfw}{\mathbf{w}}
    \newcommand{\bfx}{\mathbf{x}}
    \newcommand{\bfy}{\mathbf{y}}
    \newcommand{\bfz}{\mathbf{z}}
    \newcommand{\bfA}{\mathbf{A}}
    \newcommand{\bfW}{\mathbf{W}}
    \newcommand{\bfX}{\mathbf{X}}
    \newcommand{\bfzero}{\boldsymbol{0}}
    \newcommand{\bfmu}{\boldsymbol{\mu}}
    \newcommand{\TP}{\text{TP}}
    \newcommand{\TN}{\text{TN}}
    \newcommand{\FP}{\text{FP}}
    \newcommand{\FN}{\text{FN}}
    \newcommand{\rmn}[2]{\mathbb{R}^{#1 \times #2}}
    \newcommand{\dd}[2]{\frac{d #1}{d #2}}
    \newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
    \newcommand{\norm}[1]{\left\lVert \mathstrut #1 \right\rVert}
    \newcommand{\abs}[1]{\left\lvert \mathstrut #1 \right\rvert}
    \newcommand{\twonorm}[1]{\norm{#1}_2}
    \newcommand{\onenorm}[1]{\norm{#1}_1}
    \newcommand{\infnorm}[1]{\norm{#1}_\infty}
    \newcommand{\innerprod}[2]{\langle #1,#2 \rangle}
    \newcommand{\pr}[1]{^{(#1)}}
    \newcommand{\diag}{\operatorname{diag}}
    \newcommand{\sign}{\operatorname{sign}}
    \newcommand{\dist}{\operatorname{dist}}
    \newcommand{\simil}{\operatorname{sim}}
    \newcommand{\ee}{\times 10^}
    \newcommand{\floor}[1]{\lfloor#1\rfloor}
    \newcommand{\argmin}{\operatorname{argmin}}
    \newcommand{\E}[1]{\operatorname{\mathbb{E}}\left[\mathstrut #1\right]}
    \newcommand{\Cov}{\operatorname{Cov}}
    \newcommand{\logit}{\operatorname{logit}}
\]</span></p>
</div>
<div id="39035813" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> default_rng</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> shuffle</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, f1_score, balanced_accuracy_score</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, StratifiedKFold</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_validate, validation_curve</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In supervised learning, the data samples are supplied with labels, and the goal of the learner is to generalize the examples to new values. In unsupervised learning, there are no labels. Instead, the goal is to discover structure that is intrinsic to the feature matrix. Common problem types in unsupervised learning are</p>
<dl>
<dt>Clustering</dt>
<dd>
Determine whether the samples roughly divide into a small number of classes.
</dd>
<dt>Dimension reduction</dt>
<dd>
Find a reduced set of features, or create a small set of new features, that describe the data well.
</dd>
<dt>Outlier detection</dt>
<dd>
Find anomalous values in the data set and remove them or impute replacements.
</dd>
</dl>
<p>In this chapter we will look at clustering. The goal is to assign each feature vector a number from 1 to <span class="math inline">\(k\)</span>, where <span class="math inline">\(k\)</span> is much smaller than the number of samples. More formally:</p>
<div id="def-cluster-clustering" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1</strong></span> Given an <span class="math inline">\(n\times d\)</span> feature matrix with rows <span class="math inline">\(\bfX_1,\ldots,\bfX_n\)</span>, a <strong>clustering</strong> is a labelling function <span class="math inline">\(c\)</span> defined on the feature vectors such that <span class="math display">\[
c(\bfX_i) \in \{ 1,2,\ldots, k \} \text{ for all } i=1,\ldots,n,
\]</span> where <span class="math inline">\(k\)</span> is a positive integer. The <strong>cluster</strong> <span class="math inline">\(C_j\)</span> is the collection of all <span class="math inline">\(\bfX_i\)</span> such that <span class="math inline">\(c(\bfX_i)=j\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_g0aquvfp&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_um654m1q" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.0" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>The clusters <span class="math inline">\(C_1,\ldots,C_k\)</span> divide the samples into <span class="math inline">\(k\)</span> dijoint subsets. Depending on the algorithm, the number of clusters <span class="math inline">\(k\)</span> may be imposed (i. e., as a hyperparameter) or determined automatically.</p>
<p>Intuitively, we want similar examples to be clustered together (that is, to receive the same label). The motivation for clustering is often to find a way to classify the samples by intrinsic properties when no such classification is known in advance. It is <em>not</em> necessary that the clustering function <span class="math inline">\(c\)</span> be defined for vectors <span class="math inline">\(\bfx\)</span> other than the feature vectors, although many clustering algorithms can be used to do that as well.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The universe doesn’t owe you a clustering. Not all phenomena are amenable to clustering in whatever features you happen to choose.</p>
</div>
</div>
<p>While classification only requires us to separate different classes of examples, clustering is more specific and more demanding: samples in a cluster need to be more like each other, or the “average” cluster member, than they are like members of other clusters. We should expect that edge cases will look ambiguous.</p>
<p>In order to get a feeling for the algorithms, we will apply them to three illustrative datasets:</p>
<ul>
<li><dl>
<dt>blobs</dt>
<dd>
This dataset has one distinct blob, plus two that kind of overlap a bit:
</dd>
</dl></li>
</ul>
<div id="0355f79a" class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blobs_data():</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> make_blobs(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        n_samples<span class="op">=</span>[<span class="dv">60</span>, <span class="dv">50</span>, <span class="dv">40</span>],</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        centers<span class="op">=</span>[[<span class="op">-</span><span class="dv">2</span>,<span class="dv">3</span>], [<span class="dv">3</span>,<span class="fl">1.5</span>], [<span class="dv">1</span>,<span class="op">-</span><span class="dv">3</span>]],</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        cluster_std<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">1.2</span>],</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        random_state <span class="op">=</span> <span class="dv">19716</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">"x1"</span>: X[:,<span class="dv">0</span>], <span class="st">"x2"</span>: X[:,<span class="dv">1</span>]}), pd.Series(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-clustering-blobs" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>y, palette<span class="op">=</span><span class="st">"Dark2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-clustering-blobs" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clustering-blobs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="clustering_files/figure-html/fig-clustering-blobs-output-1.png" width="526" height="468" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clustering-blobs-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.1: Blobs dataset with reference clustering shown by colors.
</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li><dl>
<dt>stripes</dt>
<dd>
In this dataset, there is clear separation along one axis and a continuous blob along the other:
</dd>
</dl></li>
</ul>
<div id="81be138e" class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stripes_data():</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> default_rng(<span class="dv">9</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    x1,x2,cls <span class="op">=</span> [],[],[]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        x1.extend( rng.uniform(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, size<span class="op">=</span><span class="dv">200</span>) )</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        x2.extend( <span class="fl">2.5</span><span class="op">*</span>i<span class="op">+</span>rng.uniform(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span><span class="dv">200</span>) )</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        cls.extend( [i]<span class="op">*</span><span class="dv">200</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">"x1"</span>: x1, <span class="st">"x2"</span>: x2}), pd.Series(cls)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-clustering-stripes" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>y, palette<span class="op">=</span><span class="st">"Dark2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-clustering-stripes" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clustering-stripes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="clustering_files/figure-html/fig-clustering-stripes-output-1.png" width="528" height="468" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clustering-stripes-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.2: Stripes dataset with reference clustering shown by colors.
</figcaption>
</figure>
</div>
</div>
</div>
<ul>
<li><dl>
<dt>bullseye</dt>
<dd>
This is the most challenging dataset, because the clusters can’t be separated by anything like straight lines:
</dd>
</dl></li>
</ul>
<div id="f2bde3ae" class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bullseye_data():</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> default_rng(<span class="dv">6</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    inner <span class="op">=</span> <span class="fl">0.8</span><span class="op">*</span>rng.normal(size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">2</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> rng.uniform(<span class="dv">3</span>, <span class="dv">4</span>, size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    middle <span class="op">=</span> np.vstack((r<span class="op">*</span>np.cos(theta), r<span class="op">*</span>np.sin(theta))).T</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> rng.uniform(<span class="dv">5</span>,<span class="dv">6</span>,size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    outer <span class="op">=</span> np.vstack((r<span class="op">*</span>np.cos(theta), r<span class="op">*</span>np.sin(theta))).T</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    cls <span class="op">=</span> np.hstack( ([<span class="dv">1</span>]<span class="op">*</span><span class="dv">100</span>, [<span class="dv">2</span>]<span class="op">*</span><span class="dv">200</span>, [<span class="dv">3</span>]<span class="op">*</span><span class="dv">200</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pd.DataFrame( np.vstack((inner,middle,outer)), columns<span class="op">=</span>[<span class="st">"x1"</span>, <span class="st">"x2"</span>] )</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, pd.Series(cls)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-fig-clustering-bullseye" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> bullseye_data()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>y, palette<span class="op">=</span><span class="st">"Dark2"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>p.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div id="fig-clustering-bullseye" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-clustering-bullseye-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="clustering_files/figure-html/fig-clustering-bullseye-output-1.png" width="524" height="468" class="figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-clustering-bullseye-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.3: Bullseye dataset with reference clustering shown by colors.
</figcaption>
</figure>
</div>
</div>
</div>
<section id="similarity" class="level2 page-columns page-full" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="similarity"><span class="header-section-number">6.1</span> Similarity</h2>
<p>Ideally, samples within a cluster are more similar to each other than they are to samples in other clusters. The first decision we have to make is how to define <em>similarity</em> between an arbitrary pair.</p>
<p>When a distance metric is available, we intuitively expect similarity to be inversely related to distance. There are various ways to quantify the relationship, but we will not use them. Instead, we will take “maximize similarity” to be equivalent to “minimize distance.”</p>
<!-- 
Here is one of the most common ways to realize that ideal.

::::{#def-cluster-gauss}
Given a distance function between pairs of vectors as $\dist(\bfx,\bfy)$, we define the **Gaussian similarity** between two vectors as
$$
\simil(\bfx,\bfy) = \exp \left[ - \frac{\dist(\bfx,\bfy)^2}{2\sigma^2}  \right].
$$ {#eq-similar-similarity}
::::

::: {.callout-note}
Up to a constant scaling factor, the function in the Gaussian similarity definition is the same as the PDF of the normal distribution with mean 0 and standard deviation $\sigma$. 
:::

Thus, for any vector $\bfx$,
$$
\simil(\bfx,\bfx) = 1.
$$
This is the largest possible value. The similarity tends to zero as distance increases. The scaling parameter $\sigma$ controls the rate of decrease; for instance, when the distance is $\sigma$, the similarity is $e^{-1/2}\approx 0.6$. 

::: {.callout-tip}
When the distance function in Gaussian similarity is based on a vector norm, it's advisable to use standardization of the features, either by using z-scores or some other normalization.
::: -->
<section id="distance-matrix" class="level3 page-columns page-full" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="distance-matrix"><span class="header-section-number">6.1.1</span> Distance matrix</h3>
<div id="def-cluster-distmat" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.2</strong></span> Given the feature vectors <span class="math inline">\(\bfX_1,\ldots,\bfX_n\)</span>, the pairwise distances between them are collected in the <span class="math inline">\(n\times n\)</span> <strong>distance matrix</strong></p>
<p><span class="math display">\[
D_{ij} = \text{dist}(\bfX_i,\bfX_j).
\]</span></p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:400px">
<div style="position:relative;padding-bottom:71.25%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_ib6r7f6v&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_bnic1gqh" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.1.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>Note that <span class="math inline">\(D_{ii}=0\)</span> and <span class="math inline">\(D_{ji}=D_{ij}\)</span>.</p>
<div id="exm-cluster-distmat-small" class="theorem example" data-chapter="6" type="✍️" data-description="Distance matrix">
<p><span class="theorem-title"><strong>Example 6.1</strong></span> Using 1-norm for distance, find the distance matrix for the feature matrix <span class="math display">\[
\bfX = \begin{bmatrix}
1 &amp; 2 &amp; 1 &amp; -2 \\ 0 &amp; 3 &amp; 3 &amp; 1 \\ 1 &amp; -1 &amp; 0 &amp; 4
\end{bmatrix}.
\]</span></p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>There are <span class="math inline">\(n=3\)</span> feature vectors. <span class="math display">\[
\begin{split}
D_{12} &amp;=  \abs{1} + \abs{-1} + \abs{-2} + \abs{-3} = 7 \\
D_{13} &amp;=  \abs{0} + \abs{3} + \abs{1} + \abs{-6}  = 10 \\
D_{23} &amp;=  \abs{-1} + \abs{4} + \abs{3} + \abs{-3}  = 11.
\end{split}
\]</span> Hence, <span class="math display">\[
\mathbf{D} = \begin{bmatrix}
0 &amp; 7 &amp; 10 \\ 7 &amp; 0 &amp; 11 \\ 10 &amp; 11 &amp; 0
\end{bmatrix}.
\]</span></p>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Many clustering algorithms allow supplying <span class="math inline">\(\mathbf{D}\)</span> in lieu of the feature vectors. When you consider that in many problems, <span class="math inline">\(n\)</span> is much larger than <span class="math inline">\(d\)</span>, it can be faster and easier to work with.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A practical advantage of working with similarity rather than distance is that small values of similarity can be rounded down to zero. Such rounding has negligible effect on the results, but it can create big gains in execution time and memory usage.</p>
</div>
</div>
<p>The scikit-learn function <code>pairwise_distances</code> computes a distance matrix efficiently.</p>
<div id="exm-cluster-distmat" class="theorem example" data-chapter="6" type="💻" data-description="Usage of `pairwise_distances`">
<p><span class="theorem-title"><strong>Example 6.2</strong></span> The distance matrix of our <em>bullseye</em> dataset has some interesting structure:</p>
<div id="5257abbc" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> pairwise_distances</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> bullseye_data()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>D2 <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"euclidean"</span>)   <span class="co"># use 2-norm metric</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.heatmap(D2)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-9-output-1.png" width="494" height="430" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Because we set up three geometrically distinct groups of points, the distances of pairs within and between groups are fairly homogeneous. The lower-right corner, for example, shows that points in the outermost ring tend to be separated by the greatest distance.</p>
<p>In the 1-norm, the <em>stripes</em> dataset is also interesting:</p>
<div id="67506d39" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>D1 <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"manhattan"</span>)   <span class="co"># use 1-norm metric</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.heatmap(D1)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-10-output-1.png" width="494" height="430" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Points in different stripes are always separated by at least the inter-stripe distance, while points within the same stripe have a range of possible distances.</p>
</div>
</section>
<section id="angular-distance" class="level3 page-columns page-full" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="angular-distance"><span class="header-section-number">6.1.2</span> Angular distance</h3>
<p>An alternative to vector norms for distances is <strong>angular distance</strong>. In three dimensions, imagine drawing a ray from the origin to each of the two given vectors. Those rays lie in a common plane, and we can let <span class="math inline">\(\theta\)</span> be the (smallest) angle between them. The general formula in any number of dimensions is <span id="eq-cluster-angle"><span class="math display">\[
\cos(\theta) = \frac{\mathbf{u}^T\mathbf{v}}{\twonorm{\mathbf{u}} \, \twonorm{\mathbf{v}}}.
\tag{6.1}\]</span></span></p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_ol2razoi&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_vi5hg8ct" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.1.2" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>We then let <span class="math inline">\(\dist(\bfx,\bfy)=\theta \in [0,\pi]\)</span>, using the standard branch of arccos.</p>
<p>However, arccos is a relatively expensive computational operation, and sometimes we use the pseudodistance <span class="math display">\[
d(\bfx,\bfy) = 1 - \cos(\theta)
\]</span> in place of <span class="math inline">\(\theta\)</span>. This function (which ranges between 0 and 2) is not a true distance function, though, as <span class="math inline">\(d\)</span> fails to satisfy the triangle inequality in all cases (see <a href="#exr-cluster-cosine" class="quarto-xref">Exercise&nbsp;<span>6.2</span></a>).</p>
<p>Angular distance and cosine pseudodistance are useful when we want to ignore the magnitudes of vectors and consider only their directions.</p>
<div id="exm-cluster-angular" class="theorem example" data-chapter="6" type="💻" data-description="Angular distance">
<p><span class="theorem-title"><strong>Example 6.3</strong></span> We might represent a text document by a vector of the number of occurrences of certain keywords. In sentiment analysis, these would be terms such as <em>happy</em>, <em>excited</em>, <em>sad</em>, <em>angry</em>, <em>confused</em>, and so on. For sake of argument, suppose three documents have the feature matrix <span class="math display">\[
\bfX =
\begin{bmatrix}
6 &amp; 1 &amp; 10 &amp; 2 &amp; 5 \\  14 &amp; 0 &amp; 23 &amp; 3 &amp; 7 \\ 2 &amp; 3 &amp; 1 &amp; 5 &amp; 0
\end{bmatrix}.
\]</span> Using the 2-norm, we get the distances:</p>
<div id="5a526b3f" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array( [[<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">5</span>], [<span class="dv">14</span>,<span class="dv">0</span>,<span class="dv">23</span>,<span class="dv">3</span>,<span class="dv">7</span>], [<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">0</span>]] )</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>pairwise_distances( X, metric<span class="op">=</span><span class="st">"euclidean"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([[ 0.        , 15.45962483, 11.61895004],
       [15.45962483,  0.        , 26.26785107],
       [11.61895004, 26.26785107,  0.        ]])</code></pre>
</div>
</div>
<p>Accoding to this metric, the most similar documents are the first and the last. However, cosine distance tells a different story:</p>
<div id="2b57ab4e" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>pairwise_distances( X, metric<span class="op">=</span><span class="st">"cosine"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([[0.        , 0.01532383, 0.56500757],
       [0.01532383, 0.        , 0.62231412],
       [0.56500757, 0.62231412, 0.        ]])</code></pre>
</div>
</div>
<p>Now the first two documents look most similar. Looking at the vectors, we see that these two have the most similar relative frequencies, which is related to what we want to measure. The apparent dissimilarity in the 2-norm arises solely because the total word counts are very different.</p>
</div>
<!-- Categorical variables can be included in distance metrics. An ordinal variable is easily converted to equally spaced numerical values, which then may get a standard treatment. Nominal features are often compared using **Hamming distance**, which is just the total number of features that have different values in the two vectors. -->
<!-- 
### Probability distributions

::::{#def-cluster-probdist}
A **discrete probability distribution** is a vector $\bfx$ whose components are nonnegative and satisfying $\onenorm{\bfx}=1$. 
::::

Such a vector can be interpreted as frequencies or probabilities of observing different classes. For example, when computing Gini impurity, we define $p_k$ as the observed frequency of label $k$ appearing in a given set.

While we can treat probability vectors like any other vector and apply vector norms, doing so fails to take advantage of their special properties. There are alternatives suggested from information theory that are usually preferred.

::::{#def-cluster-probdist}
If $\bfx$ and $\bfy$ are probability vectors of length $d$, then the **Kullback–Leibler (KL) divergence** is defined as 
$$
\operatorname{KL}(\bfx,\bfy) = \sum_{i=1}^d x_i \log\left( \frac{x_i}{y_i} \right).
$$
Whenever $0\cdot \log(0)$ is encountered in the sum, it equals zero, in accordance with its limiting value from calculus. If, for some $i$, $x_i\neq 0$ and $y_i=0$, then $KL(\bfx,\bfy) = \infty$. 

We define the **information radius** as 
$$
\operatorname{IR}(\bfx,\bfy) = \frac{1}{2} \bigl[ \operatorname{KL}(\bfx,\bfz) + \operatorname{KL}(\bfy,\bfz) \bigr],
$$
where $\bfz=(\bfx+\bfy)/2$.
::::

Of these, only IR is symmetric in the sense that $\operatorname{IR}(\bfx,\bfy)=\operatorname{IR}(\bfy,\bfx)$. The square root of IR is a distance metric. Typically one uses a base-2 logarithm, in which case IR ranges between 0 (for identical distributions) and 1. 

::::{#exm-cluster-IR}
Let $\bfx=\frac{1}{4}[1,3]$ and $\bfy=\frac{1}{4}[3,1]$. Using base-2 logs, find the IR between them.

:::{.solution}
We compute 
$$
\bfz=\tfrac{1}{2}(\bfx+\bfy) = [\tfrac{1}{2},\tfrac{1}{2}]. 
$$
Thus,
$$
\begin{split}
    \operatorname{KL}(\bfx,\bfz)  &= \tfrac{1}{4} \cdot \log \left( \frac{1/4}{1/2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3/4}{1/2} \right) \\ &= \tfrac{1}{4} \cdot \log \left( \frac{1}{2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3}{2} \right) = -\tfrac{1}{4} + \tfrac{3}{4} (\log(3)-1),\\
    \operatorname{KL}(\bfy,\bfz)  &= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4},\\
    \operatorname{IR}(\bfx,\bfy)  &= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4} \approx 0.1887.
\end{split}
$$
:::
:::: -->
</section>
<section id="distance-in-high-dimensions" class="level3 page-columns page-full" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="distance-in-high-dimensions"><span class="header-section-number">6.1.3</span> Distance in high dimensions</h3>
<p>High-dimensional space <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">does not conform to some intuitions</a> formed by our experiences in 2D and 3D.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_fjvhcqjb&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_41w8wqra" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.1.3" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>For example, consider the unit hyperball <span class="math inline">\(\twonorm{\bfx}\le 1\)</span> in <span class="math inline">\(d\)</span> dimensions. We’ll take it as given that scaling a <span class="math inline">\(d\)</span>-dimensional object by a number <span class="math inline">\(r\)</span> will scale the volume by <span class="math inline">\(r^d\)</span>. Then for any <span class="math inline">\(r&lt;1\)</span>, the fraction of the unit hyperball’s volume lying <em>outside</em> the smaller hyperball of fixed radius <span class="math inline">\(r\)</span> is <span class="math inline">\(1-r^d\)</span>, which approaches <span class="math inline">\(1\)</span> as <span class="math inline">\(d\to \infty\)</span>. That is, <em>if we choose points randomly within a hyperball, almost all of them will be near the outer boundary</em>.</p>
<p>The volume of the unit hyperball also vanishes as <span class="math inline">\(d\to \infty\)</span>. This is because the inequality</p>
<p><span class="math display">\[
x_1^2 + x_2^2 + \cdots + x_d^2 \le 1,
\]</span></p>
<p>where each <span class="math inline">\(x_i\)</span> is chosen randomly in <span class="math inline">\([-1,1]\)</span>, becomes ever harder to satisfy as the number of terms in the sum grows, and the relative occurrence of such points is increasingly rare.</p>
<p>There are other, similar mathematical results demonstrating the weirdness of distances in high-dimensional space. These results and their implications go under the colorful name of <strong>the curse of dimensionality</strong>. Using distances in high-dimensional space is difficult not just for clustering but in nearest-neighbor and other algorithms as well.</p>
<p>It turns out that in most practical problems, there is a way out of the mess: the data points are usually not uniformly or randomly distributed. That means that distance-based approaches can be made to work. Often, though, a first step is to find a way to reduce the dimensionality without changing the essential structure of the data, a step that is beyond what we go into here.</p>
</section>
</section>
<section id="performance-measures" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="performance-measures"><span class="header-section-number">6.2</span> Performance measures</h2>
<p>Before we start generating clusterings, we will discuss how to evaluate them. A clustering is essentially a partitioning of the samples into disjoint subsets. We will use some nonstandard terminology that makes the definitions a bit easier to state and read.</p>
<div id="def-cluster-buddies" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3</strong></span> We say that two sample points in a clustering are <strong>buddies</strong> if they are in the same cluster, and <strong>strangers</strong> otherwise.</p>
</div>
<section id="rand-index-and-ari" class="level3 page-columns page-full" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="rand-index-and-ari"><span class="header-section-number">6.2.1</span> Rand index and ARI</h3>
<p>A clustering can be interpreted as a classification, and vice versa. If a reference classification is available, then we can compare any clustering result to it. This allows us to use classification datasets as proving grounds for clustering.</p>
<div id="def-cluster-rand" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.4</strong></span> We define the <strong>Rand index</strong> between two clusterings of <span class="math inline">\(n\)</span> samples as</p>
<p><span id="eq-cluster-RI"><span class="math display">\[
\text{RI} = \frac{b+s}{\binom{n}{2}},
\tag{6.2}\]</span></span></p>
<p>where <span class="math inline">\(b\)</span> is the number of pairs that are buddies in both clusterings and <span class="math inline">\(s\)</span> is the number of pairs that are strangers in both clusterings.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_7ods82y2&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_kkgr0ber" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.2.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>The reason for the denominator in <a href="#eq-cluster-RI" class="quarto-xref">Equation&nbsp;<span>6.2</span></a> is that there are <span class="math inline">\(\binom{n}{2}\)</span> distinct pairs of <span class="math inline">\(n\)</span> sample points, so we know that <span class="math inline">\(0 \le \text{RI} \le 1\)</span>.</p>
<p>The Rand index has some attractive features:</p>
<ul>
<li>The value is between 0 (complete disagreement) and 1 (complete agreement).</li>
<li>It is symmetric in the two clusterings; i. e., it doesn’t matter which is considered the reference.</li>
<li>There is no need to find a correspondence between the clusters in the two clusterings. In fact, the clusterings need not even have the same number of clusters.</li>
</ul>
<div id="exm-cluster-rand" class="theorem example" data-chapter="6" type="✍️" data-description="Rand index by hand">
<p><span class="theorem-title"><strong>Example 6.4</strong></span> Suppose that samples <span class="math inline">\(\bfX_1,\bfX_2,\bfX_4\)</span> are truly classified as positive, and <span class="math inline">\(\bfX_3,\bfX_5\)</span> are truly classified as negative. This implies the reference clustering <span class="math inline">\(C_1=\{\bfX_1,\bfX_2,\bfX_4\}\)</span> and <span class="math inline">\(C_2=\{\bfX_3,\bfX_5\}\)</span>.</p>
<p>Compute the Rand index relative to the reference of another clustering given by</p>
<p><span class="math display">\[
\widetilde{C}_1=\{\bfX_1,\bfX_2\}, \quad \widetilde{C}_2=\{\bfX_3\}, \quad \widetilde{C}_3=\{\bfX_4,\bfX_5\}
\]</span></p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>Here is a table showing, for every pair of samples, the buddy–stranger status in the reference and in the new clustering:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(\bfX_1\)</span></th>
<th><span class="math inline">\(\bfX_2\)</span></th>
<th><span class="math inline">\(\bfX_3\)</span></th>
<th><span class="math inline">\(\bfX_4\)</span></th>
<th><span class="math inline">\(\bfX_5\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\bfX_1\)</span></td>
<td></td>
<td>B/B</td>
<td>S/S</td>
<td>B/S</td>
<td>S/S</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\bfX_2\)</span></td>
<td></td>
<td></td>
<td>S/S</td>
<td>B/S</td>
<td>S/S</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\bfX_3\)</span></td>
<td></td>
<td></td>
<td></td>
<td>S/S</td>
<td>B/S</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\bfX_4\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>S/B</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\bfX_5\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Hence, we have <span class="math inline">\(b=1\)</span> and <span class="math inline">\(s=5\)</span>, and the Rand index is 6/10 = 0.6.</p>
</div>
</div>
<p>A weakness of the Rand index is that it can be fairly close to 1 even when samples are assigned to clusters randomly. The <strong>adjusted Rand index</strong> rescales the result by comparing it to how a random clustering would fare. It can be written as</p>
<p><span class="math display">\[
\text{ARI} = \frac{\text{RI} - \E{\text{RI}}}{\text{max}(\text{RI}) - \E{\text{RI}}},
\]</span></p>
<p>where the expectation and max operations are performed over all possible clusterings. (These values can be worked out exactly using combinatorics, but we do not give them here.)</p>
<p>An ARI of 0 indicates no better agreement than a random clustering, while an ARI of 1 is complete agreement. Unlike the RI, the ARI value can be negative, indicating a performance worse than on average.</p>
<div id="exm-cluster-ARI-blobs" class="theorem example" data-chapter="6" type="💻" data-description="Adjusted Rand index">
<p><span class="theorem-title"><strong>Example 6.5</strong></span> Our <em>blobs</em> dataset has a reference clustering shown in <a href="#fig-clustering-blobs" class="quarto-xref">Figure&nbsp;<span>6.1</span></a> that we take to be the ground truth. Let’s create another clustering based entirely on the quadrants of the plane:</p>
<div id="4a5e6af7" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quad(x):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x[<span class="dv">0</span>] <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x[<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>: <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: <span class="cf">return</span> <span class="dv">4</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x[<span class="dv">1</span>] <span class="op">&gt;</span> <span class="dv">0</span>: <span class="cf">return</span> <span class="dv">2</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: <span class="cf">return</span> <span class="dv">3</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co"># find the quadrant for every sample</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> X.<span class="bu">apply</span>(quad, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>q.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>0    1
1    1
2    4
3    2
4    4
dtype: int64</code></pre>
</div>
</div>
<p>Because this toy dataset has only 2 features, it’s easy to visualize the clusters by plotting all the samples:</p>
<div id="b6f5582b" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>q, palette<span class="op">=</span><span class="st">"Dark2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-14-output-1.png" width="526" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As you see above, there are four clusters, divided along the coordinate axes. The reference clustering has only three classes, but we can still compare them with the adjusted Rand index:</p>
<div id="e6d7175d" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>adjusted_rand_score(y, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="14">
<pre><code>0.904092765401111</code></pre>
</div>
</div>
<p>Not surprisingly, the clusterings are found to be quite similar.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_ypfju76b&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_r2mmdh2s" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.5" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div></section>
<section id="silhouettes" class="level3 page-columns page-full" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="silhouettes"><span class="header-section-number">6.2.2</span> Silhouettes</h3>
<p>If no reference clustering/classification is available, then the only way we can assess the quality of a clustering is to check how well it creates clusters whose members are more similar to their buddies than to strangers.</p>
<div id="def-cluster-silhouette" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.5</strong></span> Suppose <span class="math inline">\(\bfX_i\)</span> is a sample in a clustering. Let <span class="math inline">\(\bar{b}_i\)</span> be the mean distance between <span class="math inline">\(\bfX_i\)</span> and its buddies, and let <span class="math inline">\(\bar{r}_i\)</span> be the mean distance between <span class="math inline">\(\bfX_i\)</span> and the members of the nearest cluster of strangers. Then the <strong>silhouette value</strong> of <span class="math inline">\(\bfX_i\)</span> is</p>
<p><span class="math display">\[
s_i = \frac{\bar{r}_i-\bar{b}_i}{\max\{\bar{r}_i,\bar{b}_i\}}.
\]</span></p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_jpupxgag&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_wy8zyaj1" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.2.2" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>The value <span class="math inline">\(s_i\)</span> is always in the interval <span class="math inline">\([-1,1]\)</span>. A negative value indicates that the sample seems to be more similar to a cluster other than its own.</p>
<div id="exm-cluster-silhouette" class="theorem example" data-chapter="6" type="✍️" data-description="Silhouette values">
<p><span class="theorem-title"><strong>Example 6.6</strong></span> Suppose we have one-dimensional features, and they are divided into three clusters as follows:</p>
<p><span class="math display">\[
A=\{-4,-1,1\}, \quad B=\{2,6\}, \quad C=\{8,10\}
\]</span></p>
<p>To compute the silhouette score for the sample <span class="math inline">\(X_1=-4\)</span>, we first find the mean distance between it and its buddies:</p>
<p><span class="math display">\[
\bar{b}_1 = \frac{1}{2} \bigl( \abs{-4+1} + \abs{-4-1} \bigr) = 4.
\]</span></p>
<p>Now we need to find the mean distance between <span class="math inline">\(X_1\)</span> and the two stranger clusters:</p>
<p><span class="math display">\[
\bar{r}_{1,B} = \frac{1}{2} \bigl( \abs{-4-2} + \abs{-4-6} \bigr) = 8, \quad \bar{r}_{1,C} = \frac{1}{2} \bigl( \abs{-4-8} + \abs{-4-10} \bigr) = 13.
\]</span></p>
<p>The closest group of strangers is therefore cluster B, and <span class="math inline">\(\bar{r}_1 = 8\)</span>. Hence, the silhouette value for <span class="math inline">\(X_1\)</span> is</p>
<p><span class="math display">\[
s_1 = \frac{8-4}{8} = \frac{1}{2}.
\]</span></p>
<p>Let’s also compute the silhouette value for <span class="math inline">\(X_5=6\)</span>. We have</p>
<p><span class="math display">\[
\bar{b}_5 = \frac{1}{1} \abs{6-2} = 4,
\]</span></p>
<p>and</p>
<p><span class="math display">\[
\bar{r}_{5,A} = \frac{1}{3} \bigl( \abs{6+4} + \abs{6+1} + \abs{6-1} \bigr) = \frac{22}{3}, \quad \bar{r}_{5,C} = \frac{1}{2} \bigl( \abs{6-8} + \abs{6-10} \bigr) = 3.
\]</span></p>
<p>Hence <span class="math inline">\(\bar{r}_5 = 3\)</span> and <span class="math inline">\(s_5 = (3 - 4) / 3 = -1/3\)</span>. A negative silhouette value indicates that the sample may be in the wrong cluster; here it seems that <span class="math inline">\(X_5\)</span> would be a better fit in cluster C.</p>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Most sources define an overall <em>silhouette score</em> as the mean of the <span class="math inline">\(s_i\)</span>. However, because the distributions of these values are often asymmetric and have significant outliers, I happen to think that medians are preferred when a summary score is needed. It’s hard to fight tradition, though, and <code>silhouette_score</code> in scikit-learn reports the mean values.</p>
</div>
</div>
<div id="exm-cluster-sil-blobs" class="theorem example" data-chapter="6" type="💻" data-description="Silhouette values calculation">
<p><span class="theorem-title"><strong>Example 6.7</strong></span> Let’s compute the silhouette scores for the reference cluster assignments in our blobs dataset.</p>
<div id="a4f58958" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_samples</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>sil <span class="op">=</span> pd.Series( silhouette_samples(X, y) )</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>sil.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>0    0.722419
1    0.765660
2    0.678717
3    0.865919
4    0.705436
dtype: float64</code></pre>
</div>
</div>
<p>Because these samples are 2-dimensional, it’s easy to visualize their scores by varying the size of the points used to plot them:</p>
<div id="91e4110e" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span>y, size<span class="op">=</span>sil, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-17-output-1.png" width="547" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The samples that don’t belong comfortably with their cluster have negative scores and therefore the smallest dots. We can find the median score in each cluster through a grouping:</p>
<div id="96b7f51f" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>sil.groupby(y).median()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>0    0.827518
1    0.692668
2    0.639722
dtype: float64</code></pre>
</div>
</div>
<p>We can also compute the silhouette scores for the quadrant-based clustering defined in <a href="#exm-cluster-ARI-blobs" class="quarto-xref">Example&nbsp;<span>6.5</span></a>:</p>
<div id="7082c06a" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>sil <span class="op">=</span> pd.Series( silhouette_samples(X, q) )</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span>q, size<span class="op">=</span>sil, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-19-output-1.png" width="554" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="f1b03210" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>sil.groupby(q).median()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>1    0.701266
2    0.829030
3    0.391247
4    0.156942
dtype: float64</code></pre>
</div>
</div>
<p>The scores for clusters labelled as 1 and 2 are pretty good and almost the same as for the original reference clustering. But the other two clusters score much more poorly separately than they did when they are considered a single cluster.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_o43p6lb3&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_4h7o3i1m" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.7" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div id="exm-cluster-perform-digits" class="theorem example" data-chapter="6" type="💻" data-description="Silhouettes as performance metric">
<p><span class="theorem-title"><strong>Example 6.8</strong></span> <code>sklearn</code> has a well-known dataset that contains labeled handwritten digits. Let’s extract the examples for just the numerals 4, 5, and 6:</p>
<div id="4e43c2a1" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>keep <span class="op">=</span> digits[<span class="st">"target"</span>].isin([<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])  <span class="co"># keep these samples</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> digits[keep]</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.drop(<span class="st">"target"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>y.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<pre><code>5    182
4    181
6    181
Name: target, dtype: int64</code></pre>
</div>
</div>
<p>We can check the silhouette scores for the reference labelling in order to set expectations for how well a clustering method might do:</p>
<div id="b7b01321" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>digits[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X,y)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>digits.groupby(<span class="st">"target"</span>)[<span class="st">"sil"</span>].median()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="21">
<pre><code>target
4    0.207595
5    0.245201
6    0.342689
Name: sil, dtype: float64</code></pre>
</div>
</div>
<p>A look at the distributions of the silhouette values reveals more details:</p>
<div id="5d90badf" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>sns.catplot(data<span class="op">=</span>digits, x<span class="op">=</span><span class="st">"target"</span>, y<span class="op">=</span><span class="st">"sil"</span>,</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"violin"</span></span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-23-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The values are mostly positive, which indicates nearly all of the samples for a digit are at least somewhat closer to each other than to the other samples. The 6s are the most distinct. However, the existence of scores close to and below zero suggest that a clustering algorithm ia unlikely to reproduce the true classification perfectly. To put it another way, the features chosen to represent the digits don’t seem to create three clear, well-separated balls of points representing the three different types of digits.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_o43p6lb3&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_4h7o3i1m" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.7" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>Silhouette values are fairly easy to use, but they are an imperfect tool. They favor clusters that have a compact, roughly spherical shape, in which all the members of the cluster are mutually close to one another.</p>
<div id="exm-cluster-sil-bullseye" class="theorem example" data-chapter="6" type="💻" data-description="Limitation of silhouettes">
<p><span class="theorem-title"><strong>Example 6.9</strong></span> The bullseye dataset shown in <a href="#fig-clustering-bullseye" class="quarto-xref">Figure&nbsp;<span>6.3</span></a> has only one central cluster that silhouette values approve of:</p>
<div id="899ecfcf" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> bullseye_data()</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> silhouette_samples(X, y)</span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> pd.DataFrame({<span class="st">"cluster"</span>: y, <span class="st">"s value"</span>: s})</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>sns.catplot(data<span class="op">=</span>clusters, x<span class="op">=</span><span class="st">"cluster"</span>, y<span class="op">=</span><span class="st">"s value"</span>, </span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"violin"</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-24-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The central cluster scores well, but the rings score poorly because they are not compact and ball-shaped.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_y47kfwrc&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_hfq0mpjd" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.9" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>While we have no trouble validating good clusterings by eye in two dimensions in cases such as <a href="#fig-clustering-stripes" class="quarto-xref">Figure&nbsp;<span>6.2</span></a> and <a href="#fig-clustering-bullseye" class="quarto-xref">Figure&nbsp;<span>6.3</span></a>, it can be challenging to assess such nonspherical clusters in higher dimensions if there is no reference or validation available.</p>
</div>
</div>
</section>
</section>
<section id="k-means" class="level2 page-columns page-full" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="k-means"><span class="header-section-number">6.3</span> k-means</h2>
<p>The <strong><span class="math inline">\(k\)</span>-means algorithm</strong> is one of the best-known clustering methods. It is often the method of first resort, even though it has some significant limitations.</p>
<p>Given a sample matrix <span class="math inline">\(\bfX\)</span> with <span class="math inline">\(n\)</span> rows <span class="math inline">\(\bfX_i\)</span>, the algorithm divides the sample points into disjoint sets <span class="math inline">\(C_1,\ldots,C_k\)</span>, where <span class="math inline">\(k\)</span> is a hyperparameter. We need a pair of key definitions.</p>
<div id="def-cluster-kmeans" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.6</strong></span> The <strong>centroid</strong> of cluster <span class="math inline">\(C_j\)</span>, denoted by <span class="math inline">\(\bfmu_j\)</span>, is the mean of the vectors in <span class="math inline">\(C_j\)</span>: <span class="math display">\[
\bfmu_j = \frac{1}{\abs{C_j}} \sum_{\bfx\in C_j} \bfx.
\]</span> The <strong>inertia</strong> of <span class="math inline">\(C_j\)</span> is <span id="eq-cluster-inertia"><span class="math display">\[
I_j = \sum_{\bfx\in C_j} \norm{ \bfx - \bfmu_j }_2^2.
\tag{6.3}\]</span></span></p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_vi87asd8&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_mgkg39iq" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.3" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>The goal of the <span class="math inline">\(k\)</span>-means algorithm is to choose the clusters in order to minimize the total inertia,</p>
<p><span class="math display">\[
I = \sum_{j=1}^k I_j.
\]</span></p>
<!-- For any cluster, its centroid is the point that minimizes the inertia of the cluster. Suppose that $C_j$ is split into two parts $A$ and $B$ that have centroids $\bfmu_A$ and $\bfmu_B$. Those centroids minimize the inertias of the subclusters. Hence, 

$$
\sum_{\bfx\in A} \norm{ \bfx - \bfmu_A }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_B }^2 
\le  \sum_{\bfx\in A} \norm{ \bfx - \bfmu_j }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_j}^2  = I_j. 
$$

We conclude that splitting a cluster will make the total inertia decrease. In fact, if each sample point is put into its own cluster, the inertia is 0.  -->
<div id="exm-cluster-inertia" class="theorem example" data-chapter="6" type="✍️" data-description="Inertia">
<p><span class="theorem-title"><strong>Example 6.10</strong></span> Given the one-dimensional samples <span class="math inline">\(-3,-2,-1,2,5,7\)</span>, find the inertia of the clustering <span class="math display">\[
C_1=\{-3,-2,-1\}, \qquad C_2=\{2,5,7\},
\]</span> and of the clustering <span class="math display">\[
C_1=\{-3,-2,-1,2\}, \qquad C_2=\{5,7\}.  
\]</span></p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span>The first clustering has centroids</p>
<p><span class="math display">\[
\mu_1= \frac{1}{3}\bigl( -3 -2 -1 \bigr) = -2, \quad \mu_2= \frac{1}{3}\bigl(2+5 +7 \bigr) =14/3.
\]</span></p>
<p>This gives inertia</p>
<p><span class="math display">\[
\begin{split}
I_1 &amp;= (-3+2)^2 + (-2+2)^2 + (-1+2)^2 = 2, \\
I_2 &amp;= \bigl(2-\tfrac{14}{3}\bigr)^2 + \bigl(5-\tfrac{14}{3}\bigr)^2 + \bigl(7-\tfrac{14}{3}\bigr)^2 = 13.78,
\end{split}
\]</span></p>
<p>for a total inertia of 15.78.</p>
<p>The second clustering has centroids</p>
<p><span class="math display">\[
\mu_1= \frac{1}{4}\bigl( -3 -2 -1 + 2\bigr) = -1, \quad \mu_2= \frac{1}{2}\bigl(5 +7 \bigr) =6,
\]</span></p>
<p>with cluster inertias</p>
<p><span class="math display">\[
\left[   \right]  + \left[   \right] = 14 + 2 = 16.
\]</span></p>
<p><span class="math display">\[
\begin{split}
I_1 &amp;= (-3+1)^2 + (-2+1)^2 + (-1+1)^2  + (2+1)^2 = 14, \\
I_2 &amp;=  (5-6)^2 + (7-6)^2  = 2,
\end{split}
\]</span></p>
<p>for a total inertia of 16. The first clustering would be considered better because it has lower total inertia.</p>
</div>
</div>
<p>Finding the minimum inertia among all possible <span class="math inline">\(k\)</span>-clusterings is an infeasible problem to solve exactly at any practical size. Instead, the computational approach is to iteratively improve on a starting clustering.</p>
<section id="lloyds-algorithm" class="level3 page-columns page-full" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="lloyds-algorithm"><span class="header-section-number">6.3.1</span> Lloyd’s algorithm</h3>
<p>The standard iteration is known as <strong>Lloyd’s algorithm</strong>. Starting with values for the <span class="math inline">\(k\)</span> centroids, each iteration consists of two steps:</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_zyb6uloq&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_0heo7jnf" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.3.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><ol type="1">
<li>Each sample point is assigned to the cluster whose centroid is the nearest. (Ties are broken randomly.)</li>
<li>Recalculate the centroids based on the cluster assignments: <span class="math display">\[
\bfmu_j^+ = \frac{1}{|C_j|} \sum_{\bfx\in C_j} \bfx.
\]</span></li>
</ol>
<p>These steps are repeated alternately until the assignment step does not change any of the clusters. In practice, this almost always happens quickly. Here is a demonstration:</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="480" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="./_media/kmeans_demo.mp4"></video></div>
<p>If Lloyd’s algorithm converges, it finds a local minimum of total inertia, in the sense that small changes to the cluster assignments cannot decrease it. But there is no guarantee of converging to the global minimum, and often, this does not happen.</p>
</section>
<section id="practical-issues" class="level3 page-columns page-full" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="practical-issues"><span class="header-section-number">6.3.2</span> Practical issues</h3>
<ul>
<li><strong>Initialization</strong>. The performance of <span class="math inline">\(k\)</span>-means depends a great deal on the initial set of centroids. Traditionally, the centroids were chosen as random members of the sample set, but more reliable heuristics, such as <em><span class="math inline">\(k\)</span>-means++</em>, have since become dominant.</li>
<li><strong>Multiple runs</strong>. All the initialization methods include an element of randomness, and since the Lloyd algorithm usually converges quickly, it is often run with multiple instances of the initialization. The run with the lowest final inertia is kept.</li>
<li><strong>Distance metric</strong>. The Lloyd algorithm often fails to converge for norms other than the 2-norm and must be modified if another norm is preferred.</li>
</ul>
<p>The algorithm treats <span class="math inline">\(k\)</span> as a hyperparameter. Increasing <span class="math inline">\(k\)</span> tends to lower the total inertia—at the extreme, taking <span class="math inline">\(k=n\)</span> puts each sample in its own cluster, with a total inertia of zero. Hence, one should use silhouette scores or some other measure in order to optimize <span class="math inline">\(k\)</span>. Occam’s Razor dictates preferring smaller values to large ones. There are many suggestions on how to find the choice that gives the most “bang for the buck,” but none is foolproof.</p>
<p>Because of its dependence on the norm, the inertia criterion of <span class="math inline">\(k\)</span>-means disfavors long, skinny clusters and clusters of unequal dispersion. Essentially, it wants to find spherical blobs of roughly equal size.</p>
<div id="exm-cluster-kmeans-blobs" class="theorem example" data-chapter="6" type="💻" data-description="k-means on *blobs* dataset">
<p><span class="theorem-title"><strong>Example 6.11</strong></span> We apply <span class="math inline">\(k\)</span>-means to the <em>blobs</em> dataset that has 3 reference clusters, starting with <span class="math inline">\(k=2\)</span> clusters. For illustration, we instruct the algorithm to try 3 initializations and report on its progress:</p>
<div id="cb6b1c01" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>km2 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>, n_init<span class="op">=</span><span class="dv">3</span>, verbose<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>km2.fit(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initialization complete
Iteration 0, inertia 1059.6987886807747.
Iteration 1, inertia 731.1354331556078.
Iteration 2, inertia 710.0834960683221.
Converged at iteration 2: strict convergence.
Initialization complete
Iteration 0, inertia 991.8887143415382.
Iteration 1, inertia 742.3898509432538.
Iteration 2, inertia 710.0834960683221.
Converged at iteration 2: strict convergence.
Initialization complete
Iteration 0, inertia 1379.944166430521.
Iteration 1, inertia 917.3997019815874.
Iteration 2, inertia 903.6145145551955.
Iteration 3, inertia 845.0278508241745.
Iteration 4, inertia 736.5347784099483.
Iteration 5, inertia 710.0834960683221.
Converged at iteration 5: strict convergence.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="24">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KMeans(n_clusters=2, n_init=3, random_state=302, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">KMeans</label><div class="sk-toggleable__content"><pre>KMeans(n_clusters=2, n_init=3, random_state=302, verbose=1)</pre></div></div></div></div></div>
</div>
</div>
<p>The fitted clustering object can tell us the final inertia and cluster centroids:</p>
<div id="5fccec75" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"final inertia: </span><span class="sc">{</span>km2<span class="sc">.</span>inertia_<span class="sc">:.5g}</span><span class="ss">"</span>)</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"cluster centroids:"</span>)</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(km2.cluster_centers_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>final inertia: 710.08
cluster centroids:
[[-2.01977462  2.86247767]
 [ 2.01902057 -0.69722874]]</code></pre>
</div>
</div>
<p>There is a <code>predict</code> method that can make cluster assignments for arbitrary points in feature space. But to get the cluster assignments for the samples, it’s easier to use the <code>labels_</code> property of the fitted clustering object.</p>
<div id="5c9834cb" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>c <span class="op">=</span> pd.Series( km2.labels_ )</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>c.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="26">
<pre><code>1    89
0    61
dtype: int64</code></pre>
</div>
</div>
<p>Here, we encapsulate using those values to compute and then show the silhouette values. Again, since we only have 2 features, it’s easy to visualize the clusters directly.</p>
<div id="8d46f444" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report(clustering):</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>    result <span class="op">=</span> X.copy()</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> clustering.labels_    <span class="co"># cluster assignments</span></span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">"cluster"</span>] <span class="op">=</span> y_hat</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    result[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X, y_hat)</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"inertia: </span><span class="sc">{</span>clustering<span class="sc">.</span>inertia_<span class="sc">:.5g}</span><span class="ss">"</span>)</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"silhouette medians by cluster:"</span>)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>( result.groupby(<span class="st">"cluster"</span>)[<span class="st">"sil"</span>].median() )</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>    sns.catplot(data<span class="op">=</span>result, x<span class="op">=</span><span class="st">"cluster"</span>, y<span class="op">=</span><span class="st">"sil"</span>,</span>
<span id="cb39-11"><a href="#cb39-11" aria-hidden="true" tabindex="-1"></a>        kind<span class="op">=</span><span class="st">"violin"</span>, height<span class="op">=</span><span class="dv">3</span></span>
<span id="cb39-12"><a href="#cb39-12" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span>
<span id="cb39-13"><a href="#cb39-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb39-14"><a href="#cb39-14" aria-hidden="true" tabindex="-1"></a>    sns.relplot(data<span class="op">=</span>result, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb39-15"><a href="#cb39-15" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, size<span class="op">=</span><span class="st">"sil"</span>,</span>
<span id="cb39-16"><a href="#cb39-16" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="dv">3</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb39-17"><a href="#cb39-17" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span>
<span id="cb39-18"><a href="#cb39-18" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> result</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="1c06ad53" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>report(km2)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 710.08
silhouette medians by cluster:
cluster
0    0.839082
1    0.440536
Name: sil, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-29-output-2.png" width="277" height="276" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-29-output-3.png" width="346" height="276" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s clear from both plots above that one cluster is more tightly packed than the other. Now let’s repeat the computation for <span class="math inline">\(k=3\)</span> clusters:</p>
<div id="987d7798" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>km3 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, n_init<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>km3.fit(X)</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>report(km3)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 203.3
silhouette medians by cluster:
cluster
0    0.828146
1    0.688014
2    0.645840
Name: sil, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-30-output-2.png" width="277" height="276" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-30-output-3.png" width="346" height="276" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>This result shows a modest reduction in silhouette scores for the original good cluster, but improvement for the problematic one.</p>
<p>Moving on to <span class="math inline">\(k=4\)</span> clusters shows clear degradation of the silhouette values:</p>
<div id="4c1c0bd4" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>km4 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>, n_init<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>km4.fit(X)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>report(km4)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 166.02
silhouette medians by cluster:
cluster
0    0.829120
1    0.268430
2    0.423066
3    0.647724
Name: sil, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-31-output-2.png" width="277" height="276" class="figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-31-output-3.png" width="346" height="276" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Based on silhouette values, we would be justified to stop at <span class="math inline">\(k=3\)</span> clusters.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_2vl75uac&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_1uhuj1qk" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.11" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div id="exm-cluster-kmeans-stripes" class="theorem example" data-chapter="6" type="💻" data-description="k-means on *stripes* dataset">
<p><span class="theorem-title"><strong>Example 6.12</strong></span> The <em>stripes</em> dataset does not conform to compact clusters of equal size, and <span class="math inline">\(k\)</span>-means performs poorly on it:</p>
<div id="5b6b4f39" class="cell" data-execution_count="32">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]:</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, n_init<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.concat( (</span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>        results, </span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>            pd.DataFrame( {</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a>            <span class="st">"x1"</span>: X[<span class="st">"x1"</span>], <span class="st">"x2"</span>: X[<span class="st">"x2"</span>], </span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>            <span class="st">"cluster"</span>: km.labels_,</span>
<span id="cb46-11"><a href="#cb46-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">"k"</span>: k</span>
<span id="cb46-12"><a href="#cb46-12" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb46-13"><a href="#cb46-13" aria-hidden="true" tabindex="-1"></a>        ) )</span>
<span id="cb46-14"><a href="#cb46-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb46-15"><a href="#cb46-15" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb46-16"><a href="#cb46-16" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb46-17"><a href="#cb46-17" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"k"</span>, </span>
<span id="cb46-18"><a href="#cb46-18" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">3</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb46-19"><a href="#cb46-19" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-32-output-1.png" width="914" height="275" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>It’s usually a good idea to standardize the data before clustering. But that’s not much help here:</p>
<div id="66323266" class="cell" data-execution_count="33">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]:</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> make_pipeline(</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>        StandardScaler(), </span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>        KMeans(n_clusters<span class="op">=</span>k, n_init<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.concat( (</span>
<span id="cb47-9"><a href="#cb47-9" aria-hidden="true" tabindex="-1"></a>        results, </span>
<span id="cb47-10"><a href="#cb47-10" aria-hidden="true" tabindex="-1"></a>            pd.DataFrame( {</span>
<span id="cb47-11"><a href="#cb47-11" aria-hidden="true" tabindex="-1"></a>            <span class="st">"x1"</span>: X[<span class="st">"x1"</span>], <span class="st">"x2"</span>: X[<span class="st">"x2"</span>], </span>
<span id="cb47-12"><a href="#cb47-12" aria-hidden="true" tabindex="-1"></a>            <span class="st">"cluster"</span>: km[<span class="dv">1</span>].labels_,</span>
<span id="cb47-13"><a href="#cb47-13" aria-hidden="true" tabindex="-1"></a>            <span class="st">"k"</span>: k</span>
<span id="cb47-14"><a href="#cb47-14" aria-hidden="true" tabindex="-1"></a>            })</span>
<span id="cb47-15"><a href="#cb47-15" aria-hidden="true" tabindex="-1"></a>        ) )</span>
<span id="cb47-16"><a href="#cb47-16" aria-hidden="true" tabindex="-1"></a>     </span>
<span id="cb47-17"><a href="#cb47-17" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb47-18"><a href="#cb47-18" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb47-19"><a href="#cb47-19" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"k"</span>, </span>
<span id="cb47-20"><a href="#cb47-20" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">3</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb47-21"><a href="#cb47-21" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-33-output-1.png" width="914" height="275" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Clustering is hard!</p>
</div>
<div id="exm-cluster-kmeans-digits" class="theorem example" data-chapter="6" type="💻" data-description="k-means on digits dataset">
<p><span class="theorem-title"><strong>Example 6.13</strong></span> We return to the handwriting recognition dataset. Again we keep only the samples labeled 4, 5, or 6:</p>
<div id="98762300" class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb48-2"><a href="#cb48-2" aria-hidden="true" tabindex="-1"></a>keep <span class="op">=</span> digits[<span class="st">"target"</span>].isin([<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])</span>
<span id="cb48-3"><a href="#cb48-3" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> digits[keep]</span>
<span id="cb48-4"><a href="#cb48-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb48-5"><a href="#cb48-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.drop(<span class="st">"target"</span>, axis<span class="op">=</span><span class="st">"columns"</span>)</span>
<span id="cb48-6"><a href="#cb48-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits[<span class="st">"target"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first fit 3 clusters to the feature matrix:</p>
<div id="5152be2f" class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>km <span class="op">=</span> make_pipeline( StandardScaler(), KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>, n_init<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">302</span>) )</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>km.fit(X)</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span>  km[<span class="dv">1</span>].labels_</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>digits[<span class="st">"cluster number"</span>] <span class="op">=</span> y_hat</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>digits[[<span class="st">"target"</span>, <span class="st">"cluster number"</span>]].head(<span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">target</th>
<th data-quarto-table-cell-role="th">cluster number</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>4</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>5</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>6</td>
<td>1</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">14</td>
<td>4</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">15</td>
<td>5</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">16</td>
<td>6</td>
<td>1</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">24</td>
<td>4</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">25</td>
<td>5</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">26</td>
<td>6</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The adjusted Rand index suggests that we have reproduced the classification very well:</p>
<div id="824afe96" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(y, y_hat)</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ARI: 0.9457</code></pre>
</div>
</div>
<p>However, that conclusion benefits from our prior knowledge of the classification. What if we have no such reference to check against? Let’s look over a range of <span class="math inline">\(k\)</span> choices, recording the median silhouette values for each:</p>
<div id="5dfcc2f9" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>kvals <span class="op">=</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">7</span>)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> kvals:</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, n_init<span class="op">=</span><span class="dv">3</span>, random_state<span class="op">=</span><span class="dv">19716</span>)</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>    y_hat <span class="op">=</span> km.labels_</span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-8"><a href="#cb52-8" aria-hidden="true" tabindex="-1"></a>    sil <span class="op">=</span> np.median( silhouette_samples(X, y_hat) )</span>
<span id="cb52-9"><a href="#cb52-9" aria-hidden="true" tabindex="-1"></a>    results.append(sil)</span>
<span id="cb52-10"><a href="#cb52-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-11"><a href="#cb52-11" aria-hidden="true" tabindex="-1"></a>pd.Series(results, index<span class="op">=</span>kvals)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>2    0.237310
3    0.258975
4    0.257826
5    0.249603
6    0.204824
dtype: float64</code></pre>
</div>
</div>
<p>The silhouette score is maximized at <span class="math inline">\(k=3\)</span>, which could be considered a reason to choose 3 clusters. While the score for 4 clusters is close to it, we should prefer the less complex model.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_c3zy37yk&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_woo23rll" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.13" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div></section>
</section>
<section id="hierarchical-clustering" class="level2 page-columns page-full" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="hierarchical-clustering"><span class="header-section-number">6.4</span> Hierarchical clustering</h2>
<p>The idea behind hierarchical clustering is to organize all the sample points into a tree structure called a <strong>dendrogram</strong>. At the root of the tree is the entire sample set, while each leaf of the tree is a single sample. Groups of similar samples are connected as nearby relatives in the tree, with less-similar groups located as more distant relatives.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_415gg3ez&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_qypfj8iu" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.4(a)" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>Dendrograms can be found by starting with the root and recursively splitting, or by starting at the leaves and recursively merging. We will describe the latter approach, known as <strong>agglomerative clustering</strong>.</p>
<p>The algorithm begins with <span class="math inline">\(n\)</span> singleton clusters, i.e., <span class="math inline">\(C_i=\{\bfX_i\}\)</span> for <span class="math inline">\(i=1,\dots,n\)</span>. Then, the similarity or distance between each pair of clusters is determined. The pair with the minimum distance is merged, and the process repeats. Effectively, we end up with an entire family of clusterings; we can stop at any number of clusters between <span class="math inline">\(n\)</span> and 1.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_ftn4hgyr&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_9w182z23" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 6.4(b)" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>We now need to specify how to compute the distance between clusters, a quantity called their <strong>linkage</strong>.</p>
<div id="def-cluster-linkage" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.7</strong></span> Various ways to define the distance between clusters <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span> are:</p>
<ul>
<li><dl>
<dt>single linkage</dt>
<dd>
(also called <em>minimum linkage</em>) <span id="eq-cluster-linkage-single"><span class="math display">\[
\displaystyle \min_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy } \}
\tag{6.4}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>complete linkage</dt>
<dd>
(also called <em>maximum linkage</em>) <span id="eq-cluster-linkage-complete"><span class="math display">\[
\displaystyle \max_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy} \}
\tag{6.5}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>average linkage</dt>
<dd>
<span id="eq-cluster-linkage-average"><span class="math display">\[
\displaystyle \frac{1}{|C_i|\,|C_j|} \sum_{\bfx\in C_i,\,\bfy\in C_j} \norm{ \bfx-\bfy }
\tag{6.6}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>Ward linkage</dt>
<dd>
Defined as the increase in inertia resulting from merging <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>, which is equal to <span id="eq-cluster-linkage-ward"><span class="math display">\[
\frac{ |C_i|\,|C_j| }{|C_i| + |C_j|} \norm{\bfmu_i - \bfmu_j}_2^2,
\tag{6.7}\]</span></span> where <span class="math inline">\(\bfmu_i\)</span> and <span class="math inline">\(\bfmu_j\)</span> are the centroids of <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>.
</dd>
</dl></li>
</ul>
</div>
<p>Single linkage only pays attention to the gaps between clusters, not the size or spread of them. Complete linkage, on the other hand, wants to keep every cluster packed tightly together. Average linkage is a compromise between these extremes. Agglomerative clustering with Ward linkage amounts to trying to minimize the increase of inertia with each merger. In that sense, it has a similar objective to <span class="math inline">\(k\)</span>-means, but it is usually not as successful at minimizing inertia.</p>
<p>Both <span class="math inline">\(k\)</span>-means and Ward linkage require the ability to compute changing centroid locations, which in turn requires knowing all the sample locations. By contrast, agglomerative clustering with single, complete, or average linkage needs only the pairwise distances between samples and can be found from just a distance matrix in place of the samples themselves.</p>
<div id="exm-cluster-linkage-tiny" class="theorem example" data-chapter="6" type="✍️" data-description="Linkage">
<p><span class="theorem-title"><strong>Example 6.14</strong></span> Given clusters <span class="math inline">\(C_1=\{-3,-2,-1\}\)</span> and <span class="math inline">\(C_2=\{3,4,5\}\)</span>, find the different linkages between them.</p>
<div class="proof solution">
<p><span class="proof-title"><em>Solution</em>. </span><strong>Ward.</strong> The centroids of the clusters are <span class="math inline">\(-2\)</span> and <span class="math inline">\(4\)</span>. So the linkage is</p>
<p><span class="math display">\[
\frac{3\cdot 3}{3+3} \, 6^2 = 54.
\]</span></p>
<p><strong>Single.</strong> The pairwise distances between members of <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> form a <span class="math inline">\(3\times 3\)</span> matrix:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>-3</th>
<th>-2</th>
<th>-1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>6</td>
<td>5</td>
<td>4</td>
</tr>
<tr class="even">
<td>4</td>
<td>7</td>
<td>6</td>
<td>5</td>
</tr>
<tr class="odd">
<td>5</td>
<td>8</td>
<td>7</td>
<td>6</td>
</tr>
</tbody>
</table>
<p>(Note that this is a submatrix of the full <span class="math inline">\(6\times 6\)</span> distance matrix.) The single linkage is therefore 4.</p>
<p><strong>Complete.</strong> The maximum of the matrix above is 8.</p>
<p><strong>Average.</strong> The average value of the matrix entries is <span class="math inline">\(54/9\)</span>, which is 6.</p>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_63xey2da&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_dhna2z18" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.14" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
<p>(video example is different from the text)</p>
</div></div><div id="exm-cluster-hier-toy" class="theorem example" data-chapter="6" type="💻" data-description="Agglomerative for toy example">
<p><span class="theorem-title"><strong>Example 6.15</strong></span> Let’s use 5 sample points in the plane, and agglomerate them by single linkage. The <code>pairwise_distances</code> function converts sample points into a distance matrix:</p>
<div id="8288512f" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array( [[<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>] ,[<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>], [<span class="dv">1</span>,<span class="fl">0.5</span>], [<span class="dv">0</span>,<span class="dv">2</span>], [<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>]] )</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"euclidean"</span>)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>D</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="37">
<pre><code>array([[0.        , 1.        , 3.35410197, 3.60555128, 2.23606798],
       [1.        , 0.        , 3.90512484, 4.47213595, 3.16227766],
       [3.35410197, 3.90512484, 0.        , 1.80277564, 2.06155281],
       [3.60555128, 4.47213595, 1.80277564, 0.        , 1.41421356],
       [2.23606798, 3.16227766, 2.06155281, 1.41421356, 0.        ]])</code></pre>
</div>
</div>
<p>The minimum value in the upper triangle of the distance matrix is in row 0, column 1. So our first merge results in the cluster <span class="math inline">\(C_1=\{\bfX_0,\bfX_1\}\)</span>. The next-smallest entry in the upper triangle is at position <span class="math inline">\((3,4)\)</span>, so we want to merge those samples together next, resulting in <span class="math display">\[
C_1=\{\bfX_0,\bfX_1\},\, C_2 = \{\bfX_3,\bfX_4\},\, C_3=\{\bfX_2\}.
\]</span> The next-smallest element in the matrix is at <span class="math inline">\((2,3)\)</span>, resulting in <span class="math display">\[
C_1=\{\bfX_0,\bfX_1\},\, C_2 = \{\bfX_2,\bfX_3,\bfX_4\}.
\]</span> The final merge is to combine these.</p>
<p>The entire dendrogram can be visualized with seaborn’s <code>clustermap</code>:</p>
<div id="5aa864d3" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1" aria-hidden="true" tabindex="-1"></a>sns.clustermap(X, </span>
<span id="cb56-2"><a href="#cb56-2" aria-hidden="true" tabindex="-1"></a>    col_cluster<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb56-3"><a href="#cb56-3" aria-hidden="true" tabindex="-1"></a>    dendrogram_ratio<span class="op">=</span>(<span class="fl">.75</span>,<span class="fl">.15</span>),</span>
<span id="cb56-4"><a href="#cb56-4" aria-hidden="true" tabindex="-1"></a>    figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>)</span>
<span id="cb56-5"><a href="#cb56-5" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-39-output-1.png" width="568" height="385" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The root of the tree above is at the left, with the leaves at the right. The leaves have been ordered so that the lines in the dendrogram don’t ever cross. The two colored columns show the values of the features of the samples.</p>
<p>The horizontal position in the dendrogram indicates the linkage value, which is largest at the left. Working from right to left, we first see the merger of samples 0 and 1 at the minimum linkage in the entire distance matrix. The next merger is between <span class="math inline">\(\bfX_3\)</span> and <span class="math inline">\(\bfX_4\)</span>, and so on.</p>
<p>We can choose to stop at any horizontal position (linkage value). Based on the distance matrix, if we stop at a linkage value of 2.0, then we perform only the first 2 merges and get 3 clusters. The largest gap between merges is in the merge from 2 clusters to 1, which could be used to justify <span class="math inline">\(k=2\)</span> as the best number of clusters.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_2cz7adyp&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_yjsi5575" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.15" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>The <code>AgglomerativeClustering</code> class in <code>sklearn.cluster</code> performs the clustering process to create a learner object.</p>
<div id="exm-cluster-linkage" class="theorem example" data-chapter="6" type="💻" data-description="Comparing linkages on simple datasets">
<p><span class="theorem-title"><strong>Example 6.16</strong></span> We define a function that allows us to run all three linkages for a given feature matrix:</p>
<div id="ec2a9084" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_experiment(X):</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> X.copy()</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> linkage <span class="kw">in</span> [<span class="st">"single"</span>, <span class="st">"complete"</span>, <span class="st">"ward"</span>]:</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>        agg <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span>linkage)</span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>        agg.fit(X)</span>
<span id="cb57-9"><a href="#cb57-9" aria-hidden="true" tabindex="-1"></a>        data[<span class="st">"cluster"</span>] <span class="op">=</span> agg.labels_</span>
<span id="cb57-10"><a href="#cb57-10" aria-hidden="true" tabindex="-1"></a>        data[<span class="st">"linkage"</span>] <span class="op">=</span> linkage</span>
<span id="cb57-11"><a href="#cb57-11" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> pd.concat( (results, data) )</span>
<span id="cb57-12"><a href="#cb57-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first try the <em>blobs</em> dataset:</p>
<div id="97807efb" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb58-3"><a href="#cb58-3" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb58-4"><a href="#cb58-4" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb58-5"><a href="#cb58-5" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.5</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb58-6"><a href="#cb58-6" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-41-output-1.png" width="769" height="227" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>As you can see, the simple linkage was badly confused by the two blobs that nearly run together. The others are good at reproducing the ball-like clusters.</p>
<p>Next, we try the <em>stripes</em> data:</p>
<div id="bf0e3a6a" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.5</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-42-output-1.png" width="770" height="227" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now the tendency of complete and Ward linkages to find compact, roughly spherical clusters is a bug, not a feature. They group together points across stripes rather than clusters extending lengthwise. The geometric flexibility of single linkage pays off here.</p>
<p>Finally, we try the most demanding test, <em>bullseye</em>:</p>
<div id="af6b5b32" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> bullseye_data()</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(data<span class="op">=</span>results, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.5</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-43-output-1.png" width="729" height="227" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The single linkage is the only one to cluster the rings properly. However, it’s a delicate situation, and it can be sensitive to individual samples. Here, for instance, we add just one sample to the dataset and get a major change:</p>
<div id="b029549c" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.concat( ( X, pd.DataFrame({<span class="st">"x1"</span>: [<span class="dv">0</span>], <span class="st">"x2"</span>: [<span class="fl">2.25</span>]}) ) )</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.relplot(data<span class="op">=</span>results, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.5</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>fig.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-44-output-1.png" width="729" height="227" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>That instability can make single linkage difficult to work with.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_5zsztoyl&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_53kpvohg" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.16" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div id="exm-cluster-penguins" class="theorem example" data-chapter="6" type="💻" data-description="Agglomerative for penguins dataset">
<p><span class="theorem-title"><strong>Example 6.17</strong></span> Let’s try agglomerative clustering to “discover” the species of the penguins, pretending we don’t know them in advance. First, let’s recall how many of each species we actually do have:</p>
<div id="e9b97de0" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>).dropna()</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"bill_length_mm"</span>, <span class="st">"bill_depth_mm"</span>, <span class="st">"flipper_length_mm"</span>, <span class="st">"body_mass_g"</span>]</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[features]</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> penguins[<span class="st">"species"</span>]</span>
<span id="cb62-5"><a href="#cb62-5" aria-hidden="true" tabindex="-1"></a>y.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>Adelie       146
Gentoo       119
Chinstrap     68
Name: species, dtype: int64</code></pre>
</div>
</div>
<p>Our first attempt is single linkage. Because 2-norm distances are involved, we use standardization in a pipeline with the clustering method. After fitting, the <code>labels_</code> property of the cluster object is a vector of cluster assignments.</p>
<div id="c177e124" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb64-2"><a href="#cb64-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb64-3"><a href="#cb64-3" aria-hidden="true" tabindex="-1"></a>single <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span><span class="st">"single"</span>)</span>
<span id="cb64-4"><a href="#cb64-4" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), single)</span>
<span id="cb64-5"><a href="#cb64-5" aria-hidden="true" tabindex="-1"></a>pipe.fit(X)</span>
<span id="cb64-6"><a href="#cb64-6" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> single.labels_        <span class="co"># cluster assignments</span></span>
<span id="cb64-7"><a href="#cb64-7" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"single"</span>] <span class="op">=</span> y_hat</span>
<span id="cb64-8"><a href="#cb64-8" aria-hidden="true" tabindex="-1"></a>penguins.loc[::<span class="dv">26</span>, [<span class="st">"species"</span>, <span class="st">"single"</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="45">
<div>


<table class="dataframe table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">species</th>
<th data-quarto-table-cell-role="th">single</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">31</td>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">58</td>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">84</td>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">110</td>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">136</td>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">162</td>
<td>Chinstrap</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">188</td>
<td>Chinstrap</td>
<td>0</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">214</td>
<td>Chinstrap</td>
<td>0</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">240</td>
<td>Gentoo</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">267</td>
<td>Gentoo</td>
<td>2</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">294</td>
<td>Gentoo</td>
<td>2</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">320</td>
<td>Gentoo</td>
<td>2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Perhaps <em>Gentoo</em> is associated with cluster number 2, but the situation with the other species is less clear. Here are the value counts:</p>
<div id="a66d6bb4" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"single linkage results:"</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"single"</span>].value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>single linkage results:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="46">
<pre><code>0    213
2    119
1      1
Name: single, dtype: int64</code></pre>
</div>
</div>
<p>As we saw with the <em>bullseye</em> dataset in <a href="#exm-cluster-linkage" class="quarto-xref">Example&nbsp;<span>6.16</span></a>, single linkage is susceptible to declaring one isolated point to be a cluster, while grouping together other points we would like to separate. Here is the ARI for this clustering, compared to the true classification:</p>
<div id="571510a3" class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(y, y_hat)</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"single linkage ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>single linkage ARI: 0.6506</code></pre>
</div>
</div>
<p>Now let’s try Ward linkage (which is the default):</p>
<div id="6fa006e1" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a>ward <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span><span class="st">"ward"</span>)</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), ward)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>pipe.fit(X)</span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> ward.labels_</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"ward"</span>] <span class="op">=</span> y_hat</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ward linkage results:"</span>)</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(penguins[<span class="st">"ward"</span>].value_counts())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ward linkage results:
1    157
0    119
2     57
Name: ward, dtype: int64</code></pre>
</div>
</div>
<p>This result looks more promising. The ARI confirms that hunch:</p>
<div id="b7c57109" class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(y, penguins[<span class="st">"ward"</span>])</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ward linkage ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ward linkage ARI: 0.9132</code></pre>
</div>
</div>
<p>If we guess at the likely correspondence between the cluster numbers and the different species, then we can find the confusion matrix:</p>
<div id="0d7686d4" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert cluster numbers into labels:</span></span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>replacements <span class="op">=</span> {<span class="dv">1</span>: <span class="st">"Adelie"</span>, <span class="dv">0</span>: <span class="st">"Gentoo"</span>, <span class="dv">2</span>: <span class="st">"Chinstrap"</span>}</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> penguins[<span class="st">"ward"</span>].replace(replacements) </span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> confusion_matrix(y, y_hat)</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay(C, display_labels<span class="op">=</span>y.unique()).plot()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="clustering_files/figure-html/cell-51-output-1.png" width="560" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_7zcbk4s8&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_6e29kvah" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 6.17" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>While the performance of clustering as a classifier in <a href="#exm-cluster-penguins" class="quarto-xref">Example&nbsp;<span>6.17</span></a> is inferior to supervised methods we used for that purpose, the clustering process reveals that the Gentoo penguins are the most dissimilar from the other species. By inspecting the mislabeled cases in the confusion matrix, further insight might be gained about what particular traits can make the other species difficult to distinguish. In clustering, the goal is often more about insight than prediction.</p>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-cluster-angular" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.1</strong></span> (6.1) Prove that the angular distance between any nonzero vector and itself is zero.</p>
</div>
<div id="exr-cluster-cosine" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.2</strong></span> (6.1) Find an example for which the cosine distance does not satisfy the triangle inequality. That is, find three vectors <span class="math inline">\(\bfx\)</span>, <span class="math inline">\(\bfy\)</span>, and <span class="math inline">\(\bfz\)</span> such that <span class="math display">\[
\dist(\bfx,\bfy) &gt; \dist(\bfx,\bfz) + \dist(\bfz,\bfy).
\]</span> (Hint: it’s enough to play around with some simple vectors in two dimensions. Use vectors for which the angle between is a multiple of <span class="math inline">\(\pi/6\)</span> or <span class="math inline">\(\pi/4\)</span>.)</p>
</div>
<div id="exr-cluster-distmatrix" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.3</strong></span> (6.1) Here is a feature matrix. <span class="math display">\[
\bfX = \begin{bmatrix}
-1 &amp; -1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 2 &amp; 0 &amp; -2 \\ 1 &amp; 3 &amp; 1
\end{bmatrix}.
\]</span></p>
<p><strong>(a)</strong> Find the associated distance matrix using the 1-norm.</p>
<p><strong>(b)</strong> Find the associated distance matrix using the infinity-norm.</p>
</div>
<div id="exr-cluster-performance" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.4</strong></span> (6.2) Here are two clusterings on 6 samples: <span class="math display">\[
\begin{split}
\text{A}:\: &amp; C_1 = \{\bfX_1,\bfX_4\}, \; C_2=\{\bfX_2,\bfX_3,\bfX_5,\bfX_6 \}  \\
\text{B}:\: &amp; C_1 = \{\bfX_1,\bfX_2\}, \; C_2=\{\bfX_3,\bfX_4\}, \; C_3 = \{\bfX_5,\bfX_6 \}
\end{split}
\]</span></p>
<p>Here is a distance matrix for these samples: <span class="math display">\[
\begin{bmatrix}
0 &amp; 2 &amp; 2 &amp; 1 &amp; 4 &amp; 1 \\ 2 &amp; 0 &amp; 3 &amp; 5&amp; 1  &amp; 2 \\ 2 &amp; 3 &amp; 0 &amp; 6 &amp; 2 &amp; 1 \\ 1 &amp; 5 &amp; 6 &amp; 0 &amp; 8 &amp; 4 \\ 4 &amp; 1 &amp; 2 &amp; 8 &amp; 0 &amp; 3 \\ 1 &amp; 2 &amp; 1 &amp; 4 &amp; 3 &amp; 0
\end{bmatrix}
\]</span></p>
<p><strong>(a)</strong> Find the Rand index between these clusterings.</p>
<p><strong>(b)</strong> Find the silhouette values for all samples in clustering A.</p>
</div>
<div id="exr-cluster-inertia" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.5</strong></span> (6.3) Let <span class="math inline">\(z\)</span> be a positive number, and consider the 12 planar samples <span class="math display">\[
\begin{split}
&amp; [z,-3],\, [z,-2],\, [z,-1],\,[z,1],\,[z,2],\,[z,3], \\
&amp; [-z,-3],\, [-z,-2],\, [-z,-1],\,[-z,1],\,[-z,2],\,[-z,3].
\end{split}
\]</span> See <a href="#fig-exercise6_5" class="quarto-xref">Figure&nbsp;<span>6.4</span></a>.</p>
<div id="fig-exercise6_5" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-exercise6_5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="_media/exer6_5.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-exercise6_5-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6.4: Samples for <a href="#exr-cluster-inertia" class="quarto-xref">Exercise&nbsp;<span>6.5</span></a>.
</figcaption>
</figure>
</div>
<p><strong>(a)</strong> Using the 2-norm, calculate the inertia, which may depend on <span class="math inline">\(z\)</span>, of the clustering <span class="math display">\[
C_1 = \{ [z,j]: j=-3,-2,-1,1,2,3\}, \qquad C_2 = \{ [-z,j]: j=-3,-2,-1,1,2,3\}.
\]</span> (This divides the sample points by the <span class="math inline">\(y\)</span>-axis.)</p>
<p><strong>(b)</strong> Using the 2-norm, calculate the inertia, which may depend on <span class="math inline">\(z\)</span>, of the clustering <span class="math display">\[
C_1 = \{ [-z,j] \text{ and } [z,j]: j=-3,-2,-1\}, \qquad C_2 = \{ [-z,j] \text{ and } [z,j]: j=1,2,3\}.
\]</span> (This divides the sample points by the <span class="math inline">\(x\)</span>-axis.)</p>
<p><strong>(c)</strong> For which values of <span class="math inline">\(z\)</span>, if any, does clustering from part (a) have less inertia than the clustering from part (b)?</p>
</div>
<div id="exr-cluster-linkage" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.6</strong></span> (6.4) Here is a distance matrix for samples <span class="math inline">\(\bfX_1,\ldots,\bfX_5\)</span>:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccccc}
0 &amp; 2 &amp; 3.5 &amp; 5 &amp; 6 \\
2 &amp; 0 &amp; 2.5 &amp; 3 &amp; 4 \\
3.5 &amp; 2.5 &amp; 0 &amp; 1 &amp; 1.5 \\
5 &amp; 3 &amp; 1 &amp; 0 &amp; 5.5 \\
6 &amp; 4 &amp; 1.5 &amp; 5.5 &amp; 0 \\
\end{array}
\right]
\]</span></p>
<p><strong>(a)</strong> Compute the average linkage between the clusters <span class="math inline">\(C_1=\{\bfX_1,\bfX_3\}\)</span> and <span class="math inline">\(C_2=\{\bfX_2,\bfX_4,\bfX_5\}\)</span>.</p>
<p><strong>(b)</strong> Compute the single linkage between the clusters <span class="math inline">\(C_1=\{\bfX_1,\bfX_3,\bfX_4\}\)</span> and <span class="math inline">\(C_2=\{\bfX_2,\bfX_5\}\)</span>.</p>
</div>
<div id="exr-cluster-agglom" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.7</strong></span> (6.4) Find the agglomerative clustering, using complete linkage, for the samples given in <a href="#exr-cluster-linkage" class="quarto-xref">Exercise&nbsp;<span>6.6</span></a>. (This means finding four individual merge steps based on the distance matrix.)</p>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./regression.html" class="pagination-link  aria-label=" &lt;span="">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./networks.html" class="pagination-link" aria-label="<span class='chapter-number'>7</span>&nbsp; <span class='chapter-title'>Networks</span>">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Data Science 1
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Toby Driscoll
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>




</body></html>