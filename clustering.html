<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Science 1 - 6&nbsp; Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./networks.html" rel="next">
<link href="./regression.html" rel="prev">
<link href="./_media/logo_small.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script> 
MathJax = {
  chtml: {
    scale: 0.92,
  }
}
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="quarto-sidebar-header"><div class="sidebar-header-item">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="_media/logo_small.png" height="120" class="figure-img"></p>
</figure>
</div>
</div></div>
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science 1</a> 
        <div class="sidebar-tools-main">
    <a href="./Data-Science-1.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">Resources</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./starting.html" class="sidebar-item-text sidebar-link">Getting started</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Representation of data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./networks.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>Copyright 2023 by Toby Driscoll</p>
</div></div></nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#similarity" id="toc-similarity" class="nav-link active" data-scroll-target="#similarity"><span class="toc-section-number">6.1</span>  Similarity</a>
  <ul class="collapse">
  <li><a href="#distance-matrix" id="toc-distance-matrix" class="nav-link" data-scroll-target="#distance-matrix"><span class="toc-section-number">6.1.1</span>  Distance matrix</a></li>
  <li><a href="#angular-distance" id="toc-angular-distance" class="nav-link" data-scroll-target="#angular-distance"><span class="toc-section-number">6.1.2</span>  Angular distance</a></li>
  <li><a href="#distance-in-high-dimensions" id="toc-distance-in-high-dimensions" class="nav-link" data-scroll-target="#distance-in-high-dimensions"><span class="toc-section-number">6.1.3</span>  Distance in high dimensions</a></li>
  </ul></li>
  <li><a href="#performance-measures" id="toc-performance-measures" class="nav-link" data-scroll-target="#performance-measures"><span class="toc-section-number">6.2</span>  Performance measures</a>
  <ul class="collapse">
  <li><a href="#rand-index-and-ari" id="toc-rand-index-and-ari" class="nav-link" data-scroll-target="#rand-index-and-ari"><span class="toc-section-number">6.2.1</span>  Rand index and ARI</a></li>
  <li><a href="#silhouettes" id="toc-silhouettes" class="nav-link" data-scroll-target="#silhouettes"><span class="toc-section-number">6.2.2</span>  Silhouettes</a></li>
  </ul></li>
  <li><a href="#k-means" id="toc-k-means" class="nav-link" data-scroll-target="#k-means"><span class="toc-section-number">6.3</span>  k-means</a>
  <ul class="collapse">
  <li><a href="#lloyds-algorithm" id="toc-lloyds-algorithm" class="nav-link" data-scroll-target="#lloyds-algorithm"><span class="toc-section-number">6.3.1</span>  Lloyd’s algorithm</a></li>
  <li><a href="#practical-issues" id="toc-practical-issues" class="nav-link" data-scroll-target="#practical-issues"><span class="toc-section-number">6.3.2</span>  Practical issues</a></li>
  </ul></li>
  <li><a href="#hierarchical-clustering" id="toc-hierarchical-clustering" class="nav-link" data-scroll-target="#hierarchical-clustering"><span class="toc-section-number">6.4</span>  Hierarchical clustering</a></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="hidden">
<p><span class="math display">\[
    \newcommand{\float}{\mathbb{F}}
    \newcommand{\real}{\mathbb{R}}
    \newcommand{\complex}{\mathbb{C}}
    \newcommand{\nat}{\mathbb{N}}
    \newcommand{\integer}{\mathbb{Z}}
    \newcommand{\bfa}{\mathbf{a}}
    \newcommand{\bfe}{\mathbf{e}}
    \newcommand{\bfh}{\mathbf{h}}
    \newcommand{\bfp}{\mathbf{p}}
    \newcommand{\bfq}{\mathbf{q}}
    \newcommand{\bfu}{\mathbf{u}}
    \newcommand{\bfv}{\mathbf{v}}
    \newcommand{\bfw}{\mathbf{w}}
    \newcommand{\bfx}{\mathbf{x}}
    \newcommand{\bfy}{\mathbf{y}}
    \newcommand{\bfz}{\mathbf{z}}
    \newcommand{\bfA}{\mathbf{A}}
    \newcommand{\bfW}{\mathbf{W}}
    \newcommand{\bfX}{\mathbf{X}}
    \newcommand{\bfzero}{\boldsymbol{0}}
    \newcommand{\bfmu}{\boldsymbol{\mu}}
    \newcommand{\TP}{\text{TP}}
    \newcommand{\TN}{\text{TN}}
    \newcommand{\FP}{\text{FP}}
    \newcommand{\FN}{\text{FN}}
    \newcommand{\rmn}[2]{\mathbb{R}^{#1 \times #2}}
    \newcommand{\dd}[2]{\frac{d #1}{d #2}}
    \newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
    \newcommand{\norm}[1]{\left\lVert \mathstrut #1 \right\rVert}
    \newcommand{\abs}[1]{\left\lvert \mathstrut #1 \right\rvert}
    \newcommand{\twonorm}[1]{\norm{#1}_2}
    \newcommand{\onenorm}[1]{\norm{#1}_1}
    \newcommand{\infnorm}[1]{\norm{#1}_\infty}
    \newcommand{\innerprod}[2]{\langle #1,#2 \rangle}
    \newcommand{\pr}[1]{^{(#1)}}
    \newcommand{\diag}{\operatorname{diag}}
    \newcommand{\sign}{\operatorname{sign}}
    \newcommand{\dist}{\operatorname{dist}}
    \newcommand{\simil}{\operatorname{sim}}
    \newcommand{\ee}{\times 10^}
    \newcommand{\floor}[1]{\lfloor#1\rfloor}
    \newcommand{\argmin}{\operatorname{argmin}}
    \newcommand{\E}[1]{\operatorname{\mathbb{E}}\left[\mathstrut #1\right]}
    \newcommand{\Cov}{\operatorname{Cov}}
    \newcommand{\logit}{\operatorname{logit}}
\]</span></p>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> default_rng</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> shuffle</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, f1_score, balanced_accuracy_score</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, StratifiedKFold</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_validate, validation_curve</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>In supervised learning, the data samples are supplied with labels, and the goal of the learner is to generalize the examples to new values. In unsupervised learning, there are no labels. Instead, the goal is to discover structure that is intrinsic to the feature matrix. Common problem types in unsupervised learning are</p>
<dl>
<dt>Clustering</dt>
<dd>
Determine whether the samples roughly divide into a small number of classes.
</dd>
<dt>Dimension reduction</dt>
<dd>
Find a reduced set of features, or create a small set of new features, that describe the data well.
</dd>
<dt>Outlier detection</dt>
<dd>
Find anomalous values in the data set and remove them or impute replacements.
</dd>
</dl>
<p>In this chapter we will look at clustering. The goal is to assign each feature vector a number from 1 to <span class="math inline">\(k\)</span>, where <span class="math inline">\(k\)</span> is much smaller than the number of samples. More formally:</p>
<div id="def-cluster-clustering" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.1 </strong></span>Given an <span class="math inline">\(n\times d\)</span> feature matrix with rows <span class="math inline">\(\bfX_1,\ldots,\bfX_n\)</span>, a <strong>clustering</strong> is a labelling function <span class="math inline">\(c\)</span> defined on the feature vectors such that <span class="math display">\[
c(\bfX_i) \in \{ 1,2,\ldots, k \} \text{ for all } i=1,\ldots,n,
\]</span> where <span class="math inline">\(k\)</span> is a positive integer. The <strong>cluster</strong> <span class="math inline">\(C_j\)</span> is the collection of all <span class="math inline">\(\bfX_i\)</span> such that <span class="math inline">\(c(\bfX_i)=j\)</span>.</p>
</div>
<p>The clusters <span class="math inline">\(C_1,\ldots,C_k\)</span> divide the samples into <span class="math inline">\(k\)</span> dijoint subsets. Depending on the algorithm, the number of clusters <span class="math inline">\(k\)</span> may be imposed (i. e., as a hyperparameter) or determined automatically.</p>
<p>Intuitively, we want similar examples to be clustered together (that is, to receive the same label). The motivation for clustering is often to find a way to classify the samples by intrinsic properties when no such classification is known in advance. It is <em>not</em> necessary that the clustering function <span class="math inline">\(c\)</span> be defined for vectors <span class="math inline">\(\bfx\)</span> other than the feature vectors, although many clustering algorithms can be used to do that as well.</p>
<p>In order to get a feeling for the algorithms, we will apply them to three illustrative datasets:</p>
<ul>
<li><dl>
<dt>blobs</dt>
<dd>
This dataset has one distinct blob, plus two that kind of overlap a bit:
</dd>
</dl></li>
</ul>
<div class="cell" data-execution_count="3">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_blobs</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> blobs_data():</span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>    X, y <span class="op">=</span> make_blobs(</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>        n_samples<span class="op">=</span>[<span class="dv">60</span>, <span class="dv">50</span>, <span class="dv">40</span>],</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>        centers<span class="op">=</span>[[<span class="op">-</span><span class="dv">2</span>,<span class="dv">3</span>], [<span class="dv">3</span>,<span class="fl">1.5</span>], [<span class="dv">1</span>,<span class="op">-</span><span class="dv">3</span>]],</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>        cluster_std<span class="op">=</span>[<span class="fl">0.5</span>, <span class="fl">0.9</span>, <span class="fl">1.2</span>],</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>        random_state <span class="op">=</span> <span class="dv">19716</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">"x1"</span>: X[:,<span class="dv">0</span>], <span class="st">"x2"</span>: X[:,<span class="dv">1</span>]}), pd.Series(y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>y, palette<span class="op">=</span><span class="st">"Dark2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-4-output-1.png" width="526" height="468"></p>
</div>
</div>
<ul>
<li><dl>
<dt>stripes</dt>
<dd>
In this dataset, there is clear separation along one axis and a continuous blob along the other:
</dd>
</dl></li>
</ul>
<div class="cell" data-execution_count="5">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stripes_data():</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> default_rng(<span class="dv">9</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    x1,x2,cls <span class="op">=</span> [],[],[]</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">3</span>):</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>        x1.extend( rng.uniform(<span class="op">-</span><span class="dv">10</span>, <span class="dv">10</span>, size<span class="op">=</span><span class="dv">200</span>) )</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>        x2.extend( <span class="fl">2.5</span><span class="op">*</span>i<span class="op">+</span>rng.uniform(<span class="dv">0</span>, <span class="dv">1</span>, size<span class="op">=</span><span class="dv">200</span>) )</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        cls.extend( [i]<span class="op">*</span><span class="dv">200</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame({<span class="st">"x1"</span>: x1, <span class="st">"x2"</span>: x2}), pd.Series(cls)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>y, palette<span class="op">=</span><span class="st">"Dark2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-6-output-1.png" width="528" height="468"></p>
</div>
</div>
<ul>
<li><dl>
<dt>bullseye</dt>
<dd>
This is the most challenging dataset, because the clusters can’t be separated by anything like straight lines:
</dd>
</dl></li>
</ul>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> bullseye_data():</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    rng <span class="op">=</span> default_rng(<span class="dv">6</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    inner <span class="op">=</span> <span class="fl">0.8</span><span class="op">*</span>rng.normal(size<span class="op">=</span>(<span class="dv">100</span>, <span class="dv">2</span>))</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    theta <span class="op">=</span> rng.uniform(<span class="dv">0</span>, <span class="dv">2</span><span class="op">*</span>np.pi, size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> rng.uniform(<span class="dv">3</span>, <span class="dv">4</span>, size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    middle <span class="op">=</span> np.vstack((r<span class="op">*</span>np.cos(theta), r<span class="op">*</span>np.sin(theta))).T</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>    r <span class="op">=</span> rng.uniform(<span class="dv">5</span>,<span class="dv">6</span>,size<span class="op">=</span><span class="dv">200</span>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    outer <span class="op">=</span> np.vstack((r<span class="op">*</span>np.cos(theta), r<span class="op">*</span>np.sin(theta))).T</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    cls <span class="op">=</span> np.hstack( ([<span class="dv">1</span>]<span class="op">*</span><span class="dv">100</span>, [<span class="dv">2</span>]<span class="op">*</span><span class="dv">200</span>, [<span class="dv">3</span>]<span class="op">*</span><span class="dv">200</span>))</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> pd.DataFrame( np.vstack((inner,middle,outer)), columns<span class="op">=</span>[<span class="st">"x1"</span>, <span class="st">"x2"</span>] )</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X, pd.Series(cls)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> bullseye_data()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>y, palette<span class="op">=</span><span class="st">"Dark2"</span>)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>p.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-8-output-1.png" width="524" height="468"></p>
</div>
</div>
<section id="similarity" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="similarity"><span class="header-section-number">6.1</span> Similarity</h2>
<p>Ideally, samples within a cluster are more similar to each other than they are to samples in other clusters. The first decision we have to make is how to define <em>similarity</em> between an arbitrary pair.</p>
<p>When a distance metric is available, we intuitively expect similarity to be inversely related to distance. There are various ways to quantify the relationship, but we will not use them. Instead, we will take “maximize similarity” to be equivalent to “minimize distance.”</p>
<!-- 
Here is one of the most common ways to realize that ideal.

::::{#def-cluster-gauss}
Given a distance function between pairs of vectors as $\dist(\bfx,\bfy)$, we define the **Gaussian similarity** between two vectors as
$$
\simil(\bfx,\bfy) = \exp \left[ - \frac{\dist(\bfx,\bfy)^2}{2\sigma^2}  \right].
$$ {#eq-similar-similarity}
::::

::: {.callout-note}
Up to a constant scaling factor, the function in the Gaussian similarity definition is the same as the PDF of the normal distribution with mean 0 and standard deviation $\sigma$. 
:::

Thus, for any vector $\bfx$,
$$
\simil(\bfx,\bfx) = 1.
$$
This is the largest possible value. The similarity tends to zero as distance increases. The scaling parameter $\sigma$ controls the rate of decrease; for instance, when the distance is $\sigma$, the similarity is $e^{-1/2}\approx 0.6$. 

::: {.callout-tip}
When the distance function in Gaussian similarity is based on a vector norm, it's advisable to use standardization of the features, either by using z-scores or some other normalization.
::: -->
<section id="distance-matrix" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="distance-matrix"><span class="header-section-number">6.1.1</span> Distance matrix</h3>
<div id="def-cluster-distmat" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.2 </strong></span>Given the feature vectors <span class="math inline">\(\bfX_1,\ldots,\bfX_n\)</span>, the pairwise distances between them are collected in the <span class="math inline">\(n\times n\)</span> <strong>distance matrix</strong></p>
<p><span class="math display">\[
D_{ij} = \text{dist}(\bfX_i,\bfX_j).
\]</span></p>
</div>
<p>Note that <span class="math inline">\(D_{ii}=0\)</span> and <span class="math inline">\(D_{ji}=D_{ij}\)</span>.</p>
<div class="callout callout-style-default callout-tip callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Many clustering algorithms allow supplying <span class="math inline">\(\mathbf{D}\)</span> in lieu of the feature vectors. When you consider that in many problems, <span class="math inline">\(n\)</span> is much larger than <span class="math inline">\(d\)</span>, it can be faster and easier to work with.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>A practical advantage of working with similarity rather than distance is that small values of similarity can be rounded down to zero. Such rounding has negligible effect on the results, but it can create big gains in execution time and memory usage.</p>
</div>
</div>
<p>The scikit-learn function <code>pairwise_distances</code> computes a distance matrix efficiently.</p>
<div id="exm-cluster-distmat" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.1 </strong></span>The distance matrix of our <em>bullseye</em> dataset has some interesting structure:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> pairwise_distances</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> bullseye_data()</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>D2 <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"euclidean"</span>)   <span class="co"># use 2-norm metric</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.heatmap(D2)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-9-output-1.png" width="494" height="430"></p>
</div>
</div>
<p>Because we set up three geometrically distinct groups of points, the distances of pairs within and between groups are fairly homogeneous. The lower-right corner, for example, shows that points in the outermost ring tend to be separated by the greatest distance.</p>
<p>In the 1-norm, the <em>stripes</em> dataset is also interesting:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>D1 <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"manhattan"</span>)   <span class="co"># use 1-norm metric</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>ax <span class="op">=</span> sns.heatmap(D1)</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>ax.set_aspect(<span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-10-output-1.png" width="494" height="430"></p>
</div>
</div>
<p>Points in different stripes are always separated by at least the inter-stripe distance, while points within the same stripe have a range of possible distances.</p>
</div>
</section>
<section id="angular-distance" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="angular-distance"><span class="header-section-number">6.1.2</span> Angular distance</h3>
<p>An alternative to vector norms for distances is <strong>angular distance</strong>. In three dimensions, imagine drawing a ray from the origin to each of the two given vectors. Those rays lie in a common plane, and we can let <span class="math inline">\(\theta\)</span> be the (smallest) angle between them. The general formula in any number of dimensions is <span id="eq-cluster-angle"><span class="math display">\[
\cos(\theta) = \frac{\mathbf{u}^T\mathbf{v}}{\twonorm{\mathbf{u}} \, \twonorm{\mathbf{v}}}.
\tag{6.1}\]</span></span></p>
<p>We then let <span class="math inline">\(\dist(\bfx,\bfy)=\theta \in [0,\pi]\)</span>, using the standard branch of arccos.</p>
<p>However, arccos is a relatively expensive computational operation, and sometimes we use the pseudodistance <span class="math display">\[
d(\bfx,\bfy) = \tfrac{1}{2}\left[ 1-\cos(\theta) \right]
\]</span> in place of <span class="math inline">\(\theta\)</span>. This is not a true distance function, though, as <span class="math inline">\(d\)</span> fails to satisfy the triangle inequality in all cases (see <a href="#exr-cluster-cosine">Exercise&nbsp;<span>6.2</span></a>).</p>
<p>Angular distance and cosine pseudodistance are useful when we want to ignore the magnitudes of vectors and consider only their directions.</p>
<div id="exm-cluster-angular" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.2 </strong></span>We might represent a text document by a vector of the number of occurrences of certain keywords. In sentiment analysis, these would be terms such as <em>happy</em>, <em>excited</em>, <em>sad</em>, <em>angry</em>, <em>confused</em>, and so on. For sake of argument, suppose three documents have the feature matrix <span class="math display">\[
\bfX =
\begin{bmatrix}
6 &amp; 1 &amp; 10 &amp; 2 &amp; 5 \\  14 &amp; 0 &amp; 23 &amp; 3 &amp; 7 \\ 2 &amp; 3 &amp; 1 &amp; 5 &amp; 0
\end{bmatrix}.
\]</span> Using the 2-norm, we get the distances:</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array( [[<span class="dv">6</span>,<span class="dv">1</span>,<span class="dv">10</span>,<span class="dv">2</span>,<span class="dv">5</span>], [<span class="dv">14</span>,<span class="dv">0</span>,<span class="dv">23</span>,<span class="dv">3</span>,<span class="dv">7</span>], [<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">0</span>]] )</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>pairwise_distances( X, metric<span class="op">=</span><span class="st">"euclidean"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="10">
<pre><code>array([[ 0.        , 15.45962483, 11.61895004],
       [15.45962483,  0.        , 26.26785107],
       [11.61895004, 26.26785107,  0.        ]])</code></pre>
</div>
</div>
<p>Accoding to this metric, the most similar documents are the first and the last. However, cosine distance tells a different story:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>pairwise_distances( X, metric<span class="op">=</span><span class="st">"cosine"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>array([[0.        , 0.01532383, 0.56500757],
       [0.01532383, 0.        , 0.62231412],
       [0.56500757, 0.62231412, 0.        ]])</code></pre>
</div>
</div>
<p>Now the first two documents look most similar. Looking at the vectors, we see that these two have the most similar relative frequencies, which is related to what we want to measure. The apparent dissimilarity in the 2-norm arises solely because the total word counts are very different.</p>
</div>
<!-- Categorical variables can be included in distance metrics. An ordinal variable is easily converted to equally spaced numerical values, which then may get a standard treatment. Nominal features are often compared using **Hamming distance**, which is just the total number of features that have different values in the two vectors. -->
<!-- 
### Probability distributions

::::{#def-cluster-probdist}
A **discrete probability distribution** is a vector $\bfx$ whose components are nonnegative and satisfying $\onenorm{\bfx}=1$. 
::::

Such a vector can be interpreted as frequencies or probabilities of observing different classes. For example, when computing Gini impurity, we define $p_k$ as the observed frequency of label $k$ appearing in a given set.

While we can treat probability vectors like any other vector and apply vector norms, doing so fails to take advantage of their special properties. There are alternatives suggested from information theory that are usually preferred.

::::{#def-cluster-probdist}
If $\bfx$ and $\bfy$ are probability vectors of length $d$, then the **Kullback–Leibler (KL) divergence** is defined as 
$$
\operatorname{KL}(\bfx,\bfy) = \sum_{i=1}^d x_i \log\left( \frac{x_i}{y_i} \right).
$$
Whenever $0\cdot \log(0)$ is encountered in the sum, it equals zero, in accordance with its limiting value from calculus. If, for some $i$, $x_i\neq 0$ and $y_i=0$, then $KL(\bfx,\bfy) = \infty$. 

We define the **information radius** as 
$$
\operatorname{IR}(\bfx,\bfy) = \frac{1}{2} \bigl[ \operatorname{KL}(\bfx,\bfz) + \operatorname{KL}(\bfy,\bfz) \bigr],
$$
where $\bfz=(\bfx+\bfy)/2$.
::::

Of these, only IR is symmetric in the sense that $\operatorname{IR}(\bfx,\bfy)=\operatorname{IR}(\bfy,\bfx)$. The square root of IR is a distance metric. Typically one uses a base-2 logarithm, in which case IR ranges between 0 (for identical distributions) and 1. 

::::{#exm-cluster-IR}
Let $\bfx=\frac{1}{4}[1,3]$ and $\bfy=\frac{1}{4}[3,1]$. Using base-2 logs, find the IR between them.

:::{.solution}
We compute 
$$
\bfz=\tfrac{1}{2}(\bfx+\bfy) = [\tfrac{1}{2},\tfrac{1}{2}]. 
$$
Thus,
$$
\begin{split}
    \operatorname{KL}(\bfx,\bfz)  &= \tfrac{1}{4} \cdot \log \left( \frac{1/4}{1/2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3/4}{1/2} \right) \\ &= \tfrac{1}{4} \cdot \log \left( \frac{1}{2} \right) + \tfrac{3}{4} \cdot \log \left( \frac{3}{2} \right) = -\tfrac{1}{4} + \tfrac{3}{4} (\log(3)-1),\\
    \operatorname{KL}(\bfy,\bfz)  &= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4},\\
    \operatorname{IR}(\bfx,\bfy)  &= \tfrac{3}{4} (\log(3)-1) -\tfrac{1}{4} \approx 0.1887.
\end{split}
$$
:::
:::: -->
</section>
<section id="distance-in-high-dimensions" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="distance-in-high-dimensions"><span class="header-section-number">6.1.3</span> Distance in high dimensions</h3>
<p>High-dimensional space <a href="https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf">does not conform to some intuitions</a> formed by our experiences in 2D and 3D.</p>
<p>For example, consider the unit hyperball <span class="math inline">\(\twonorm{\bfx}\le 1\)</span> in <span class="math inline">\(d\)</span> dimensions. We’ll take it as given that scaling a <span class="math inline">\(d\)</span>-dimensional object by a number <span class="math inline">\(r\)</span> will scale the volume by <span class="math inline">\(r^d\)</span>. Then for any <span class="math inline">\(r&lt;1\)</span>, the fraction of the unit hyperball’s volume lying <em>outside</em> the smaller hyperball of fixed radius <span class="math inline">\(r\)</span> is <span class="math inline">\(1-r^d\)</span>, which approaches <span class="math inline">\(1\)</span> as <span class="math inline">\(d\to \infty\)</span>. That is, <em>if we choose points randomly within a hyperball, almost all of them will be near the outer boundary</em>.</p>
<p>The volume of the unit hyperball also vanishes as <span class="math inline">\(d\to \infty\)</span>. This is because the inequality</p>
<p><span class="math display">\[
x_1^2 + x_2^2 + \cdots + x_d^2 \le 1,
\]</span></p>
<p>where each <span class="math inline">\(x_i\)</span> is chosen randomly in <span class="math inline">\([-1,1]\)</span>, becomes ever harder to satisfy as the number of terms in the sum grows, and the relative occurrence of such points is increasingly rare.</p>
<p>There are other, similar mathematical results demonstrating the weirdness of distances in high-dimensional space. These go under the colorful name <em>curse of dimensionality</em>, and the advice given in response to them is sometimes stated flatly as, “Don’t use distance metrics in high-dimensional space.”</p>
<p>But that form of the advice may be overstated. The curse is essentially about <em>randomly</em> chosen points, and it is correct that dimensions of noisy or irrelevant features will make most learning algorithms less effective. But when features carry useful information, adding them usually makes matters better, not worse.</p>
</section>
</section>
<section id="performance-measures" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="performance-measures"><span class="header-section-number">6.2</span> Performance measures</h2>
<p>Before we start generating clusterings, we will discuss how to evaluate them. A clustering is essentially a partitioning of the samples into disjoint subsets. We will use some nonstandard terminology that makes the definitions a bit easier to state and read.</p>
<div id="def-cluster-buddies" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.3 </strong></span>We say that two sample points in a clustering are <strong>buddies</strong> if they are in the same cluster, and <strong>strangers</strong> otherwise.</p>
</div>
<section id="rand-index-and-ari" class="level3" data-number="6.2.1">
<h3 data-number="6.2.1" class="anchored" data-anchor-id="rand-index-and-ari"><span class="header-section-number">6.2.1</span> Rand index and ARI</h3>
<p>A clustering can be interpreted as a classification, and vice versa. If a reference classification is available, then we can compare any clustering result to it. This allows us to use classification datasets as proving grounds for clustering.</p>
<p>Let <span class="math inline">\(b\)</span> be the number of pairs that are buddies in both clusterings, and let <span class="math inline">\(s\)</span> be the number of pairs that are strangers in both clusterings. We define the <strong>Rand index</strong> by</p>
<p><span class="math display">\[
\text{RI} = \frac{b+s}{\binom{n}{2}}.
\]</span></p>
<p>The reason for the denominator is that there are <span class="math inline">\(\binom{n}{2}\)</span> distinct pairs of <span class="math inline">\(n\)</span> sample points, so we know that <span class="math inline">\(0 \le \text{RI} \le 1\)</span>.</p>
<p>One way to interpret the Rand index is through binary classification. If we define a positive result on a pair of samples to mean “in the same cluster” and a negative result to mean “in different clusters”, then the Rand index is the accuracy of that classifier over all pairs of samples, taking the reference clustering as providing ground truth.</p>
<div id="exm-cluster-rand" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.3 </strong></span>Suppose that samples <span class="math inline">\(\bfX_1,\bfX_2,\bfX_4\)</span> are classified as blue, and <span class="math inline">\(\bfX_3,\bfX_5\)</span> are classified as red. Compute the Rand index relative to the reference classification for the clustering <span class="math inline">\(A=\{\bfX_1,\bfX_2\}\)</span> and <span class="math inline">\(B=\{\bfX_3,\bfX_4,\bfX_5\}\)</span>. (Note that the actual feature vectors themselves are not important in this setting.)</p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>Here is a table showing which pairs of samples are buddies in both clusterings (indicated as TP), strangers in both (TN), or neither (F).</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th><span class="math inline">\(\bfX_1\)</span></th>
<th><span class="math inline">\(\bfX_2\)</span></th>
<th><span class="math inline">\(\bfX_3\)</span></th>
<th><span class="math inline">\(\bfX_4\)</span></th>
<th><span class="math inline">\(\bfX_5\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\bfX_1\)</span></td>
<td></td>
<td>TP</td>
<td>TN</td>
<td>F</td>
<td>TN</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\bfX_2\)</span></td>
<td></td>
<td></td>
<td>TN</td>
<td>F</td>
<td>TN</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\bfX_3\)</span></td>
<td></td>
<td></td>
<td></td>
<td>F</td>
<td>TP</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\bfX_4\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>F</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(\bfX_5\)</span></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>Hence the Rand index is 6/10 = 0.6.</p>
</div>
</div>
<p>The Rand index has some attractive features:</p>
<ul>
<li>It is symmetric in the two clusterings; it doesn’t matter which is considered the reference.</li>
<li>There is no need to find a correspondence between the clusters in the two clusterings. In fact, the clusterings need not even have the same number of clusters.</li>
<li>The value is between 0 (complete disagreement) and 1 (complete agreement).</li>
</ul>
<p>A weakness of the Rand index is that it can be fairly close to 1 even for a random clustering. The <strong>adjusted Rand index</strong> rescales the result by comparing it to how a random clustering would fare. It can be written as</p>
<p><span class="math display">\[
\text{ARI} = \frac{\text{RI} - \E{\text{RI}}}{\text{max}(\text{RI}) - \E{\text{RI}}},
\]</span></p>
<p>where the expectation and max operations are performed over all possible clusterings. (These values can be worked out exactly using combinatorics, but we do not give them here.)</p>
<p>An ARI of 0 indicates no better agreement than a random clustering, while an ARI of 1 is complete agreement. Unlike the RI, the ARI value can be negative, indicating a performance worse than on average.</p>
<div id="exm-cluster-ARI-blobs" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.4 </strong></span>Our <em>blobs</em> dataset has a reference clustering that we take to be the gold standard. Let’s create another clustering based entirely on the quadrants of the plane:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> quad(x1, x2):</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x1 <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x2 <span class="op">&gt;</span> <span class="dv">0</span>: <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: <span class="cf">return</span> <span class="dv">4</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x2 <span class="op">&gt;</span> <span class="dv">0</span>: <span class="cf">return</span> <span class="dv">2</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>: <span class="cf">return</span> <span class="dv">3</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> pd.Series( [ quad(x1, x2) <span class="cf">for</span> (x1, x2) <span class="kw">in</span> <span class="bu">zip</span>(X[<span class="st">"x1"</span>], X[<span class="st">"x2"</span>]) ] )</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, hue<span class="op">=</span>q, palette<span class="op">=</span><span class="st">"Dark2"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-13-output-1.png" width="526" height="468"></p>
</div>
</div>
<p>The reference clustering has three classes, and there are four quadrants. Yet we can still compare them by adjusted Rand index:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>adjusted_rand_score(y, q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<pre><code>0.904092765401111</code></pre>
</div>
</div>
<p>Not surprisingly, they are seen as fairly similar.</p>
</div>
</section>
<section id="silhouettes" class="level3" data-number="6.2.2">
<h3 data-number="6.2.2" class="anchored" data-anchor-id="silhouettes"><span class="header-section-number">6.2.2</span> Silhouettes</h3>
<p>If no reference clustering/classification is available, then the only way we can assess the quality of a clustering is to check how well it creates clusters whose members are more similar to their buddies than to strangers.</p>
<p>Suppose <span class="math inline">\(\bfX_i\)</span> is a sample point. Let <span class="math inline">\(\bar{b}_i\)</span> be the mean distance between <span class="math inline">\(\bfX_i\)</span> and its buddies, and let <span class="math inline">\(\bar{r}_i\)</span> be the mean distance between <span class="math inline">\(\bfX_i\)</span> and the members of the nearest cluster of strangers. Then the <strong>silhouette value</strong> of <span class="math inline">\(\bfX_i\)</span> is</p>
<p><span class="math display">\[
s_i = \frac{\bar{r}_i-\bar{b}_i}{\max\{\bar{r}_i,\bar{b}_i\}}.
\]</span></p>
<p>This value is between <span class="math inline">\(-1\)</span> (worst) and <span class="math inline">\(1\)</span> (best) for every sample point. A <strong>silhouette score</strong> is derived by taking a mean of the silhouette values, either per cluster or overall, depending on the usage.</p>
<div id="exm-cluster-silhouette" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.5 </strong></span>Suppose that two clusters in one dimension are defined as <span class="math inline">\(A=\{-4,-1,1\}\)</span> and <span class="math inline">\(B=\{2,6\}\)</span>. Find the silhouette values of all the samples, the silhouette scores of the clusters, and the overall silhouette score.</p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span></p>
<table class="table">
<colgroup>
<col style="width: 32%">
<col style="width: 23%">
<col style="width: 26%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span class="math inline">\(X_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\bar{b}_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(\bar{r}_i\)</span></th>
<th style="text-align: center;"><span class="math inline">\(s_i\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(-4\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3+5}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{6+10}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{8-4}{8}=\frac{1}{2}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(-1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3+2}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3+7}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{5-2.5}{5}=\frac{1}{2}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(1\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{5+2}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{1+5}{2}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{3-3.5}{3.5}=-\frac{1}{7}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span class="math inline">\(2\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{4}{1}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{6+3+1}{3}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{(10/3)-4}{4}=-\frac{1}{6}\)</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span class="math inline">\(6\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{4}{1}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{10+7+5}{3}\)</span></td>
<td style="text-align: center;"><span class="math inline">\(\frac{(22/3)-4}{(22/3)}=\frac{5}{11}\)</span></td>
</tr>
</tbody>
</table>
<p>The silhouette score of cluster <span class="math inline">\(A\)</span> is</p>
<p><span class="math display">\[
\frac{1}{3}\left( \frac{1}{2} + \frac{1}{2} - \frac{1}{7} \right) \approx 0.286,
\]</span></p>
<p>and of cluster <span class="math inline">\(B\)</span> is</p>
<p><span class="math display">\[
\frac{1}{2}\left( \frac{5}{11} - \frac{1}{6} \right) \approx 0.144.
\]</span></p>
<p>The overall score is the mean of the five values in the last column, which is about <span class="math inline">\(0.229\)</span>.</p>
</div>
</div>
<p>The silhouette score is fairly easy to understand and use. However, it relies on distances and tends to favor convex, compact clusters.</p>
<div id="exm-cluster-sil-blobs" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.6 </strong></span>Let’s use the predefined cluster assignments in our blobs dataset. We will add a column to the data frame that records the silhouette score for each point:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_samples</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X, y)</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X,</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span>y, size<span class="op">=</span><span class="st">"sil"</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-15-output-1.png" width="547" height="468"></p>
</div>
</div>
<p>In the plot above, the size of each dot shows its silhouette coefficient. Those points which don’t belong comfortably with their cluster have negative scores and the smallest dots. We can find the average score in each cluster through a grouped mean:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>X.groupby(y)[<span class="st">"sil"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<pre><code>0    0.815086
1    0.641591
2    0.582871
Name: sil, dtype: float64</code></pre>
</div>
</div>
<p>These values are ordered as we would expect. We can also compute the silhouette scores for the quadrant-based clustering defined in <a href="#exm-cluster-ARI-blobs">Example&nbsp;<span>6.4</span></a>:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X, q)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>X, </span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span>q, size<span class="op">=</span><span class="st">"sil"</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-17-output-1.png" width="554" height="468"></p>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>X.groupby(q)[<span class="st">"sil"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="17">
<pre><code>1    0.653534
2    0.816034
3    0.356596
4    0.094035
Name: sil, dtype: float64</code></pre>
</div>
</div>
<p>The scores for clusters labelled as 1 and 2 are pretty good and almost the same as for the original reference clustering. But the other two clusters score much more poorly separately than they did when they were considered a single cluster.</p>
</div>
<div id="exm-cluster-perform-digits" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.7 </strong></span><code>sklearn</code> has a well-known dataset that contains labeled handwritten digits. Let’s extract the examples for just the numerals 4, 5, and 6:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>keep <span class="op">=</span> digits[<span class="st">"target"</span>].isin([<span class="dv">4</span>, <span class="dv">5</span>, <span class="dv">6</span>])</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> digits[keep]</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.drop(<span class="st">"target"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits.target</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>y.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="18">
<pre><code>5    182
4    181
6    181
Name: target, dtype: int64</code></pre>
</div>
</div>
<p>We can check the silhouette scores for the reference labelling in order to set expectations for how well a clustering method might do:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>digits[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X,y)</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>digits.groupby(<span class="st">"target"</span>)[<span class="st">"sil"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>target
4    0.194477
5    0.225677
6    0.327088
Name: sil, dtype: float64</code></pre>
</div>
</div>
<p>As usual, means can tell us only so much. A look at the distributions of the values reveals more details:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>sns.catplot(data<span class="op">=</span>digits,</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"target"</span>, y<span class="op">=</span><span class="st">"sil"</span>,</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"violin"</span></span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-21-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>The values are mostly positive, which indicates nearly all of the samples for a digit are at least somewhat closer to each other than to the other samples. The 6s are the most distinct. However, the existence of scores close to and below zero suggest that a clustering algorithm ia unlikely to reproduce the true classification perfectly. To put it another way, the features chosen to represent the digits don’t seem to create three clear, well-separated balls of points representing the three different types of digits.</p>
</div>
<div class="callout callout-style-default callout-important callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The universe doesn’t owe you a clustering. Not all phenomena are amenable to clustering in whatever features you happen to choose.</p>
</div>
</div>
<p>While classification only requires us to separate different classes of examples, clustering is more specific and more demanding: samples in a cluster need to be more like each other, or the “average” cluster member, than they are like members of other clusters. We should expect that edge cases will look ambiguous.</p>
</section>
</section>
<section id="k-means" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="k-means"><span class="header-section-number">6.3</span> k-means</h2>
<p>The <strong><span class="math inline">\(k\)</span>-means algorithm</strong> is one of the best-known and most widely used clustering methods, although it has some significant limitations.</p>
<p>Given a sample matrix <span class="math inline">\(\bfX\)</span> with <span class="math inline">\(n\)</span> rows <span class="math inline">\(\bfX_i\)</span>, the algorithm divides the sample points into disjoint sets <span class="math inline">\(C_1,\ldots,C_k\)</span>, where <span class="math inline">\(k\)</span> is a hyperparameter. We need a pair of key definitions.</p>
<div id="def-cluster-kmeans" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 6.4 </strong></span>The <strong>centroid</strong> of cluster <span class="math inline">\(C_j\)</span>, denoted by <span class="math inline">\(\bfmu_j\)</span>, is the mean of the vectors in <span class="math inline">\(C_j\)</span>. The <strong>inertia</strong> of <span class="math inline">\(C_j\)</span> is <span id="eq-cluster-inertia"><span class="math display">\[
I_j = \sum_{\bfx\in C_j} \norm{ \bfx - \bfmu_j }_2^2.
\tag{6.2}\]</span></span></p>
</div>
<p>The goal of the <span class="math inline">\(k\)</span>-means algorithm is to choose the clusters in order to minimize the total inertia,</p>
<p><span class="math display">\[
I = \sum_{j=1}^k I_j.
\]</span></p>
<!-- For any cluster, its centroid is the point that minimizes the inertia of the cluster. Suppose that $C_j$ is split into two parts $A$ and $B$ that have centroids $\bfmu_A$ and $\bfmu_B$. Those centroids minimize the inertias of the subclusters. Hence, 

$$
\sum_{\bfx\in A} \norm{ \bfx - \bfmu_A }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_B }^2 
\le  \sum_{\bfx\in A} \norm{ \bfx - \bfmu_j }^2 + \sum_{\bfx\in B} \norm{ \bfx - \bfmu_j}^2  = I_j. 
$$

We conclude that splitting a cluster will make the total inertia decrease. In fact, if each sample point is put into its own cluster, the inertia is 0.  -->
<div id="exm-cluster-inertia" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.8 </strong></span>Given the samples <span class="math inline">\(-3,-2,-1,2,5,7\)</span>, find the inertia of the clustering <span class="math display">\[
C_1=\{-3,-2,-1\}, \qquad C_2=\{2,5,7\},
\]</span> and of the clustering <span class="math display">\[
C_1=\{-3,-2,-1,2\}, \qquad C_2=\{5,7\}.  
\]</span></p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>The total inertia of the first clustering is <span class="math display">\[
\left[  (-3+2)^2 + (-2+2)^2 + (-1+2)^2   \right]  + \left[  \bigl(2-\tfrac{14}{3}\bigr)^2 + \bigl(5-\tfrac{14}{3}\bigr)^2 + \bigl(7-\tfrac{14}{3}\bigr)^2   \right] = 2 + \frac{124}{9} = 15.78.
\]</span></p>
<p>The total inertia of the second clustering is</p>
<p><span class="math display">\[
\left[  (-3+1)^2 + (-2+1)^2 + (-1+1)^2  + (2+1)^2 \right]  + \left[   (5-6)^2 + (7-6)^2   \right] = 14 + 2 = 16.
\]</span></p>
</div>
</div>
<p>Finding the minimum inertia among all possible <span class="math inline">\(k\)</span>-clusterings is an infeasible problem to solve exactly at any practical size. Instead, the approach is to iteratively improve on a starting clustering.</p>
<section id="lloyds-algorithm" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="lloyds-algorithm"><span class="header-section-number">6.3.1</span> Lloyd’s algorithm</h3>
<p>The most-used iteration is known as <strong>Lloyd’s algorithm</strong>. Starting with values for the <span class="math inline">\(k\)</span> centroids, each iteration consists of two steps:</p>
<ol type="1">
<li>Each sample point is assigned to the cluster whose centroid is the nearest. (Ties are broken randomly.)</li>
<li>Recalculate the centroids based on the cluster assignments: <span class="math display">\[
\bfmu_j^+ = \frac{1}{|C_j|} \sum_{\bfx\in C_j} \bfx.
\]</span></li>
</ol>
<p>These steps are repeated alternately until the assignment step does not change any of the clusters. In practice, this almost always happens quickly. Here is a demonstration:</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="480" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="_media/kmeans_demo.mp4"></video></div>
<p>If Lloyd’s algorithm converges, it finds a local minimum of total inertia, in the sense that small changes to the cluster assignments cannot decrease it. But there is no guarantee of converging to the global minimum, and often, this does not happen.</p>
</section>
<section id="practical-issues" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="practical-issues"><span class="header-section-number">6.3.2</span> Practical issues</h3>
<ul>
<li><strong>Initialization</strong>. The performance of <span class="math inline">\(k\)</span>-means depends a great deal on the initial set of centroids. Traditionally, the centroids were chosen as random members of the sample set, but more reliable heuristics, such as <em><span class="math inline">\(k\)</span>-means++</em>, have since become dominant.</li>
<li><strong>Multiple runs</strong>. All the initialization methods include an element of randomness, and since the Lloyd algorithm usually converges quickly, it is often run with multiple instances of the initialization. The run with the lowest final inertia is kept.</li>
<li><strong>Distance metric</strong>. The Lloyd algorithm often fails to converge for norms other than the 2-norm and must be modified if another norm is preferred.</li>
</ul>
<p>The algorithm treats <span class="math inline">\(k\)</span> as a hyperparameter. Increasing <span class="math inline">\(k\)</span> tends to lower the total inertia—at the extreme, taking <span class="math inline">\(k=n\)</span> puts each sample in its own cluster, with a total inertia of zero. Hence, one should use silhouette scores or some other measure in order to optimize <span class="math inline">\(k\)</span>. Occam’s Razor dictates preferring smaller values to large ones. There are many suggestions on how to find the choice that gives the most “bang for the buck,” but none is foolproof.</p>
<p>Because of its dependence on the norm, the inertia criterion of <span class="math inline">\(k\)</span>-means disfavors long, skinny clusters and clusters of unequal dispersion. Essentially, it wants to find spherical blobs of roughly equal size.</p>
<div id="exm-cluster-kmeans-blobs" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.9 </strong></span>We apply <span class="math inline">\(k\)</span>-means to the <em>blobs</em> dataset that has 3 reference clusters, starting with <span class="math inline">\(k=2\)</span> clusters. For illustration, we instruct the algorithm to try 3 initializations and report on its progress:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>km2 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>, n_init<span class="op">=</span><span class="dv">3</span>, verbose<span class="op">=</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>km2.fit(X)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Initialization complete
Iteration 0, inertia 1059.6987886807747.
Iteration 1, inertia 731.1354331556078.
Iteration 2, inertia 710.0834960683221.
Converged at iteration 2: strict convergence.
Initialization complete
Iteration 0, inertia 991.8887143415382.
Iteration 1, inertia 742.3898509432538.
Iteration 2, inertia 710.0834960683221.
Converged at iteration 2: strict convergence.
Initialization complete
Iteration 0, inertia 1379.944166430521.
Iteration 1, inertia 917.3997019815874.
Iteration 2, inertia 903.6145145551955.
Iteration 3, inertia 845.0278508241745.
Iteration 4, inertia 736.5347784099483.
Iteration 5, inertia 710.0834960683221.
Converged at iteration 5: strict convergence.</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="21">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KMeans(n_clusters=2, n_init=3, random_state=302, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">KMeans</label><div class="sk-toggleable__content"><pre>KMeans(n_clusters=2, n_init=3, random_state=302, verbose=1)</pre></div></div></div></div></div>
</div>
</div>
<p>The fitted clustering object can tell us the final inertia and cluster centroids:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"final inertia: </span><span class="sc">{</span>km2<span class="sc">.</span>inertia_<span class="sc">:.5g}</span><span class="ss">"</span>)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"cluster centroids:"</span>)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(km2.cluster_centers_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>final inertia: 710.08
cluster centroids:
[[-2.01977462  2.86247767]
 [ 2.01902057 -0.69722874]]</code></pre>
</div>
</div>
<p>There is a <code>predict</code> method that can make cluster assignments for arbitrary points in feature space. In <span class="math inline">\(k\)</span>-means, this prediction simply tells you which centroid is closest:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a>km2.predict([ [<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>], [<span class="dv">1</span>,<span class="dv">2</span>] ])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>array([0, 1], dtype=int32)</code></pre>
</div>
</div>
<p>For the training samples, we don’t need to call <code>predict</code> to get the cluster assignments. Every fitted clustering object has a <code>labels_</code> property that lists the cluster index values. We can use those labels to compute silhouette scores:</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> report(clustering):</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>    X[<span class="st">"cluster"</span>] <span class="op">=</span> clustering.labels_</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    X[<span class="st">"sil"</span>] <span class="op">=</span> silhouette_samples(X, X[<span class="st">"cluster"</span>])</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"inertia: </span><span class="sc">{</span>clustering<span class="sc">.</span>inertia_<span class="sc">:.5g}</span><span class="ss">"</span>)</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"silhouette scores by cluster:"</span>)</span>
<span id="cb34-6"><a href="#cb34-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>( X.groupby(<span class="st">"cluster"</span>)[<span class="st">"sil"</span>].mean() )</span>
<span id="cb34-7"><a href="#cb34-7" aria-hidden="true" tabindex="-1"></a>    sns.catplot(data<span class="op">=</span>X,</span>
<span id="cb34-8"><a href="#cb34-8" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"cluster"</span>, y<span class="op">=</span><span class="st">"sil"</span>,</span>
<span id="cb34-9"><a href="#cb34-9" aria-hidden="true" tabindex="-1"></a>        kind<span class="op">=</span><span class="st">"violin"</span>, height<span class="op">=</span><span class="dv">3</span></span>
<span id="cb34-10"><a href="#cb34-10" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span>
<span id="cb34-11"><a href="#cb34-11" aria-hidden="true" tabindex="-1"></a>    sns.relplot(data<span class="op">=</span>X,</span>
<span id="cb34-12"><a href="#cb34-12" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb34-13"><a href="#cb34-13" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, size<span class="op">=</span>X[<span class="st">"sil"</span>], </span>
<span id="cb34-14"><a href="#cb34-14" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="dv">3</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb34-15"><a href="#cb34-15" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span>
<span id="cb34-16"><a href="#cb34-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> X </span>
<span id="cb34-17"><a href="#cb34-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb34-18"><a href="#cb34-18" aria-hidden="true" tabindex="-1"></a>report(km2)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 710.08
silhouette scores by cluster:
cluster
0    0.824350
1    0.437237
Name: sil, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-25-output-2.png" width="277" height="276"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-25-output-3.png" width="346" height="276"></p>
</div>
</div>
<p>It’s clear in both plots that one cluster is more tightly packed than the other. Let’s repeat the computation for <span class="math inline">\(k=3\)</span> clusters:</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>km3 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>km3.fit(X)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>report(km3)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 205.17
silhouette scores by cluster:
cluster
0    0.660228
1    0.819895
2    0.642309
Name: sil, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-26-output-2.png" width="277" height="276"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-26-output-3.png" width="339" height="276"></p>
</div>
</div>
<p>This result shows a modest reduction in silhouette scores for original good cluster, but improvement for the problematic one.</p>
<p>Moving on to <span class="math inline">\(k=4\)</span> clusters shows clear degradation of the silhouette scores:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>km4 <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>km4.fit(X)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>report(km4)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>inertia: 165.65
silhouette scores by cluster:
cluster
0    0.812891
1    0.566701
2    0.458589
3    0.489471
Name: sil, dtype: float64</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-27-output-2.png" width="277" height="276"></p>
</div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-27-output-3.png" width="346" height="276"></p>
</div>
</div>
<p>Based on silhouette scores, we would be justified to stop at <span class="math inline">\(k=3\)</span> clusters.</p>
</div>
<div id="exm-cluster-kmeans-stripes" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.10 </strong></span>The <em>stripes</em> dataset does not conform to compact clusters of equal size, and <span class="math inline">\(k\)</span>-means performs poorly on it:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]:</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    X[<span class="st">"cluster"</span>] <span class="op">=</span> km.labels_</span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a>    X[<span class="st">"k"</span>] <span class="op">=</span> k</span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.concat( (results, X) )</span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"k"</span>, </span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">3</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-28-output-1.png" width="914" height="275"></p>
</div>
</div>
<p>It’s generally a good idea to standardize the data. But that’s no help here:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb41-1"><a href="#cb41-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb41-2"><a href="#cb41-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> [<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]:</span>
<span id="cb41-3"><a href="#cb41-3" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> make_pipeline(</span>
<span id="cb41-4"><a href="#cb41-4" aria-hidden="true" tabindex="-1"></a>        StandardScaler(), </span>
<span id="cb41-5"><a href="#cb41-5" aria-hidden="true" tabindex="-1"></a>        KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb41-6"><a href="#cb41-6" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb41-7"><a href="#cb41-7" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb41-8"><a href="#cb41-8" aria-hidden="true" tabindex="-1"></a>    X[<span class="st">"cluster"</span>] <span class="op">=</span> km[<span class="dv">1</span>].labels_</span>
<span id="cb41-9"><a href="#cb41-9" aria-hidden="true" tabindex="-1"></a>    X[<span class="st">"k"</span>] <span class="op">=</span> k</span>
<span id="cb41-10"><a href="#cb41-10" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.concat( (results, X) )</span>
<span id="cb41-11"><a href="#cb41-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb41-12"><a href="#cb41-12" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb41-13"><a href="#cb41-13" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>,</span>
<span id="cb41-14"><a href="#cb41-14" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"k"</span>, </span>
<span id="cb41-15"><a href="#cb41-15" aria-hidden="true" tabindex="-1"></a>    height<span class="op">=</span><span class="dv">3</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb41-16"><a href="#cb41-16" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-29-output-1.png" width="914" height="275"></p>
</div>
</div>
<p>Clustering is hard!</p>
</div>
<div id="exm-cluster-kmeans-digits" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.11 </strong></span>We return to the handwriting recognition dataset. Again we keep only the samples labeled 4, 5, or 6:</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> datasets.load_digits(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>keep <span class="op">=</span> digits[<span class="st">"target"</span>].isin([<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>])</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>digits <span class="op">=</span> digits[keep]</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> digits.drop(<span class="st">"target"</span>,axis<span class="op">=</span><span class="st">"columns"</span>)</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> digits[<span class="st">"target"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We fit 3 clusters to the feature matrix:</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>km.fit(X)</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>digits[<span class="st">"kmeans3"</span>] <span class="op">=</span> km.labels_</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>digits[[<span class="st">"target"</span>, <span class="st">"kmeans3"</span>]].head(<span class="dv">9</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="30">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>target</th>
<th>kmeans3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>4</th>
<td>4</td>
<td>0</td>
</tr>
<tr class="even">
<th>5</th>
<td>5</td>
<td>2</td>
</tr>
<tr class="odd">
<th>6</th>
<td>6</td>
<td>1</td>
</tr>
<tr class="even">
<th>14</th>
<td>4</td>
<td>0</td>
</tr>
<tr class="odd">
<th>15</th>
<td>5</td>
<td>2</td>
</tr>
<tr class="even">
<th>16</th>
<td>6</td>
<td>1</td>
</tr>
<tr class="odd">
<th>24</th>
<td>4</td>
<td>0</td>
</tr>
<tr class="even">
<th>25</th>
<td>5</td>
<td>2</td>
</tr>
<tr class="odd">
<th>26</th>
<td>6</td>
<td>1</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The adjusted Rand index suggests that we have reproduced the classification very well:</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(y, digits[<span class="st">"kmeans3"</span>])</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ARI: 0.9618</code></pre>
</div>
</div>
<p>However, that conclusion benefits from our prior knowledge. What if we did not know how many clusters to look for? Let’s look over a range of <span class="math inline">\(k\)</span> values, recording the mean silhouette score for each:</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> silhouette_score</span>
<span id="cb46-2"><a href="#cb46-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb46-3"><a href="#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">7</span>):</span>
<span id="cb46-4"><a href="#cb46-4" aria-hidden="true" tabindex="-1"></a>    km <span class="op">=</span> KMeans(n_clusters<span class="op">=</span>k, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb46-5"><a href="#cb46-5" aria-hidden="true" tabindex="-1"></a>    km.fit(X)</span>
<span id="cb46-6"><a href="#cb46-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-7"><a href="#cb46-7" aria-hidden="true" tabindex="-1"></a>    sil <span class="op">=</span> silhouette_score(X, km.labels_)</span>
<span id="cb46-8"><a href="#cb46-8" aria-hidden="true" tabindex="-1"></a>    results.append( [k, sil] )</span>
<span id="cb46-9"><a href="#cb46-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb46-10"><a href="#cb46-10" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(results, columns<span class="op">=</span>[<span class="st">"k"</span>, <span class="st">"mean silhouette"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>k</th>
<th>mean silhouette</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>2</td>
<td>0.226800</td>
</tr>
<tr class="even">
<th>1</th>
<td>3</td>
<td>0.251904</td>
</tr>
<tr class="odd">
<th>2</th>
<td>4</td>
<td>0.245855</td>
</tr>
<tr class="even">
<th>3</th>
<td>5</td>
<td>0.235199</td>
</tr>
<tr class="odd">
<th>4</th>
<td>6</td>
<td>0.188324</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The silhouette score is maximized at <span class="math inline">\(k=3\)</span>, which could be considered a reason to choose 3 clusters. While the score for 4 clusters is fairly close, we should prefer the less complex model.</p>
</div>
</section>
</section>
<section id="hierarchical-clustering" class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="hierarchical-clustering"><span class="header-section-number">6.4</span> Hierarchical clustering</h2>
<p>The idea behind hierarchical clustering is to organize all the sample points into a tree structure called a <strong>dendrogram</strong>. At the root of the tree is the entire sample set, while each leaf of the tree is a single sample. Groups of similar samples are connected as nearby relatives in the tree, with less-similar groups located as more distant relatives.</p>
<p>Dendrograms can be found by starting with the root and recursively splitting, or by starting at the leaves and recursively merging. We will describe the latter approach, known as <strong>agglomerative clustering</strong>.</p>
<p>The algorithm begins with <span class="math inline">\(n\)</span> singleton clusters, i.e., <span class="math inline">\(C_i=\{\bfX_i\}\)</span> for <span class="math inline">\(i=1,\dots,n\)</span>. Then, the similarity or distance between each pair of clusters is determined. The pair with the minimum distance is merged, and the process repeats. Effectively, we end up with an entire family of clusterings; we can stop at any number of clusters between <span class="math inline">\(n\)</span> and 1.</p>
<p>Common ways to define the distance between two clusters <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span> are:</p>
<ul>
<li><dl>
<dt>single linkage</dt>
<dd>
(also called <em>minimum linkage</em>) <span id="eq-cluster-linkage-single"><span class="math display">\[
\displaystyle \min_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy } \}
\tag{6.3}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>complete linkage</dt>
<dd>
(also called <em>maximum linkage</em>) <span id="eq-cluster-linkage-complete"><span class="math display">\[
\displaystyle \max_{\bfx\in C_i,\,\bfy\in C_j} \{ \norm{\bfx-\bfy} \}
\tag{6.4}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>average linkage</dt>
<dd>
<span id="eq-cluster-linkage-average"><span class="math display">\[
\displaystyle \frac{1}{|C_i|\,|C_j|} \sum_{\bfx\in C_i,\,\bfy\in C_j} \norm{ \bfx-\bfy }
\tag{6.5}\]</span></span>
</dd>
</dl></li>
<li><dl>
<dt>Ward linkage</dt>
<dd>
The increase in inertia resulting from merging <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>, which is equal to <span id="eq-cluster-linkage-ward"><span class="math display">\[
\frac{ |C_i|\,|C_j| }{|C_i| + |C_j|} \norm{\bfmu_i - \bfmu_j}_2^2,
\tag{6.6}\]</span></span> where <span class="math inline">\(\bfmu_i\)</span> and <span class="math inline">\(\bfmu_j\)</span> are the centroids of <span class="math inline">\(C_i\)</span> and <span class="math inline">\(C_j\)</span>.
</dd>
</dl></li>
</ul>
<p>Single linkage only pays attention to the gaps between clusters, not the size or spread of them. Complete linkage, on the other hand, wants to keep every cluster packed tightly together. Average linkage is a compromise between these extremes. Agglomerative clustering with Ward linkage amounts to trying to minimize the increase of inertia with each merger. In that sense, it has the same objective as <span class="math inline">\(k\)</span>-means, but it is usually not as successful at minimizing inertia.</p>
<p>Unlike <span class="math inline">\(k\)</span>-means, which has to be able to compute the distances between the samples and changing centroid locations, agglomerative clustering with single, complete, or average linkage needs only the pairwise distances between samples and can be found from a distance matrix in place of the samples themselves.</p>
<div id="exm-cluster-linkage-tiny" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.12 </strong></span>Given clusters <span class="math inline">\(C_1=\{-3,-2,-1\}\)</span> and <span class="math inline">\(C_2=\{3,4,5\}\)</span>, find the different linkages between them.</p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span><strong>Ward.</strong> The centroids of the clusters are <span class="math inline">\(-2\)</span> and <span class="math inline">\(4\)</span>. So the linkage is</p>
<p><span class="math display">\[
\frac{3\cdot 3}{3+3} \, 6^2 = 54.
\]</span></p>
<p><strong>Single.</strong> The pairwise distances between members of <span class="math inline">\(C_1\)</span> and <span class="math inline">\(C_2\)</span> form a <span class="math inline">\(3\times 3\)</span> matrix:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>-3</th>
<th>-2</th>
<th>-1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>3</td>
<td>6</td>
<td>5</td>
<td>4</td>
</tr>
<tr class="even">
<td>4</td>
<td>7</td>
<td>6</td>
<td>5</td>
</tr>
<tr class="odd">
<td>5</td>
<td>8</td>
<td>7</td>
<td>6</td>
</tr>
</tbody>
</table>
<p>(Note that this is a submatrix of the full <span class="math inline">\(6\times 6\)</span> distance matrix.) The single linkage is therefore 4.</p>
<p><strong>Complete.</strong> The maximum of the matrix above is 8.</p>
<p><strong>Average.</strong> The average value of the matrix entries is <span class="math inline">\(54/9\)</span>, which is 6.</p>
</div>
</div>
<div id="exm-cluster-hier-toy" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.13 </strong></span>Let’s use 5 sample points in the plane, and agglomerate them by single linkage. The <code>pairwise_distances</code> function converts sample points into a distance matrix:</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.array( [[<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>] ,[<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>], [<span class="dv">1</span>,<span class="fl">0.5</span>], [<span class="dv">0</span>,<span class="dv">2</span>], [<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>]] )</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>D <span class="op">=</span> pairwise_distances(X, metric<span class="op">=</span><span class="st">"euclidean"</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>D</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>array([[0.        , 1.        , 3.35410197, 3.60555128, 2.23606798],
       [1.        , 0.        , 3.90512484, 4.47213595, 3.16227766],
       [3.35410197, 3.90512484, 0.        , 1.80277564, 2.06155281],
       [3.60555128, 4.47213595, 1.80277564, 0.        , 1.41421356],
       [2.23606798, 3.16227766, 2.06155281, 1.41421356, 0.        ]])</code></pre>
</div>
</div>
<p>The minimum value in the upper triangle of the distance matrix is in row 0, column 1. So our first merge results in the cluster <span class="math inline">\(C_1=\{\bfX_0,\bfX_1\}\)</span>. The next-smallest entry in the upper triangle is at position <span class="math inline">\((3,4)\)</span>, so we want to merge those samples together next, resulting in <span class="math display">\[
C_1=\{\bfX_0,\bfX_1\},\, C_2 = \{\bfX_3,\bfX_4\},\, C_3=\{\bfX_2\}.
\]</span> The next-smallest element in the matrix is at <span class="math inline">\((2,3)\)</span>, resulting in <span class="math display">\[
C_1=\{\bfX_0,\bfX_1\},\, C_2 = \{\bfX_2,\bfX_3,\bfX_4\}.
\]</span> The final merge is to combine these.</p>
<p>The entire dendrogram can be visualized with seaborn:</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a>sns.clustermap(X, </span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>    col_cluster<span class="op">=</span><span class="va">False</span>,</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>    dendrogram_ratio<span class="op">=</span>(<span class="fl">.75</span>,<span class="fl">.15</span>),</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>)</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-35-output-1.png" width="568" height="385"></p>
</div>
</div>
<p>The root of the tree above is at the left, with the leaves at the right. The leaves have been ordered so that the lines in the dendrogram don’t ever cross. The two colored columns show the values of the features of the samples.</p>
<p>The horizontal position in the dendrogram indicates the linkage value, which is largest at the left. Working from right to left, we first see the merger of samples 0 and 1 at the minimum linkage in the entire distance matrix. The next merger is between <span class="math inline">\(\bfX_3\)</span> and <span class="math inline">\(\bfX_4\)</span>, and so on.</p>
<p>We can choose to stop at any horizontal position (linkage value). Based on the distance matrix, if we stop at a linkage value of 2.0, then we perform only the first 2 merges and get 3 clusters. The largest gap between merges is in the merge from 2 clusters to 1, which could be used to justify <span class="math inline">\(k=2\)</span> as the best number of clusters.</p>
</div>
<p>The <code>AgglomerativeClustering</code> class in <code>sklearn.cluster</code> performs the clustering process to create a learner object.</p>
<div id="exm-cluster-linkage" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.14 </strong></span>We define a function that allows us to run all three linkages for a given feature matrix:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> run_experiment(X):</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a>    results <span class="op">=</span> pd.DataFrame()</span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> X.copy()</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> linkage <span class="kw">in</span> [<span class="st">"single"</span>, <span class="st">"complete"</span>, <span class="st">"ward"</span>]:</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>        agg <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span>linkage)</span>
<span id="cb50-8"><a href="#cb50-8" aria-hidden="true" tabindex="-1"></a>        agg.fit( X )</span>
<span id="cb50-9"><a href="#cb50-9" aria-hidden="true" tabindex="-1"></a>        data[<span class="st">"cluster"</span>] <span class="op">=</span> agg.labels_</span>
<span id="cb50-10"><a href="#cb50-10" aria-hidden="true" tabindex="-1"></a>        data[<span class="st">"linkage"</span>] <span class="op">=</span> linkage</span>
<span id="cb50-11"><a href="#cb50-11" aria-hidden="true" tabindex="-1"></a>        results <span class="op">=</span> pd.concat( (results, data) )</span>
<span id="cb50-12"><a href="#cb50-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We first try the <em>blobs</em> dataset:</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> blobs_data()</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.7</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-37-output-1.png" width="827" height="247"></p>
</div>
</div>
<p>As you can see, the simple linkage was badly confused by the two blobs that nearly run together. The others are good at reproducing the ball-like clusters.</p>
<p>Next, we try the <em>stripes</em> data:</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>X, y <span class="op">=</span> stripes_data()</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.7</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb52-7"><a href="#cb52-7" aria-hidden="true" tabindex="-1"></a>        )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-38-output-1.png" width="827" height="247"></p>
</div>
</div>
<p>Now the tendency of complete and Ward linkages to find compact, roughly spherical clusters is a bug, not a feature. They group together points across stripes rather than clusters extending lengthwise. The geometric flexibility of single linkage pays off here.</p>
<p>Finally, we try the most demanding test, <em>bullseye</em>:</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>X ,y <span class="op">=</span> bullseye_data()</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb53-6"><a href="#cb53-6" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.7</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb53-7"><a href="#cb53-7" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb53-8"><a href="#cb53-8" aria-hidden="true" tabindex="-1"></a>p.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-39-output-1.png" width="811" height="247"></p>
</div>
</div>
<p>The single linkage is the only one to cluster the rings properly. However, it’s a delicate situation, and it can be sensitive to individual samples. Here, for instance, we add just one sample to the dataset and get a major change:</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pd.concat( ( X, pd.DataFrame({<span class="st">"x1"</span>: [<span class="dv">0</span>], <span class="st">"x2"</span>: [<span class="fl">2.25</span>]}) ) )</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> run_experiment(X)</span>
<span id="cb54-3"><a href="#cb54-3" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb54-4"><a href="#cb54-4" aria-hidden="true" tabindex="-1"></a>        x<span class="op">=</span><span class="st">"x1"</span>, y<span class="op">=</span><span class="st">"x2"</span>, </span>
<span id="cb54-5"><a href="#cb54-5" aria-hidden="true" tabindex="-1"></a>        hue<span class="op">=</span><span class="st">"cluster"</span>, col<span class="op">=</span><span class="st">"linkage"</span>, </span>
<span id="cb54-6"><a href="#cb54-6" aria-hidden="true" tabindex="-1"></a>        height<span class="op">=</span><span class="fl">2.7</span>, palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb54-7"><a href="#cb54-7" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb54-8"><a href="#cb54-8" aria-hidden="true" tabindex="-1"></a>p.<span class="bu">set</span>(aspect<span class="op">=</span><span class="dv">1</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-40-output-1.png" width="811" height="247"></p>
</div>
</div>
<p>That instability can make single linkage difficult to work with.</p>
</div>
<div id="exm-cluster-penguins" class="theorem example">
<p><span class="theorem-title"><strong>Example 6.15 </strong></span>Let’s try agglomerative clustering to “discover” the species of the penguins, pretending we don’t know them in advance. First, let’s recall how many of each species we actually do have:</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>).dropna()</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"bill_length_mm"</span>, <span class="st">"bill_depth_mm"</span>, <span class="st">"flipper_length_mm"</span>, <span class="st">"body_mass_g"</span>]</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[features]</span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"species"</span>].value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>Adelie       146
Gentoo       119
Chinstrap     68
Name: species, dtype: int64</code></pre>
</div>
</div>
<p>Our first attempt is single linkage. Because 2-norm distances are involved, we use standardization in a pipeline with the clustering method. After fitting, the <code>labels_</code> property of the cluster object is a vector of cluster assignments.</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>single <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span><span class="st">"single"</span>)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(),single)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>pipe.fit(X)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"single"</span>] <span class="op">=</span> single.labels_         <span class="co"># cluster assignments</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>penguins.loc[::<span class="dv">26</span>, [<span class="st">"species"</span>, <span class="st">"single"</span>]]   <span class="co"># print out some rows</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>species</th>
<th>single</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="even">
<th>31</th>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="odd">
<th>58</th>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="even">
<th>84</th>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="odd">
<th>110</th>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="even">
<th>136</th>
<td>Adelie</td>
<td>0</td>
</tr>
<tr class="odd">
<th>162</th>
<td>Chinstrap</td>
<td>0</td>
</tr>
<tr class="even">
<th>188</th>
<td>Chinstrap</td>
<td>0</td>
</tr>
<tr class="odd">
<th>214</th>
<td>Chinstrap</td>
<td>0</td>
</tr>
<tr class="even">
<th>240</th>
<td>Gentoo</td>
<td>2</td>
</tr>
<tr class="odd">
<th>267</th>
<td>Gentoo</td>
<td>2</td>
</tr>
<tr class="even">
<th>294</th>
<td>Gentoo</td>
<td>2</td>
</tr>
<tr class="odd">
<th>320</th>
<td>Gentoo</td>
<td>2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Perhaps <em>Gentoo</em> is associated with cluster number 2, but the situation with the other species is less clear. Here are the value counts:</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"single linkage results:"</span>)</span>
<span id="cb58-2"><a href="#cb58-2" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"single"</span>].value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>single linkage results:</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>0    213
2    119
1      1
Name: single, dtype: int64</code></pre>
</div>
</div>
<p>As we saw with the <em>bullseye</em> dataset in <a href="#exm-cluster-linkage">Example&nbsp;<span>6.14</span></a>, single linkage is susceptible to declaring one isolated point to be a cluster, while grouping together other points we would like to separate. Here is the ARI for this clustering, compared to the true classification:</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> adjusted_rand_score</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(penguins[<span class="st">"species"</span>], penguins[<span class="st">"single"</span>])</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"single linkage ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>single linkage ARI: 0.6506</code></pre>
</div>
</div>
<p>Now let’s try Ward linkage (which is the default):</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a>ward <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">3</span>, linkage<span class="op">=</span><span class="st">"ward"</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), ward)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>pipe.fit(X)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>penguins[<span class="st">"ward"</span>] <span class="op">=</span> ward.labels_</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-6"><a href="#cb63-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Ward linkage results:"</span>)</span>
<span id="cb63-7"><a href="#cb63-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(penguins[<span class="st">"ward"</span>].value_counts())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ward linkage results:
1    157
0    119
2     57
Name: ward, dtype: int64</code></pre>
</div>
</div>
<p>This result looks more promising. The ARI confirms that hunch:</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>ARI <span class="op">=</span> adjusted_rand_score(penguins[<span class="st">"species"</span>], penguins[<span class="st">"ward"</span>])</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Ward linkage ARI: </span><span class="sc">{</span>ARI<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Ward linkage ARI: 0.9132</code></pre>
</div>
</div>
<p>If we guess at the likely correspondence between the cluster numbers and the different species, then we can find the confusion matrix:</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> penguins[<span class="st">"species"</span>]</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert cluster numbers into labels:</span></span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>replacements <span class="op">=</span> {<span class="dv">1</span>: <span class="st">"Adelie"</span>, <span class="dv">0</span>: <span class="st">"Gentoo"</span>, <span class="dv">2</span>: <span class="st">"Chinstrap"</span>}</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>y_hat <span class="op">=</span> penguins[<span class="st">"ward"</span>].replace(replacements) </span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>ConfusionMatrixDisplay(</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>    confusion_matrix(y, y_hat),</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    display_labels<span class="op">=</span>y.unique()</span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    ).plot()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="clustering_files/figure-html/cell-47-output-1.png" width="560" height="429"></p>
</div>
</div>
</div>
<p>While the performance of clustering as a classifier in <a href="#exm-cluster-penguins">Example&nbsp;<span>6.15</span></a> is inferior to supervised methods we used for that purpose, the clustering process reveals that the Gentoo penguins are the most dissimilar from the other species. By inspecting the mislabeled cases in the confusion matrix, further insight might be gained about what particular traits can make the other species difficult to distinguish. In clustering, the goal is often more about insight than prediction.</p>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-cluster-angular" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.1 </strong></span>(6.1) Prove that the angular distance between any nonzero vector and itself is zero.</p>
</div>
<div id="exr-cluster-cosine" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.2 </strong></span>(6.1) Find an example for which the cosine distance does not satisfy the triangle inequality. That is, find three vectors <span class="math inline">\(\bfx\)</span>, <span class="math inline">\(\bfy\)</span>, and <span class="math inline">\(\bfz\)</span> such that <span class="math display">\[
\dist(\bfx,\bfy) &gt; \dist(\bfx,\bfz) + \dist(\bfz,\bfy).
\]</span> (Hint: it’s enough to play around with some simple vectors in two dimensions. Use vectors for which the angle between is a multiple of <span class="math inline">\(\pi/6\)</span> or <span class="math inline">\(\pi/4\)</span>.)</p>
</div>
<div id="exr-cluster-distmatrix" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.3 </strong></span>(6.1) Here is a feature matrix. <span class="math display">\[
\bfX = \begin{bmatrix}
-1 &amp; -1 &amp; 0 \\ 1 &amp; 1 &amp; 1 \\ 2 &amp; 0 &amp; -2 \\ 1 &amp; 3 &amp; 1
\end{bmatrix}.
\]</span></p>
<p><strong>(a)</strong> Find the associated distance matrix using the 1-norm.</p>
<p><strong>(b)</strong> Find the associated distance matrix using the infinity-norm.</p>
</div>
<div id="exr-cluster-performance" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.4 </strong></span>(6.2) Here are two clusterings on 6 samples. <span class="math display">\[
\begin{align*}
\text{A}:\: &amp; C_1 = \{\bfX_1,\bfX_4\}, \; C_2=\{\bfX_2,\bfX_3,\bfX_5,\bfX_6 \}  \\
\text{B}:\: &amp; C_1 = \{\bfX_1,\bfX_2\}, \; C_2=\{\bfX_3,\bfX_4\}, \; C_3 = \{\bfX_5,\bfX_6 \}
\end{align*}
\]</span></p>
<p><strong>(a)</strong> Find the Rand index between these clusterings. <strong>(b)</strong> Find the silhouette values for all samples in clustering <span class="math inline">\(A\)</span>.</p>
</div>
<div id="exr-cluster-inertia" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.5 </strong></span>(6.3) Let <span class="math inline">\(z\)</span> be a positive number, and consider the 12 planar samples <span class="math display">\[
\begin{gather*}
[z,-3],\, [z,-2],\, [z,-1],\,[z,1],\,[z,2],\,[z,3],
[-z,-3],\, [-z,-2],\, [-z,-1],\,[-z,1],\,[-z,2],\,[-z,3].
\end{gather*}
\]</span></p>
<p><strong>(a)</strong> Using the 2-norm, calculuate the inertia, as a function of <span class="math inline">\(z\)</span>, of the clustering <span class="math display">\[
C_1 = \{ [z,j]: j=-3,-2,-1,1,2,3\}, \qquad C_2 = \{ [-z,j]: j=-3,-2,-1,1,2,3\}.
\]</span> (This divides the sample points by the <span class="math inline">\(x\)</span>-axis.)</p>
<p><strong>(b)</strong> Using the 2-norm, calculuate the inertia, as a function of <span class="math inline">\(z\)</span>, of the clustering <span class="math display">\[
C_1 = \{ [-z,j] \text{ and } [z,j]: j=-3,-2,-1\}, \qquad C_2 = \{ [-z,j] \text{ and } [z,j]: j=1,2,3\}.
\]</span> (This divides the sample points by the <span class="math inline">\(y\)</span>-axis.)</p>
<p><strong>(c)</strong> For which values of <span class="math inline">\(c\)</span>, if any, does clustering from part (a) have less inertia than the clustering from part (b)?</p>
</div>
<div id="exr-cluster-linkage" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.6 </strong></span>(6.4) Here is a distance matrix for samples <span class="math inline">\(\bfX_1,\ldots,\bfX_5\)</span>:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccccc}
0 &amp; 2 &amp; 3.5 &amp; 5 &amp; 6 \\
2 &amp; 0 &amp; 2.5 &amp; 3 &amp; 4 \\
3.5 &amp; 2.5 &amp; 0 &amp; 1 &amp; 1.5 \\
5 &amp; 3 &amp; 1 &amp; 0 &amp; 1 \\
6 &amp; 4 &amp; 1.5 &amp; 1 &amp; 0 \\
\end{array}
\right]
\]</span></p>
<p><strong>(a)</strong> Compute the average linkage between the clusters <span class="math inline">\(C_1=\{\bfX_1,\bfX_3\}\)</span> and <span class="math inline">\(C_2=\{\bfX_2,\bfX_4,\bfX_5\}\)</span>.</p>
<p><strong>(b)</strong> Compute the simple linkage between the clusters <span class="math inline">\(C_1=\{\bfX_1,\bfX_3,\bfX_4\}\)</span> and <span class="math inline">\(C_2=\{\bfX_2,\bfX_5\}\)</span>.</p>
</div>
<div id="exr-cluster-agglom" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 6.7 </strong></span>(6.4) Find the agglomerative clustering, using complete linkage, for the samples given in <a href="#exr-cluster-linkage">Exercise&nbsp;<span>6.6</span></a>. (This means finding four individual merge steps based on the distance matrix.)</p>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./regression.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./networks.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Data Science 1
  </li>  
</ul>
    </div>   
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Toby Driscoll
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>



</body></html>