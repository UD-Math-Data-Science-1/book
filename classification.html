<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Science 1 - 3&nbsp; Classification</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./selection.html" rel="next">
<link href="./stats.html" rel="prev">
<link href="./favicon.ico" rel="icon">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script> 
MathJax = {
  chtml: {
    scale: 0.92,
  }
}
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science 1</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./intro.html" class="sidebar-item-text sidebar-link">Resources</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Representation of data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./networks.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#classification-basics" id="toc-classification-basics" class="nav-link active" data-scroll-target="#classification-basics"><span class="toc-section-number">3.1</span>  Classification basics</a>
  <ul class="collapse">
  <li><a href="#encoding-qualitative-data" id="toc-encoding-qualitative-data" class="nav-link" data-scroll-target="#encoding-qualitative-data"><span class="toc-section-number">3.1.1</span>  Encoding qualitative data</a></li>
  <li><a href="#walkthrough" id="toc-walkthrough" class="nav-link" data-scroll-target="#walkthrough"><span class="toc-section-number">3.1.2</span>  Walkthrough</a></li>
  </ul></li>
  <li><a href="#classifier-performance" id="toc-classifier-performance" class="nav-link" data-scroll-target="#classifier-performance"><span class="toc-section-number">3.2</span>  Classifier performance</a>
  <ul class="collapse">
  <li><a href="#traintest-paradigm" id="toc-traintest-paradigm" class="nav-link" data-scroll-target="#traintest-paradigm"><span class="toc-section-number">3.2.1</span>  Train–test paradigm</a></li>
  <li><a href="#binary-classifiers" id="toc-binary-classifiers" class="nav-link" data-scroll-target="#binary-classifiers"><span class="toc-section-number">3.2.2</span>  Binary classifiers</a></li>
  <li><a href="#multiclass-classifiers" id="toc-multiclass-classifiers" class="nav-link" data-scroll-target="#multiclass-classifiers"><span class="toc-section-number">3.2.3</span>  Multiclass classifiers</a></li>
  </ul></li>
  <li><a href="#decision-trees" id="toc-decision-trees" class="nav-link" data-scroll-target="#decision-trees"><span class="toc-section-number">3.3</span>  Decision trees</a>
  <ul class="collapse">
  <li><a href="#gini-impurity" id="toc-gini-impurity" class="nav-link" data-scroll-target="#gini-impurity"><span class="toc-section-number">3.3.1</span>  Gini impurity</a></li>
  <li><a href="#partitioning" id="toc-partitioning" class="nav-link" data-scroll-target="#partitioning"><span class="toc-section-number">3.3.2</span>  Partitioning</a></li>
  <li><a href="#toy-example" id="toc-toy-example" class="nav-link" data-scroll-target="#toy-example"><span class="toc-section-number">3.3.3</span>  Toy example</a></li>
  <li><a href="#penguin-data" id="toc-penguin-data" class="nav-link" data-scroll-target="#penguin-data"><span class="toc-section-number">3.3.4</span>  Penguin data</a></li>
  <li><a href="#limitations" id="toc-limitations" class="nav-link" data-scroll-target="#limitations"><span class="toc-section-number">3.3.5</span>  Limitations</a></li>
  </ul></li>
  <li><a href="#nearest-neighbors" id="toc-nearest-neighbors" class="nav-link" data-scroll-target="#nearest-neighbors"><span class="toc-section-number">3.4</span>  Nearest neighbors</a>
  <ul class="collapse">
  <li><a href="#norms" id="toc-norms" class="nav-link" data-scroll-target="#norms"><span class="toc-section-number">3.4.1</span>  Norms</a></li>
  <li><a href="#algorithm" id="toc-algorithm" class="nav-link" data-scroll-target="#algorithm"><span class="toc-section-number">3.4.2</span>  Algorithm</a></li>
  <li><a href="#knn-in-sklearn" id="toc-knn-in-sklearn" class="nav-link" data-scroll-target="#knn-in-sklearn"><span class="toc-section-number">3.4.3</span>  KNN in sklearn</a></li>
  <li><a href="#standardization" id="toc-standardization" class="nav-link" data-scroll-target="#standardization"><span class="toc-section-number">3.4.4</span>  Standardization</a></li>
  <li><a href="#pipelines" id="toc-pipelines" class="nav-link" data-scroll-target="#pipelines"><span class="toc-section-number">3.4.5</span>  Pipelines</a></li>
  </ul></li>
  <li><a href="#sec-class-quant" id="toc-sec-class-quant" class="nav-link" data-scroll-target="#sec-class-quant"><span class="toc-section-number">3.5</span>  Quantifying votes</a>
  <ul class="collapse">
  <li><a href="#roc-curve" id="toc-roc-curve" class="nav-link" data-scroll-target="#roc-curve"><span class="toc-section-number">3.5.1</span>  ROC curve</a></li>
  <li><a href="#auc" id="toc-auc" class="nav-link" data-scroll-target="#auc"><span class="toc-section-number">3.5.2</span>  AUC</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="hidden">
<p><span class="math display">\[
    \newcommand{\float}{\mathbb{F}}
    \newcommand{\real}{\mathbb{R}}
    \newcommand{\complex}{\mathbb{C}}
    \newcommand{\nat}{\mathbb{N}}
    \newcommand{\integer}{\mathbb{Z}}
    \newcommand{\bfa}{\mathbf{a}}
    \newcommand{\bfe}{\mathbf{e}}
    \newcommand{\bfh}{\mathbf{h}}
    \newcommand{\bfp}{\mathbf{p}}
    \newcommand{\bfq}{\mathbf{q}}
    \newcommand{\bfu}{\mathbf{u}}
    \newcommand{\bfv}{\mathbf{v}}
    \newcommand{\bfw}{\mathbf{w}}
    \newcommand{\bfx}{\mathbf{x}}
    \newcommand{\bfy}{\mathbf{y}}
    \newcommand{\bfz}{\mathbf{z}}
    \newcommand{\bfA}{\mathbf{A}}
    \newcommand{\bfW}{\mathbf{W}}
    \newcommand{\bfX}{\mathbf{X}}
    \newcommand{\bfzero}{\boldsymbol{0}}
    \newcommand{\bfmu}{\boldsymbol{\mu}}
    \newcommand{\TP}{\text{TP}}
    \newcommand{\TN}{\text{TN}}
    \newcommand{\FP}{\text{FP}}
    \newcommand{\FN}{\text{FN}}
    \newcommand{\rmn}[2]{\mathbb{R}^{#1 \times #2}}
    \newcommand{\dd}[2]{\frac{d #1}{d #2}}
    \newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
    \newcommand{\norm}[1]{\left\lVert \mathstrut #1 \right\rVert}
    \newcommand{\abs}[1]{\left\lvert \mathstrut #1 \right\rvert}
    \newcommand{\twonorm}[1]{\norm{#1}_2}
    \newcommand{\onenorm}[1]{\norm{#1}_1}
    \newcommand{\infnorm}[1]{\norm{#1}_\infty}
    \newcommand{\innerprod}[2]{\langle #1,#2 \rangle}
    \newcommand{\pr}[1]{^{(#1)}}
    \newcommand{\diag}{\operatorname{diag}}
    \newcommand{\sign}{\operatorname{sign}}
    \newcommand{\dist}{\operatorname{dist}}
    \newcommand{\simil}{\operatorname{sim}}
    \newcommand{\ee}{\times 10^}
    \newcommand{\floor}[1]{\lfloor#1\rfloor}
    \newcommand{\argmin}{\operatorname{argmin}}
    \newcommand{\E}[1]{\operatorname{\mathbb{E}}\left[\mathstrut #1\right]}
    \newcommand{\Cov}{\operatorname{Cov}}
    \newcommand{\logit}{\operatorname{logit}}
\]</span></p>
</div>
<p>Machine learning is the use of data to tune algorithms for making decisions or predictions. Unlike deduction based on reasoning from principles governing the application, machine learning is a “black box” that just adapts via training.</p>
<p>We divide machine learning into three major forms:</p>
<dl>
<dt>Supervised learning</dt>
<dd>
The training data only examples that include the answer (or <strong>label</strong>) we expect to get. The goals are to find important effects and/or to predict labels for previously unseen examples.
</dd>
<dt>Unsupervised learning</dt>
<dd>
The data is unlabeled, and the goal is to discover structure and relationships inherent to the data set.
</dd>
<dt>Reinforcement learning</dt>
<dd>
The data is unlabeled, but there are known rules and goals that can be encouraged through penalties and rewards.
</dd>
</dl>
<p>We start with supervised learning, which can be subdivided into two major areas:</p>
<ul>
<li><strong>Classification</strong>, in which the algorithm is expected to choose from among a finite set of options.</li>
<li><strong>Regression</strong>, in which the algorithm should predict the value of a quantitative variable.</li>
</ul>
<p>Most algorithms for one of these problems have counterparts in the other.</p>
<section id="classification-basics" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="classification-basics"><span class="header-section-number">3.1</span> Classification basics</h2>
<p>A single training example or sample is characterized by a <strong>feature vector</strong> <span class="math inline">\(\bfx\)</span> of <span class="math inline">\(d\)</span> real numbers and a <strong>label</strong> <span class="math inline">\(y\)</span> drawn from a finite set <span class="math inline">\(L\)</span>. If <span class="math inline">\(L\)</span> has only two members (say, “true” and “false”), we have a <strong>binary classification</strong> problem; otherwise, we have a <strong>multiclass</strong> problem.</p>
<p>When we have <span class="math inline">\(n\)</span> training samples, it’s natural to collect them into columns of a <strong>feature matrix</strong> <span class="math inline">\(\bfX\)</span> with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(d\)</span> columns. Using subscripts to represent the indexes of the matrix, we can write</p>
<p><span class="math display">\[
\bfX = \begin{bmatrix}
X_{11} &amp; X_{12} &amp; \cdots &amp; X_{1d} \\
X_{21} &amp; X_{22} &amp; \cdots &amp; X_{2d} \\
\vdots &amp; \vdots &amp;&amp; \vdots \\
X_{n1} &amp; X_{n2} &amp; \cdots &amp; X_{nd}
\end{bmatrix}.
\]</span></p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>A 2D array or matrix has elements that are addressed by two subscripts. These are always given in order <em>row</em>, <em>column</em>.</p>
<p>In math, we usually start the row and column indexes at 1, but Python starts them at 0.</p>
</div>
</div>
<p>Each row of the feature matrix is a single feature vector, while each column is the value for a single feature over the entire training set.</p>
<div id="exm-class-basketball" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.1 </strong></span>Suppose we want to train an algorithm to predict whether a basketball shot will score. For one shot, we might collect three coordinates to represent the launch point, three to represent the launch velocity, and three to represent the initial angular rotation (axis and magnitude). Thus each shot will require a feature vector of length 9. A collection of 200 sample shots would be encoded as a <span class="math inline">\(200\times 9\)</span> feature matrix.</p>
</div>
<p>We can also collect the associated training labels into the <strong>label vector</strong></p>
<p><span class="math display">\[
\bfy = \begin{bmatrix}
y_1 \\ y_2 \\ \vdots \\ y_n
\end{bmatrix}
\]</span></p>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>In linear algebra, the default shape for a vector is usually as a single column. In Python, a vector doesn’t exactly have a row or column orientation, though when it matters, a row shape is usually preferred.</p>
</div>
</div>
<p>Each component <span class="math inline">\(y_i\)</span> of the label vector is drawn from the label set <span class="math inline">\(L\)</span>.</p>
<!-- ### One-vs-rest

Many classification algorithms are derived for binary classification problems, in which there are only two unique labels. There are automatic ways of upgrading this capability to a multiclass problem. Probably the most useful one is the **one versus rest** or *one versus all* method. 

Suppose the label set $L$ has $m > 2$ members, which we can arbitrarily denote by the integers $1$ through $m$. We can define $m$ separate binary classification problems with the true/false label sets
$$
L_k = \{ y=k, y\neq k\},
$$
for each $k=1,2,\ldots,m$.  -->
<section id="encoding-qualitative-data" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="encoding-qualitative-data"><span class="header-section-number">3.1.1</span> Encoding qualitative data</h3>
<p>We have defined the features as numerical values. What should we do with qualitative data? We could arbitrarily assign numbers to possible values, such as “0=red”, “1=blue”, “2=yellow,” and so on. But this is not ideal: most of the time, we would not want to say that yellow is twice as far from red as it is from blue!</p>
<p>A better strategy is to use the one-hot or dummy encoding. If a particular feature can take on <span class="math inline">\(k\)</span> unique values, then we introduce <span class="math inline">\(k\)</span> new features indicating which value is present. (We can use <span class="math inline">\(k-1\)</span> dummy features if we interpret all-zeros to mean the <span class="math inline">\(k\)</span>th possibility.)</p>
</section>
<section id="walkthrough" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="walkthrough"><span class="header-section-number">3.1.2</span> Walkthrough</h3>
<p>The <em>scikit-learn</em> package <code>sklearn</code> is a collection of well-known machine learning algorithms and tools. It includes a few classic example datasets. We will load one derived from automatic recognition of handwritten digits.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets </span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>ds <span class="op">=</span> datasets.load_digits()        <span class="co"># loads a well-known dataset</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>X, dig <span class="op">=</span> ds[<span class="st">"data"</span>], ds[<span class="st">"target"</span>]      <span class="co"># assign feature matrix and label vector</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The feature matrix has shape"</span>, X.shape)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"The label vector has shape"</span>, dig.shape)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n, d <span class="op">=</span> X.shape</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"there are"</span>, d, <span class="st">"features and"</span>, n, <span class="st">"samples"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>The feature matrix has shape (1797, 64)
The label vector has shape (1797,)
there are 64 features and 1797 samples</code></pre>
</div>
</div>
<p>It happens that the 64 features are the pixel grayscale values from an <span class="math inline">\(8\times 8\)</span> bitmap of a handwritten digit. The labels are the integer values 0 through 9, indicating the true value of the digit.</p>
<p>Let’s consider the binary problem, “is this digit a 6?” That implies the following label vector:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> (dig <span class="op">==</span> <span class="dv">6</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Number of sixes in training set:"</span>, <span class="bu">sum</span>(y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Number of sixes in training set: 181</code></pre>
</div>
</div>
<p>The process of training a classifier is called <strong>fitting</strong>. We first have to import a particular classifier type, then create an instance of that type. Here, we choose one that we will study in a future section:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">20</span>)    <span class="co"># specification of the model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can fit the learner to the training data:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>knn.fit(X, y)                                 <span class="co"># training of the model</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="4">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=20)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=20)</pre></div></div></div></div></div>
</div>
</div>
<p>At this point, the classifier object <code>knn</code> has figured out what it needs from the training data. It has methods we can now call to make predictions and evaluate the quality of the results.</p>
<p>Each new prediction is for a <strong>query vector</strong> with 64 features. In practice, we can use a list in place of a vector for the query.</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> [<span class="dv">20</span>]<span class="op">*</span>d    <span class="co"># list with d copies of 20  </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The <code>predict</code> method of the classifier allows specifying multiple query vectors as rows of an array. In fact, it expects a 2D array in all cases, even if there is just one row.</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>Xq <span class="op">=</span> [ query ]    <span class="co"># 2D array with a single row</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The result of the prediction will be a vector of labels, one per row of the query.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get vector of predictions:</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>knn.predict(Xq)  </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="7">
<pre><code>array([False])</code></pre>
</div>
</div>
<div class="callout-important callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <code>predict</code> method requires a vector or list of query vectors or lists and it outputs a vector of classes. This is true even if there is just a single query.</p>
</div>
</div>
<p>At the moment, we don’t have any realistic query data at hand other than the training data. But we can investigate the question of how well the classifier does on that data, simply by using the feature matrix as the query:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get vector of predictions for the training set:</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> knn.predict(X)    </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we simply count up the number of correctly predicted labels and divide by the total number of samples to get the <strong>accuracy</strong> of the classifier.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> <span class="bu">sum</span>(yhat <span class="op">==</span> y) <span class="op">/</span> n    <span class="co"># fraction of correct predictions</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"accuracy is </span><span class="sc">{</span>acc<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy is 99.9%</code></pre>
</div>
</div>
<p>Not surprisingly, sklearn has functions for doing this measurement in fewer steps. The <code>metrics</code> module has functions that can compare true labels with predictions. In addition, each classifier object has a <code>score</code> method that allows you to skip finding the predictions vector yourself.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> accuracy_score</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Compare original labels to predictions:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> accuracy_score(y, yhat)    </span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"accuracy score is </span><span class="sc">{</span>acc<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Same result, if we don't want to keep the predicted values around:</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> knn.score(X, y)    </span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"knn score is </span><span class="sc">{</span>acc<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy score is 99.9%</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>knn score is 99.9%</code></pre>
</div>
</div>
<p>Does this mean that the classifier is a good one? The raw number looks great, but that question is more subtle than you would expect.</p>
</section>
</section>
<section id="classifier-performance" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="classifier-performance"><span class="header-section-number">3.2</span> Classifier performance</h2>
<p>Let’s return to the (previously cleaned) loan applications dataset.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>loans <span class="op">=</span> pd.read_csv(<span class="st">"loan_clean.csv"</span>)</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>loans.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>loan_amnt</th>
      <th>int_rate</th>
      <th>installment</th>
      <th>annual_inc</th>
      <th>dti</th>
      <th>delinq_2yrs</th>
      <th>delinq_amnt</th>
      <th>percent_funded</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>5000</td>
      <td>10.65</td>
      <td>162.87</td>
      <td>24000.0</td>
      <td>27.65</td>
      <td>0</td>
      <td>0</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2500</td>
      <td>15.27</td>
      <td>59.83</td>
      <td>30000.0</td>
      <td>1.00</td>
      <td>0</td>
      <td>0</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2400</td>
      <td>15.96</td>
      <td>84.33</td>
      <td>12252.0</td>
      <td>8.72</td>
      <td>0</td>
      <td>0</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>10000</td>
      <td>13.49</td>
      <td>339.31</td>
      <td>49200.0</td>
      <td>20.00</td>
      <td>0</td>
      <td>0</td>
      <td>100.0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3000</td>
      <td>12.69</td>
      <td>67.79</td>
      <td>80000.0</td>
      <td>17.94</td>
      <td>0</td>
      <td>0</td>
      <td>100.0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>We create a binary classification problem by labelling whether each loan was at least 95% funded. The other columns will form the features for the predictions.</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> loans.drop(<span class="st">"percent_funded"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> loans[<span class="st">"percent_funded"</span>] <span class="op">&gt;</span> <span class="dv">95</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout-tip callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Scikit-learn works as seamlessly with pandas data frames as it does with numerical arrays. In an application, it’s often worthwhile to refer to quantities by memorable names, rather than relying on numerical indexes into a matrix.</p>
</div>
</div>
<section id="traintest-paradigm" class="level3" data-number="3.2.1">
<h3 data-number="3.2.1" class="anchored" data-anchor-id="traintest-paradigm"><span class="header-section-number">3.2.1</span> Train–test paradigm</h3>
<p>It seems desirable for a classifier to work well on the samples it was trained on. But we probably want to do more than that.</p>
<div id="def-class-generalization" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.1 </strong></span>The performance of a predictor on previously unseen data is known as the <strong>generalization</strong> of the predictor.</p>
</div>
<p>In order to gauge generalization, we hold back some of the labeled data from training and use it only to test the performance. An <code>sklearn</code> helper function called <code>train_test_split</code> allows us to split off 20% of the data to use for testing. It’s usually recommended to shuffle the order of the samples before the split, and in order to make the results reproducible, we give a specific random seed to the RNG used for the shuffle.</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(X, y,</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>  test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>  shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a>  random_state<span class="op">=</span><span class="dv">3</span></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"There are"</span>, X_tr.shape[<span class="dv">0</span>], <span class="st">"training samples."</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"There are"</span>, X_te.shape[<span class="dv">0</span>], <span class="st">"testing samples."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>There are 31773 training samples.
There are 7944 testing samples.</code></pre>
</div>
</div>
<p>We can check that the test and train labels have similar characteristics:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"labels in the training set:"</span>)</span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( pd.Series(y_tr).describe() )</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">labels in the testing set:"</span>)</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( pd.Series(y_te).describe() )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>labels in the training set:
count     31773
unique        2
top        True
freq      30351
Name: percent_funded, dtype: object

labels in the testing set:
count     7944
unique       2
top       True
freq      7575
Name: percent_funded, dtype: object</code></pre>
</div>
</div>
<p>Now we train on the training data…</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>knn.fit(X_tr, y_tr)    <span class="co"># fit only to train set</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="15">
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier(n_neighbors=9)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked=""><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier(n_neighbors=9)</pre></div></div></div></div></div>
</div>
</div>
<p>…and test on the rest.</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> knn.score(X_te, y_te)    <span class="co"># score only on test set</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"test accuracy is </span><span class="sc">{</span>acc<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>test accuracy is 95.6%</code></pre>
</div>
</div>
<p>This seems like a high accuracy, perhaps. But consider that the vast majority of loans were funded:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>funded <span class="op">=</span> <span class="bu">sum</span>(y)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>funded<span class="op">/</span><span class="bu">len</span>(y)<span class="sc">:.1%}</span><span class="ss"> were funded"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>95.5% were funded</code></pre>
</div>
</div>
<p>Therefore, an algorithm that simply “predicts” funding every single loan could do as well as the trained classifier!</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> metrics</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>generous <span class="op">=</span> [<span class="va">True</span>]<span class="op">*</span><span class="bu">len</span>(y_te)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> metrics.accuracy_score(y_te, generous)</span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"fund-them-all accuracy is </span><span class="sc">{</span>acc<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>fund-them-all accuracy is 95.4%</code></pre>
</div>
</div>
<p>In this context, our trained classifier is not impressive at all. We need a metric other than accuracy.</p>
</section>
<section id="binary-classifiers" class="level3" data-number="3.2.2">
<h3 data-number="3.2.2" class="anchored" data-anchor-id="binary-classifiers"><span class="header-section-number">3.2.2</span> Binary classifiers</h3>
<p>A binary classifier is one that produces just two unique labels, which we call “yes” and “no” here. To fully understand the performance of a binary classifier, we have to account for four cases:</p>
<ul>
<li>True positives (TP): Predicts “yes”, actually is “yes”</li>
<li>False positives (FP): Predicts “yes”, actually is “no”</li>
<li>True negatives (TN): Predicts “no”, actually is “no”</li>
<li>False negatives (FN): Predicts “no”, actually is “yes”</li>
</ul>
<p>The four cases correspond to a 2×2 table according to the states of the prediction and <em>ground truth</em>, which is the accepted correct value (i.e., the given label). The table can be filled with counts or percentages of tested instances, to create a <strong>confusion matrix</strong>, as illustrated in <a href="#fig-class-confusion">Figure&nbsp;<span class="quarto-unresolved-ref">fig-class-confusion</span></a>.</p>
<div id="fig-class-confusion" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="confusion.svg" class="img-fluid figure-img" alt="A binary confusion matrix."></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;3.1: Confusion matrix</figcaption><p></p>
</figure>
</div>
<p><code>sklearn</code> makes it straightforward to compute a confusion matrix:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> knn.predict(X_te)</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> metrics.confusion_matrix(y_te, yhat, labels<span class="op">=</span>[<span class="va">True</span>,<span class="va">False</span>])</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>lbl <span class="op">=</span> [<span class="st">"fund"</span>, <span class="st">"reject"</span>]</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>metrics.ConfusionMatrixDisplay(C, display_labels<span class="op">=</span>lbl).plot()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-20-output-1.png" width="542" height="429"></p>
</div>
</div>
<div class="callout-caution callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>It’s advisable to call <code>confusion_matrix</code> with the <code>labels</code> argument, even though it is optional, in order to have control over the ordering within the matrix. In particular, <code>False</code> &lt; <code>True</code>, so the default is to count the upper left corner of the matrix as “true negatives,” assuming that <code>False</code> represents a negative result.</p>
</div>
</div>
<p>Hence, there are 7570 true positives. Therefore, the <strong>accuracy</strong> is</p>
<p><span class="math display">\[
\text{accuracy} = \frac{\TP + \TN}{\TP + \FP + \TN + \FN} = \frac{7570}{7944} \approx 0.95292
\]</span></p>
<p>i.e., 95.3%. However, there are four other useful quantities defined by putting a “number correct” value in the numerator and the sum of a confusion matrix row or column in the denominator:</p>
<p><span class="math display">\[
\begin{split}
\text{recall (aka sensitivity)} &amp;= \frac{\TP}{\TP + \FN} \\[2mm]
\text{specificity} &amp;= \frac{\TN}{\TN + \FP} \\[2mm]
\text{precision} &amp;= \frac{\TP}{\TP + \FP} \\[2mm]
\text{negative predictive value (NPV)} &amp;= \frac{\TN}{\TN + \FN}
\end{split}
\]</span></p>
<p>In words, these metrics answer the following questions:</p>
<ul>
<li><strong>recall</strong> How often are actual “yes” cases predicted correctly?</li>
<li><strong>specificity</strong> How often are actual “no” cases predicted correctly?</li>
<li><strong>precision</strong> How often are the “yes” predictions correct?</li>
<li><strong>NPV</strong> How often are the “no” predictions correct?</li>
</ul>
<p>For our loan classifier, here are the scores:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>TP,FN,FP,TN <span class="op">=</span> C.ravel()    <span class="co"># grab the 4 values in the confusion matrix</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"recall = </span><span class="sc">{</span>TP<span class="op">/</span>(TP<span class="op">+</span>FN)<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"specificity = </span><span class="sc">{</span>TN<span class="op">/</span>(TN<span class="op">+</span>FP)<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"precision = </span><span class="sc">{</span>TP<span class="op">/</span>(TP<span class="op">+</span>FP)<span class="sc">:.1%}</span><span class="ss">"</span>)</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"NPV = </span><span class="sc">{</span>TN<span class="op">/</span>(TN<span class="op">+</span>FN)<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>recall = 99.9%
specificity = 7.0%
precision = 95.7%
NPV = 83.9%</code></pre>
</div>
</div>
<p>The high recall rate means that few who ought to get a loan will go away disappointed. However, the low specificity would be concerning to those providing the funds, because almost all of those who should be rejected will be approved by the classifier.</p>
<p>In <code>sklearn.metrics</code> there are functions to compute recall and precision without reference to the confusion matrix. You must put the ground-truth labels before the predicted labels, and you should also specify which of the labels corresponds to a “positive” result. Swapping the “positive” role effectively swaps recall with specificity, and precision with NPV.</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pos <span class="kw">in</span> [<span class="va">True</span>,<span class="va">False</span>]:</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="st">"With"</span>, pos, <span class="st">"as positive:"</span>)</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>  s <span class="op">=</span> metrics.recall_score(y_te, yhat, pos_label<span class="op">=</span>pos)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"    recall is </span><span class="sc">{</span>s<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>  s <span class="op">=</span> metrics.precision_score(y_te, yhat, pos_label<span class="op">=</span>pos) </span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>(<span class="ss">f"    precision is </span><span class="sc">{</span>s<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>  <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>With True as positive:</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>    recall is 0.999</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>    precision is 0.957

With False as positive:
    recall is 0.070
    precision is 0.839
</code></pre>
</div>
</div>
<p>There are several ways to combine the measures above into a single value. None is universally best, because different applications emphasize different aspects of performance. One of the most popular is the <strong><em>F</em>₁ score</strong>, which is the harmonic mean of the precision and the recall:</p>
<p><span class="math display">\[
F_1 = \left[ \frac{1}{2} \left(\frac{\TP + \FN}{\TP} + \frac{\TP+\FP}{\TP} \right)  \right]^{-1} = \frac{2\TP}{2\TP+\FN+\FP}.
\]</span></p>
<p>This score varies between 0 (poor) and 1 (ideal). If one of the quantities is much smaller than the other, their harmonic mean will be close to the small value. Thus, <em>F</em>₁ score punishes a classifier if either recall or precision is poor.</p>
<p>Another composite score is <strong>balanced accuracy</strong>, which is the arithmetic mean of recall and specificity. It also ranges from 0 to 1, with 1 meaning perfect accuracy.</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( <span class="st">"F1:"</span>, metrics.f1_score(y_te, yhat) )</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( <span class="st">"Balanced:"</span>, metrics.balanced_accuracy_score(y_te, yhat) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>F1: 0.9775309917355371
Balanced: 0.5349003193002227</code></pre>
</div>
</div>
<p>The loan classifier trained above has excellent recall, respectable precision, and terrible specificity, resulting in a good <em>F</em>₁ score and a low balanced accuracy score.</p>
<div id="exm-class-scores" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.2 </strong></span>If <span class="math inline">\(k\)</span> of the <span class="math inline">\(n\)</span> testing samples were funded loans, then the fund-them-all loan classifier has</p>
<p><span class="math display">\[
\TP = k,\, \TN = 0,\, \FP = n-k,\, \FN = 0.
\]</span></p>
<p>Its <em>F</em>₁ score is thus</p>
<p><span class="math display">\[
\frac{2\TP}{2\TP+\FN+\FP} = \frac{2k}{2k+n-k} = \frac{2k}{k+n}.
\]</span></p>
<p>If the fraction of funded samples in the test set is <span class="math inline">\(k/n=a\)</span>, then the accuracy of this classifier is <span class="math inline">\(a\)</span>. Its <em>F</em>₁ score is <span class="math inline">\(2a/(1+a)\)</span>, which is larger than <span class="math inline">\(a\)</span> unless <span class="math inline">\(a=1\)</span>. That’s because the true positives greatly outweigh the other confusion matrix values.</p>
<p>The balanced accuracy is</p>
<p><span class="math display">\[
\frac{1}{2} \left(\frac{\TP}{\TP+\FN} + \frac{\TN}{\TN+\FP} \right)  = \frac{1}{2},
\]</span></p>
<p>independently of <span class="math inline">\(a\)</span>. This quantity is sensitive to the low specificity.</p>
</div>
</section>
<section id="multiclass-classifiers" class="level3" data-number="3.2.3">
<h3 data-number="3.2.3" class="anchored" data-anchor-id="multiclass-classifiers"><span class="header-section-number">3.2.3</span> Multiclass classifiers</h3>
<p>When there are more than two unique possible labels, these metrics can be applied using the <strong>one-vs-rest</strong> paradigm. For <span class="math inline">\(K\)</span> unique labels, this paradigm poses <span class="math inline">\(K\)</span> binary questions: “Is it in class 1, or not?”, “Is it in class 2, or not?”, etc. The confusion matrix becomes <span class="math inline">\(K\times K\)</span>.</p>
<p>It’s easiest to see how this works by an example. We will load a dataset on the characteristics of cars and use quantitative factors to predict the region of origin.</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a>cars <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>cars.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="23">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>mpg</th>
      <th>cylinders</th>
      <th>displacement</th>
      <th>horsepower</th>
      <th>weight</th>
      <th>acceleration</th>
      <th>model_year</th>
      <th>origin</th>
      <th>name</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>18.0</td>
      <td>8</td>
      <td>307.0</td>
      <td>130.0</td>
      <td>3504</td>
      <td>12.0</td>
      <td>70</td>
      <td>usa</td>
      <td>chevrolet chevelle malibu</td>
    </tr>
    <tr>
      <th>1</th>
      <td>15.0</td>
      <td>8</td>
      <td>350.0</td>
      <td>165.0</td>
      <td>3693</td>
      <td>11.5</td>
      <td>70</td>
      <td>usa</td>
      <td>buick skylark 320</td>
    </tr>
    <tr>
      <th>2</th>
      <td>18.0</td>
      <td>8</td>
      <td>318.0</td>
      <td>150.0</td>
      <td>3436</td>
      <td>11.0</td>
      <td>70</td>
      <td>usa</td>
      <td>plymouth satellite</td>
    </tr>
    <tr>
      <th>3</th>
      <td>16.0</td>
      <td>8</td>
      <td>304.0</td>
      <td>150.0</td>
      <td>3433</td>
      <td>12.0</td>
      <td>70</td>
      <td>usa</td>
      <td>amc rebel sst</td>
    </tr>
    <tr>
      <th>4</th>
      <td>17.0</td>
      <td>8</td>
      <td>302.0</td>
      <td>140.0</td>
      <td>3449</td>
      <td>10.5</td>
      <td>70</td>
      <td>usa</td>
      <td>ford torino</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"cylinders"</span>, <span class="st">"horsepower"</span>, <span class="st">"weight"</span>, <span class="st">"acceleration"</span>, <span class="st">"mpg"</span>]</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars[features]</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pd.Categorical(cars[<span class="st">"origin"</span>])</span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(X.shape[<span class="dv">0</span>], <span class="st">"samples and"</span>, X.shape[<span class="dv">1</span>], <span class="st">"features"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>392 samples and 5 features</code></pre>
</div>
</div>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>  X, y, </span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a>  test_size<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a>  shuffle<span class="op">=</span><span class="va">True</span>, </span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a>  random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">8</span>)</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>knn.fit(X_tr, y_tr)</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> knn.predict(X_te)</span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"accuracy is </span><span class="sc">{</span>metrics<span class="sc">.</span>accuracy_score(y_te, yhat)<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy is 77.2%</code></pre>
</div>
</div>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>labels <span class="op">=</span> y.categories</span>
<span id="cb44-2"><a href="#cb44-2" aria-hidden="true" tabindex="-1"></a>C <span class="op">=</span> metrics.confusion_matrix(y_te, yhat, labels<span class="op">=</span>labels)</span>
<span id="cb44-3"><a href="#cb44-3" aria-hidden="true" tabindex="-1"></a>metrics.ConfusionMatrixDisplay(C, display_labels<span class="op">=</span>labels).plot()<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-27-output-1.png" width="534" height="429"></p>
</div>
</div>
<p>From the confusion matrix above we can see that, for example, out of 54 predictions of “usa” on the test set, there are 8 total false positives, in the sense that the actual labels were otherwise.</p>
<p>We also get <span class="math inline">\(K\)</span> versions of the metrics like accuracy, recall, <em>F</em>₁ score, and so on. We can get all the individual precision scores, say, automatically:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a>prec <span class="op">=</span> metrics.precision_score(y_te, yhat, average<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (i,p) <span class="kw">in</span> <span class="bu">enumerate</span>(prec): </span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>labels[i]<span class="sc">}</span><span class="ss">: </span><span class="sc">{</span>p<span class="sc">:.1%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>europe: 62.5%
japan: 58.8%
usa: 85.2%</code></pre>
</div>
</div>
<p>To get a composite precision score, we have to specify an averaging method. The <code>"macro"</code> option simply takes the mean of the vector above.</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>mac <span class="op">=</span> metrics.precision_score(y_te, yhat, average<span class="op">=</span><span class="st">"macro"</span>)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(mac)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.6883623819898329</code></pre>
</div>
</div>
<p>There are other ways to perform the averaging, depending on whether poorly represented classes should be weighted more weakly than others.</p>
</section>
</section>
<section id="decision-trees" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="decision-trees"><span class="header-section-number">3.3</span> Decision trees</h2>
<p>A decision tree is much like playing “Twenty Questions.” A question is asked, and the answer reduces the possible results, leading to a new question. <strong>CART</strong> (Classification And Regression Tree) is a popular method for systematizing the idea.</p>
<p>Given feature vectors <span class="math inline">\(\bfx_1,\ldots,\bfx_n\)</span> with labels <span class="math inline">\(y_1,\ldots,y_n\)</span>, the immediate goal is to partition the samples into subsets whose labels are as uniform as possible. The process is then repeated recursively on the subsets. Defining a measurement of label uniformity is a key step.</p>
<section id="gini-impurity" class="level3" data-number="3.3.1">
<h3 data-number="3.3.1" class="anchored" data-anchor-id="gini-impurity"><span class="header-section-number">3.3.1</span> Gini impurity</h3>
<p>Let <span class="math inline">\(S\)</span> be a subset of the samples, given as a list of indices into the original set. Suppose there are <span class="math inline">\(K\)</span> unique labels, which we denote <span class="math inline">\(1,2,\ldots,K\)</span>. Define</p>
<p><span class="math display">\[
p_k = \frac{1}{ |S| } \sum_{i\in S} \mathbb{1}_k(y_i),
\]</span></p>
<p>where <span class="math inline">\(|S|\)</span> is the number of elements in <span class="math inline">\(S\)</span> and <span class="math inline">\(\mathbb{1}_k\)</span> is the <strong>indicator function</strong></p>
<p><span class="math display">\[
\mathbb{1}_k(t) = \begin{cases}
  1, &amp; \text{if } t=k, \\
  0, &amp; \text{otherwise.}
  \end{cases}
\]</span></p>
<p>In words, <span class="math inline">\(p_k\)</span> is the proportion of samples in <span class="math inline">\(S\)</span> that have label <span class="math inline">\(k\)</span>. Then the <strong>Gini impurity</strong> is defined as</p>
<p><span class="math display">\[
H(S) = \sum_{k=1}^K p_k(1-p_k).
\]</span></p>
<p>If one of the <span class="math inline">\(p_k\)</span> is 1, then the others are all zero and <span class="math inline">\(H(S)=0\)</span>. This is considered optimal. At the other extreme, if <span class="math inline">\(p_k=1/K\)</span> for all <span class="math inline">\(k\)</span>, then</p>
<p><span class="math display">\[
H(S) = \sum_{k=1}^K \frac{1}{K} \left(1 - \frac{1}{K} \right) = K\cdot \frac{1}{K}\cdot\frac{K-1}{K} = \frac{K-1}{K} &lt; 1.
\]</span></p>
<div id="exm-class-gini" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.3 </strong></span>Suppose a set <span class="math inline">\(S\)</span> has <span class="math inline">\(n\)</span> members with label 1, 1 member with label 2, and 1 member with label 3. What is the Gini impurity of <span class="math inline">\(S\)</span>?</p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>We have <span class="math inline">\(p_1=n/(n+2)\)</span>, <span class="math inline">\(p_2=p_3=1/(n+2)\)</span>. Hence</p>
<p><span class="math display">\[
\begin{split}
    H(S) &amp;= \frac{n}{n+2}\left( 1 - \frac{n}{n+2} \right) + 2 \frac{1}{n+2}\left( 1 - \frac{1}{n+2} \right) \\
    &amp;= \frac{n}{n+2}\frac{2}{n+2} + \frac{2}{n+2}\frac{n+1}{n+2} \\
    &amp;= \frac{4n+2}{(n+2)^2}.
\end{split}
\]</span></p>
<p>This value is 1/2 for <span class="math inline">\(n=0\)</span> and approaches zero as <span class="math inline">\(n\to\infty\)</span>.</p>
</div>
</div>
</section>
<section id="partitioning" class="level3" data-number="3.3.2">
<h3 data-number="3.3.2" class="anchored" data-anchor-id="partitioning"><span class="header-section-number">3.3.2</span> Partitioning</h3>
<p>Now we can describe the partition process. If <span class="math inline">\(j\)</span> is a dimension (feature) number and <span class="math inline">\(\theta\)</span> is a numerical threshold, then the sample set can be partitioned into complementary sets <span class="math inline">\(S_L\)</span>, in which <span class="math inline">\(x_j \le \theta\)</span>, and <span class="math inline">\(S_R\)</span>, in which <span class="math inline">\(x_j &gt; \theta\)</span>. Define the <strong>total impurity</strong> of the partition to be</p>
<p><span class="math display">\[
Q(j,\theta) = \lvert S\rvert\, H(S) + \lvert T \rvert \, H(T).
\]</span></p>
<p>Choose the <span class="math inline">\((j,\theta)\)</span> that minimize <span class="math inline">\(Q\)</span>, and then recursively partition <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span>.</p>
<div id="exm-class-tree-partition" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.4 </strong></span>Suppose the 1D real samples <span class="math inline">\(x_i=i\)</span> for <span class="math inline">\(i=0,1,2,3\)</span> have labels A,B,A,B. What is the optimal partition?</p>
<div class="solution proof">
<p><span class="proof-title"><em>Solution</em>. </span>There are three ways to partition them.</p>
<ul>
<li><span class="math inline">\(S=\{0\}\)</span>, <span class="math inline">\(T=\{1,2,3\}\)</span>. We have <span class="math inline">\(H(S)=0\)</span> and <span class="math inline">\(H(T)=(2/3)(1/3)+(1/3)(2/3)=4/9\)</span>. Hence the total impurity for this partition is <span class="math inline">\((1)(0) + (3)(4/9) = 4/3\)</span>.</li>
<li><span class="math inline">\(S=\{0,1\}\)</span>, <span class="math inline">\(T=\{2,3\}\)</span>. Then <span class="math inline">\(H(S)=H(T)=2(1/2)(1/2)=1/2\)</span>, and the total impurity is <span class="math inline">\((2)(1/2)+(2)(1/2)=2\)</span>.</li>
<li><span class="math inline">\(S=\{0,1,2\}\)</span>, <span class="math inline">\(T=\{3\}\)</span>. This arrangement is the same as the first case with <span class="math inline">\(A↔B\)</span>.</li>
</ul>
<p>The best partition threshold is <span class="math inline">\(x\le 0\)</span> (or <span class="math inline">\(x\le 2\)</span>, which is equivalent).</p>
</div>
</div>
</section>
<section id="toy-example" class="level3" data-number="3.3.3">
<h3 data-number="3.3.3" class="anchored" data-anchor-id="toy-example"><span class="header-section-number">3.3.3</span> Toy example</h3>
<p>We first create a toy dataset with 20 random points, with two subsets of 10 that are shifted left/right a bit.</p>
<div class="cell" data-execution_count="30">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> default_rng</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> default_rng(<span class="dv">1</span>)</span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> rng.random((<span class="dv">10</span>,<span class="dv">2</span>))</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>x1[:,<span class="dv">0</span>] <span class="op">-=</span> <span class="fl">0.25</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> rng.random((<span class="dv">10</span>,<span class="dv">2</span>))</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>x2[:,<span class="dv">0</span>] <span class="op">+=</span> <span class="fl">0.25</span></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack((x1,x2))</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.hstack(([<span class="dv">1</span>]<span class="op">*</span><span class="dv">10</span>,[<span class="dv">2</span>]<span class="op">*</span><span class="dv">10</span>))</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame( {<span class="st">"x₁"</span>:X[:,<span class="dv">0</span>], <span class="st">"x₂"</span>:X[:,<span class="dv">1</span>], <span class="st">"y"</span>:y} )</span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">"x₁"</span>, y<span class="op">=</span><span class="st">"x₂"</span>, hue<span class="op">=</span><span class="st">"y"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-30-output-1.png" width="589" height="429"></p>
</div>
</div>
<p>Now we create a decision tree for these samples.</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier, plot_tree</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb50-3"><a href="#cb50-3" aria-hidden="true" tabindex="-1"></a>t.fit(X,y)</span>
<span id="cb50-4"><a href="#cb50-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb50-5"><a href="#cb50-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb50-6"><a href="#cb50-6" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">11</span>), dpi<span class="op">=</span><span class="dv">160</span>)</span>
<span id="cb50-7"><a href="#cb50-7" aria-hidden="true" tabindex="-1"></a>plot_tree(t, feature_names<span class="op">=</span>[<span class="st">"x₁"</span>, <span class="st">"x₂"</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-31-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The root of the tree (at the top) shows that the best split was found at the vertical line <span class="math inline">\(x_1=0.644\)</span>. To the right of that line is a Gini value of zero: 8 samples, all with label 2. Thus, any future prediction by this tree will immediately return label 2 if the first feature of the input exceeds 0.644. Otherwise, it moves to the left child node and tests whether the second feature is greater than <span class="math inline">\(0.96\)</span>. This splits along a horizontal line, above which there is a single sample with label 2. And so on.</p>
<p>Notice that the bottom right node has a nonzero Gini impurity. This node could be partitioned, but the classifier was constrained to stop at a depth of 3. If a prediction ends up here, then the classifier returns label 1, which is the most likely outcome.</p>
<p>Because we can follow the decision tree’s logic step by step, we say it is highly <strong>interpretable</strong>. The transparency of the prediction algorithm is an attractive aspect of decision trees, although this advantage can weaken as the numbers of features and observations increase.</p>
</section>
<section id="penguin-data" class="level3" data-number="3.3.4">
<h3 data-number="3.3.4" class="anchored" data-anchor-id="penguin-data"><span class="header-section-number">3.3.4</span> Penguin data</h3>
<p>We return to the penguins. There is no need to standardize the columns for a decision tree, because each feature is considered on its own.</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>pen <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>)</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a>pen <span class="op">=</span> pen.dropna()</span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">"bill_length_mm"</span>,</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>  <span class="st">"bill_depth_mm"</span>,</span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>  <span class="st">"flipper_length_mm"</span>,</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a>  <span class="st">"body_mass_g"</span></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> pen[features]</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> pen[<span class="st">"species"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We get some interesting information from looking at the top levels of a decision tree trained on the full dataset.</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>dt <span class="op">=</span> DecisionTreeClassifier(max_depth<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>dt.fit(X, y)</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">18</span>,<span class="dv">11</span>), dpi<span class="op">=</span><span class="dv">160</span>)</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>plot_tree(dt, max_depth<span class="op">=</span><span class="dv">2</span>, feature_names<span class="op">=</span>features)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-33-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The most determinative feature for identifying the species is the flipper length. If it exceeds 206.5 mm, then the penguin is rather likely to be a Gentoo.</p>
<p>We can measure the relative importance of each feature by comparing their total contributions to reducing the Gini index. This is known as <strong>Gini importance</strong>.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>pd.Series(dt.feature_importances_, index<span class="op">=</span>features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>bill_length_mm       0.367310
bill_depth_mm        0.065186
flipper_length_mm    0.553866
body_mass_g          0.013638
dtype: float64</code></pre>
</div>
</div>
<p>Flipper length alone accounts for about half of the resolving power of the tree, followed in importance by the bill length. The other measurements apparently have little discriminative value.</p>
<p>In order to assess the effectiveness of the tree, we use the train–test paradigm.</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix,classification_report</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>  X, y,</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>  test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>  shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a>  random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>dt.fit(X_tr, y_tr)</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> dt.predict(X_te)</span>
<span id="cb55-13"><a href="#cb55-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( confusion_matrix(y_te, yhat) )</span>
<span id="cb55-14"><a href="#cb55-14" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( classification_report(y_te, yhat) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[38  1  0]
 [ 3  7  0]
 [ 0  0 18]]
              precision    recall  f1-score   support

      Adelie       0.93      0.97      0.95        39
   Chinstrap       0.88      0.70      0.78        10
      Gentoo       1.00      1.00      1.00        18

    accuracy                           0.94        67
   macro avg       0.93      0.89      0.91        67
weighted avg       0.94      0.94      0.94        67
</code></pre>
</div>
</div>
<p>The performance is quite good, although the Chinstrap case is hindered by the relatively low number of training examples:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a>y_tr.value_counts()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>Adelie       107
Gentoo       101
Chinstrap     58
Name: species, dtype: int64</code></pre>
</div>
</div>
</section>
<section id="limitations" class="level3" data-number="3.3.5">
<h3 data-number="3.3.5" class="anchored" data-anchor-id="limitations"><span class="header-section-number">3.3.5</span> Limitations</h3>
<p>Decision trees depend sensitively on the sample locations. A tree trained on one data subset may not do well on a new set. A small change can completely rewrite large parts of the tree, which gives a caveat about interpretation. Also, the partition algorithm, which is <em>greedy</em> by doing the best thing at the moment, does not necessarily find a globally optimal tree, or even a nearby one.</p>
</section>
</section>
<section id="nearest-neighbors" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="nearest-neighbors"><span class="header-section-number">3.4</span> Nearest neighbors</h2>
<p>Our first learning algorithm is conceptually simple: Given a new point to classify, survey the nearest known examples and choose the most frequently occurring class. This is called the <strong><span class="math inline">\(k\)</span> nearest neighbors</strong> (KNN) algorithm, where <span class="math inline">\(k\)</span> is the number of neighboring examples to survey.</p>
<section id="norms" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="norms"><span class="header-section-number">3.4.1</span> Norms</h3>
<p>The existence of “closest” examples means that we need to define a notion of distance in spaces of any dimension. Let <span class="math inline">\(\real^d\)</span> be the space of vectors with <span class="math inline">\(d\)</span> real components, and let <span class="math inline">\(\bfzero\)</span> be the vector of all zeros.</p>
<div id="def-class-norm" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.2 </strong></span>A <strong>norm</strong> is a function <span class="math inline">\(\norm{\bfx}\)</span> on <span class="math inline">\(\real^d\)</span> that satisfies the following properties:</p>
<p><span class="math display">\[
\begin{split}
\norm{\bfzero} &amp;= 0,  \\
\norm{\bfx} &amp;&gt; 0 \text{ if $\bfx$ is a nonzero vector}, \\
\norm{c\bfx} &amp;= |c| \, \norm{x} \text{ for any real number $c$}, \\
\norm{\bfx + \bfy } &amp;\le \norm{\bfx} + \norm{\bfy}
\end{split}
\]</span></p>
</div>
<p>The last inequality above is called the <strong>triangle inequality</strong>. It turns out that these four characteristics are all we expect from a function that behaves like a distance.</p>
<p>On the number line (i.e., <span class="math inline">\(\real^1\)</span>), the distance between two values is just the absolute value of their difference, <span class="math inline">\(\abs{x-y}\)</span>. In <span class="math inline">\(\real^d\)</span>, the distance between two vectors is the norm of their difference, <span class="math inline">\(\norm{ \bfx - \bfy }\)</span>.</p>
<p>There are three commonly used norms:</p>
<ul>
<li>The <strong>2-norm</strong> or Euclidean norm: <span class="math display">\[
\twonorm{\bfx} = \bigl(x_1^2 + x_2^2 + \cdots + x_d^2\bigr)^{1/2}.
\]</span></li>
<li>The <strong>1-norm</strong> or Manhattan norm: <span class="math display">\[
\onenorm{\bfx} = |x_1| + |x_2| + \cdots + |x_d|.
\]</span></li>
<li>The <strong><span class="math inline">\(\infty\)</span>-norm</strong> or max norm: <span class="math display">\[\infnorm{\bfx} = \max_i \abs{x_i}.\]</span></li>
</ul>
<p>The Euclidean norm generalizes ordinary geometric distance in <span class="math inline">\(\real^2\)</span> and <span class="math inline">\(\real^3\)</span> and is usually considered the default. One of its most important features is that <span class="math inline">\(\twonorm{\bfx}^2\)</span> is a differentiable function of the components of $%.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>When <span class="math inline">\(\norm{\,}\)</span> is used with no subscript, it’s usually meant to be the 2-norm, but sometimes it means a generic, unspecified norm.</p>
</div>
</div>
</section>
<section id="algorithm" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="algorithm"><span class="header-section-number">3.4.2</span> Algorithm</h3>
<p>As data, we are given labeled examples <span class="math inline">\(\bfx_1,\ldots,\bfx_n\)</span> in <span class="math inline">\(\real^d\)</span>. Given a new query vector <span class="math inline">\(\bfx\)</span>, find the <span class="math inline">\(k\)</span> labeled vectors closest to <span class="math inline">\(\bfx\)</span> and choose the most frequently occurring label among them. Ties can be broken randomly.</p>
<p>KNN divides up the feature space into domains that are dominated by nearby instances. The boundaries between those domains, called <strong>decision boundaries</strong>, can be fairly complicated, though, as shown in the animation below.</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" class="video-js vjs-default-skin vjs-fluid" controls="" preload="auto" data-setup="{}" title=""><source src="_media/knn_demo.mp4"></video></div>
<p>Implementation of KNN is straightforward for small data sets, but requires care to get reasonable execution efficiency for large sets.</p>
</section>
<section id="knn-in-sklearn" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="knn-in-sklearn"><span class="header-section-number">3.4.3</span> KNN in sklearn</h3>
<p>Let’s revisit the penguins. We use <code>dropna</code> to drop any rows with missing values.</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> penguins.dropna()</span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a>penguins</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>species</th>
      <th>island</th>
      <th>bill_length_mm</th>
      <th>bill_depth_mm</th>
      <th>flipper_length_mm</th>
      <th>body_mass_g</th>
      <th>sex</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.1</td>
      <td>18.7</td>
      <td>181.0</td>
      <td>3750.0</td>
      <td>Male</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.5</td>
      <td>17.4</td>
      <td>186.0</td>
      <td>3800.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>40.3</td>
      <td>18.0</td>
      <td>195.0</td>
      <td>3250.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>36.7</td>
      <td>19.3</td>
      <td>193.0</td>
      <td>3450.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Adelie</td>
      <td>Torgersen</td>
      <td>39.3</td>
      <td>20.6</td>
      <td>190.0</td>
      <td>3650.0</td>
      <td>Male</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>338</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>47.2</td>
      <td>13.7</td>
      <td>214.0</td>
      <td>4925.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>340</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>46.8</td>
      <td>14.3</td>
      <td>215.0</td>
      <td>4850.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>341</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>50.4</td>
      <td>15.7</td>
      <td>222.0</td>
      <td>5750.0</td>
      <td>Male</td>
    </tr>
    <tr>
      <th>342</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>45.2</td>
      <td>14.8</td>
      <td>212.0</td>
      <td>5200.0</td>
      <td>Female</td>
    </tr>
    <tr>
      <th>343</th>
      <td>Gentoo</td>
      <td>Biscoe</td>
      <td>49.9</td>
      <td>16.1</td>
      <td>213.0</td>
      <td>5400.0</td>
      <td>Male</td>
    </tr>
  </tbody>
</table>
<p>333 rows × 7 columns</p>
</div>
</div>
</div>
<p>The data set has four quantitative columns that we use as features, and the species name is the label.</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>  <span class="st">"bill_length_mm"</span>,</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>  <span class="st">"bill_depth_mm"</span>,</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>  <span class="st">"flipper_length_mm"</span>,</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>  <span class="st">"body_mass_g"</span></span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins[features]</span>
<span id="cb60-8"><a href="#cb60-8" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> penguins[<span class="st">"species"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Each type of classifier has to be imported before its first use in a session. (Importing more than once does no harm.)</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>knn.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">
<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>KNeighborsClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked=""><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">KNeighborsClassifier</label><div class="sk-toggleable__content"><pre>KNeighborsClassifier()</pre></div></div></div></div></div>
</div>
</div>
<p>We can manually find the neighbors of a new vector. However, we have to make the query in the form of a data frame, since that is how the training data was provided. Here we make a query frame for values very close to the ones in the first row of the data.</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>vals <span class="op">=</span> [<span class="dv">39</span>, <span class="dv">19</span>, <span class="dv">180</span>, <span class="dv">3750</span>]</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> pd.DataFrame([vals], columns<span class="op">=</span>features)</span>
<span id="cb62-3"><a href="#cb62-3" aria-hidden="true" tabindex="-1"></a>dist, idx <span class="op">=</span> knn.kneighbors(query)</span>
<span id="cb62-4"><a href="#cb62-4" aria-hidden="true" tabindex="-1"></a>idx[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="39">
<pre><code>array([  0, 143,  53, 100, 153])</code></pre>
</div>
</div>
<p>The result above indicates that the first sample (index 0) was the closest, followed by four others. We can look up the labels of these points:</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1" aria-hidden="true" tabindex="-1"></a>y[ idx[<span class="dv">0</span>] ]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="40">
<pre><code>0         Adelie
143       Adelie
53        Adelie
100       Adelie
153    Chinstrap
Name: species, dtype: object</code></pre>
</div>
</div>
<p>By a vote of 4–1, then, the classifier should choose Adelie as the result at this location.</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>knn.predict(query)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>array(['Adelie'], dtype=object)</code></pre>
</div>
</div>
<p>Note that points can be outvoted by their neighbors. In other words, the classifier won’t necessarily be correct on every training sample. For example:</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb68"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb68-1"><a href="#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Predicted:"</span>)</span>
<span id="cb68-2"><a href="#cb68-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( knn.predict(X.iloc[:<span class="dv">5</span>,:]) )</span>
<span id="cb68-3"><a href="#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb68-4"><a href="#cb68-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Data:"</span>)</span>
<span id="cb68-5"><a href="#cb68-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( y.iloc[:<span class="dv">5</span>].values )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Predicted:
['Adelie' 'Adelie' 'Chinstrap' 'Adelie' 'Chinstrap']

Data:
['Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie']</code></pre>
</div>
</div>
<p>Next, we split into training and test sets to gauge the performance of the classifier. The <code>classification_report</code> function creates a summary of some of the important metrics.</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report,confusion_matrix</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-4"><a href="#cb70-4" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(X,y,test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb70-5"><a href="#cb70-5" aria-hidden="true" tabindex="-1"></a>knn.fit(X_tr,y_tr)</span>
<span id="cb70-6"><a href="#cb70-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb70-7"><a href="#cb70-7" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> knn.predict(X_te)</span>
<span id="cb70-8"><a href="#cb70-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_te,yhat))</span>
<span id="cb70-9"><a href="#cb70-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_te,yhat))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[25  0  1]
 [11  4  1]
 [ 0  1 24]]
              precision    recall  f1-score   support

      Adelie       0.69      0.96      0.81        26
   Chinstrap       0.80      0.25      0.38        16
      Gentoo       0.92      0.96      0.94        25

    accuracy                           0.79        67
   macro avg       0.81      0.72      0.71        67
weighted avg       0.80      0.79      0.76        67
</code></pre>
</div>
</div>
<p>The default norm in the KNN learner is the 2-norm. To use the 1-norm instead, add <code>metric="manhattan"</code> to the classifier construction call.</p>
</section>
<section id="standardization" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="standardization"><span class="header-section-number">3.4.4</span> Standardization</h3>
<p>The values in the columns of the penguin frame are scaled quite differently. In particular, the values in the body mass column are more than 20x larger than the other columns on average:</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>X.mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>bill_length_mm         43.992793
bill_depth_mm          17.164865
flipper_length_mm     200.966967
body_mass_g          4207.057057
dtype: float64</code></pre>
</div>
</div>
<p>Consequently, the mass feature will dominate the distance calculations. To remedy this issue, we can transform the data into z-scores:</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> X.transform( <span class="kw">lambda</span> x: (x <span class="op">-</span> x.mean()) <span class="op">/</span> x.std() )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>In this instance, standardization makes performance dramatically better:</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb75-1"><a href="#cb75-1" aria-hidden="true" tabindex="-1"></a>Z_tr, Z_te, y_tr, y_te <span class="op">=</span> train_test_split(Z,y,test_size<span class="op">=</span><span class="fl">0.2</span>)</span>
<span id="cb75-2"><a href="#cb75-2" aria-hidden="true" tabindex="-1"></a>knn.fit(Z_tr,y_tr)</span>
<span id="cb75-3"><a href="#cb75-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb75-4"><a href="#cb75-4" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> knn.predict(Z_te)</span>
<span id="cb75-5"><a href="#cb75-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(confusion_matrix(y_te,yhat))</span>
<span id="cb75-6"><a href="#cb75-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(classification_report(y_te,yhat))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[[31  0  0]
 [ 1 10  0]
 [ 0  0 25]]
              precision    recall  f1-score   support

      Adelie       0.97      1.00      0.98        31
   Chinstrap       1.00      0.91      0.95        11
      Gentoo       1.00      1.00      1.00        25

    accuracy                           0.99        67
   macro avg       0.99      0.97      0.98        67
weighted avg       0.99      0.99      0.98        67
</code></pre>
</div>
</div>
</section>
<section id="pipelines" class="level3" data-number="3.4.5">
<h3 data-number="3.4.5" class="anchored" data-anchor-id="pipelines"><span class="header-section-number">3.4.5</span> Pipelines</h3>
<p>One nuisance of the standardization step above is that it must be performed again for any new query vector that comes along. This means we need to keep track of the mean and std of the original training set.</p>
<p>Scikit-learn allows us to create a <strong>pipeline</strong> that automates the transformation. Pipelines make it easy to chain together data transformations followed by a learner. The composite object acts much like a regular learner.</p>
<p>As you might guess, standardization of data is so common that it is predefined:</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb77-1"><a href="#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb77-2"><a href="#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler   <span class="co"># converts to z-scores</span></span>
<span id="cb77-3"><a href="#cb77-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-4"><a href="#cb77-4" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb77-5"><a href="#cb77-5" aria-hidden="true" tabindex="-1"></a>  X, y, </span>
<span id="cb77-6"><a href="#cb77-6" aria-hidden="true" tabindex="-1"></a>  test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb77-7"><a href="#cb77-7" aria-hidden="true" tabindex="-1"></a>  shuffle<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb77-8"><a href="#cb77-8" aria-hidden="true" tabindex="-1"></a>  random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb77-9"><a href="#cb77-9" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb77-10"><a href="#cb77-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-11"><a href="#cb77-11" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb77-12"><a href="#cb77-12" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), knn)</span>
<span id="cb77-13"><a href="#cb77-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb77-14"><a href="#cb77-14" aria-hidden="true" tabindex="-1"></a>pipe.fit(X_tr, y_tr)</span>
<span id="cb77-15"><a href="#cb77-15" aria-hidden="true" tabindex="-1"></a>pipe.score(X_te, y_te)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>0.9701492537313433</code></pre>
</div>
</div>
<p>We can look under the hood of the pipeline. For example, we can see that the mean and variance of each of the original data columns is stored in the first part of the pipeline:</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( pipe[<span class="dv">0</span>].mean_ )</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( pipe[<span class="dv">0</span>].var_ )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[  44.49774436   17.13496241  201.80827068 4253.28947368]
[2.86849573e+01 4.06520620e+00 1.88493315e+02 6.27702995e+05]</code></pre>
</div>
</div>
</section>
</section>
<section id="sec-class-quant" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-class-quant"><span class="header-section-number">3.5</span> Quantifying votes</h2>
<p>Both kNN and decision trees base classification on a voting procedure—for kNN, the <span class="math inline">\(k\)</span> nearest neighbors cast votes, and for a decision tree, the values at a leaf cast votes. So far, we have interpreted the voting results in a winner-takes-all sense, i.e., the class with the most votes wins. But that interpretation discards a lot of potentially valuable information.</p>
<div id="def-class-phat" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.3 </strong></span>Let <span class="math inline">\(\bfx\)</span> be a query vector in a vote-based classification method. The <strong>probability vector</strong> <span class="math inline">\(\hat{p}(\bfx)\)</span> is the vector of vote fraction received by each class.</p>
</div>
<div id="exm-class-phat" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.5 </strong></span>Suppose we have trained a kNN classifier with <span class="math inline">\(k=10\)</span> for data with three classes, called A, B, and C, and that the votes at the testing points are as follows:</p>
<div class="cell" data-execution_count="50">
<div class="cell-output cell-output-display" data-execution_count="49">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0</td>
      <td>8</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>5</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>The values of <span class="math inline">\(\hat{p}\)</span> over the test set form a <span class="math inline">\(5\times 3\)</span> matrix: <!-- $$
[0.9,0,0.1],\,[0.3,0.3,0.4],\,[0.6,0.1,0.3],\,[0.2,0,0.8],\,[0.4,0.5,0.1]. 
$$ --></p>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> np.array( [</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.9</span>, <span class="dv">0</span>, <span class="fl">0.1</span>],</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.5</span>, <span class="fl">0.3</span>, <span class="fl">0.2</span>],</span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.6</span>, <span class="fl">0.1</span>, <span class="fl">0.3</span>],</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.2</span>, <span class="dv">0</span>, <span class="fl">0.8</span>],</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    [<span class="fl">0.4</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span>]</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    ] )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<p>It’s natural to interpret <span class="math inline">\(\hat{p}\)</span> as predicting the probability of each label at any query point, since the values are nonnegative and sum to 100%. Given <span class="math inline">\(\hat{p}\)</span>, we can still output a predicted class; it’s just that the information we get is upstream in the process.</p>
<div id="exm-class-prob" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.6 </strong></span>Consider the penguin species classification problem:</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a>penguins <span class="op">=</span> sns.load_dataset(<span class="st">"penguins"</span>).dropna()</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Select only numeric columns for features:</span></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> penguins.loc[:, penguins.dtypes<span class="op">==</span><span class="st">"float64"</span>]  </span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> penguins[<span class="st">"species"</span>].astype(<span class="st">"category"</span>)</span>
<span id="cb82-5"><a href="#cb82-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-6"><a href="#cb82-6" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb82-7"><a href="#cb82-7" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb82-8"><a href="#cb82-8" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb82-9"><a href="#cb82-9" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">5</span></span>
<span id="cb82-10"><a href="#cb82-10" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We can train a kNN classifier and then retrieve the probabilities via <code>predict_proba</code>:</p>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>knn.fit(X_tr, y_tr)</span>
<span id="cb83-3"><a href="#cb83-3" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> knn.predict_proba(X_te)</span>
<span id="cb83-4"><a href="#cb83-4" aria-hidden="true" tabindex="-1"></a>p_hat[:<span class="dv">6</span>,:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>array([[0.8, 0.2, 0. ],
       [0.8, 0.2, 0. ],
       [0. , 0. , 1. ],
       [0. , 0. , 1. ],
       [0.8, 0.2, 0. ],
       [0.6, 0.4, 0. ]])</code></pre>
</div>
</div>
<p>From the output above we see that, for example, while the third and fourth test cases led to unanimous votes for <em>Gentoo</em>, the sixth case is deemed <em>Adelie</em> in a 3–2 squeaker (or is it a squawker?):</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> knn.predict(X_te)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a>yhat[:<span class="dv">6</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>array(['Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Adelie', 'Adelie'],
      dtype=object)</code></pre>
</div>
</div>
</div>
<section id="roc-curve" class="level3" data-number="3.5.1">
<h3 data-number="3.5.1" class="anchored" data-anchor-id="roc-curve"><span class="header-section-number">3.5.1</span> ROC curve</h3>
<p>Having values between the classes means that we can fine-tune how we decide to assign them.</p>
<div id="def-class-hits" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 3.4 </strong></span>Let <span class="math inline">\(\theta\)</span> be a number in the interval <span class="math inline">\([0,1]\)</span>. We say that a class <span class="math inline">\(T\)</span> <strong>hits</strong> at level <span class="math inline">\(\theta\)</span> at a query point if the fraction of votes that <span class="math inline">\(T\)</span> receives at that point is at least <span class="math inline">\(\theta\)</span>.</p>
</div>
<div id="exm-class-hits" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.7 </strong></span>Continuing with the data in <a href="#exm-class-phat">Example&nbsp;<span class="quarto-unresolved-ref">exm-class-phat</span></a>, we find that at <span class="math inline">\(\theta=0\)</span>, everything always hits:</p>
<div class="cell" data-execution_count="55">
<div class="cell-output cell-output-display" data-execution_count="54">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>At <span class="math inline">\(\theta=0.05\)</span>, say, we lose all the cases where no votes were received:</p>
<div class="cell" data-execution_count="56">
<div class="cell-output cell-output-display" data-execution_count="55">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>At <span class="math inline">\(\theta=0.15\)</span>, we have also lost all those receiving 1 out of 10 votes:</p>
<div class="cell" data-execution_count="57">
<div class="cell-output cell-output-display" data-execution_count="56">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>By the time we get to <span class="math inline">\(\theta=0.7\)</span>, there are only two hits left:</p>
<div class="cell" data-execution_count="58">
<div class="cell-output cell-output-display" data-execution_count="57">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</div>
<p>The probability vector <span class="math inline">\(\hat{p}(\bfx)\)</span> holds the largest possible <span class="math inline">\(\theta\)</span> values for which each class hits at <span class="math inline">\(\bfx\)</span>. Looking at it another way, <span class="math inline">\(\theta=0\)</span> represents maximum credulity—everybody’s a winner!—while <span class="math inline">\(\theta=1\)</span> represents maximum skepticism—unanimous winners only.</p>
<p>The <strong>ROC curve</strong>, or <em>receiver operator characteristic</em> curve, is a way to visualize the hits as a function of <span class="math inline">\(\theta\)</span> over a fixed testing set. The name is a little misleading, since the multiclass case requires multiple curves. The idea is to tally, at each value of <span class="math inline">\(\theta\)</span>, all the hits within each class that represent true positives and false positives.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The name of the ROC curve is a throwback to the early days of radar, when the idea was first developed.</p>
</div>
</div>
<div id="exm-class-roc-simple" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.8 </strong></span>We continue with the data from <a href="#exm-class-phat">Example&nbsp;<span class="quarto-unresolved-ref">exm-class-phat</span></a>, but now we add ground truth to the queries:</p>
<div class="cell" data-execution_count="59">
<div class="cell-output cell-output-display" data-execution_count="58">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>A</th>
      <th>B</th>
      <th>C</th>
      <th>truth</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>9</td>
      <td>0</td>
      <td>1</td>
      <td>A</td>
    </tr>
    <tr>
      <th>1</th>
      <td>5</td>
      <td>3</td>
      <td>2</td>
      <td>B</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6</td>
      <td>1</td>
      <td>3</td>
      <td>A</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>0</td>
      <td>8</td>
      <td>C</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>5</td>
      <td>1</td>
      <td>A</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Let’s look at class A. At <span class="math inline">\(\theta=0.05\)</span>, class A hits in every case, giving TP=3 and FP=2. At <span class="math inline">\(\theta=0.25\)</span>, the fourth query drops out; we still have TP=3, but now FP=1. Here is the table of all the unique values of TP and FP that we can achieve as <span class="math inline">\(\theta\)</span> varies between 0 and 1:</p>
<div class="cell" data-execution_count="60">
<div class="cell-output cell-output-display" data-execution_count="59">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>theta</th>
      <th>FP</th>
      <th>TP</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.05</td>
      <td>2</td>
      <td>3</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.25</td>
      <td>1</td>
      <td>3</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.45</td>
      <td>1</td>
      <td>2</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.55</td>
      <td>0</td>
      <td>2</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.65</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0.95</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>In order to make a graph, we convert the raw TP and FP numbers to rates. Since there are 2 positive and 3 negative over the entire test set, we can represent the rows above as the points <span class="math display">\[
\left(\tfrac{2}{2},\tfrac{3}{3}\right), \, \left(\tfrac{1}{2},\tfrac{3}{3}\right), \, \left(\tfrac{1}{2},\tfrac{2}{3}\right), \, \left(\tfrac{0}{2},\tfrac{2}{3}\right), \, \left(\tfrac{0}{2},\tfrac{1}{3}\right)\, \left(\tfrac{0}{2},\tfrac{0}{3}\right).
\]</span> The ROC curve for class A is just connect-the-dots for these points:</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> pd.DataFrame({<span class="st">"FP rate"</span>: [<span class="dv">1</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>], <span class="st">"TP rate"</span>: [<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span><span class="op">/</span><span class="dv">3</span>,<span class="dv">2</span><span class="op">/</span><span class="dv">3</span>,<span class="dv">1</span><span class="op">/</span><span class="dv">3</span>,<span class="dv">0</span>]})</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>data, </span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"FP rate"</span>, y<span class="op">=</span><span class="st">"TP rate"</span>, </span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"line"</span>, estimator<span class="op">=</span><span class="va">None</span></span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-61-output-1.png" width="469" height="468"></p>
</div>
</div>
</div>
<p>Unsurprisingly, <code>sklearn</code> can compute the points defining the ROC curve automatically, which greatly simplifies drawing them.</p>
<div id="exm-class-roc-penguin" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.9 </strong></span>Continuing <a href="#exm-class-prob">Example&nbsp;<span class="quarto-unresolved-ref">exm-class-prob</span></a>, we will plot ROC curves for the three species in the penguin data:</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb88-2"><a href="#cb88-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb88-3"><a href="#cb88-3" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> knn.predict_proba(X_te)</span>
<span id="cb88-4"><a href="#cb88-4" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb88-5"><a href="#cb88-5" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(knn.classes_):</span>
<span id="cb88-6"><a href="#cb88-6" aria-hidden="true" tabindex="-1"></a>    actual <span class="op">=</span> (y_te<span class="op">==</span>label)</span>
<span id="cb88-7"><a href="#cb88-7" aria-hidden="true" tabindex="-1"></a>    fp, tp, theta <span class="op">=</span> roc_curve(actual,p_hat[:,i])</span>
<span id="cb88-8"><a href="#cb88-8" aria-hidden="true" tabindex="-1"></a>    results.extend( [(label,fp,tp) <span class="cf">for</span> fp,tp <span class="kw">in</span> <span class="bu">zip</span>(fp,tp)] )</span>
<span id="cb88-9"><a href="#cb88-9" aria-hidden="true" tabindex="-1"></a>roc <span class="op">=</span> pd.DataFrame( results, columns<span class="op">=</span>[<span class="st">"label"</span>,<span class="st">"FP rate"</span>,<span class="st">"TP rate"</span>] )</span>
<span id="cb88-10"><a href="#cb88-10" aria-hidden="true" tabindex="-1"></a>roc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>label</th>
      <th>FP rate</th>
      <th>TP rate</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Adelie</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Adelie</td>
      <td>0.027027</td>
      <td>0.233333</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Adelie</td>
      <td>0.054054</td>
      <td>0.566667</td>
    </tr>
    <tr>
      <th>3</th>
      <td>Adelie</td>
      <td>0.216216</td>
      <td>0.900000</td>
    </tr>
    <tr>
      <th>4</th>
      <td>Adelie</td>
      <td>0.405405</td>
      <td>0.966667</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Adelie</td>
      <td>0.459459</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>Adelie</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>7</th>
      <td>Chinstrap</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>8</th>
      <td>Chinstrap</td>
      <td>0.019231</td>
      <td>0.333333</td>
    </tr>
    <tr>
      <th>9</th>
      <td>Chinstrap</td>
      <td>0.153846</td>
      <td>0.733333</td>
    </tr>
    <tr>
      <th>10</th>
      <td>Chinstrap</td>
      <td>0.423077</td>
      <td>0.933333</td>
    </tr>
    <tr>
      <th>11</th>
      <td>Chinstrap</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>12</th>
      <td>Gentoo</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>13</th>
      <td>Gentoo</td>
      <td>0.000000</td>
      <td>0.909091</td>
    </tr>
    <tr>
      <th>14</th>
      <td>Gentoo</td>
      <td>0.022222</td>
      <td>0.909091</td>
    </tr>
    <tr>
      <th>15</th>
      <td>Gentoo</td>
      <td>0.022222</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>16</th>
      <td>Gentoo</td>
      <td>0.088889</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>17</th>
      <td>Gentoo</td>
      <td>0.200000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>18</th>
      <td>Gentoo</td>
      <td>1.000000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>The table above holds all of the key points on the ROC curves:</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>roc, </span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"FP rate"</span>, y<span class="op">=</span><span class="st">"TP rate"</span>, </span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"label"</span>, kind<span class="op">=</span><span class="st">"line"</span>, estimator<span class="op">=</span><span class="va">None</span></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-63-output-1.png" width="574" height="468"></p>
</div>
</div>
<p>Each curve starts in the lower left corner and ends at the upper right corner. The ideal situation is in the top left corner of the plot, corresponding to perfect recall and specificity. All of the curves explicitly show the tradeoff between recall and specificity as the decision threshold is varied. The <em>Gentoo</em> curve comes closest to the ideal.</p>
<p>If we weight neighbors’ votes inversely to their distances from the query point, then the thresholds aren’t restricted to multiples of <span class="math inline">\(\tfrac{1}{5}\)</span>:</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>knnw <span class="op">=</span> KNeighborsClassifier(n_neighbors<span class="op">=</span><span class="dv">5</span>, weights<span class="op">=</span><span class="st">"distance"</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>knnw.fit(X_tr, y_tr)</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> knnw.predict_proba(X_te)</span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(knn.classes_):</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>    actual <span class="op">=</span> (y_te<span class="op">==</span>label)</span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>    fp, tp, theta <span class="op">=</span> roc_curve(actual,p_hat[:,i])</span>
<span id="cb90-9"><a href="#cb90-9" aria-hidden="true" tabindex="-1"></a>    results.extend( [(label,fp,tp) <span class="cf">for</span> fp,tp <span class="kw">in</span> <span class="bu">zip</span>(fp,tp)] )</span>
<span id="cb90-10"><a href="#cb90-10" aria-hidden="true" tabindex="-1"></a>roc <span class="op">=</span> pd.DataFrame( results, columns<span class="op">=</span>[<span class="st">"label"</span>,<span class="st">"FP rate"</span>,<span class="st">"TP rate"</span>] )</span>
<span id="cb90-11"><a href="#cb90-11" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>roc, </span>
<span id="cb90-12"><a href="#cb90-12" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"FP rate"</span>, y<span class="op">=</span><span class="st">"TP rate"</span>, </span>
<span id="cb90-13"><a href="#cb90-13" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"label"</span>, kind<span class="op">=</span><span class="st">"line"</span>, estimator<span class="op">=</span><span class="va">None</span></span>
<span id="cb90-14"><a href="#cb90-14" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="classification_files/figure-html/cell-64-output-1.png" width="574" height="468"></p>
</div>
</div>
</div>
</section>
<section id="auc" class="level3" data-number="3.5.2">
<h3 data-number="3.5.2" class="anchored" data-anchor-id="auc"><span class="header-section-number">3.5.2</span> AUC</h3>
<p>ROC curves lead to another classification performance metric known as <strong>area under ROC curve (AUC)</strong>. Its name tells you exactly what it is, and it ranges between 0 (bad) and 1 (ideal). Unlike the other classification metrics we have encountered, AUC tries to account not just for the result of the classification at a single threshold, but over the full range from credulous to skeptical. You might think of it as grading with partial credit.</p>
<div id="exm-class-auc" class="theorem example">
<p><span class="theorem-title"><strong>Example 3.10 </strong></span>The AUC metric allows us to compare the standard and weighted kNN classifiers from <a href="#exm-class-roc-penguin">Example&nbsp;<span class="quarto-unresolved-ref">exm-class-roc-penguin</span></a>. Note that the function for computing them, <code>roc_auc_score</code>, requires a keyword argument when there are more than two classes, to specify “one vs.&nbsp;rest” (our usual) or “one vs.&nbsp;one” matchups.</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_auc_score</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>s <span class="op">=</span> roc_auc_score(</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>    y_te, knn.predict_proba(X_te), </span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>    multi_class<span class="op">=</span><span class="st">"ovr"</span>, average<span class="op">=</span><span class="va">None</span></span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-7"><a href="#cb91-7" aria-hidden="true" tabindex="-1"></a>sw <span class="op">=</span> roc_auc_score(</span>
<span id="cb91-8"><a href="#cb91-8" aria-hidden="true" tabindex="-1"></a>  y_te, knnw.predict_proba(X_te), </span>
<span id="cb91-9"><a href="#cb91-9" aria-hidden="true" tabindex="-1"></a>  multi_class<span class="op">=</span><span class="st">"ovr"</span>, average<span class="op">=</span><span class="va">None</span></span>
<span id="cb91-10"><a href="#cb91-10" aria-hidden="true" tabindex="-1"></a>  )</span>
<span id="cb91-11"><a href="#cb91-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-12"><a href="#cb91-12" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(</span>
<span id="cb91-13"><a href="#cb91-13" aria-hidden="true" tabindex="-1"></a>    {<span class="st">"standard"</span>: s, <span class="st">"weighted"</span>: sw},</span>
<span id="cb91-14"><a href="#cb91-14" aria-hidden="true" tabindex="-1"></a>    index<span class="op">=</span>knn.classes_</span>
<span id="cb91-15"><a href="#cb91-15" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>standard</th>
      <th>weighted</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Adelie</th>
      <td>0.903153</td>
      <td>0.935586</td>
    </tr>
    <tr>
      <th>Chinstrap</th>
      <td>0.857051</td>
      <td>0.883333</td>
    </tr>
    <tr>
      <th>Gentoo</th>
      <td>0.997980</td>
      <td>0.998990</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Based on the above scores, the weighted classifier seems to be better at identifying all three species.</p>
</div>
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<p>For these exercises, you may use computer help to work on a problem, but your answer should be self-contained without reference to computer output (unless stated otherwise).</p>
<div id="exr-class-dankness" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.1 </strong></span>Here is a confusion matrix for a classifier of meme dankness.</p>
<p><img src="_media/dankness.png" class="img-fluid"></p>
<p>Calculate the <strong>(a)</strong> recall, <strong>(b)</strong> precision, <strong>(c)</strong> specificity, <strong>(d)</strong> accuracy, and <strong>(e)</strong> <span class="math inline">\(F_1\)</span> score of the classifier, where <em>dank</em> is the positive outcome.</p>
</div>
<div id="exr-class-flavors" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.2 </strong></span>Here is a confusion matrix for a classifier of ice cream flavors.</p>
<p><img src="_media/flavors.png" class="img-fluid"></p>
<p><strong>(a)</strong> Calculate the recall rate for chocolate.</p>
<p><strong>(b)</strong> Find the precision for vanilla.</p>
<p><strong>(c)</strong> Find the accuracy for strawberry.</p>
</div>
<div id="exr-class-gini" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.3 </strong></span>Find the Gini impurity of this set: <span class="math display">\[ \{ A, B, B, C, C, C \}.\]</span></p>
</div>
<div id="exr-class-partition" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.4 </strong></span>Given <span class="math inline">\(x_i=i\)</span> for <span class="math inline">\(i=0,\ldots,5\)</span>, with labels <span class="math display">\[
y_1=y_5=y_6=A, \quad y_2=y_3=y_4=B,
\]</span> find an optimal partition threshold using Gini impurity.</p>
</div>
<div id="exr-class-manhattan" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.5 </strong></span>Carefully sketch the set of all points in <span class="math inline">\(\real^2\)</span> whose 1-norm distance from the origin equals 1. This is a <em>Manhattan unit circle</em>.</p>
</div>
<div id="exr-class-triangle" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.6 </strong></span>Three points in the plane lie at the vertices of an equilateral triangle. One is labeled A and the other two are B. Carefully sketch the decision boundary of <span class="math inline">\(k\)</span>-nearest neighbors with <span class="math inline">\(k=1\)</span>, using the 2-norm.</p>
</div>
<div id="exr-class-ellipse" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 3.7 </strong></span>Define points on an ellipse by <span class="math inline">\(x_k=a\cos(\theta_k)\)</span> and <span class="math inline">\(y_k=b\sin(\theta_k)\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are positive and <span class="math inline">\(\theta_k=2k\pi/8\)</span> for <span class="math inline">\(k=0,1,\ldots,7\)</span>. Show that if the <span class="math inline">\(x_k\)</span> and <span class="math inline">\(y_k\)</span> are standardized into z-scores, then the resulting points all lie on a circle centered at the origin. (Standardizing points into z-scores is sometimes called <em>sphereing</em> them.)</p>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./stats.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./selection.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<script>videojs(video_shortcode_videojs_video1);</script>



</body></html>