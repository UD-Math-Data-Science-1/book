# Representation of data

::: {.content-visible unless-format="pdf"}
{{< include _macros.qmd >}}
:::

First we have to discuss how to represent data, both abstractly and in Python.

## Quantitative data

:::{#def-data-quantitative}
A **quantitative** value is one that is numerical and supports meaningful comparison and arithmetic operations.
:::

 Quantitative data is further divided into **continuous** and **discrete** types. The difference is the same as between real numbers and integers.

::::{#exm-data-quantitative chapter=1 description="Quantitative data" type=‚úçÔ∏è}
Some continuous quantitative data sources:

* Temperature at noon at a given airport
* Your height
* Voltage across the terminals of a battery

Examples of discrete quantitative data:

* The number of shoes you own
* Number of people at a restaurant table
* Score on an exam
::::

:::{.column-margin}
{{< video https://www.dropbox.com/s/l9fs3fa43hup08l/Example1_1.mp4?raw=1 >}}
:::

:::{.callout-note}
Sometimes there may be room for interpretation or context. For example, the retail price of a gallon of milk might be regarded as discrete data, since it technically represents a whole number of pennies. But in finance, transactions are regularly computed to much higher precision, so it might make more sense to interpret prices as continuous values. As a rule of thumb, if there are many possible values, or the values are the result of a measurement, the continuous interpretation is usually more appropriate.
:::

::::{.callout-caution}
Not all numerical values represent truly quantitative data. ZIP codes (postal codes) in the U.S. are 5-digit numbers, and while there is some logic to how they were assigned, there is no clearly meaningful interpretation of averaging them, for instance.
::::

For both continuous and discrete quantities, it makes sense to order different values, compute averages of them, etc. (However, averages of discrete quantities are continuous.)

Mathematically, the real and integer number sets are infinite, but computers are finite machines. Integers are represented exactly within some range that is determined by how many binary bits are dedicated. The computational analog of real numbers are **floating-point numbers**, or more simply, **floats**. These are bounded in range as well as discretized. The details are complicated, but essentially, the floating-point numbers have about 16 significant decimal digits by default‚Äîwhich is virtually always far more precision than real data offers.

```{python}
# This is an integer (int type)
print(3, "is a", type(3))

# This is a real number (float type)
print(3.0, "is a", type(3.0))

# Convert int to float (generally no change to numerical value)
print( "float(3) creates",float(3) )

# Truncate float to int
print( "int(3.14) creates", int(3.14) )
```

### Inf and NaN

There are two additional quasi-numerical `float` values to be aware of as well.

::: {.callout-important}
For numerical work in Python, the NumPy package indispensible. We will use it often, and it is also loaded and used by most other scientifically oriented packages.
:::

The value `inf` stands for infinity. It's greater than every finite number. Some arithmetic with infinity is well-defined:

```{python}
import numpy as np
print( "np.inf + 5 is", np.inf + 5 )
print( "np.inf + np.inf is", np.inf + np.inf )
print( "5 - np.inf is", 5 - np.inf )
```

However, in calculus you learned that some expressions with infinity are considered to be undefined without additional information to apply (e.g., L'H√¥pital's Rule):

```{python}
print( "np.inf / np.inf is", np.inf / np.inf )
```

The result `nan` stands for *Not a Number*. It is the result of indeterminate arithmetic operations, like $\infty/\infty$ and $\infty - \infty$. It is also used sometimes as a placeholder for missing data.

:::{.callout-warning}
By definition, every operation that involves a `NaN` value results in a `NaN`. One notorious consequence of this behavior is that `nan==nan` is `nan`, not `True`!
:::


### Dates and times

Handling times and dates can be tricky. Aside from headaches such as time zones and leap years, there are many different ways people and machines represent dates, and with varying amounts of precision. Python has its own inbuilt system for handling dates and times, but we will show the facilities provided by the NumPy package, which is more comprehensive.

There are two basic types:

::::{#def-data-time}
A **datetime** is a representation of an instant in time. A **time delta** is a representation of a duration; i.e., a difference between two datetimes.
::::

::: {.callout-important}
Native Python uses a `datetime` type, while NumPy uses `datetime64`.
:::

```{python}
import numpy as np
np.datetime64("2020-01-17")    # YYYY-MM-DD 
```

```{python}
np.datetime64("1969-07-20T20:17")    # YYYY-MM-DDThh:mm
```

```{python}
# Current date and time, down to the second
np.datetime64("now")
```

A time delta in NumPy indicates its units (granularity).

```{python}
np.datetime64("1969-07-20T20:17") - np.datetime64("today")
```

### Random numbers

Generating truly random numbers on a computer is not simple. Mostly we rely on *pseudorandom* numbers, which are generated by deterministic functions called **random number generators** (RNGs) that have extremely long periods. One nice consequence is repeatability. By specifying the starting state of the RNG, you can get exactly the same pseudorandom sequence every time.

We will rely on pseudorandom numbers in two ways. First, many algorithms in data science have at least one random aspect (dividing data into subsets, for example). The library routines we will be using allow you to specify the random state and get repeatable results. Occasionally, though, we might want to generate random values for our own use.

```{python}
from numpy.random import default_rng
rng = default_rng(19716)    # giving an initial state
```

The `uniform` generator method produces numbers distributed uniformly (i.e., every value is equally likely) between two limits you specify.

```{python}
for _ in range(5):
    print( rng.uniform( -1, 1 ) )
```

Another common type of random value is generated by `normal`, which produces real values distributed according to the normal or Gaussian distribution. We'll get into that in more detail later.

```{python}
for _ in range(5):
    print( rng.normal() )
```

::::{#exm-data-normal chapter=1 description="Mean of a normal distribution" type=üíª}
In the long run, the average value of normally distributed numbers will be zero. Here is an experiment on 100,000 of them:

```{python}
s = 0
for _ in range(100000):
    s += rng.normal()

s/100000
```
::::

## Arrays

Most interesting phenomena are characterized and influenced by more than one factor. Collections of values therefore play a central role in data science. The workhorse type for collections in base Python is the list. However, we're going to need some more powerful metaphors and tools as well. 

### Vectors

::::{#def-data-vector}
A **vector** is a collection of values called **elements**, all of the same type, indexed by consecutive integers.
::::

::: {.callout-important}
In math, vector indexes usually begin with 1. In Python, they begin with 0. 
:::

A vector with $n$ elements is often referred to as an $n$-vector, and we say that $n$ is the **length** of the vector. In math we often use $\real^n$ to denote the set of all $n$-vectors with real-valued elements.

The usual way to work with arrays in Python is through NumPy:

```{python}
import numpy as np

x = np.array( [1,2,3,4,5] )
x
```

:::{.column-margin}
{{< video https://www.dropbox.com/s/eq4n2508uhtwjir/Section1_2_a.mp4?raw=1 >}}
:::

A vector has a data type for its elements:

```{python}
x.dtype
```

Any float values in the vector cause the data type of the entire vector to be `float`:

```{python}
y = np.array( [1.0,2,3,4,5] )
y.dtype
```

Use `len` to determine the length of a vector:

```{python}
len(x)
```

You can create a special type of vector called a *range* that has equally spaced elements:

```{python}
np.arange(0,5)
```

```{python}
np.arange(1,3,0.5)
```

The syntax here is `(start,stop,step)`. Note something critical and counterintuitive from the above results:

::: {.callout-caution}
In base Python and in NumPy, the last element of a range is omitted. This is guaranteed to cause confusion if you are used to just about any other computer language.
:::

#### Access and slicing

:::{.column-margin}
{{< video https://www.dropbox.com/s/adsliy9ncce8ivx/Section1_2_b.mp4?raw=1 >}}
:::

Use square brackets to refer to an element of a vector:

```{python}
x[0]
```

```{python}
x[4]
```

Negative values for the index are counted from the end. The last element of a vector always has index `-1`, and more-negative values move backward through the elements:

```{python}
x[-1]
```

```{python}
x[-3]
```

```{python}
x[-len(x)]
```

Element references can also be on the left side of an assignment:

```{python}
x[2] = -3
x
```

Note, however, that once the data type of a vector is set, it can't be changed:

```{python}
x[0] = 1.234
x    # float was truncated to int, without warning!!!
```

You can also use a list in square brackets to access multiple elements at once:

```{python}
x[ [0, 2, 4] ]
```

::: {.callout-important}
The result of a list reference is a new vector, not a number. Note the difference here:
```{python}
x[0]
```

```{python}
x[ [0] ]
```
:::

Accessing multiple elements via a range is known as **slicing**:

```{python}
x[0:3]
```

```{python}
x[-3:-1]
```

As with ranges, the syntax of a slice is `start:stop:step`, and the last element of the range is *not* included. This causes headaches and bugs, though it does imply that the range `i:j` has $j-i$ elements, not $j-i+1$. 

When the `start` of the range is omitted, it means "from the beginning", and when `stop` is omitted, it means "through to the end." Hence, `[:k]` means "first $k$ elements" and `[-k:]` means "last $k$ elements":

```{python}
x[:3]
```

```{python}
x[-3:]
```

And we also have this idiom:

```{python}
x[::-1]   # reverse the vector
```

NumPy will happily allow you to reference invalid indexes. It will just return as much as is available without warning or error.
:::

```{python}
x[:10]
```

### Multiple dimensions

A vector is an important special case of a more general construct.

::::{#def-data-array}
An **array** is a collection of values called **elements**, all of the same type, indexed by one or more sets of consecutive integers. The number of indexes needed to specify a value is the **dimension** of the array.
::::

::: {.callout-note}
A dimension is called an `axis` in NumPy and related packages.
:::

::: {.callout-note}
The term **matrix** is often used simply to mean a 2D array. Technically, though, a matrix should have only numerical values, and matrices obey certain properties that make them important mathematical objects. These properties and their consequences are studied in linear algebra.
:::

#### Construction

:::{.column-margin}
{{< video https://www.dropbox.com/s/xk5nyx52miba267/Section1_2_c.mp4?raw=1 >}}
:::

One way to construct an array is by a list comprehension:

```{python}
A = np.array([ [j-i for j in range(6)] for i in range(4) ])
A
```

The `shape` of an array is what we would often call the *size*:

```{python}
A.shape
```

There is no difference between a vector and a 1D array:

```{python}
x.shape
```

There is also no difference between a 2D array and a vector of vectors giving the rows of the array.

```{python}
R = np.array( [ [1, 2, 3], [4, 5, 6] ])
R
```

Here are some other common ways to construct arrays.

```{python}
np.ones(5)
```

```{python}
np.zeros( (3, 6) )
```

```{python}
# See earlier section for definition of rng
rng.normal( size=(3, 4) )
```

```{python}
np.repeat(np.pi, 3)
```

You can also stack arrays vertically or horizontally to create new arrays.

```{python}
np.hstack( ( np.ones((2, 2)), np.zeros((2, 3)) ) )
```

```{python}
np.vstack( (range(5), range(5, 0, -1)) )
```

#### Indexing and slicing

:::{.column-margin}
{{< video https://www.dropbox.com/s/b8187f40a2vghjv/Section1_2_d.mp4?raw=1 >}}
:::

We can use successive brackets to refer to an element, row then column:

```{python}
R[1][2]    # second row, third column
```

But it's more convenient to use a single bracket set with indexes separated by commas:

```{python}
R[1, 2]    # second row, third column
```

You can slice in each dimension individually.

```{python}
R[:1, -2:]    # first row, last two columns
```

The result above is another 2D array. Note how this result is subtly different:

```{python}
R[0, -2:]
```

Because we accessed an individual row, not a slice, the result is one dimension lower---a vector. Finally, a `:` in one slice position means to keep everything in that dimension.

```{python}
A[:, :2]    # all rows, first 2 columns
```

#### Reductions

:::{.column-margin}
{{< video https://www.dropbox.com/s/nvt91p4okytb33e/Section1_2_e.mp4?raw=1 >}}
:::

A common task is to *reduce* an array along one dimension, called an *axis* in numpy, resulting in an array of one less dimension. It's easiest to explain by some examples.

```{python}
np.sum(A, axis=0)    # sum along the rows
```

```{python}
np.sum(A, axis=1)    # sum along the columns
```

If you don't specify an axis, the reduction occurs over all directions at once, resulting in a single number.

```{python}
np.sum(A)
```

You can also do reductions with `np.max`, `np.min`, `np.mean`, etc.

<!-- #### Broadcasting

You can add together arrays of the same shape, element by element, just as you would expect:

```{python}
x = np.array( [1,2,3,4,5] )
x + x
```

Generally, you cannot do this for arrays of different shapes:

```{python}
#| error: true
y = np.array ( [10,20] )
x+y
```


There is a major exception, though. When one of the operands can be repeated along an axis to become the same shape as the other, then that *broadcasting* is done automatically. This makes some common operations easy to express, such as adding a value to every element of an array:

```{python}
x+3
```

For a less trivial example, suppose that within each column of an array, we want to subtract off the average value in that column. This becomes easy to do via broadcasting:

```{python}
A - np.mean(A, axis=0)
```
 -->

## Qualitative data

A qualitative value is one that is not quantitative. There are numerous types, but we will consider only a few of them here. Ultimately, we need to be able to represent qualitative data in a quantitative way for use in algorithms.

### Categorical

::::{#def-data-categorical}

**Categorical** data has values drawn from a finite set $S$ of categories. If the members of $S$ support meaningful ordering comparisons, then the data is **ordinal**; otherwise, it is **nominal**.
::::

::::{#exm-data-categorical chapter=1 description="Ordinal data" type=‚úçÔ∏è}
Examples of ordinal categorical data:

* Seat classes on a commercial airplane (e.g., economy, business, first)
* Letters of the alphabet

Examples of nominal categorical data:

* Yes/No responses
* Marital status
* Make of a car

There are nuanced cases. For instance, letter grades are themselves ordinal categorical data. However, schools convert them to discrete quantitative data and then compute a continuous quantitative GPA.
::::

One way to quantify ordinal categorical data is to assign integer values to the categories in a manner that preserves ordering. This approach can be suspect, though, when it comes to operations such as averaging or computing a distance between values. 

#### Dummy variables 

Another means of quantifying categorical data is called **dummy variables** in classical statistics and **one-hot encoding** in much of machine learning. Suppose a variable $x$ has values in a category set that has $m$ members, i. e., $S = \{c_1,\ldots,c_m\}$. There are two ways to replace the values with dummy variables.

1. Introduce $m$ variables $x_1,\ldots,x_m$, where for a given categorical value $v$ we have 

$$
x_i = \begin{cases} 1, & v = c_i, \\ 0, & v \neq c_i. \end{cases}
$$

2. Introduce only $m-1$ variables, leaving out $x_m$. If $x_1=x_2=\ldots=x_{m-1}=0$, then we know that the value is $c_m$. (This variant is important in statistics because otherwise, $x_m$ has to be correlated with the other $x_i$.) 

::::{#exm-data-dummy chapter=1 description="Dummy variables" type=‚úçÔ∏è}
Suppose that the *stooge* variable can take the values `Moe`, `Larry`, `Curly`, or `Shemp`, and we have the dataset

```python
[Curly, Moe, Curly, Shemp]
```

If we use 4 dummy variables, the data would be replaced by the array

```python
[ [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1] ]
```

If we use only 3 dummy variables, we would get

```python
[ [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 0] ]
```
::::

### Text 

Text is a ubiquitous data source. One way to quantify text is to use a dictionary of interesting keywords $w_1,\ldots,w_n$. Given a collection of documents $d_1,\ldots,d_m$, we can define an $m\times n$ **document--term matrix** $T$ by letting $T_{ij}$ be the number of times term $j$ appears in document $i$. 

### Images

The most straightforward way to represent an image is as a 3D array of values representing intensities representing of red, green and blue in each pixel. Sometimes it might be preferable to represent the image by a vector of statistics about these values, or by presence or absence of detected objects, etc.

## Series and frames

The most popular Python package for manipulating and analyzing data is [pandas](https:pandas.pydata.org). We will use the paradigm it presents, which is fairly well understood throughout data science.

::::{#def-data-frame}
A **series** is a vector that is indexed by a finite ordered set. A **data frame** is a collection of series that all share the same index set. 
::::

We can conceptualize a series as a vector plus an index list, and a data frame as a 2D array with index sets for the rows and the columns. In that sense, they are simply syntactic sugar. However, since people are much better at remembering the meaning of words than arbitrarily assigned integers, data frames serve to prevent errors and misunderstandings.

::::{#exm-data-frames chapter=1 description="Series and data frames" type=‚úçÔ∏è}
Some data that can be viewed as series:

* The length, width, and height of a box can be expressed as a 3-vector of positive real numbers. If we index the vector by the names of the measurements, it becomes a series.
* The number of steps taken by an individual over the course of a week can be expressed as a 7-vector of nonnegative integers. We could index it by the integers 1--7, or in a series by the names of the days of the week.
* The bid prices of a stock at the end of each trading day can be represented as a *time series*, in which the index is drawn from timestamps. 
* The scores of gymnasts on multiple apparatus types can be represented as a data frame whose rows are indexed by the names of the gymnasts and whole columns are indexed by the names of the apparatuses. 
::::

::::{#exm-data-colors chapter=1 description="Series and frames in pandas" type=üíª}
Here is a pandas series for the wavelengths of light corresponding to rainbow colors:
```{python}
import pandas as pd

wavelength = pd.Series( 
  [400, 470, 520, 580, 610, 710],    # values
  index=["violet", "blue", "green", "yellow", "orange", "red"],
  name="wavelength"
  )

wavelength
```

We can use an index value (one of the colors) to access a value in the series:
```{python}
wavelength["blue"]
```

If we access multiple values, we get a series that is a subset of the original:
```{python}
wavelength[ ["violet", "red"] ]
```

We can also use the `iloc` property to access the underlying vector by NumPy slicing:
```{python}
wavelength.iloc[:4]
```

Here is a series of NFL teams based on the same index:
```{python}
team = pd.Series(
  ["Vikings", "Bills", "Eagles", "Chargers", "Bengals", "Cardinals"],
  index = wavelength.index,
  name="team"
  )

team["green"]
```

Now we can create a data frame using these two series as columns:
```{python}
rainbow = pd.DataFrame( {"wavelength": wavelength, "team name": team} )
rainbow
```

::: {.callout-tip}
Curly braces `{ }` are used to construct a dictionary in Python.
:::

We can access a single column using simple bracket notation:

```{python}
rainbow["team name"]
```

We can add a column after the fact by using a bracket access on the left side of the assignment:
```{python}
rainbow["flower"] = [
    "Lobelia", 
    "Cornflower", 
    "Bells-of-Ireland", 
    "Daffodil",
    "Butterfly weed",
    "Rose"
    ]

rainbow
```

We can access a row by using brackets with the `loc` property of the frame, getting a series indexed by the column names of the frame:
```{python}
rainbow.loc["orange"]
```

We are also free to strip away the index and get an ordinary array:
```{python}
rainbow.loc["red"].to_numpy()
```
::::

:::{.column-margin}
{{< video https://www.dropbox.com/s/t74l0mltts3s9vz/Section1_4_a.mp4?raw=1 >}}
:::

::::{#exm-data-create-frame chapter=1 description="Creating a data frame" type=üíª}
Here are some more direct ways to construct a data frame in pandas. 

In this case, we give a list of rows, plus (optionally) the index and the names of the columns:
```{python}
pd.DataFrame( 
    [ ("white", 4), ("brown", 8), ("pink", 7) ], 
    columns=["Color", "Rating"],
    index=["vanilla", "chocolate", "strawberry"]
    )
```

We can also specify data as columns by using a dictionary:

```{python}
pd.DataFrame( 
    { "Color": ["white", "brown", "pink"], "Rating": [4, 8, 7] }, 
    index=["vanilla", "chocolate", "strawberry"]
    )
```
::::

### Categorical data in pandas

Pandas has many facilities for dealing with categorical variables.

:::{.column-margin}
{{< video https://www.dropbox.com/s/fjf4gj8bj5hwvwm/Section1_5_c.mp4?raw=1 >}}
:::

::::{#exm-data-ordinal chapter=1 description="Ordinal data" type=üíª}
Here is a [dataset](https://ggplot2.tidyverse.org/reference/diamonds.html) about features and prices of diamonds:

```{python}
import seaborn as sns
diamonds = sns.load_dataset("diamonds")
diamonds.head()
```

As you can see above, some of the features (cut, color, clarity) have string designations. We could convert *clarity*, for example, to a categorical variable like so: 

```{python}
pd.Categorical(diamonds["clarity"])
```

However, these values actually have a specific ordering in this context, and that is lost in the nominal variable above. We can fix that by specifying the ordering when we create the categorical variable:

```{python}
clarities = ["I1", "SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF"]
pd.Categorical(diamonds["clarity"], categories=clarities, ordered=True)
```

You often want to simply replace the ordered categories with integers:

```{python}
cl_num = pd.Categorical(diamonds["clarity"], categories=clarities, ordered=True)
diamonds["clarity"] = cl_num.codes
diamonds.head()
```

In such cases, it's more direct to use string replacement:

```{python}
cuts = ["Fair", "Good", "Very Good", "Premium", "Ideal"]
diamonds["cut"] = diamonds["cut"].replace(
    cuts,              # to be replaced
    range(1, 6),       # replacements
)
diamonds.head()
```
::::

::::{#exm-data-chess chapter=1 description="Categorical data in pandas" type=üíª}
Here is a vector of chess pieces at the start of a game:

```{python}
pieces = pd.Series( [
    "pawn", "pawn", "pawn", "pawn", "pawn", "pawn", "pawn", "pawn",
    "knight", "bishop", "rook", "knight", "bishop", "rook",
    "queen", "king"
] )
```

We can use pandas to convert this variable to dummy variables:

```{python}
pd.get_dummies(pieces)
```

If we wanted to use a specific ordering in the dummy variables, then we would first have to create an ordinal variable as in @exm-data-ordinal.
::::

### Common operations

| Description                  | Syntax                   | Result         |
| :--------------------------- | :----------------------- | :------------- |
| First or last entries        | `s.head()`, `s.tail()`   | Series         |
| Length                       | `len(s)`                 | integer        |
| Number of entries            | `s.shape`                | tuple          |
| All of the values            | `s.values`               | array          |
| Convert to list              | `list(s)`                | list           |
| Index                        | `s.index`                | Index          |
| Unique values                | `s.unique()`             | array          |
| Appearance counts for values | `s.value_counts()`       | Series         |
| Extreme values               | `s.min()`, `s.max()`     | number         |
| Sum                          | `s.sum()`                | number         |
| Comparison                   | `s > 1`,  `s=="foo"`     | boolean Series |
| Locate missing               | `s.isna()`               | boolean Series |
| Arithmetic                   | `s + 1`,  `s * t`        | Series         |
| Delete one or more rows      | `s.drop(0)`              | Series         |

: Operations on pandas series {#tbl-data-series}

Pandas offers many methods for manipulating series and data frames. @tbl-data-series shows common operations that can be applied to a series. Since a column of a data frame is a series, these operations can be applied in that context, too. There is an [exhaustive list](https://pandas.pydata.org/docs/reference/series.html) in the pandas documentation.

::: {.callout-caution}
As always in Python, you need to pay attention to the difference between applying a function, like `foo(bar)`, accessing a property, like `foo.bar`, and calling an object method, `foo.bar()`. Extra or missing parentheses groups can cause errors.
:::

| Description                | Syntax                   | Result              |
| :------------------------- | :----------------------- | :------------------ |
| First or last rows         | `df.head()`, `df.tail()` | DataFrame           |
| Number of rows             | `len(df)`                | integer             |
| Number of rows and columns | `df.shape`               | tuple               |
| All of the values          | `df.values`              | array               |
| Row index                  | `df.index`               | Index               |
| Column names               | `list(df)`               | list                |
| Column names               | `df.columns`             | Index               |
| Access by column name(s)   | `df["name"]`             | Series or DataFrame |
| Access by name             | `df.loc[rows, "name"]`   | (varies)            |
| Access by position         | `df.iloc[i, j]`          | (varies)            |
| Sum                        | `df.sum()`               | Series              |
| Comparison                 | `s > 1`, `s=="foo"`      | boolean DataFrame   |
| Delete a column            | `df.drop(name, axis=1)`  | DataFrame           |
| Apply columnwise           | `df.apply(myfun)`        | (varies)            |

: Operations on pandas data frames {#tbl-data-frames}

@tbl-data-frames shows additional operations that can be applied to an entire data frame (or to any subset). It takes a lot of space to demonstrate all the ways in which these can be used, as is done in the [pandas user guide](https://pandas.pydata.org/docs/user_guide/basics.html), so they are just collected here for future reference. There is also an [exhaustive list](https://pandas.pydata.org/docs/reference/frame.html) of them in the pandas documentation.

### Loading from files 

Datasets can be presented in many forms. In this course, we assume that they are given as spreadsheets or in *comma-separated value* (CSV) files. These files can be read by pandas locally or over the web.

::::{#exm-data-read chapter=1 description="Importing data with pandas" type=üíª}
The pandas function we use to load a dataset is `read_csv`. Here we use it to read a file that is available over the web:

```{python}
ads = pd.read_csv("https://raw.githubusercontent.com/tobydriscoll/ds1book/master/advertising.csv")
ads.head(6)    # show the first 6 rows
```

Note above that we used `head(6)` to see just the first 6 rows.

A data frame has a few properties that describe its contents:
```{python}
ads.shape    # number of rows, number of columns
```

```{python}
ads.columns   # names of the columns
```

```{python}
ads.dtypes    # data types of the columns
```
::::

:::{.column-margin}
{{< video  https://www.dropbox.com/s/1r9is5thcryuarl/Section1_4_b.mp4?raw=1 >}}
:::

It's also possible to import data from most other general-purpose formats you might encounter, such as Excel spreadsheets (though possibly requiring one-time installation of additional libraries). There are many functions starting with `pd.read_` showing the formats pandas understands.

## Selecting rows and columns

In a data frame, we access columns by giving a string name, or a list of names, in square brackets. We can access a row by the `loc` property with an index value, or `iloc` with an absolute row number starting from zero. 

::::{#exm-data-row-iloc chapter=1 description="iloc for rows in a frame" type=üíª}
Here's a local file that contains daily weather summaries from Newark, Delaware:
```{python}
weather = pd.read_csv("_datasets/ghcn_newark.csv")
weather.head()
```

There are lots of columns here. We will work with a small subset of them:

```{python}
columns = ["DATE", "PRCP", "SNOW", "TMAX", "TMIN"]
weather = weather[columns]
weather.head()
```

The first row has absolute position zero:

```{python}
weather.iloc[0]
```

The result above is a series, with the column names of the original data frame serving as the index. The following result is subtly different:

```{python}
weather.iloc[:1]
```

The slicing syntax `[:1]` means "the first 1 rows". The result is a one-row data frame---*not* a series. To access the last 4 rows, we can use a negative index:

```{python}
weather.iloc[-4:]
```
::::

In @exm-data-row-iloc, we did not specify an index for the data frame, so the row numbers were the index. If we do give the data frame an index, we can use `loc` to access rows by the index value.

::::{#exm-data-row-loc chapter=1 description="loc for rows in a frame" type=üíª}
A natural candidate for an index in the weather data is the *DATE* column. This can be specified when the file is read, but here we do it after the fact:

```{python}
dated_weather = weather.set_index("DATE")
dated_weather.head()
```

Notice above that the leftmost column of `dated_weather` is the index, which still has the column name *DATE*. We can access a row by giving a date string to `loc`:

```{python}
dated_weather.loc["1979-03-28"]
```

We can extract a row and column simultaneously by giving both to `loc`:

```{python}
dated_weather.loc["1979-03-28", "TMAX"]
```

When the index is a date, as is the case here, we can even use a range of dates:

```{python}
dated_weather.loc["1979-03-01":"1979-03-06"]
```
::::

Frequently we want to select rows from a data frame based on criteria applied to the data itself. We can use relational operators such as `>`, `<`, `==`, etc. to select rows based on the values in a column.

::::{#exm-data-row-select chapter=1 description="Selecting rows in a frame" type=üíª}
Here is how we can select all the rows in which the value of *PRCP* is greater than 1200:

```{python}
weather.loc[weather["PRCP"] > 1200]
```

There are two steps above. The expression inside the square brackets produces a series of Boolean values:

```{python}
is_prcp = (weather["PRCP"] > 1200)
is_prcp.head()
```

This series can be used to select rows with `loc`. Actually, it works without `loc` as well, because pandas knows it is a row selector:

```{python}
print("There are",is_prcp.sum(),"rows with PRCP > 1200.")
weather[is_prcp]
```

We can use logical operators `&` (AND), `|` (OR), and `~` (NOT) on these Boolean series in order to combine criteria:
```{python}
weather[(weather["TMAX"] < 0) & (weather["SNOW"] > 0)]
```
::::

:::{.column-margin}
{{< video https://www.dropbox.com/s/uwrmgpz15mm5aj9/Example1_10.mp4?raw=1 >}}
:::


:::{.column-margin}
{{< video https://www.dropbox.com/s/h3ezvnpyn8xe5ch/Section1_4_c.mp4?raw=1 >}}
:::

::: {.callout-tip}
The pandas user guide has a handy [section on selections](https://pandas.pydata.org/docs/user_guide/10min.html?highlight=boolean#selection).
:::

::: {.callout-tip}
For practice with pandas fundamentals, try the [Kaggle course](https://www.kaggle.com/learn/pandas).
:::

## Data preparation

Raw data often needs to be manipulated into a useable format before algorithms can be applied. Preprocessing data so that it is suitable for machine analysis is known as **data wrangling** or **data munging**. A related process is **data cleaning**, where missing and anomalous values are removed or replaced.

### Missing values

In real data sets, we often must cope with data series that have missing values. This is a common source of mistakes and confusion, especially because there is no universal practice. Sometimes zero is used to represent a missing number. It's also common to use an impossible value, such as $-999$ to represent weight, to signify missing data. 

:::{.column-margin}
{{< video https://www.dropbox.com/s/wz9vpncyyyw2xex/Section1_5_a.mp4?raw=1 >}}
:::

Formally, the most natural way to represent missing data in Python is as `nan` or `NaN`, and pandas makes it easy to find and manipulate such values. 

::::{#exm-data-missing chapter=1 description="Handling missing values in pandas" type=üíª}

Here is a well-known data set about penguins:

```{python}
import seaborn as sns
penguins = sns.load_dataset("penguins")
penguins.head()
```

Note above that the fourth row of the frame is missing measurements. We can discover how many such rows there are using `isna`:

```{python}
penguins.isna()
```

```{python}
penguins.isna().sum()
```

Sometimes one replaces missing values with average or other representative values, a process called *imputation*. But it's often prudent to simply toss them out, as follows:

```{python}
print("original counts:")
print( penguins.count() )
penguins.dropna( inplace=True )
print()
print("after removals:")
print( penguins.count() )
```

::: {.callout-tip}
Operations that make changes to or retrieve subsets from a data frame work on copies of the frame. When `inplace=True` is given, though, the operation changes the original frame. The difference is important when dealing with very large datasets that you want to avoid copying within memory, even temporarily.
:::

### Loans example

To demonstrate algorithms in later sections, we will be using a [dataset describing loans](https://www.kaggle.com/datasets/imsparsh/lending-club-loan-dataset-2007-2011) made on the crowdfunding site LendingClub. 

:::{.callout-tip}
It's possible to import datasets from the Web directly into pandas. However, web sources and links change and disappear frequently, so if storing the dataset is not a problem, you may want to download your own copy before working on it.
:::

:::{.column-margin}
{{< video https://www.dropbox.com/s/t56b2blgzn6s9a6/Section1_5_b.mp4?raw=1 >}}
:::


::::{#exm-data-loans chapter=1 description="Cleaning a data frame" type=üíª}

First, we load the raw data from a CSV (comma separated values) file.
```{python}
import pandas as pd
loans = pd.read_csv("_datasets/loan.csv")
loans.head()
```

The `int_rate` column, which gives the interest rate on the loan, has been interpreted as strings due to the percent sign. We'll strip out those percent signs and convert them to floats.

:::{.callout-tip}
As you see below, we often end up with chains of methods separated by dots. Python works from left to right, evaluating a sub-expression and then replacing it with the object for the next segment in the chain. We could write these as a sequence of separate lines having intermediate results assigned to variable names, but it's often considered better style to chain them.
:::

```{python}
num_rate = loans["int_rate"].str.strip('%').astype(float)
print(num_rate.head())
loans["int_rate"] = num_rate
```
Let's add a column for the percentage of the loan request that was eventually funded. This will be a target for some of our learning methods.

```{python}
loans["percent_funded"] = 100 * loans["funded_amnt"] / loans["loan_amnt"]
```

We will only use a small subset of the numerical columns: 

```{python}
columns = [ "percent_funded", "loan_amnt", "int_rate", "installment", "annual_inc",
    "dti", "delinq_2yrs", "delinq_amnt" ]
loans = loans[columns]
```

Let's verify that there are no missing values in those columns:
```{python}
loans.isna().sum()
```

Finally, we'll output this cleaned data frame to its own CSV file. The index row is an ID number with no inherent meaning, so we instruct pandas to exclude it from the new file:

```{python}
loans.to_csv("loan_clean.csv", index=False)
```
::::

## Exercises {.unnumbered}

::::{#exr-data-types}
For each type of data, classify it as discrete quantitative, continuous quantitative, categorical, or other.

a) How many students are enrolled at a university
a) Your favorite day of the week
a) How many inches of rain fall at an airport during one day
a) Weight of a motor vehicle
a) Manufacturer of a motor vehicle
a) Text of all Yelp reviews for a restaurant
a) Star ratings from all Yelp reviews for a restaurant
a) Size of the living area of an apartment
a) DNA nucleotide sequence of a cell
::::

::::{#exr-data-vectors}
Give the length of each vector or series.

a. Morning waking times every day for a week
a. Number of siblings (max 12) for each student in a class of 30
a. Position and momentum of a roller coaster car
::::

::::{#exr-test-dummy}
Describe a scheme for creating dummy variables for the days of the week. Use your scheme to encode the vector:

``` 
[Tuesday, Sunday, Friday, Tuesday, Monday]
```
::::
