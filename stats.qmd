# Descriptive statistics
::: {.content-visible unless-format="pdf"}
{{< include _macros.qmd >}}
:::

When confronted with a new dataset, it's crucial to get a sense of its characteristics before attempting to draw conclusions or predictions from it. There are two aspects of this exploration: numerical and graphical. We begin with numbers.


## Summary statistics

The idea of descriptive statistics is to reduce a large collection of numerical values to a much smaller one, or even a single number, that captures essential features. You have probably encountered at least a few of these statistics before. In many cases, the statistical results connect to mathematical theory that makes it possible to reason abstractly about the data. 

::::{#exm-stats-summary chapter=2 type=üíª description="Summary statistics in pandas"}
Here a classic dataset about flowers:

```{python}
import pandas as pd
iris = pd.read_csv('https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv')
iris.info()
```

The `describe` method of a data frame gives a bunch of summary statistics for each column of quantitative data:

```{python}
iris.describe()
```
::::

:::{.column-margin}
![](_media/memes/panda.jpg)

:::

We now discuss the definitions and interpretations of the values prodcued by `describe`.

### Mean and dispersion

Here are the Big Three of summary statistics: mean, variance, and standard deviation.

::::{#def-stats-meanvarstd}
Given data values $x_1,\ldots,x_n$, their **mean** is
$$
\mu = \frac{1}{n}\sum_{i=1}^n x_i,
$$ {#eq-stats-mean}

their **variance** is 
$$
\sigma^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \mu)^2,
$$ {#eq-stats-var}

and their **standard deviation** (STD) is $\sigma$, the square root of the variance.
::::

:::{.column-margin}
<div style="max-width:304px"><div style="position:relative;padding-bottom:75.986842105263%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_geff87ds&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_d2z5o00p" width="304" height="231" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 2.1.1" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

Mean is a measurement of central tendency. Variance and STD are measures of spread or **dispersion** in the data.

::::{#exm-stats-maxvar chapter=2 type=‚úçÔ∏è description="Standard deviation by hand"}
Suppose that $x_1=0$, $x_2=t$, and $x_3=-t$, where $|t| \le 6$. What are the minimum and maximum possible values of the standard deviation? 

:::{.solution}
The mean is $\mu=0$, hence
$$
\sigma^2 = \frac{1}{3}\left[ 0^2 + t^2 + (-t)^2 \right] = \frac{2}{3} t^2.
$$
From this we conclude 
$$
\sigma =  \sqrt{\tfrac{2}{3}} |t|.
$$
Given that $0 \le |t| \le 6$, we see that the minimum value of $\sigma$ is 0 and the maximum is $2\sqrt{6}$.
:::

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_o10wj99b&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_3dy7ythh" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.2" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

::: {.callout-note}
Variance is in units that are the square of the data, which can be hard to interpret. But it has other characteristics that are useful mathematically.
:::

### Populations and samples

In statistics one refers to the **population** as the entire universe of available values. Thus, the ages of adults on Earth at some instant has a particular population mean and standard deviation. However, we can usually only measure a **sample** of the population directly, and we use the sample mean and standard deviation as estimates of the population values.

The definitions in @def-stats-meanvarstd are for populations. When they are used on a sample rather than a population, we change the notation a bit as a reminder.

::::{#def-stats-sample-mean}
Given a sample of data values $x_1,\ldots,x_n$, the **sample mean** is

$$
\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i,
$$ {#eq-stats-mean-sample}

and the **sample variance** is
$$
s_n^2 = \frac{1}{n}\sum_{i=1}^n (x_i - \bar{x})^2.
$$ 

::::

:::{.column-margin}
![](_media/memes/sample.jpg)
:::

It can be proved that the sample mean is an accurate way to estimate the population mean, in the following precise sense. If, in a thought experiment, we could average $\bar{x}$ over all possible samples of size $n$, the result would be exactly the population mean $\mu$. That is, we say that $\bar{x}$ is an **unbiased estimator** for $\mu$.

The situation with sample variance is more subtle. If $s_n^2$ is averaged over all possible sample sets, we do *not* get the population variance $\sigma^2$; hence, $s_n^2$ is called a **biased estimator** of the population variance. This can be proved by a calculation we won't go into here, but the essential issue is that the sample mean used in $s_n^2$ is itself only an estimate of the population mean. A minor adjustment fixes that issue.

::::{#thm-stats-sample-var}
An unbiased estimator for $\sigma^2$ is

$$
s_{n-1}^2 = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2.
$$ {#eq-stats-var-sample}

::::

:::{.callout-warning}
Sources are not always uniform using this terminology. Some use *sample variance* to mean $s_{n-1}^2$, not $s_n^2$, and many even omit the subscripts. You should check any source to understand its conventions.
:::

::::{#exm-stats-sample chapter=2 type=‚úçÔ∏è description="Sample variance"}
The values [1, 4, 9, 16, 25] have mean $\bar{x}=55/5 = 11$. The sample variance is 

$$
\begin{split}
    s_n^2 &= \frac{(1-11)^2+(4-11)^2+(9-11)^2+(16-11)^2+(25-11)^2}{5} \\ 
    & = \frac{374}{5} = 74.8.
\end{split}
$$

By contrast, the unbiased estimate of population variance from this sample is 

$$
s_{n-1}^2 = \frac{374}{4} = 93.5.
$$

::::

As you can see from the formulas and in @exm-stats-sample, $s_n^2$ is always an overestimate of the variance. However, the difference becomes negligible as the sample size $n$ increases, and in any practical dataset the distinction is basically academic.

:::{.callout-caution}
NumPy computes the biased estimator of variance by default, while pandas computes the unbiased version. Whee! 
:::

For standard deviation, *neither* $s_n$ *nor* $s_{n-1}$ is an unbiased estimator of $\sigma$, and there is no simple fix. Again, though, it's not usually something we worry about in practice. 

::::{#exm-stats-meanvar-pandas chapter=2 type=üíª description="Mean, variance, STD in pandas"}
While the `describe` method computes all the summary statistics, you can also obtain them individually using the `mean`, `var`, and `std` methods:

```{python}
iris.mean()
```

The result above is a series with the mean of each column. When given a series, the result is just a number:

```{python}
iris["sepal_length"].var()
```

::::

### z-scores

Comparing or operating on datasets that have different ranges can be difficult. One common step is to **standardize** the data.

:::{#def-stats-zscores}
Given data values $x_1,\ldots,x_n$, we define the **standardized scores** or **z-scores** by

$$
z_i = \frac{x_i-\mu}{\sigma}, \qquad i=1,\ldots,n.
$$ {#eq-stats-zscores}
:::

::: {.callout-note}
@eq-stats-zscores requires the population statistics $\mu$ and $\sigma$. If sample estimates are used, the results are technically called *t-scores* in classical statistics, but this distinction is generally ignored for large datasets in practice.

:::

In physical terms, z-scores are dimensionless, i.e., not dependent on the physical units chosen to express the data. The following characteristics are important:

:::{#thm-stats-zscores}
The z-scores have mean equal to zero and variance equal to 1. 

:::

::: {.proof}
Using the definition @eq-stats-zscores, we calculate the mean of the z-scores as

$$
\begin{split}
    \frac{1}{n}\sum_{i=1}^n z_i &= \frac{1}{n}\sum_{i=1}^n \frac{x_i-\mu}{\sigma} \\
    &= \frac{1}{n\sigma}\sum_{i=1}^n x_i - \frac{1}{n\sigma}\sum_{i=1}^n \mu \\
    &= \frac{1}{n\sigma}\sum_{i=1}^n x_i - \frac{1}{n\sigma}(n\mu) \\
    &= \frac{1}{n\sigma}\sum_{i=1}^n x_i - \frac{\mu}{\sigma} \\
    &= \frac{1}{\sigma}\left( \frac{1}{n}\sum_{i=1}^n x_i - \mu \right) = 0.
\end{split}
$$

A longer but similar direct calculation shows that the variance is 1.

:::

::::{#exm-stats-zscores chapter=2 type="‚úçÔ∏è, üíª" description="Z scores"}
Continuing with the values from @exm-stats-maxvar, we assume without losing generality that $t\ge 0$. (Otherwise, we can just swap $x_2$ and $x_3$.) Then we have the z-scores
$$
z_1 = \frac{0-0}{2t\sqrt{6}} = 0, \quad z_2 = \frac{t-0}{2t\sqrt{6}} = \frac{1}{2\sqrt{6}} \quad z_3 = \frac{-t-0}{2t\sqrt{6}} = \frac{-1}{2\sqrt{6}}. 
$$

These are independent of $t$, which just scales the original values. 
::::

::::{#exm-stats-zscores-py chapter=2 type=üíª description="Z scores in Python"}
We can write a little function to compute z-scores in Python:
```{python}
def standardize(x):
    return (x - x.mean()) / x.std()
```

This function can be applied to any series `x` and returns a series of the same length. If we apply it to a data frame, it standardizes each column:

```{python}
iris["z"] = standardize( iris["petal_length"] )
iris[ ["petal_length", "z"] ].describe()
```

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_btuf0vym&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_d85ascs9" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.6" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

:::{.callout-caution}
Since floating-point values are rounded off, it's unlikely that a value derived from them that is meant to be zero will actually be exactly zero. Above, the mean value of about $-10^{-15}$ should be seen as reasonable for values that have been rounded off in the 15th digit or so.
:::


### Median and quantiles

Mean and variance are not the most relevant summary statistics for every dataset. There are important alternatives.

:::{#def-stats-percentile}
For any $0 < p < 1$, the $100p$-**percentile** or **quantile** is the value of $x$ such that $p$ is the probability of observing a population value less than or equal to $x$. 

The 50th percentile is known as the **median** of the population.
:::

<!-- In other words, percentiles are the inverse function of the CDF.  -->

::: {.callout-note}
Some sources reserve the term *quantile* for another meaning, but since pandas offers `quantile` to compute percentiles, we don't draw a distinction.
:::

The unbiased sample median of $x_1,\ldots,x_n$ can be computed by sorting the values into $y_1,\ldots,y_n$. If $n$ is odd, then $y_{(n+1)/2}$ is the sample median; otherwise, the average of $y_{n/2}$ and $y_{1+(n/2)}$ is the sample median. 

::::{#exm-stats-median chapter=2 type=‚úçÔ∏è description="Median"}
For the sorted sample values $1,3,3,4,5,5,5$, we have $n=7$ and the sample median is $4$. 

If the sample values are $1,3,3,4,5,5,5,9$, then $n=8$ and the sample median is $(4+5)/2=4.5$.
::::

::::{#exm-stats-quantiles chapter=2 type=üíª description="Quantiles"}
Here we find the 90th quantile (percentile) of the *loan_amnt* variable in the `loans` dataset:

```{python}
loans = pd.read_csv("_datasets/loans_small.csv")
x = loans["loan_amnt"]
pct90 = x.quantile(0.9)
print(pct90)
```

Equivalently, 90 percent of the values are no greater than that value: 
```{python}
sum(x <= pct90) / len(x)
```

The 50th percentile is the same thing as the median:
```{python}
print(x.median())
print(x.quantile(0.50))
```

::::

The median is a measure of central tendency that can substitute for the mean. To calculate an alternative to the standard deviation, we can use two other quantiles.

::::{#def-stats-quartile}
The 25th, 50th, and 75th percentiles are the called first, second, and third **quartiles** of the distribution. The **interquartile range (IQR)** is the difference between the 75th percentile and the 25th percentile.
::::

<!-- Sometimes the definition above is extended to the *zeroth quartile*, which is the minimum sample value, and the *fourth quartile*, which is the maximum sample value. -->

For some samples, the median and IQR might be a good substitute for the mean and standard deviation.

::::{#exm-stats-describe chapter=2 type=üíª description="Quartiles and IQR"}
The dataframe `describe` method includes mean, standard deviation, and the quartiles:

```{python}
loans.describe()
```

It's easy to write a function that computes and returns the IQR of a series:
```{python}
def IQR(x):
    Q25, Q75 = x.quantile( [0.25, 0.75] )
    return Q75 - Q25 

IQR(loans["loan_amnt"])
```

That function can also be applied columnwise to a data frame:

```{python}
loans.apply(IQR)
```

::::

## Empirical distributions

Mean and STD, or median and IQR, attempt to summarize quantitative data with a couple of numbers. At the other extreme, we can express the distribution values more precisely using functions and graphical tools. 

Python has many graphics packages with different niches. The most widespread is **Matplotlib**, for which you must explicitly specify most aspects of how the plots will look. 

We will instead use **seaborn**, which is built on top of Matplotlib. It's built around a declarative style, meaning that you describe what you want to see and seaborn makes the decisions about how it looks. Most of the time, the decisions it makes are just fine. To use seaborn, you import it:

```{python}
import seaborn as sns
```

There are three major commands that you call in seaborn:

`displot`
: How values of a single variable are distributed.

`catplot`
: How categorical values are distributed within and across categories.

`relplot`
: How values of two variables are related to each other.

These three commands then call other functions to create the actual plots. (You can call those directly yourself, but then you will be missing a lot of automated setup and formatting that seaborn provides.)

### Histograms

By definition, we know that if $a<b$, $\hat{F}(b) - \hat{F}(a)$ is the number of observations in the half-open interval $(a,b]$. This leads into the next definition.

::::{#def-stats-histogram}
Select the ordered values $t_1 < t_2 < \cdots < t_m$, called **edges**, and define **bins** as the intervals
$$
B_k = (t_k,t_{k+1}], \qquad k=0,\ldots,m,
$$

where we adopt the convention that $t_0=-\infty$ and  $t_{m+1}=\infty$. Let $c_k$ be the number of data values in $B_k$. Then a **histogram** relative to the bins is the list of $(B_0,c_0),\ldots,(B_m,c_m)$.
::::

Seaborn uses `displot` to show empirical distributions, and the default is to show a histogram.

::::{#exm-stats-ecdf-penguins chapter=2 type=üíª description="ECDF of penguins"}
Here is a histogram from the `penguins` dataset:

```{python}
penguins = sns.load_dataset("penguins")
sns.displot(data=penguins, x="body_mass_g");
```

We can specify the number of bins to use, or their edges:

```{python}
sns.displot(data=penguins, x="body_mass_g", bins=20);
```

Sometimes we'd rather show the proportions of values rather than the counts:

```{python}
sns.displot(data=penguins, x="body_mass_g", stat="proportion");
```

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_4eybb266&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_yqdbpjv0" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.10" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

### Empirical CDF

Given a sample of a population, we can characterize it using an important function defined by the data.

::::{#def-stats-ecdf}
The **empirical cumulative distribution function** or ECDF of a sample is the function $\hat{F}(t)$ whose value at $t$ equals the proportion of the sample values that are less than or equal to $t$.

::::

Note that the value of $\hat{F}$ ranges between 0 and 1 (inclusive) and is a nondecreasing function of $t$.

::::{#exm-stats-uniform-discrete chapter=2 type=‚úçÔ∏è description="ECDF of a simple distribution"}
Consider the population

$$
x_1=1/4, \quad x_2=2/4, \quad x_3=3/4, \quad x_4=1. 
$$

Since the smallest value is at 1/4, we have $F(t)=0$ when $t < 1/4$.  At $t=1/4$ we have passed 1 of the 4 values, so $F(1/4)=1/4$. This value of $F$ remains constant until we get to $t=1/2$, at which point it jumps to the value $1/2$, and so on, leading to the graph in @fig-stats-cdf-a

![CDF for a population of size 4.](_media/ch2_cdf_a.svg){width=75% #fig-stats-cdf-a}

Now consider the population 

$$
x_1=1/10, \quad x_2=2/10, \quad x_3=3/10, \quad \ldots, \quad x_{10}=1. 
$$

Using the same reasoning as above, we get the graph in @fig-stats-cdf-b.

![CDF for a population of size 10.](_media/ch2_cdf_b.svg){width=75% #fig-stats-cdf-b}

::::

::::{#exm-stats-ecdf-penguins chapter=2 type=üíª description="ECDF of penguins"}
A variation of the seaborn `displot` function creates an ECDF. Here is one for the `penguins` dataset:
```{python}
penguins = sns.load_dataset("penguins")
sns.displot(data=penguins, x="body_mass_g", kind="ecdf");
```

It's a stairstep plot, because the ECDF jumps at each data value.

::::

There is a simple relationship between the ECDF $\hat{F}$ and histograms. If there are $n$ total observations in the sample and $c_k$ is the count in bin $(t_k,t_{k+1}]$, then 

$$
c_k = n[\hat{F}(t_{k+1}) - \hat{F}(t_k)].
$$ {#eq-stats-hist-ecdf}


## Continuous distributions

Paradoxically, sometimes infinity makes things easier. For instance, a Riemann sum is a long, complicated expression, but in the limit of infinitely many rectangles, it becomes a simple definite integral. Something similar happens when we think about populations so large that they can be considered infinite.

### CDF

::::{#exm-stats-uniform-continuous chapter=2 type=‚úçÔ∏è description="CDF of a uniform distribution"}
It's natural and easy to generalize @exm-stats-uniform-discrete to samples of any size $n$: $x_i=i/n$ for $i=1,\ldots,n$. Intuitively you cam see that as $n$ increases,     the stairs in the plots in @fig-stats-cdf-a and @fig-stats-cdf-b become narrower and shallower, eventually becoming a smooth ramp:

![CDF for an infinite population.](_media/ch2_cdf_c.svg){width=75% #fig-stats-cdf-c}

The formal expression of the function plotted in @fig-stats-cdf-c is

$$
F(t) = \begin{cases} 
0, & t < 0, \\ 
t,& 0 \le t \le 1, \\ 
1,& t > 1.
\end{cases}
$$  {#eq-stats-uniform-cdf}

Conceptually, this function represents a distribution in which every value between 0 and 1 is equally likely. This is known as the **uniform distribution** over the interval $[0,1]$.

::::

We now define the continuous counterpart of the ECDF.

::::{#def-stats-cdf-continuous}
The **cumulative distribution function** (CDF) of a distribution is the function 
$$
F(t) = \text{probability that a randomly chosen value is $\le t$},
$$

where $t$ is any real number.

::::

For a finite population, the CDF and the ECDF are the same. 

::::{#exm-stats-temps-ecdf chapter=2 type=üíª description="ECDF of temperatures"}
Here are maximum daily temperatures (in tenths of Celsius degrees) for Newark, DE, starting in 1894:
```{python}
weather = pd.read_csv("_datasets/ghcn_newark.csv")
sns.displot(weather, x="TMAX", kind="ecdf");
```

More than 29,000 observations are summarized in this ECDF. There are still small steps visible in the graph above because the temperature values are discrete. Even so, there would be little changed by approximating the distribution with a smooth one.

::::

Conceptually we may interpret a large sample as an approximation to an infinite population. In this case, the ECDF is an approximation to the CDF.

### PDF

In @eq-stats-hist-ecdf we connected a histogram to the ECDF via ${c_k} = n[\hat{F}(t_{k+1}) - \hat{F}(t_k)]$, where $c_k$ is the count in bin $(t_k,t_{k+1}]$. If we let $p_k = c_k/n$ be the fraction of observations in that bin, then we can write

$$
\frac{p_k}{t_{k+1}-t_k} = \frac{\hat{F}(t_{k+1})-\hat{F}(t_k)}{t_{k+1}-t_k}. 
$$  {#eq-stats-pdf}

We can interpret the left-hand side of @eq-stats-pdf as the density of observations in the bin. If we take the limit $t_{k+1} \to t_k$, we get a derivative on the right-hand side, which motivates the following definition.

::::{#def-stats-pdf}
The **probability density function** or **PDF** of a distribution, denoted $f(t)$, is the derivative of the CDF, i.e.,

$$
f(t) = F'(t).
$$ {#eq-stats-pdf-def}

::::

The intuitive interpretation of $f(t)$ is as the probability of observing a value in a tiny interval around $t$. 

::::{#exm-stats-temps-pdf chapter=2 type=‚úçÔ∏è description="PDF of temperature distribution"}
The histogram of the temperature distribution in @exm-stats-temps-ecdf is an approximation to the continuous PDF, if we normalize the counts properly:

```{python}
sns.displot(weather, x="TMIN", bins=20, stat="density");
```

There is an algorithm known as **kernel density estimation** (KDE) that can be used to estimate the true PDF from a sample. It's another option in the `displot` function:

```{python}
sns.displot(weather, x="TMIN", kind="kde");
```

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_a1g6pze9&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_86alwtnf" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.15" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

::::{#exm-stats-uniform-pdf chapter=2 type=‚úçÔ∏è description="PDF of a uniform distribution"}
For a uniform distribution over $[0,1]$, we can take the derivative of the CDF in @eq-stats-uniform-cdf to get

$$
F'(t) = \begin{cases}
0, & t < 0, \\
1, & 0 < t < 1, \\
0, & t > 1.
\end{cases}
$$ {#eq-stats-uniform-pdf}

We're sweeping some details under the rug here, because $F$ is not differentiable at $t=0$ and $t=1$. It turns out that we can mostly ignore such points in practice.

::::

The Fundamental Theorem of Calculus and @eq-stats-pdf-def imply that the CDF is the integral of the PDF. Specifically, for any value of $a$,

$$
F(t) = F(a) + \int_{a}^t f(s) \, ds.
$$ {#eq-stats-cdf-integral}

### Random numbers in Python

Generating truly random numbers on a computer is not simple. We rely on *pseudorandom* numbers, which are generated by deterministic functions called **random number generators** (RNGs) that have extremely long periods. One consequence is repeatability: by specifying the starting state of the RNG, you can get exactly the same pseudorandom sequence every time.

::: {.callout-caution}
There is an older way to use random numbers in NumPy than the one presented here. You'll still find it on the web and in some books, but the newer way is recommended.

:::

::::{#exm-stats-numpy-uniform chapter=2 type=üíª description="Uniform random numbers in NumPy"}
We start by creating an RNG with a specific state. Every time this code is run, the same sequence of numbers will be generated from it.

```{python}
from numpy.random import default_rng
rng = default_rng(19716)    # setting an initial state
```

The `uniform` generator method produces numbers distributed uniformly between two limits you specify.

```{python}
rng.uniform( 0, 1, size=5)
```

For a large uniform random sample, the ECDF will approximate the CDF in @fig-stats-cdf-c.

```{python}
x = rng.uniform( size=10000 )
sns.displot(x, kind="ecdf");
```

Similarly, a histogram of the sample will approximate a plot of the PDF in @eq-stats-uniform-pdf:

```{python}
sns.displot(x, bins=16, stat="density");
```

::::


:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_idm34uwa&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_zo16ot10" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.17" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>
</br>
![](_media/memes/cat-distributon.jpg){width=100%}

:::


### Normal distribution

Next to the uniform distribution, the following is the most important continuous distribution to know.

::::{#def-stats-normal}
The **normal distribution** or *Gaussian distribution* with mean $\mu$ and variance $\sigma^2$ is defined by the PDF

$$
f(t) = \frac{1}{\sigma \sqrt{2\pi}} e^{ -(t - \mu)^2/(2\sigma^2)}. 
$$ {#eq-stats-normal}

The **standard normal** distribution uses $\mu=0$ and $\sigma=1$.

:::: 

For data that are distributed normally, about 68% of the values lie within one standard deviation of the mean, and 95% lie within two standard deviations. Note however that all real values of $t$ are theoretically possible. 

One appealing fact about the normal distribution is that the effects of scaling and shifting the variable are simple:

:::{#thm-stats-normal}
If the variable $s$ has a standard normal distribution, then the variable $t = \sigma s + \mu$ has a normal distribution with mean $\mu$ and variance $\sigma^2$.

:::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_aclhtcjr&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_xhmhxyac" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Normal distribution" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>
</br>

![](_media/memes/cant-be-normal.jpg){width=60%}

:::

::::{#exm-stats-normal chapter=2 type=üíª description="Normal distribution in NumPy"}
The `normal` method of a NumPy RNG simulates a standard normal distribution.

```{python}
rng = default_rng(19716)
x = rng.normal( size=(30000,) )
sns.displot(x, bins=25, stat="density");
```

We can change the variance by multiplying by $\sigma$ and change the mean by adding $\mu$:

```{python}
df = pd.DataFrame( {"x": x, "3x-10": 3*x-10} )
df.describe()
```

A KDE plot shows us a result close to the classic bell curve:

```{python}
sns.displot(data=df, x="3x-10", kind="kde");
```

::::

## Grouping data

It's often useful to analyze data in groups defined by one or more columns. Grouping data is tied to the idea of **conditional probability**. 

::::{#exm-stats-groups chapter=2 type=üíª description="Grouping data"}
This is a dataset about the fuel efficiency of some cars:

```{python}
cars = sns.load_dataset("mpg")
cars.head()
```

Here's the distribution of the *mpg* variable in `cars` over the entire dataset:

```{python}
sns.displot(data=cars, x="mpg", bins=20);
```

Here is the probability that a randomly chosen car has *mpg* less than 20:

```{python}
(cars["mpg"] < 20).mean()
```

Now, note that one of the columns of the dataset indicates the place of manufacture:

```{python}
cars["origin"].value_counts()
```

Intuitively, it seems likely that the distribution of *mpg* will differ for different countries of origin. For example, if we narrow our focus to cars made in the U.S.A., we get a very different result for the probability of *mpg* less than 20:

```{python}
(cars.loc[cars["origin"]=="usa", "mpg"] < 20).mean()
```

There is no need to pattern your code after this example, as we will see more convenient ways to do this sort of computation in @sec-stats-groupby.

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_knofje34&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_csdtudbz" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.19" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

### Facet, box, and violin plots

In a **facet plot**, a distribution plot is repeated across columns or rows for each group, making it easy to compare the groups visually.

::: {.callout-important}
In these examples we sometimes use `height` or `width` keywords to specify the size of the plots (in inches) for this output format. You should start without them and use them only as needed.
:::

::::{#exm-stats-groups-facet chapter=2 type=üíª description="Facet plots"}
A facet plot allows us to easily group data by a categorical variable. In this case, we select the *origin* column using the `col` keyword:

```{python}
sns.displot(data=cars, x="mpg", col="origin", height=2.2);
```

A key feature of the facet plots is that the $x$ axis limits are the same in all three cases, facilitating a direct visual comparison.

::::

Other ways to visualize grouped data are offered by the `catplot` function in seaborn, including the well-known **box plot** and **violin plot**.

::::{#exm-stats-groups-box-violin chapter=2 type=üíª description="Box and violin plots"}
Here is a grouped **box plot** for *mpg*:

```{python}
sns.catplot(data=cars, x="origin", y="mpg", kind="box");
```

Each colored box shows the interquartile range, with the interior horizontal line showing the median. The whiskers and dots are explained in a later section. A related visualization is a **violin plot**:

```{python}
sns.catplot(data=cars, 
    x="mpg", y="origin", 
    kind="violin"
    );
```

In a violin plot, the inner lines show the same information as the box plot, with the thick part showing the IQR, while the sides of the "violins" are KDE estimates of the density functions.

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_4stcfpp7&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_6tx09vnr" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.21" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

### Grouped operations in pandas {#sec-stats-groupby}

In pandas, the `groupby` method splits a data frame into groups based on values in a designated (categorical) column. By itself, this method doesn't accomplish much, but it's a prelude to operating in a groupwise (i.e., conditional) manner.

#### Aggregation

When you want to compute a summary statistic for each group, you need an *aggregator*. A list of the most common predefined aggregation functions is given in @tbl-stats-agg. 

| method | effect | 
|---------|--------|
| `count` | Number of values in each group |
| `mean`  | Mean value in each group |
| `sum` |  Sum within each group  |
| `std`, `var` | Standard deviation/variance within groups |
| `min`, `max` | Min or max within groups |
| `describe` | Descriptive statistics |
| `first`, `last` | First or last of group values |

: Aggregation functions. All ignore `NaN` values. {#tbl-stats-agg}

::::{#exm-stats-groups-aggregation chapter=2 type=üíª description="Aggregating data in groups"}
Here is how we can define groups based on the categorical *origin* column:

```{python}
cars.groupby("origin")
```

As you can see from the output above, a grouped data frame isn't of much value on its own. But let's use it to find the groupwise mean of each quantitative column in the frame:

```{python}
cars.groupby("origin").mean()
```

Multiply dotted commands are common in pandas. You need to parse them from left to right. The first command above groups the data frame, and the second computes the mean within each group.

If we want to focus on just *mpg* from the dataset, we can extract that column *after* the grouping but *before* operating on the groups:

```{python}
cars.groupby("origin")["mpg"].mean()
```

To help interpret this expression, we could break it into equivalent separate steps:

```{python}
by_origin = cars.groupby("origin")
mpg_by_origin = by_origin["mpg"]
mpg_by_origin.mean()
```

However, the one-liner is considered better style most of the time.

If you want an aggregator other than those in @tbl-stats-agg, you can call `agg` with your own function. Here are the conditional probabilities of *mpg* being less than 20 for all values of *origin*:

```{python}
def less_than_20(x):
    return (x < 20).mean()

cars.groupby("origin")["mpg"].agg(less_than_20)
```

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_d0r09h5o&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_u8e4y4w5" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.22" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>
</br>
![](_media/memes/groupby.jpg){height="240" alt-text="Blue Men Group-ing"}

:::

#### Transformation

A **transformation** applies a function groupwise to a column, producing a column of the same length. 

::::{#exm-stats-groups-transformation chapter=2 type=üíª description="Transforming data in groups"}
We can convert the *mpg* values to z-scores using the population mean and variance:

```{python}
def standardize(x):
    return (x - x.mean()) / x.std()

cars["mpg_z"] = standardize(cars["mpg"])
sns.displot(data=cars, x="mpg_z", col="origin", height=2.3);
```

Above, each of the conditional distributions was shifted and scaled identically. But we might instead, want to standardize within groups separately, with each group using its own mean and standard deviation:

```{python}
by_origin = cars.groupby("origin")
cars["group_mpg_z"] = by_origin["mpg"].transform(standardize)
sns.displot(data=cars, x="group_mpg_z", col="origin", height=2.3);
```

Now, each group distribution is centered at 0 and has a standard deviation of 1.

::::


### Grouping by a quantitative variable

Grouping is easiest to think about as being conditioned on a categorical variable. But it can also be done on a quantitative variable by first binning it into intervals using `pd.cut`.

::::{#exm-stats-groups-cuts chapter=2 type=üíª description="Grouping data by cuts"}
Suppose we want to group cars by horsepower, which is quantitative. We can use `pd.cut` to create bins of width 40:

```{python}
cuts = pd.cut(cars["horsepower"], range(40, 250, 40))
by_hp = cars.groupby(cuts)
by_hp["mpg"].count()
```

The `count` method then gives the number of observations in each group. 

We can also use the cuts to make, for example, a violin plot:

```{python}
sns.catplot(data=cars, x="mpg", y=cuts, kind="violin");
```

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_uma3wh4j&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_j9x6hskn" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.24" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::


## Outliers

Informally, an **outlier** is a data value that is considered to be far from typical. In some applications, such as detecting earthquakes or cancer, outliers are the cases of real interest. But here we will treat them as unwelcome values that might result from equipment failure, confounding effects, mistyping a value, using an artificial extreme value to represent missing data, and so on. In such cases we want to minimize the effect of the outliers on the statistics. 


It is well known, for instance, that the mean is more sensitive to outliers than the median is. 

::::{#exm-stats-median-mean chapter=2 type=‚úçÔ∏è description="Outliers for mean and median"}
The values $1,2,3,4,5$ have a mean and median both equal to 3. If we change the largest value to be a lot larger, say $1,2,3,4,1000$, then the mean changes to 202. But the median is still 3!

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_fdm9u5cu&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_bcetgsog" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.25" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

</br>
![](_media/memes/outlier.jpg)

:::

If you want to use a statistic like the mean that is vulnerable to outliers, it's typical to remove such values early on. There are various ways of deciding what qualifies as an outlier, with no one-size recommendation for all applications.

### IQR

Let $Q_{25}$ and $Q_{75}$ be the first and third quartiles (i.e., 25% and 75% percentiles), and let $I=Q_{75}-Q_{25}$ be the interquartile range (IQR). Then $x$ is an outlier value if

$$ 
x < Q_{25} - 1.5I \text{ or } x > Q_{75} + 1.5I.
$$ {#eq-stats-outlier-iqr}

In a box plot, the whiskers growing from a box show the extent of the non-outlier data, and the dots beyond the whiskers represent outliers.

::::{#exm-stats-outlier-iqr chapter=2 type=üíª description="Interquartile range"}
Let's look at another data set, based on a functional MRI experiment:

```{python}
fmri = sns.load_dataset("fmri")
fmri.head()
```

We want to focus on the *signal* column, splitting according to the *event*.

```{python}
by_event = fmri.groupby("event")
by_event["signal"].describe()
```

Here is a box plot of the signal for these groups.

```{python}
sns.catplot(data=fmri, x="event", y="signal", kind="box");
```

The dots lying outside the whiskers in the plot can be considered outliers satisfying one of the inequalities in @eq-stats-outlier-iqr.

Let's now remove the outliers. We start with a function that computes a Boolean-valued series for a given input. This function is applied as a `transform` to the data as grouped by *events*: 

```{python}
def is_outlier(x):
    Q25, Q75 = x.quantile([.25,.75])
    I = Q75 - Q25
    return (x < Q25 - 1.5*I) |  (x > Q75 + 1.5*I)

outliers = by_event["signal"].transform(is_outlier)
fmri.loc[outliers,"event"].value_counts()
```

You can see above that there are 66 outliers. To negate the outlier indicator, we can use `~outs` as a row selector.

```{python}
cleaned = fmri[~outliers]
```

The median values are barely affected by the omission of the outliers:

```{python}
print( "medians with outliers:" )
print( by_event["signal"].median() )
print( "\nmedians without outliers:" )
print( cleaned.groupby("event")["signal"].median() )
```

The means, though, show much greater change:

```{python}
print( "means with outliers:" )
print( by_event["signal"].mean() )
print( "\nmeans without outliers:" )
print( cleaned.groupby("event")["signal"].mean() )
```

For the *stim* case in particular, the mean value changes by almost 200%, including a sign change. (Relative to the standard deviation, it's closer to a 20% change.)

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_op8fm8cw&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_2e45ik5q" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.26" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

### Mean and STD

In normal distributions, values more than twice the standard deviation $\sigma$ from the mean could be considered to be outliers; this would exclude 5% of the values, on average. A less aggressive criterion is to allow a distance of $3\sigma$, which excludes only about 0.3% of the values. The IQR criterion above corresponds to about $2.7\sigma$ in the normal distribution case.

:::{.callout-note}
A criticism of classical statistics is that much of it is conditioned on the assumption of normal distributions. This assumption is often violated by real datasets; quantities that depend on normality should be used judiciously.

:::

::::{#exm-stats-outlier-std chapter=2 type=üíª description="Outliers"}
The following plot shows the outlier cutoffs for 2000 samples from a normal distribution, using the criteria for 2œÉ (red), 3œÉ (blue), and 1.5 IQR (black).

```{python}
#| code-fold: true
import matplotlib.pyplot as plt
from numpy.random import default_rng
randn = default_rng(1).normal 

x = pd.Series(randn(size=2000))
sns.displot(data=x,bins=30);
m,s = x.mean(),x.std()
plt.axvline(m-2*s,color='r')
plt.axvline(m+2*s,color='r')
plt.axvline(m-3*s,color='b')
plt.axvline(m+3*s,color='b')

q1,q3 = x.quantile([.25,.75])
plt.axvline(q3+1.5*(q3-q1),color='k')
plt.axvline(q1-1.5*(q3-q1),color='k');
```

For asymmetric distributions, or those with a heavy tail, these criteria might show greater differences.

::::

## Correlation

There are often variables that we believe to be linked, either because one influences the other, or because both are influenced by some other factor. In either case, we say the quantities are **correlated**.

There are several ways to measure correlation. It's good practice to look at the data first, though, before jumping to the numbers.

### Relational plots

Thus far, we have used `displot`, or "distribution plot", to make histograms, and `catplot`, or "categorical plot", to make box and violin plots. The third major plot type in seaborn is `relplot`, or "relational plot", to show the relationships between variables.

::::{#exm-stats-relational chapter=2 type=üíª description="Relational plots"}
By default, `relplot` makes a scatter plot of two different variables:

```{python}
sns.relplot(data=cars, x="model_year", y="mpg");
```

Like in the other plot types, we can use hue (color) and marker size to indicate groups within the data:

```{python}
sns.relplot(data=cars, 
    x="model_year", y="mpg",
    hue="origin", size="weight",
    );
```

If we want to emphasize a trend rather than the individual data values, we can instead plot the average value at each $x$ with error bars:

```{python}
sns.relplot(data=cars, 
    x="model_year", y="mpg",
    kind="line", errorbar="sd"
    );
```

The error ribbon above is drawn at one standard deviation around the mean.

In order to see multiple pairwise scatter plots in one shot, we can use `pairplot` in seaborn:

```{python}
columns = [ "mpg", "horsepower", 
            "displacement", "origin" ]

sns.pairplot(data=cars[columns], hue="origin", height=2);
```

The panels along the diagonal show each quantitative variable's PDF. The other panels show scatter plots putting one pair at a time of the variables on the coordinate axes. 

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_cc1w9kgr&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_rl0diku3" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.28" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

### Correlation

::::{#def-stats-covariance}
Suppose we have two series of observations, $[x_i]$ and $[y_i]$, representing observations of random quantities $X$ and $Y$ having means $\mu_X$ and $\mu_Y$. Their **covariance** is defined as 
$$
\Cov(X,Y) = \frac{1}{n} \sum_{i=1}^n (x_i-\mu_X)(y_i-\mu_Y).
$$
::::

Note that the values $x_i-\mu_X$ and $y_i-\mu_Y$ are deviations from the means. 

It follows from the definitions that 

$$
\begin{split}
    \Cov(X,X) &= \sigma_X^2, \\ 
    \Cov(Y,Y) &= \sigma_Y^2.
\end{split}
$$

In words, self-covariance is simply variance.

Covariance is not easy to interpret. Its units are the products of the units of the two variables, and it is sensitive to rescaling the variables (e.g., grams versus kilograms).

We can remove the dependence on units and scale by applying the covariance to standardized scores for both variables, resulting in a measure called **correlation**. The following is the best-known measure of correlation.

::::{#def-stats-pearson}
For the populations of $X$ and $Y$, the **Pearson correlation coefficient** is
$$
\begin{split}
    \rho(X,Y) &= \frac{1}{n} \sum_{i=1}^n \left(\frac{x_i-\mu_X}{\sigma_X}\right)\left(\frac{y_i-\mu_Y}{\sigma_Y}\right) \\ 
    & = \frac{\Cov(X,Y)}{\sigma_X\sigma_Y},
\end{split}
$$ {#eq-stats-pearson-pop}
where $\sigma_X^2$ and $\sigma_Y^2$ are the population variances of $X$ and $Y$.

For samples from the two populations, we use
$$
r_{xy} =  \frac{\sum_{i=1}^n (x_i-\bar{x}) (y_i-\bar{y})}{\sqrt{\sum_{i=1}^n (x_i-\bar{x})^2}\,\sqrt{\sum_{i=1}^n (y_i-\bar{y})^2}},
$$ {#eq-stats-pearson-samp}
where $\bar{x}$ and $\bar{y}$ are sample means.

::::

Both $\rho_{XY}$ and $r_{xy}$ are between $-1$ and $1$, with the endpoints indicating perfect correlation (inverse or direct). 

An equivalent formula for $r_{xy}$ is 
$$
r_{xy} =  \frac{1}{n-1} \sum_{i=1}^n \left(\frac{x_i-\bar{x}}{s_x}\right)\, \left(\frac{y_i-\bar{y}}{s_y}\right),
$$ {#eq-stats-pearson-alt}
where the quantities in parentheses are z-scores.

::::{#exm-stats-pearson-cars chapter=2 type=üíª description="Pearson correlation coefficient"}
We might reasonably expect horsepower and miles per gallon to be negatively correlated:

```{python}
sns.relplot(data=cars, x="horsepower", y="mpg");
```

Covariance allows us to quantify the relationship:

```{python}
cars[ ["horsepower", "mpg"] ].cov()
```

But should these numbers considered big? The Pearson coefficient is more easily interpreted:

```{python}
cars[ ["horsepower", "mpg"] ].corr()
```

The value of about $-0.79$ suggests that knowing one of the values would allow us to predict the other one rather well using a best-fit straight line (more on that in a future chapter).

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_5sm57hj4&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_6i3phm87" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.29" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

As usual when dealing with means, however, the Pearson coefficient can be sensitive to outlier values. 

::::{#exm-stats-pearson-outlier chapter=2 type=üíª description="Outliers and the Pearson coefficient"}
The Pearson coefficient of any variable with itself is 1.
But let's correlate two series that differ in only one element: $0,1,2,\ldots,19$, and the same sequence but with the fifth value replaced by $-100$:

```{python}
x = pd.Series( range(20) )
y = x.copy()
y[4] = -100
x.corr(y)
```

Despite the change being in a single value, over half of the predictive power was lost. 

::::

### Spearman coefficient

The Spearman coefficient is one way to lessen the impact of outliers when measuring correlation. The idea is that the values are used only in their orderings. 

::::{#def-stats-rank-series}
If $x_1,\ldots,x_n$ is a series of observations, let their sorted ordering be
$$
x_{s_1},x_{s_2},\ldots,x_{s_n}.
$$
Then $s_1,s_2,\ldots,s_n$ is the **rank series** of $\bfx$. 

::::

::::{#def-stats-spearman}
The **Spearman coefficient** of two series of equal length is the Pearson coefficient of their rank series.

::::

::::{#exm-stats-spearman-outlier chapter=2 type=üíª description="Spearman correlation coefficient"}
Returning to @exm-stats-pearson-outlier, we find the Spearman coefficient is barely affected by the single outlier:

```{python}
x = pd.Series( range(20) )
y = x.copy()
y[4] = -100
x.corr(y,"spearman")
```

It's trivial in this case to produce the two rank series by hand:

```{python}
s = pd.Series( range(1,21) )    # already sorted
t = s.copy()
t[:5] = [2,3,4,5,1]     # modified sort ordering

t.corr(s)
```

As long as `y[4]` is negative, it doesn't matter what its particular value is:

```{python}
y[4] = -1000000
x.corr(y,"spearman")
```

::::

:::{.column-margin}
(different data, same idea)</br>
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_xemkvgjk&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_ek4cibrt" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.31" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

Since real data almost always features outlying or anomalous values, it's important to think about the robustness of the statistics you choose.

### Categorical correlation

An ordinal variable, such as the days of the week, is often straightforward to quantify as integers. But a nominal variable poses a different challenge. 

::::{#exm-stats-catcorr chapter=2 type=üíª description="Categorical correlation"}
Grouped histograms suggest an association between country of origin and MPG:

```{python}
sns.displot(data=cars, kind="kde",
    x="mpg", hue="origin");
```

How can we quantify the association? The first step is to convert the *origin* column into dummy variables:

```{python}
dum = pd.get_dummies(cars, columns=["origin"])
dum.head()
```

The original *origin* column has been replaced by three binary indicator columns. Now we can look for correlations between them and *mpg*:

```{python}
columns = [
    "mpg",
    "origin_europe",
    "origin_japan",
    "origin_usa"
    ]
dum[columns].corr()
```

As you can see from the above, `europe` and `japan` are positively associated with *mpg*, while `usa` is inversely associated with *mpg*.

:::: 

<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_lt0xli2d&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_6vg0qxus" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.32" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

![xckd by [Randall Munroe](https://xkcd.com/license.html)](https://imgs.xkcd.com/comics/correlation.png)

## Cautionary tales

Attaching theorem-supported numbers to real data feels precise and infallible. The theorems do what they say, of course---they're theorems!---but our intuition can be a little too ready to attach significance to the numbers, causing misconceptions or mistakes. Proper visualizations can help us navigate these waters safely.

### The Datasaurus

The Datasaurus Dozen is a collection of datasets that highlights the perils of putting blind trust into summary statistics. 

::::{#exm-stats-datasaurus chapter=2 type=üíª description="The Datasaurus"}
The Datasaurus is a set of 142 points making a handsome portrait:

```{python}
dozen = pd.read_csv("_datasets/DatasaurusDozen.tsv", delimiter="\t")
sns.relplot(data=dozen[dozen["dataset"]=="dino"], x="x", y="y");
```

However, there are 12 other datasets that all have roughly the same mean and variance for $x$ and $y$, and the same correlations between them:

```{python}
by_set = dozen.groupby("dataset")
by_set.mean()
```

```{python}
by_set.std()
```

```{python}
by_set.corr()
```

However, a plot reveals that these sets are, to put it mildly, quite distinct:

```{python}
others = dozen[ dozen["dataset"] != "dino" ]
sns.relplot(data=others,
    x="x", y="y",
    col="dataset", col_wrap=3, height=2.2
    );
```

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_kiug6i2v&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_uphqzzhe" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.33" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

:::

::: {.callout-important}
Always plot your data.

:::

Always üëè plot üëè your üëè data!

### Correlation vs. dependence

In casual speech, you might expect two variables that are uncorrelated to be unrelated. But that is not at all how the mathematical definitions play out. 

::::{#exm-stats-correlation chapter=2 type=üíª description="Correlation vs. dependence"}
For example, here are $x$ and $y$ variables that both depend on a hidden variable $\theta$:

```{python}
import numpy as np
theta = np.linspace(0,2*np.pi,50)
x = pd.Series(np.cos(theta))
y = pd.Series(np.sin(theta))
sns.relplot(x=x, y=y);
```

Neither informally nor mathematically would we say that $x$ and $y$ are independent! For example, if $y=0$, then there is only one possible value for $x$. Yet the correlation between $x$ and $y$ is zero:
```{python}
x.corr(y)
```

Correlation can only measure the extent of the relationship that is *linear*; $x$ and $y$ lying on a straight line means perfect correlation. In this case, every line you can draw that passes through $(0,0)$ is essentially equally bad at representing the data, and correlation cannot express the relationship.

::::

### Simpson's paradox {#sec-stats-simpson}

The penguin dataset contains a common paradox---or a counterintuitive phenomenon, at least. 

::::{#exm-stats-simpson chapter=2 type=üíª description="Simpson's paradox"}
Two of the variables show a fairly strong negative correlation:

```{python}
penguins = sns.load_dataset("penguins")
columns = [ "body_mass_g", "bill_depth_mm" ]
penguins[columns].corr()
```

But something surprising happens if we compute correlations *after* grouping by species.

```{python}
penguins.groupby("species")[columns].corr()
```

Within each individual species, the correlation between the variables is strongly positive!

This is an example of **Simpson's paradox**. The reason for it can be seen from a scatter plot:

```{python}
sns.relplot(data=penguins,
    x=columns[0], y=columns[1], 
    hue="species"
    );
```

Within each color, the positive association is clear. But what dominates the combination of all three species is the large difference between Gentoo and the others. Because the Gentoo are both larger and have shallower bills, the dominant relationship is negative. 

::::

:::{.column-margin}
<div style="max-width:400px"><div style="position:relative;padding-bottom:71.25%"><iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&playerId=kaltura_player&entry_id=1_ocuuaffk&flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&wid=1_i0596m4u" width="400" height="285" allowfullscreen webkitallowfullscreen mozAllowFullScreen allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 2.35" style="position:absolute;top:0;left:0;width:100%;height:100%;border:0"></iframe></div></div>

</br> ![](_media/memes/simpson.gif)
:::

As often happens in statistics, the precise framing of the question can strongly affect its answer. This can lead to honest mistakes by the naive as well as outright deception by the unscrupulous.

## Exercises {.unnumbered}

For these exercises, you may use computer help to work on a problem, but your submitted solution should be self-contained without reference to computer output (unless stated otherwise).

:::{#exr-stats-summary}
(2.1) The following parts are about the sample set of $n$ values ($n>2$) 

$$
0, 0, 0, \ldots, 0, 1000.
$$

(That is, there are $n-1$ copies of 0 and one copy of 1000.)

**(a)** Show that the sample mean is $1000/n$.

**(b)** Find the sample median when $n$ is odd. 

**(c)** Show that the corrected sample variance $s_{n-1}^2$ is $10^6/n$.

**(d)** Find the sample z-scores of all the values.
:::

:::{#exr-stats-z-scores}
(2.1) Suppose given samples $x_1,\ldots,x_n$ have the sample z-scores $z_1,\ldots,z_n$. 

**(a)** Show that $\displaystyle \sum_{i=1}^n z_i = 0.$ (Start by substituting the definition of $z_i$, using a new index variable $j$ for that sum.)

**(b)** Show that $\displaystyle \sum_{i=1}^n z_i^2 = n-1.$
:::

:::{#exr-stats-least-squares}
(2.1) Given a population of values $x_1,x_2,\ldots,x_n$, define the function 

$$
r_2(x) = \sum_{i=1}^n (x_i-x)^2.
$$

Show using calculus that $r_2$ is minimized at $x=\mu$, the population mean. (The idea is that minimizing $r_2$ is a way to find the "most representative" value for the dataset.)
:::

:::{#exr-stats-least-mad}
(2.1) Suppose that $n=2k+1$ and a population has values $x_1,x_2,\ldots,x_{n}$ in sorted order, so that the median is equal to $x_k$. Define the function 

$$
r_1(x) = \sum_{i=1}^n |x_i - x|.
$$

(This function is called the *total absolute deviation* of $x$ from the population.) Show that $r_1$ has a global minimum at $x=x_k$ by way of the following steps. 

**(a)** Explain why the derivative of $r_1$ is undefined at every $x_i$. Consequently, all of the $x_i$ are critical points of $r_1$. 

**(b)** Determine $r_1'$ within each interval $(-\infty,x_1),\, (x_1,x_2),\, (x_2,x_3),$ and so on. Explain why this shows that there cannot be any additional critical points to consider. (Note: you can replace the absolute values with a piecewise definition of $r_1$, where the formula for the pieces changes as you cross over each $x_i$.) 

**(c)** By considering the $r_1'$ values between the $x_i$, explain why it must be that

$$
r_1(x_1) > r_1(x_2) > \cdots > r_1(x_k) < r_1(x_{k+1}) < \cdots < r_1(x_n).
$$
:::

:::{#exr-stats-empirical}
(2.2) This problem is about the dataset

$$
1, 3, 4, 5, 5, 6, 7, 8.
$$

**(a)** Make a table of the values of the ECDF $\hat{F}(t)$ at the values $t=0,1,2,\ldots,10$. 

**(b)** Carefully sketch the ECDF of the dataset over the interval $[0,10]$. 

**(c)** Make a table of counts $c_k$ for the bins $(0,2],(2,4],(4,6],(6,8],(8,10]$.

**(d)** Sketch a histogram of the dataset using the bins from part (c).

**(e)** Verify the equation @eq-stats-hist-ecdf for the bins from part (c).

:::

:::{#exr-stats-integrate-pdf}
(2.3) Suppose that a distribution has continuous PDF $f(t)$ and CDF $F(t)$, and that $F(a)=0$ and $F(b)=1$. Explain why 

$$
\int_a^b f(t)\,dt = 1.
$$

:::

:::{#exr-stats-get-cdf}
(2.3) Suppose that a distribution has PDF

$$
f(t) = \begin{cases}
0, & |t| >  1, \\
\tfrac{1}{2}(1+t), & |t| \le 1.
\end{cases}
$$

Find a formula for its CDF. (Hint: It's a piecewise formula. First find it for $t< -1,$ then for $-1 \le t \le 1,$ and finally for $t>1$.)

:::

:::{#exr-stats-normal-median}
(2.3) What is the median of the normal distribution whose PDF is given by @eq-stats-normal? The answer is probably intuitively clear, but you should make a mathematical argument (though it does not require difficult calculations). Note: there is no simple antiderivative formula for the PDF, and you do not need it anyway.

:::

:::{#exr-stats-outlier}
(2.5) This exercise is about the same set of sample values as @exr-stats-summary. Suppose the 2œÉ-outlier criterion is applied using the sample mean and sample variance. 

**(a)** Show that regardless of $n$, the value 0 is never an outlier.

**(b)** Show that the value 1000 is an outlier if $n \ge 6$.

:::

:::{#exr-stats-outlier-iqr}
(2.5) Define a population by

$$
x_i = \begin{cases}
1, & 1 \le i \le 11, \\ 
2, & 12 \le i \le 14,\\ 
4, & 15 \le i \le 22, \\ 
6, & 23 \le i \le 32.
\end{cases}
$$

(That is, there are 11 values of 1, 3 values of 2, 8 values of 4, and 10 values of 6.)

**(a)** Find the median of the population.

**(b)** Find the smallest interval containing all non-outlier values according to the 1.5 IQR criterion.
:::

:::{#exr-stats-pearson-z}
(2.6) Prove that two sample sets have a Pearson correlation coefficient equal to 1 if they have identical z-scores. (Hint: Use the results of @exr-stats-z-scores.)
:::

:::{#exr-stats-pearson-anti}
(2.6) Suppose that two sample sets satisfy $y_i=-x_i$ for all $i$. Prove that the Pearson correlation coefficient between the sets equals $-1$.
:::

