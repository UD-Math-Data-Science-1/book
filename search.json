[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science 1",
    "section": "",
    "text": "This book covers material for Math 219, Data Science 1, at the University of Delaware. It should be considered prepublication and may have errors.\nSee license file for rights information.\nThis site was made with Quatro. To learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Science 1",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrior experience with single-variable calculus (basic differentiation and integration) and with base Python are expected."
  },
  {
    "objectID": "resources.html#useful-guides",
    "href": "resources.html#useful-guides",
    "title": "Resources",
    "section": "Useful guides",
    "text": "Useful guides\n\npandas user guide, pandas cheat sheet, pandas long summary\nseaborn tutorial, seaborn cheat sheet\nscikit-learn user guide, sklearn cheat sheet\nnumpy cheat sheet"
  },
  {
    "objectID": "resources.html#data-sources",
    "href": "resources.html#data-sources",
    "title": "Resources",
    "section": "Data sources",
    "text": "Data sources\nHere are places around the web with data available for download.\n\nSearch engines\nThese point to a lot of other resources.\n\nGoogle Dataset Search\nRegistry of Open Data on AWS Access to datasets used by governments and researchers that happen to be stored on Amazon‚Äôs servers. Skewed toward large datasets.\n\n\n\nPackaged\nThese feature datasets that are essentially already packaged as CSV or Excel files, plus descriptions.\n\nFive Thirty-Eight Data used to support the site‚Äôs journalism, mainly in politics and sports.\nDelaware Open Data Publicly available data from the state government.\nKaggle Long-time host of data science competitions. The formal competitions are well-curated, but user contributions vary widely.\nUCI Machine Learning Repository Well-known source for datasets that have been used extensively in machine learning research, but also recent contributions.\nOpen ML Sort of abandoned years ago, but lots of eclectic datasets remain.\nIMDB Datasets Information about movies and TVs. (Big files!)\nStanford Network Analysis Project Datasets presented as networks.\n\n\n\nOpen-ended\nThese require you to navigate an interface to select data from a large pool. Typically, you can make selections, preview the dataset, and then download in CSV or Excel format.\n\nU.S. Census Bureau Tons of demographic data about the U.S.\nData.gov Home for all open U.S. government data.\nUNICEF Portal Worldwide data about child welfare.\nWorld Bank Focuses on economic and development data.\nWorld Health Organization Information on health and disease."
  },
  {
    "objectID": "resources.html#glossary",
    "href": "resources.html#glossary",
    "title": "Resources",
    "section": "Glossary",
    "text": "Glossary\nA much more exhaustive glossary can be found here.\n\nGit\n\nGit Protocol for maintaining the entire file history of a project, including all versions and author attributions.\nrepository Collection of files needed to record the history of a git project.\nGitHub Website that hosts git repositories created by private users, along with tools to help inspect and manage them.\ncommit Collection of particular changes to the repository made by an individual and given a message.\nstage Temporary designation of locally modified files to be added to the next commit.\nmerge Automatic union of non-conflicting commits from different sources.\nconflict Disagreement between repository versions that requires human intervention to resolve.\npush Sending one or more commits from a local repository to a remote repository.\npull Receiving and merging all commits from a remote repository that are unknown to the local repository.\n\n\n\nNotebooks\n\nnotebook Self-contained collection of text, math, code, output, and graphics.\nkernel Back-end that executes code from and returns output to the notebook.\ncell Atomic unit of a notebook that contains one or more lines of text or code.\nMarkdown Simplified syntax to put boldface, italics, and other formatting within text.\nTeX/LaTeX Language used to express mathematical notation within a notebook.\nJupyter Popular format and system for interactive editing, execution, and export of notebooks.\nJupyter Lab Layer over Jupyter notebook functionality to help manage notebooks and extensions.\n\n\n\nPython\n\npackage (or wheel) Collection of Python files distributed in a portable way to provide extra functionality.\nnumpy Package of essential tools for numerical computation.\nscipy Package of tools useful in scientific and engineering computation.\ndatabase Structured collection of data, usually with a formal interface for interaction with the data.\ndata frame Tabular representation of a data set analogous to a spreadsheet, in which columns are observable quantities and rows are different observations of the quantities.\npandas Package for working with data frames.\nmatplotlib Package providing plot capabilities, modeled on MATLAB.\nseaborn Package built on matplotlib and providing commands to simplify creating plots from data frames.\nscikit-learn Package that provides capabilities for machine learning using a variety of methods.\ntensorflow, keras, pytorch Best-known packages for machine learning using neural networks.\nAnaconda Bundle of Python, most of the popular Python packages, Jupyter, and other useful tools, all within an optional point-and-click interface.\n\n\n\nEditors/IDEs\n\nVS Code (recommended) Free all-purpose editor with many extensions for working closely with git, Github, Jupyter, and Python.\nJupyter Popular format and system for interactive editing, execution, and export of notebooks.\nJupyter Lab Layer over Jupyter notebook functionality to help manage notebooks and extensions.\nGoogle Colab Free cloud-based service for jumping into Jupyter notebooks without installing any software locally.\nSpyder Free development environment that somewhat resembles MATLAB.\nPyCharm Feature-rich freemium development environment for Python, geared toward large, complex projects.\nThonny Bare-bones development environment intended to prioritize beginners."
  },
  {
    "objectID": "starting.html#back-end",
    "href": "starting.html#back-end",
    "title": "Getting started",
    "section": "Back end",
    "text": "Back end\nHere is a rundown of somes pros and cons of the usual options:\n\n\n\nYour own machine\nCloud\n\n\n\n\nAvailable offline üòÑ\nUse any device üëç\n\n\nTotal control üòé/üòï\nNo installations üòå\n\n\nChoose the front end\nBrowser only\n\n\nYours forever üòÅ\nPermanence is an illusion ‚õÑÔ∏è\n\n\nYour hardware üíµ\nYou get what you get ¬Ø_(„ÉÑ)_/¬Ø\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou want to use Python 3.x, not Python 2.x. These coexisted for a while, during a dark time for Python. If you see guidance based on Python 2, it is either outdated or the product of a disordered mind.\n\n\n\nRun on your own computer\nThe recommended option is to download Anaconda. It‚Äôs a big download, but it comes with just about everything ready to go, and you can manage things by point-and-click.\n\n\nRun on the cloud\nThere are many choices, but a popular one is Google Colab. It‚Äôs free and saves you from having to install anything. The hardware is basic but most likely fine for our purposes. You will need to download your results for submissions."
  },
  {
    "objectID": "starting.html#front-end",
    "href": "starting.html#front-end",
    "title": "Getting started",
    "section": "Front end",
    "text": "Front end\nMuch of data science is expressed using notebooks, which we will use exclusively. Specifically, you will use Jupyter notebooks.\n\n\n\n\n\n\nTip\n\n\n\nThe name IPython refers to the direct ancestor of Jupyter. The older name continues to stick to a few of the tools, though.\n\n\nA notebook is a self-contained collection of text, math, code, output, and graphics grouped into cells. The front end manages the cells and communicates with a back end called the kernel.\nIf you are using Colab, then you are using a Jupyter notebook, though in an interface customized by Google.\n\nOn your own machine\nJupyter splits into ‚Äúclassic‚Äù Jupyter and the newer Jupyter Lab. Either is fine for us, but there is no reason to prefer the older variant. Just start it up from the Anaconda dashboard, and it will open your web browser to the front-end server.\nA worthwhile alternative is to edit the notebook within VS Code, which is a powerful and popular editor for Python and other languages."
  },
  {
    "objectID": "starting.html#tips-on-jupyter-success",
    "href": "starting.html#tips-on-jupyter-success",
    "title": "Getting started",
    "section": "Tips on Jupyter success",
    "text": "Tips on Jupyter success\n\n\n\n\n\n\nWarning\n\n\n\nThe order of cells that you see in a notebook is not necessarily the order in which they were executed.\n\n\nBy far the greatest source of confusion and subtle problems in a notebook is the freedom it gives you to execute the cells in whatever order you want. As you experiment and add and delete cells to try things out, you will reach a point at which the code on the screen is no longer a recipe for reaching the current state of your workspace.\n\n\n\n\n\n\nTip\n\n\n\nBefore submitting a notebook, it‚Äôs highly advisable to restart the kernel and run all cells in order, just to make sure that everything still works as seen on screen.\n\n\nMake use of the code completion tools. If you start typing the name of a variable, data column, or python method, you can either pause or hit TAB to see a list of possible completions. This can (a) save you typing time and (b) remind you of function names that are on the tip of your tongue."
  },
  {
    "objectID": "data.html#quantitative-data",
    "href": "data.html#quantitative-data",
    "title": "1¬† Representation of data",
    "section": "1.1 Quantitative data",
    "text": "1.1 Quantitative data\n\nDefinition 1.1 A quantitative value is one that is numerical and supports meaningful comparison and arithmetic operations.\n\nQuantitative data is further divided into continuous and discrete types. The difference is the same as between real numbers and integers.\n\nExample 1.1 Some continuous quantitative data sources:\n\nTemperature at noon at a given airport\nYour height\nVoltage across the terminals of a battery\n\nExamples of discrete quantitative data:\n\nThe number of shoes you own\nNumber of people at a restaurant table\nScore on an exam\n\nIn each case, it makes sense, for example, to order different values and compute averages of them. However, averages of discrete quantities are continuous.\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes there may be room for interpretation or context. For example, the retail price of a gallon of milk might be regarded as discrete data, since it technically represents a whole number of pennies. But in finance, transactions are regularly computed to much higher precision, so it might make more sense to interpret prices as continuous values.\n\n\n\nExample 1.2 Not all numerical values represent truly quantitative data. ZIP codes (postal codes) in the U.S. are 5-digit numbers, and while there is some logic to how they were assigned, there is no clearly meaningful interpretation of averaging them, for instance.\n\nMathematically, the real and integer number sets are infinite, but that is not possible in a computer. Integers are represented exactly within some range that is determined by how many binary bits are dedicated. The computational analog of real numbers are floating-point numbers, or more simply, floats. These are bounded in range as well as discretized. The details are complicated, but essentially, the floating-point numbers have about 16 significant digits by default, which is virtually always far more precision than real data offers.\n\n# This is an integer (int type)\nprint(3, \"is a\", type(3))\n\n# This is a real number (float type)\nprint(3.0, \"is a\", type(3.0))\n\n# Convert int to float (generally no change to numerical value)\nprint( \"float(3) creates\",float(3) )\n\n# Truncate float to int\nprint( \"int(3.14) creates\", int(3.14) )\n\n3 is a <class 'int'>\n3.0 is a <class 'float'>\nfloat(3) creates 3.0\nint(3.14) creates 3\n\n\n\n1.1.1 Special values\nThere are two additional quasi-numerical float values to be aware of as well.\n\n\n\n\n\n\nImportant\n\n\n\nFor numerical work in Python, the NumPy package indispensible. We will use it often, and it is also loaded and used by most other scientifically oriented packages.\n\n\nThe value inf stands for infinity. It‚Äôs greater than every finite number. Some arithmetic with infinity is well-defined:\n\nimport numpy as np\nprint( \"np.inf + 5 is\", np.inf + 5 )\nprint( \"np.inf + np.inf is\", np.inf + np.inf )\nprint( \"5 - np.inf is\", 5 - np.inf )\n\nnp.inf + 5 is inf\nnp.inf + np.inf is inf\n5 - np.inf is -inf\n\n\nHowever, in calculus you learned that some expressions with infinity are considered to be undefined without additional information to apply (e.g., L‚ÄôH√¥pital‚Äôs Rule):\n\nprint( \"np.inf / np.inf is\", np.inf / np.inf )\n\nnp.inf / np.inf is nan\n\n\nThe result nan stands for Not a Number. It is the result of indeterminate arithmetic operations, like \\(\\infty/\\infty\\) and \\(\\infty - \\infty\\). It is also used sometimes as a placeholder for missing data, i.e.¬†to mean, ‚Äúunknown value‚Äù.\n\n\n\n\n\n\nWarning\n\n\n\nBy definition, every operation that involves a NaN value results in a NaN.\n\n\nOne notorious consequence of this behavior is that nan==nan is nan, not True!\n\n\n1.1.2 Dates and times\nHandling times and dates can be tricky. Aside from headaches such as time zones and leap years, there are many different ways people and machine represent dates, and with varying amounts of precision. Python has its own inbuilt system for handling dates and times, but we will show the facilities provided by NumPy instead.\nThere are two basic types:\n\nDefinition 1.2 A datetime is a representation of an instant in time. A time delta is a representation of a duration; i.e., a difference between two datetimes.\n\n\n\n\n\n\n\nImportant\n\n\n\nNative Python uses a datetime type, while NumPy uses datetime64.\n\n\n\nnp.datetime64(\"2020-01-17\")    # YYYY-MM-DD \n\nnumpy.datetime64('2020-01-17')\n\n\n\nnp.datetime64(\"1969-07-20T20:17\")    # YYYY-MM-DDThh:mm\n\nnumpy.datetime64('1969-07-20T20:17')\n\n\n\n# Current date and time, down to the second\nnp.datetime64(\"now\")\n\nnumpy.datetime64('2023-01-30T17:03:14')\n\n\nA time delta in NumPy indicates its units (granularity).\n\nnp.datetime64(\"1969-07-20T20:17\") - np.datetime64(\"today\")\n\nnumpy.timedelta64(-28153663,'m')\n\n\n\n\n1.1.3 Random numbers\nGenerating truly random numbers on a computer is not simple. Mostly we rely on pseudorandom numbers, which are generated by deterministic functions called random number generators (RNGs) that have extremely long periods. One nice consequence is repeatability. By specifying the starting state of the RNG, you can get exactly the same pseudorandom sequence every time.\nWe will rely on pseudorandom numbers in two ways. First, many algorithms in data science have at least one random aspect (dividing data into subsets, for example). The library routines we will be using allow you to specify the random state and get repeatable results. Occasionally, though, we might want to generate random values for our own use.\n\nfrom numpy.random import default_rng\nrng = default_rng(19716)    # giving an initial state\n\nThe uniform generator method produces numbers distributed uniformly (i.e., every value is equally likely) between two limits you specify.\n\nfor _ in range(5):\n    print( rng.uniform( -1, 1 ) )\n\n0.9516875346510687\n0.3153947867339544\n-0.6651579991995873\n0.42925720152795055\n0.960762541480505\n\n\nAnother common type of random value is generated by normal, which produces real values distributed according to the normal or Gaussian distribution, which we‚Äôll get into with more detail later.\n\nfor _ in range(5):\n    print( rng.normal() )\n\n-0.6241028855083841\n-0.2829164875756926\n-1.2277902883810283\n-0.4642829516161333\n0.602421860754439\n\n\n\nExample 1.3 In the long run, the average value of normally distributed numbers will be zero. Here is an experiment on 100,000 of them:\n\ns = 0\nfor _ in range(100000):\n    s += rng.normal()\n\ns/100000\n\n-0.00019715894764449414"
  },
  {
    "objectID": "data.html#arrays",
    "href": "data.html#arrays",
    "title": "1¬† Representation of data",
    "section": "1.2 Arrays",
    "text": "1.2 Arrays\nMost interesting phenomena are characterized and influenced by more than one factor. Collections of values therefore play a huge role in data science. The workhorse type for collections in base Python is the list. However, we‚Äôre going to need some more powerful metaphors and tools as well.\n\n1.2.1 Vectors\n\nDefinition 1.3 A vector is a collection of values called elements, all of the same type, indexed by consecutive integers.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn math, vector indexes usually begin with 1. In Python, they begin with 0.\n\n\nA vector with \\(n\\) elements is often referred to as an \\(n\\)-vector, and we say that \\(n\\) is the length of the vector. In math we often use \\(\\real^n\\) to denote the set of all \\(n\\)-vectors with real-valued elements.\nThe usual way to work with arrays in Python is through NumPy:\n\nimport numpy as np\n\nx = np.array( [1,2,3,4,5] )\nx\n\narray([1, 2, 3, 4, 5])\n\n\n\n\n\nA vector has a data type for its elements:\n\nx.dtype\n\ndtype('int64')\n\n\nAny float values in the vector cause the data type of the entire vector to be float:\n\ny = np.array( [1.0,2,3,4,5] )\ny.dtype\n\ndtype('float64')\n\n\nUse len to determine the length of a vector:\n\nlen(x)\n\n5\n\n\nYou can create a special type of vector called a range that has equally spaced elements:\n\nnp.arange(0,5)\n\narray([0, 1, 2, 3, 4])\n\n\n\nnp.arange(1,3,0.5)\n\narray([1. , 1.5, 2. , 2.5])\n\n\nThe syntax here is (start,stop,step). Note something critical and counterintuitive from the above results:\n\n\n\n\n\n\nDanger\n\n\n\nIn base Python and in NumPy, the last element of a range is omitted. This is guaranteed to cause confusion if you are used to just about any other computer language.\n\n\n\n1.2.1.1 Access and slicing\n\n\n\nUse square brackets to refer to an element of a vector:\n\nx[0]\n\n1\n\n\n\nx[4]\n\n5\n\n\nNegative values for the index are counted from the end. The last element of a vector always has index -1, and more-negative values move backward through the elements:\n\nx[-1]\n\n5\n\n\n\nx[-3]\n\n3\n\n\n\nx[-len(x)]\n\n1\n\n\nElement references can also be on the left side of an assigment:\n\nx[2] = -3\nx\n\narray([ 1,  2, -3,  4,  5])\n\n\nNote, however, that once the data type of a vector is set, it can‚Äôt be changed:\n\nx[0] = 1.234\nx    # float was truncated to int, without warning!\n\narray([ 1,  2, -3,  4,  5])\n\n\nYou can also use a list in square brackets to access multiple elements at once:\n\nx[ [0,2,4] ]\n\narray([ 1, -3,  5])\n\n\nAccessing elements via a range is known as slicing:\n\nx[0:3]\n\narray([ 1,  2, -3])\n\n\n\nx[-3:-1]\n\narray([-3,  4])\n\n\nAs with ranges, the syntax of a slice is start:stop:step, and the last element of the range is not included. This causes headaches and bugs, though it does imply that the range i:j has \\(j-i\\) elements, not \\(j-i+1\\).\nWhen the start of the range is omitted, it means ‚Äúfrom the beginning‚Äù, and when stop is omitted, it means ‚Äúthrough to the end.‚Äù Hence, [:k] means ‚Äúfirst \\(k\\) elements‚Äù and [-k:] means ‚Äúlast \\(k\\) elements‚Äù:\n\nx[:3]\n\narray([ 1,  2, -3])\n\n\n\nx[-3:]\n\narray([-3,  4,  5])\n\n\nAnd we also have this idiom:\n\nx[::-1]   # reverse the vector\n\narray([ 5,  4, -3,  2,  1])\n\n\nNumPy will happily allow you to reference invalid indexes. It will just return as much as is available without warning or error. :::\n\nx[:10]\n\narray([ 1,  2, -3,  4,  5])\n\n\n\n\n\n1.2.2 Multiple dimensions\nA vector is an important special case of a more general construct.\n\nDefinition 1.4 An array is a collection of values called elements, all of the same type, indexed by one or more sets of consecutive integers. The number of indexes needed to specify a value is the dimension of the array.\n\n\n\n\n\n\n\nNote\n\n\n\nA dimension is called an axis in NumPy and related packages.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term matrix is often used simply to mean a 2D array. Technically, though, a matrix should have only numerical values, and matrices obey certain properties that make them important mathematical objects. These properties and their consequences are studied in linear algebra.\n\n\n\n1.2.2.1 Construction\n\n\n\nOne way to construct an array is by a list comprehension:\n\nA = np.array([ [j-i for j in range(6)] for i in range(4) ])\nA\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [-1,  0,  1,  2,  3,  4],\n       [-2, -1,  0,  1,  2,  3],\n       [-3, -2, -1,  0,  1,  2]])\n\n\nThe shape of an array is what we would often call the size:\n\nA.shape\n\n(4, 6)\n\n\nThere is no difference between a vector and a 1D array:\n\nx.shape\n\n(5,)\n\n\nThere is also no difference between a 2D array and a vector of vectors that give the rows of the array.\n\nR = np.array( [ [1,2,3], [4,5,6] ])\nR\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nHere are some other common ways to construct arrays.\n\nnp.ones(5)\n\narray([1., 1., 1., 1., 1.])\n\n\n\nnp.zeros( (3,6) )\n\narray([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])\n\n\n\n# See earlier section for definition of rng\nrng.normal( size=(3,4) )\n\narray([[-1.27264664, -0.62756222, -0.30866094,  0.02293842],\n       [ 0.29444108, -0.80103727,  0.34504746, -0.11060255],\n       [-1.72285615,  0.22140352, -1.06020551, -0.05159689]])\n\n\n\nnp.repeat(np.pi, 3)\n\narray([3.14159265, 3.14159265, 3.14159265])\n\n\nYou can also stack arrays vertically or horizontally to create new arrays.\n\nnp.hstack( ( np.ones((2,2)), np.zeros((2,3)) ) )\n\narray([[1., 1., 0., 0., 0.],\n       [1., 1., 0., 0., 0.]])\n\n\n\nnp.vstack( (range(5), range(5,0,-1)) )\n\narray([[0, 1, 2, 3, 4],\n       [5, 4, 3, 2, 1]])\n\n\n\n\n1.2.2.2 Indexing and slicing\n\n\n\nWe can use successive brackets to refer to an element, row then column:\n\nR[1][2]    # second row, third column\n\n6\n\n\nBut it‚Äôs more convenient to use a single bracket set with indexes separated by commas:\n\nR[1, 2]    # second row, third column\n\n6\n\n\nYou can slice in each dimension individually.\n\nR[:1, -2:]    # first row, last two columns\n\narray([[2, 3]])\n\n\nThe result above is another 2D array. Note how this result is subtly different:\n\nR[0, -2:]\n\narray([2, 3])\n\n\nBecause we accessed an individual row, not a slice, the result is one dimension lower‚Äîa vector. Finally, a : in one slice position means to keep everything in that dimension.\n\nA[:, :2]    # all rows, first 2 columns\n\narray([[ 0,  1],\n       [-1,  0],\n       [-2, -1],\n       [-3, -2]])\n\n\n\n\n1.2.2.3 Reductions\n\n\n\nA common task is to reduce an array along one dimension, called an axis in numpy, resulting in an array of one less dimension. It‚Äôs easiest to explain by some examples.\n\nnp.sum(A, axis=0)    # sum along the rows\n\narray([-6, -2,  2,  6, 10, 14])\n\n\n\nnp.sum(A,axis=1)    # sum along the columns\n\narray([15,  9,  3, -3])\n\n\nIf you don‚Äôt give an axis, the reduction occurs over all directions at once, resulting in a single number.\n\nnp.sum(A)\n\n24\n\n\nYou can also do reductions with maximum, minimum, mean, etc."
  },
  {
    "objectID": "data.html#qualitative-data",
    "href": "data.html#qualitative-data",
    "title": "1¬† Representation of data",
    "section": "1.3 Qualitative data",
    "text": "1.3 Qualitative data\nA qualitative value is one that is not quantitative. However, in order to work with such data, we usually have to encode it in some numerical form,\n\n1.3.1 Categorical\n\nDefinition 1.5 Categorical data has values drawn from a finite set \\(S\\) of categories. If the members of \\(S\\) support meaningful ordering comparisons, then the data is ordinal; otherwise, it is nominal.\n\n\nExample 1.4 Examples of ordinal categorical data:\n\nSeat classes on a commercial airplane (e.g., economy, business, first)\nLetters of the alphabet\n\nExamples of nominal categorical data:\n\nYes/No responses\nMarital status\nMake of a car\n\nThere are nuanced cases. For instance, letter grades are themselves ordinal categorical data. However, schools convert them to discrete quantitative data and then compute a continuous quantitative GPA.\n\nOne way to quantify ordinal categorical data is to assign integer values to the categories in a manner that preserves ordering. This approach can succeed, but it can be questionable when it comes to operations such as averaging or computing a distance between values.\nAnother means of quantifying categorical data is called dummy variables in classical statistics and one-hot encoding in much of machine learning. Suppose a variable \\(x\\) has values in a category set that has \\(m\\) members we label \\(c_1,\\ldots,c_m\\). Then we can replace \\(x\\) with introduce \\(m-1\\) new variables \\(x_1,\\ldots,x_{m-1}\\), where \\[\nx_i = \\begin{cases} 1, & x=c_i, \\\\ 0, & x\\neq c_i. \\end{cases}\n\\] At most one of the \\(x_i\\) can be 1. If all of the \\(x_i\\) are 0, then we know that \\(x=c_m\\).\n\nExample 1.5 Suppose that the stooge variable can take the values Moe, Larry, Curly, or Shemp. Then the vector\n[Curly, Moe, Curly, Shemp]\nwould be replaced by the array\n[ [0,0,1], [1,0,0], [0,0,1], [0,0,0] ]\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes an \\(m\\)-fold categorical variable is replaced by \\(m\\) indicator (dummy) variables, rather than just \\(m-1\\).\n\n\n\n\n1.3.2 Text\nText is a ubiquitous data source. One way to quantify text is to use a dictionary of interesting keywords \\(w_1,\\ldots,w_n\\). Given a collection of documents \\(d_1,\\ldots,d_m\\), we can define an \\(m\\times n\\) document‚Äìterm matrix \\(T\\) by letting \\(T_{ij}\\) be the number of times term \\(j\\) appears in document \\(i\\).\n\n\n1.3.3 Images\nThe most straightforward way to represent an image is as a 3D array of values representing intensities representing of red, green and blue in each pixel. Sometimes it might be preferable to represent the image by a vector of statistics about these values, or by presence or absence of detected objects, etc."
  },
  {
    "objectID": "data.html#series-and-frames",
    "href": "data.html#series-and-frames",
    "title": "1¬† Representation of data",
    "section": "1.4 Series and frames",
    "text": "1.4 Series and frames\nThe most popular Python package for manipulating and analyzing data is pandas. We will use the paradigm it presents, which is fairly well understood throughout data science.\n\nDefinition 1.6 A series is a vector that is indexed by a finite ordered set. A data frame is a collection of series that all share the same index set.\n\nWe can conceptualize a series as a vector plus an index list, and a data frame as a 2D array with index sets for the rows and the columns. In that sense, they are simply syntactic sugar. However, since people are much better at remembering the meaning of words than arbitrarily assigned integers, data frames serve to prevent errors and misunderstandings.\n\nExample 1.6 Some data that can be viewed as series:\n\nThe length, width, and height of a box can be expressed as a 3-vector of positive real numbers. If we index the vector by the names of the measurements, it becomes a series.\nThe number of steps taken by an individual over the course of a week can be expressed as a 7-vector of nonnegative integers. We could index it by the integers 1‚Äì7, or in a series by the names of the days of the week.\nThe bid prices of a stock at the end of each trading day can be represented as a time series, in which the index is drawn from timestamps.\nThe scores of gymnasts on multiple apparatus types can be represented as a data frame whose rows are indexed by the names of the gymnasts and whole columns are indexed by the names of the apparatuses.\n\n\n\nExample 1.7 Here is a pandas series for the wavelengths of light corresponding to rainbow colors:\n\nimport pandas as pd\n\nwavelength = pd.Series( \n  [400, 470, 520, 580, 610, 710],    # values\n  index=[\"violet\", \"blue\", \"green\", \"yellow\", \"orange\", \"red\"],\n  name=\"wavelength\"\n  )\n\nwavelength\n\nviolet    400\nblue      470\ngreen     520\nyellow    580\norange    610\nred       710\nName: wavelength, dtype: int64\n\n\nWe can use an index value (one of the colors) to access a value in the series:\n\nwavelength[\"blue\"]\n\n470\n\n\nIf we access multiple values, we get a series that is a subset of the original:\n\nwavelength[ [\"violet\", \"red\"] ]\n\nviolet    400\nred       710\nName: wavelength, dtype: int64\n\n\nWe can also use the iloc property to access the underlying vector by NumPy slicing:\n\nwavelength.iloc[:4]\n\nviolet    400\nblue      470\ngreen     520\nyellow    580\nName: wavelength, dtype: int64\n\n\nHere is a series of NFL teams based on the same index:\n\nteam = pd.Series(\n  [\"Vikings\", \"Bills\", \"Eagles\", \"Chargers\", \"Bengals\", \"Cardinals\"],\n  index = wavelength.index,\n  name=\"team\"\n  )\n\nteam[\"green\"]\n\n'Eagles'\n\n\nNow we can create a data frame using these two series as columns:\n\nrainbow = pd.DataFrame( {\"wavelength\": wavelength, \"team name\": team} )\nrainbow\n\n\n\n\n\n\n\n\nwavelength\nteam name\n\n\n\n\nviolet\n400\nVikings\n\n\nblue\n470\nBills\n\n\ngreen\n520\nEagles\n\n\nyellow\n580\nChargers\n\n\norange\n610\nBengals\n\n\nred\n710\nCardinals\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCurly braces { } are used to construct a dictionary in Python.\n\n\nWe can access a single column using simple bracket notation:\n\nrainbow[\"team name\"]\n\nviolet      Vikings\nblue          Bills\ngreen        Eagles\nyellow     Chargers\norange      Bengals\nred       Cardinals\nName: team name, dtype: object\n\n\nWe can add a column after the fact by using a bracket access on the left side of the assignment:\n\nrainbow[\"flower\"] = [\n    \"Lobelia\", \n    \"Cornflower\", \n    \"Bells-of-Ireland\", \n    \"Daffodil\",\n    \"Butterfly weed\",\n    \"Rose\"\n    ]\n\nrainbow\n\n\n\n\n\n\n\n\nwavelength\nteam name\nflower\n\n\n\n\nviolet\n400\nVikings\nLobelia\n\n\nblue\n470\nBills\nCornflower\n\n\ngreen\n520\nEagles\nBells-of-Ireland\n\n\nyellow\n580\nChargers\nDaffodil\n\n\norange\n610\nBengals\nButterfly weed\n\n\nred\n710\nCardinals\nRose\n\n\n\n\n\n\n\nWe can access a row by using brackets with the loc property of the frame, getting a series indexed by the column names of the frame:\n\nrainbow.loc[\"orange\"]\n\nwavelength               610\nteam name            Bengals\nflower        Butterfly weed\nName: orange, dtype: object\n\n\nWe are also free to strip away the index and get an ordinary array:\n\nrainbow.loc[\"red\"].to_numpy()\n\narray([710, 'Cardinals', 'Rose'], dtype=object)\n\n\n\n\n\n\nHere is another way to construct a data frame in pandas. We give a list of rows, plus (optionally) the index and the names of the columns:\n\nletters = pd.DataFrame( \n    [ (\"a\", \"A\"), (\"b\", \"B\"), (\"c\", \"C\") ], \n    columns=[\"lowercase\", \"uppercase\"] \n    )\n\nletters\n\n\n\n\n\n\n\n\nlowercase\nuppercase\n\n\n\n\n0\na\nA\n\n\n1\nb\nB\n\n\n2\nc\nC\n\n\n\n\n\n\n\nWhen we specify data by column, the columns can be just lists or vectors, not necessarily series:\n\nletters = pd.DataFrame( \n    { \"lowercase\": [\"a\",\"b\",\"c\"], \"uppercase\": [\"A\",\"B\",\"C\"] }, \n    )\n\nletters\n\n\n\n\n\n\n\n\nlowercase\nuppercase\n\n\n\n\n0\na\nA\n\n\n1\nb\nB\n\n\n2\nc\nC\n\n\n\n\n\n\n\nPandas has facilities for dealing with categorical variables.\n\nExample 1.8 Here is a vector of chess pieces at the start of a game:\n\npieces = np.hstack( [\n    np.repeat(\"pawn\", 8),\n    np.repeat([\"knight\",\"bishop\",\"rook\"], 2),\n    \"queen\",\n    \"king\"\n] )\n\npieces\n\narray(['pawn', 'pawn', 'pawn', 'pawn', 'pawn', 'pawn', 'pawn', 'pawn',\n       'knight', 'knight', 'bishop', 'bishop', 'rook', 'rook', 'queen',\n       'king'], dtype='<U6')\n\n\nWe can tell pandas to regard these strings as elements of a category set:\n\npd.Categorical(pieces)\n\n['pawn', 'pawn', 'pawn', 'pawn', 'pawn', ..., 'bishop', 'rook', 'rook', 'queen', 'king']\nLength: 16\nCategories (6, object): ['bishop', 'king', 'knight', 'pawn', 'queen', 'rook']\n\n\nNotice above that the unique categories were found automatically.\n\n\n1.4.1 Loading from files\nDatasets can be presented in many forms. In this course, we assume that they are given as spreadsheets or in comma-separated value (CSV) files. These can be read by pandas locally or over the web.\n\nExample 1.9 The pandas function we use to load a dataset is read_csv. Here we use it to read a file that is availble over the web:\n\nads = pd.read_csv(\"https://raw.githubusercontent.com/tobydriscoll/ds1book/master/advertising.csv\")\nads.head(6)    # show the first 6 rows\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n5\n8.7\n48.9\n75.0\n7.2\n\n\n\n\n\n\n\nNote above that we used head(6) to see just the first 6 rows.\nA data frame has a few properties that describe its contents:\n\nads.shape    # number of rows, number of columns\n\n(200, 4)\n\n\n\nads.columns   # names of the columns\n\nIndex(['TV', 'Radio', 'Newspaper', 'Sales'], dtype='object')\n\n\n\nads.dtypes    # data types of the columns\n\nTV           float64\nRadio        float64\nNewspaper    float64\nSales        float64\ndtype: object\n\n\n\n\n\n\nIt‚Äôs also possible to import data from most other general-purpose formats you might encounter, such as Excel spreadsheets (though possibly requiring one-time installation of additional libraries). There are many functions starting with pd.read_ showing the formats pandas understands.\n\n\n1.4.2 Selecting rows\nAbove we saw that you can use the loc property to access a frame‚Äôs row by name, or iloc to access by position. It‚Äôs more common, though, to select them by some criteria.\n\nExample 1.10 Here‚Äôs a local file that contains daily weather summaries from Newark, Delaware:\n\nweather = pd.read_csv(\"_datasets/ghcn_newark.csv\")\nweather.head()\n\n\n\n\n\n\n\n\nSTATION\nDATE\nLATITUDE\nLONGITUDE\nELEVATION\nNAME\nPRCP\nPRCP_ATTRIBUTES\nSNOW\nSNOW_ATTRIBUTES\n...\nWT08\nWT08_ATTRIBUTES\nWT11\nWT11_ATTRIBUTES\nWT14\nWT14_ATTRIBUTES\nWT16\nWT16_ATTRIBUTES\nWT18\nWT18_ATTRIBUTES\n\n\n\n\n0\nUSC00076410\n1894-04-01\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nUSC00076410\n1894-04-02\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nUSC00076410\n1894-04-03\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nUSC00076410\n1894-04-04\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n165.0\n,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nUSC00076410\n1894-04-05\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 56 columns\n\n\n\nIf we apply a relational operator to a column of a frame, we get a series of Boolean values:\n\nisprecip = (weather[\"PRCP\"] > 10)\nisprecip.head()\n\n0    False\n1    False\n2    False\n3     True\n4    False\nName: PRCP, dtype: bool\n\n\nThis kind of series can then be used to select the rows that have value True:\n\nprecip = weather[isprecip]\nprecip.head()\n\n\n\n\n\n\n\n\nSTATION\nDATE\nLATITUDE\nLONGITUDE\nELEVATION\nNAME\nPRCP\nPRCP_ATTRIBUTES\nSNOW\nSNOW_ATTRIBUTES\n...\nWT08\nWT08_ATTRIBUTES\nWT11\nWT11_ATTRIBUTES\nWT14\nWT14_ATTRIBUTES\nWT16\nWT16_ATTRIBUTES\nWT18\nWT18_ATTRIBUTES\n\n\n\n\n3\nUSC00076410\n1894-04-04\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n165.0\n,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n7\nUSC00076410\n1894-04-08\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n104.0\n,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n10\nUSC00076410\n1894-04-11\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n531.0\n,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n11\nUSC00076410\n1894-04-12\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n15.0\n,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n21\nUSC00076410\n1894-04-22\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n30.0\n,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 56 columns\n\n\n\nYou can use logical operators & (and), | (or), and ~ (not) on these Boolean series:\n\niscold = (weather[\"TMAX\"] < 0)\ndry_cold = weather[ iscold & ~isprecip ]\ndry_cold.head(6)\n\n\n\n\n\n\n\n\nSTATION\nDATE\nLATITUDE\nLONGITUDE\nELEVATION\nNAME\nPRCP\nPRCP_ATTRIBUTES\nSNOW\nSNOW_ATTRIBUTES\n...\nWT08\nWT08_ATTRIBUTES\nWT11\nWT11_ATTRIBUTES\nWT14\nWT14_ATTRIBUTES\nWT16\nWT16_ATTRIBUTES\nWT18\nWT18_ATTRIBUTES\n\n\n\n\n240\nUSC00076410\n1894-12-28\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n241\nUSC00076410\n1894-12-29\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n242\nUSC00076410\n1894-12-30\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n243\nUSC00076410\n1894-12-31\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n244\nUSC00076410\n1895-01-01\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\n0.0\n,,6\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n245\nUSC00076410\n1895-01-02\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\n0.0\n,,6\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n6 rows √ó 56 columns\n\n\n\n\n\n\n\nWhen a column of a dataset is a time stamp, it can be used as the index of the data frame, which makes some time-centric operations easier.\n\nExample 1.11 Let‚Äôs reload the dataset using the DATE column (which is second in the file) as a datetime index:\n\nweather = pd.read_csv(\"_datasets/ghcn_newark.csv\", index_col=1, parse_dates=True)\nweather.head()\n\n\n\n\n\n\n\n\nSTATION\nLATITUDE\nLONGITUDE\nELEVATION\nNAME\nPRCP\nPRCP_ATTRIBUTES\nSNOW\nSNOW_ATTRIBUTES\nSNWD\n...\nWT08\nWT08_ATTRIBUTES\nWT11\nWT11_ATTRIBUTES\nWT14\nWT14_ATTRIBUTES\nWT16\nWT16_ATTRIBUTES\nWT18\nWT18_ATTRIBUTES\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1894-04-01\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1894-04-02\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1894-04-03\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1894-04-04\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n165.0\n,,6,\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1894-04-05\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 55 columns\n\n\n\nWe can extract a row by referencing the date in the index with loc:\n\nweather.loc[\"1979-03-28\"]\n\nSTATION                      USC00076410\nLATITUDE                         39.6682\nLONGITUDE                      -75.74569\nELEVATION                           32.3\nNAME               NEWARK AG FARM, DE US\nPRCP                                 0.0\nPRCP_ATTRIBUTES                 ,,0,1700\nSNOW                                 0.0\nSNOW_ATTRIBUTES                      ,,0\nSNWD                                 0.0\nSNWD_ATTRIBUTES                      ,,0\nTMAX                               111.0\nTMAX_ATTRIBUTES                      ,,0\nTMIN                               -33.0\nTMIN_ATTRIBUTES                      ,,0\nDAEV                                 NaN\nDAEV_ATTRIBUTES                      NaN\nDAPR                                 NaN\nDAPR_ATTRIBUTES                      NaN\nDAWM                                 NaN\nDAWM_ATTRIBUTES                      NaN\nEVAP                                 NaN\nEVAP_ATTRIBUTES                      NaN\nMDEV                                 NaN\nMDEV_ATTRIBUTES                      NaN\nMDPR                                 NaN\nMDPR_ATTRIBUTES                      NaN\nMDWM                                 NaN\nMDWM_ATTRIBUTES                      NaN\nMNPN                                 NaN\nMNPN_ATTRIBUTES                      NaN\nMXPN                                 NaN\nMXPN_ATTRIBUTES                      NaN\nTOBS                               100.0\nTOBS_ATTRIBUTES                 ,,0,1700\nWDMV                                 NaN\nWDMV_ATTRIBUTES                      NaN\nWT01                                 NaN\nWT01_ATTRIBUTES                      NaN\nWT03                                 NaN\nWT03_ATTRIBUTES                      NaN\nWT04                                 NaN\nWT04_ATTRIBUTES                      NaN\nWT05                                 NaN\nWT05_ATTRIBUTES                      NaN\nWT08                                 NaN\nWT08_ATTRIBUTES                      NaN\nWT11                                 NaN\nWT11_ATTRIBUTES                      NaN\nWT14                                 NaN\nWT14_ATTRIBUTES                      NaN\nWT16                                 NaN\nWT16_ATTRIBUTES                      NaN\nWT18                                 NaN\nWT18_ATTRIBUTES                      NaN\nName: 1979-03-28 00:00:00, dtype: object\n\n\nBut we can also easily select all the rows from a particular month or year:\n\nweather.loc[\"1979-03\"].head()\n\n\n\n\n\n\n\n\nSTATION\nLATITUDE\nLONGITUDE\nELEVATION\nNAME\nPRCP\nPRCP_ATTRIBUTES\nSNOW\nSNOW_ATTRIBUTES\nSNWD\n...\nWT08\nWT08_ATTRIBUTES\nWT11\nWT11_ATTRIBUTES\nWT14\nWT14_ATTRIBUTES\nWT16\nWT16_ATTRIBUTES\nWT18\nWT18_ATTRIBUTES\n\n\nDATE\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1979-03-01\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\n,,0,1700\n0.0\n,,0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1979-03-02\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n3.0\n,,0,1700\n0.0\n,,0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1979-03-03\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\n,,0,1700\n0.0\n,,0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1979-03-04\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\n,,0,1700\n0.0\n,,0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1979-03-05\nUSC00076410\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n61.0\n,,0,1700\n0.0\n,,0\n0.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 55 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe pandas user guide has a handy section on selections.\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor practice with pandas fundamentals, try the Kaggle course."
  },
  {
    "objectID": "data.html#data-preparation",
    "href": "data.html#data-preparation",
    "title": "1¬† Representation of data",
    "section": "1.5 Data preparation",
    "text": "1.5 Data preparation\nRaw data often needs to be manipulated into a useable format before algorithms can be applied. Preprocessing data so that it is suitable for machine analysis is known as data wrangling or data munging. A related process is data cleaning, where missing and anomalous values are removed or replaced.\n\n1.5.1 Missing values\nIn real data sets, we often must cope with data series that have missing values. This is a common source of mistakes and confusion, especially because there is no universal practice. Sometimes zero is used to represent a missing number. It‚Äôs also common to use an impossible value, such as \\(-999\\) to represent weight, to signify missing data.\n\n\n\nFormally, the most natural way to represent missing data in Python is as nan or NaN, and pandas makes it easy to find and manipulate such values. Here is another well-known data set, this time about penguins:\n\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nNote above that the fourth row of the frame is missing measurements. We can discover how many such rows there are using isna:\n\npenguins.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nSometimes one replaces missing values with average or other representative values, a process called imputation. It‚Äôs often prudent to simply toss them out, as follows:\n\nprint(\"original counts:\")\nprint( penguins.count() )\npenguins.dropna( inplace=True )\nprint()\nprint(\"after removals:\")\nprint( penguins.count() )\n\noriginal counts:\nspecies              344\nisland               344\nbill_length_mm       342\nbill_depth_mm        342\nflipper_length_mm    342\nbody_mass_g          342\nsex                  333\ndtype: int64\n\nafter removals:\nspecies              333\nisland               333\nbill_length_mm       333\nbill_depth_mm        333\nflipper_length_mm    333\nbody_mass_g          333\nsex                  333\ndtype: int64\n\n\n\n\n\n\n\n\nTip\n\n\n\nOperations that make changes to or retrieve subsets from a data frame work on copies of the frame. When inplace=True is given, though, the operation changes the original frame.\n\n\n\n\n1.5.2 Loans example\nTo demonstrate algortihms in later sections, we will be using a dataset describing loans made on the crowdfunding site LendingClub. First, we load the raw data from a CSV (comma separated values) file.\n\n\n\n\n\n\nTip\n\n\n\nIt‚Äôs possible to import datasets from the Web directly into pandas. However, web sources and links change and disappear frequently, so if storing the dataset is not a problem, you may want to download your own copy before working on it.\n\n\n\n\n\n\nimport pandas as pd\nloans = pd.read_csv(\"_datasets/loan.csv\")\nloans.head()\n\n\n\n\n\n\n\n\nid\nmember_id\nloan_amnt\nfunded_amnt\nfunded_amnt_inv\nterm\nint_rate\ninstallment\ngrade\nsub_grade\n...\nnum_tl_90g_dpd_24m\nnum_tl_op_past_12m\npct_tl_nvr_dlq\npercent_bc_gt_75\npub_rec_bankruptcies\ntax_liens\ntot_hi_cred_lim\ntotal_bal_ex_mort\ntotal_bc_limit\ntotal_il_high_credit_limit\n\n\n\n\n0\n1077501\n1296599\n5000\n5000\n4975.0\n36 months\n10.65%\n162.87\nB\nB2\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1077430\n1314167\n2500\n2500\n2500.0\n60 months\n15.27%\n59.83\nC\nC4\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1077175\n1313524\n2400\n2400\n2400.0\n36 months\n15.96%\n84.33\nC\nC5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1076863\n1277178\n10000\n10000\n10000.0\n36 months\n13.49%\n339.31\nC\nC1\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1075358\n1311748\n3000\n3000\n3000.0\n60 months\n12.69%\n67.79\nB\nB5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 111 columns\n\n\n\nThe int_rate column, which gives the interest rate on the loan, has been interpreted as strings due to the percent sign. We‚Äôll strip out those percent signs and convert them to floats.\n\n\n\n\n\n\nTip\n\n\n\nAs you see below, we often end up with chains of methods separated by dots. Python works from left to right, evaluating a subexpression and then replacing it with the object for the next segment in the chain. We could write these as a sequence of separate lines having intermediate results assigned to variable names, but it‚Äôs often considered better style to chain them.\n\n\n\nloans[\"int_rate\"] = loans[\"int_rate\"].str.strip('%').astype(float)\nloans.head()\n\n\n\n\n\n\n\n\nid\nmember_id\nloan_amnt\nfunded_amnt\nfunded_amnt_inv\nterm\nint_rate\ninstallment\ngrade\nsub_grade\n...\nnum_tl_90g_dpd_24m\nnum_tl_op_past_12m\npct_tl_nvr_dlq\npercent_bc_gt_75\npub_rec_bankruptcies\ntax_liens\ntot_hi_cred_lim\ntotal_bal_ex_mort\ntotal_bc_limit\ntotal_il_high_credit_limit\n\n\n\n\n0\n1077501\n1296599\n5000\n5000\n4975.0\n36 months\n10.65\n162.87\nB\nB2\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1077430\n1314167\n2500\n2500\n2500.0\n60 months\n15.27\n59.83\nC\nC4\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1077175\n1313524\n2400\n2400\n2400.0\n36 months\n15.96\n84.33\nC\nC5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1076863\n1277178\n10000\n10000\n10000.0\n36 months\n13.49\n339.31\nC\nC1\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1075358\n1311748\n3000\n3000\n3000.0\n60 months\n12.69\n67.79\nB\nB5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows √ó 111 columns\n\n\n\nLet‚Äôs add a column for the percentage of the loan request that was eventually funded. This will be a target for some of our learning methods.\n\nloans[\"percent_funded\"] = 100 * loans[\"funded_amnt\"] / loans[\"loan_amnt\"]\ntarget = [\"percent_funded\"]\n\nWe will only use a small subset of the numerical columns as features. Let‚Äôs verify that there are no missing values in those columns:\n\nfeatures = [ \"loan_amnt\", \"int_rate\", \"installment\", \"annual_inc\",\n    \"dti\", \"delinq_2yrs\", \"delinq_amnt\" ]\nloans = loans[features + target]\nloans.isna().sum()\n\nloan_amnt         0\nint_rate          0\ninstallment       0\nannual_inc        0\ndti               0\ndelinq_2yrs       0\ndelinq_amnt       0\npercent_funded    0\ndtype: int64\n\n\n\n\n\n\n\n\nNote\n\n\n\nGiven lists of columns names (or any strings), you can use + to concatenate them into a single list.\n\n\nFinally, we‚Äôll output this cleaned data frame to its own CSV file. The index row is an ID number that is meaningless to classification, so we will instruct pandas to exclude it from the new file:\n\nloans.to_csv(\"loan_clean.csv\", index=False)\n\n\n\n1.5.3 Diamonds example\n\n\n\nWe will also be using a seaborn dataset comprising features of diamonds and their prices:\n\nimport seaborn as sns\ndiamonds = sns.load_dataset(\"diamonds\")\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nAs you can see above, some of the features (cut, color, clarity) have string designations. However, these do have a specific ordering in this context. So we will replace the strings with the correct ordinal values.\nWe start with the cut column:\n\ncuts = [\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]\ndiamonds[\"cut\"].replace(\n    cuts,           # to be replaced\n    range(5),       # replacements\n    inplace=True    # change the original frame, not a copy\n    )\n\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\n4\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\n3\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\n1\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\n3\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\n1\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nAbove, you see that the cut strings have been replaced by integers. Now we do the same for the other categories:\n\ndiamonds[\"clarity\"].replace(\n    [\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"],\n    range(8),\n    inplace=True\n    )\n\ndiamonds[\"color\"].replace(\n    list(\"DEFGHIJ\"),\n    range(7),\n    inplace=True\n    )\n\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\n4\n1\n1\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\n3\n1\n2\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\n1\n1\n4\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\n3\n5\n3\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\n1\n6\n1\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nWe‚Äôll save this modified dataset to its own file for our future use:\n\ndiamonds.to_csv(\"diamonds.csv\", index=False)"
  },
  {
    "objectID": "data.html#exercises",
    "href": "data.html#exercises",
    "title": "1¬† Representation of data",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1 For each type of data, classify it as discrete quantitative, continuous quantitative, categorical, or other.\n\nHow many students are enrolled at a university\nYour favorite day of the week\nHow many inches of rain fall at an airport during one day\nWeight of a motor vehicle\nManufacturer of a motor vehicle\nText of all Yelp reviews for a restaurant\nStar ratings from all Yelp reviews for a restaurant\nSize of the living area of an apartment\nDNA nucleotide sequence of a cell\n\n\n\nExercise 1.2 Give the length of each vector or series.\n\nMorning waking times every day for a week\nNumber of siblings (max 12) for each student in a class of 30\nPosition and momentum of a roller coaster car\n\n\n\nExercise 1.3 Describe a scheme for creating dummy variables for the days of the week. Use your scheme to encode the vector:\n[Tuesday, Sunday, Friday, Tuesday, Monday]"
  },
  {
    "objectID": "stats.html#summary-statistics",
    "href": "stats.html#summary-statistics",
    "title": "2¬† Descriptive statistics",
    "section": "2.1 Summary statistics",
    "text": "2.1 Summary statistics\nWe will use data about car fuel efficiency for illustrations.\n\ncars = sns.load_dataset(\"mpg\")\n\nThe describe method of a data frame gives summary statistics for each column of quantitative data:\n\ncars.describe()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\n\ncount\n398.000000\n398.000000\n398.000000\n392.000000\n398.000000\n398.000000\n398.000000\n\n\nmean\n23.514573\n5.454774\n193.425879\n104.469388\n2970.424623\n15.568090\n76.010050\n\n\nstd\n7.815984\n1.701004\n104.269838\n38.491160\n846.841774\n2.757689\n3.697627\n\n\nmin\n9.000000\n3.000000\n68.000000\n46.000000\n1613.000000\n8.000000\n70.000000\n\n\n25%\n17.500000\n4.000000\n104.250000\n75.000000\n2223.750000\n13.825000\n73.000000\n\n\n50%\n23.000000\n4.000000\n148.500000\n93.500000\n2803.500000\n15.500000\n76.000000\n\n\n75%\n29.000000\n8.000000\n262.000000\n126.000000\n3608.000000\n17.175000\n79.000000\n\n\nmax\n46.600000\n8.000000\n455.000000\n230.000000\n5140.000000\n24.800000\n82.000000\n\n\n\n\n\n\n\nWe now discuss the definitions and interpretations of these values.\n\n2.1.1 Mean and dispersion\nYou may already know the ‚Äúbig three‚Äù summary statistics:\n\nDefinition 2.1 Given data values \\(x_1,\\ldots,x_n\\), their mean is \\[\n\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\tag{2.1}\\] their variance is \\[\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2,\n\\tag{2.2}\\] and their standard deviation (STD) is \\(\\sigma\\), the square root of the variance.\n\nMean is a measurement of central tendency. Variance and STD are measures of spread or dispersion in the data.\n\nExample 2.1 Suppose that \\(x_1=0\\), \\(x_2=t\\), and \\(x_3=-t\\), where \\(|t| \\le 5\\). What are the minimum and maximum possible values of the standard deviation?\n\nSolution. The mean is \\(\\mu=0\\), and hence \\[\n\\sigma^2 = 0^2 + t^2 + (-t)^2 = 2t^2.\n\\] The function \\(f(t)=2t^2\\) has a single critical value at \\(t=0\\). Clearly, \\(f(0)=0\\) is both a local and a global minimum. Since \\(-5\\le t \\le 5\\), we also find that \\(f(\\pm5)=50\\) is a global maximum.\n\n\n\n\n\n\n\n\nNote\n\n\n\nVariance is in units that are the square of the data, which can be harder to interpret than STD, which has units the same as the data values.\n\n\n\n\n2.1.2 z-scores\nGiven data values \\(x_1,\\ldots,x_n\\), we can define related values known as standardized scores or z-scores:\n\\[\nz_i = \\frac{x-\\mu}{\\sigma}, \\ldots i=1,\\ldots,n.\n\\]\nThe z-scores have mean zero and standard deviation equal to 1; in physical terms, they are dimensionless. This makes them attractive to work with and compare across data sets.\n\nTheorem 2.1 The z-scores have mean equal to zero and variance equal to 1.\n\n\nProof. Direct calculations.\n\n\nExample 2.2 Continuing with the values from Example¬†2.1, we assume without losing generality that \\(t\\ge 0\\). (Otherwise, we can just swap \\(x_2\\) and \\(x_3\\).) Then we have the z-scores \\[\nz_1 = \\frac{0-0}{t\\sqrt{2}} = 0, \\quad z_2 = \\frac{t-0}{t\\sqrt{2}} = \\frac{1}{\\sqrt{2}} \\quad z_3 = \\frac{-t-0}{t\\sqrt{2}} = \\frac{-1}{\\sqrt{2}}.\n\\] These are independent of \\(t\\), which just scales the original values. So, for instance, the z-scores of a collection of distances are unaffected if the distances are expressed in centimeters or kilometers.\n\nWe can write a little function to compute z-scores in Python:\n\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\ncars[\"mpg_z\"] = standardize( cars[\"mpg\"] )\ncars[ [\"mpg\", \"mpg_z\"] ].describe()\n\n\n\n\n\n\n\n\nmpg\nmpg_z\n\n\n\n\ncount\n398.000000\n3.980000e+02\n\n\nmean\n23.514573\n1.071170e-16\n\n\nstd\n7.815984\n1.000000e+00\n\n\nmin\n9.000000\n-1.857037e+00\n\n\n25%\n17.500000\n-7.695221e-01\n\n\n50%\n23.000000\n-6.583596e-02\n\n\n75%\n29.000000\n7.018217e-01\n\n\nmax\n46.600000\n2.953617e+00\n\n\n\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\nSince floating-point values are rounded off, it‚Äôs unlikely that a value derived from them that is meant to be zero will actually be exactly zero. Above, the mean value of about \\(-10^{-15}\\) should be seen as reasonable for values that have been rounded off in the 15th digit or so.\n\n\n\n\n2.1.3 Populations and samples\nIn statistics one refers to the population as the entire universe of available values. Thus, the ages of adult on Earth at some instant has a particular population mean and standard deviation. However, in order to estimate those values, we can only measure a sample of the population directly.\nWhen Equation¬†2.1 is used to compute the mean of a sample rather than a population, we change the notation a bit as a reminder: \\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\tag{2.3}\\]\nIt can be proved that the sample mean is an accurate way to estimate the population mean, in the following precise sense. If, in a thought experiment, we could average \\(\\bar{x}\\) over all possible samples of size \\(n\\), the result would be exactly the population mean \\(\\mu\\). That is, we say that \\(\\bar{x}\\) is an unbiased estimator for \\(\\mu\\).\nThe sample mean in turn can be used within Equation¬†2.2 to compute sample variance: \\[\ns_n^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\]\nHowever, sample variance is more subtle than the sample mean. If \\(s_n^2\\) is averaged over all possible sample sets, we do not get the population variance \\(\\sigma^2\\); hence, \\(s_n^2\\) is called a biased estimator of the population variance.\nAn unbiased estimator for \\(\\sigma^2\\) is\n\\[\ns_{n-1}^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\tag{2.4}\\]\n\nExample 2.3 The values [1, 4, 9, 16, 25] have mean \\(\\bar{x}=55/5 = 11\\). The sample variance is\n\\[\n\\begin{split}\n    s_n^2 &= \\frac{(1-11)^2+(4-11)^2+(9-11)^2+(16-11)^2+(25-11)^2}{5} \\\\\n    & = \\frac{374}{5} = 74.8.\n\\end{split}\n\\]\nBy contrast, the unbiased estimate of population variance from this sample is\n\\[\ns_{n-1}^2 = \\frac{374}{4} = 93.5.\n\\]\n\nAs you can see from the formulas and the example, the sample variance is always too large as an estimator, but the difference vanishes as the sample size \\(n\\) increases.\n\n\n\n\n\n\nWarning\n\n\n\nSources are not always clear about this terminology. Some use sample variance to mean \\(s_{n-1}^2\\), not \\(s_n^2\\), and many even omit the subscripts. You always have to check each source.\n\n\n\n\n\n\n\n\nDanger\n\n\n\nNumPy computes the biased estimator of variance by default, while pandas computes the unbiased version. Whee! Fortunately, most datasets today have large enough \\(n\\) to make the difference negligible.\n\n\nFor standard deviation, neither \\(s_n\\) nor \\(s_{n-1}\\) is an unbiased estimator of \\(\\sigma\\). There is no simple correction that works for all distributions. Our practice is to use \\(s_{n-1}\\), which is what std computes in pandas. Thus, for instance, a sample z-score for \\(x_i\\) is\n\\[\nz_i = \\frac{x_i-\\bar{x}}{s_{n-1}}.\n\\tag{2.5}\\]\n\n\n\n2.1.4 Median and quantiles\nMean and variance are not the most relevant summary statistics for every dataset. There are important alternatives.\n\nDefinition 2.2 For any \\(0 < p < 1\\), the \\(100p\\)-percentile is the value of \\(x\\) such that \\(p\\) is the probability of observing a population value less than or equal to \\(x\\).\nThe 50th percentile is known as the median of the population.\n\nIn other words, percentiles are the inverse function of the CDF.\nThe unbiased sample median of \\(x_1,\\ldots,x_n\\) can be computed by sorting the values into \\(y_1,\\ldots,y_n\\). If \\(n\\) is odd, then \\(y_{(n+1)/2}\\) is the sample median; otherwise, the average of \\(y_{n/2}\\) and \\(y_{1+(n/2)}\\) is the sample median.\nComputing unbiased sample estimates of percentiles other than the median is complicated, and we won‚Äôt go into the details. For large datasets, the sample values are good estimators in practice.\n\nExample 2.4 If the sorted values are \\(1,3,3,4,5,5,5\\), then \\(n=7\\) and the sample median is \\(y_4=4\\). If the sample values are \\(1,3,3,4,5,5,5,9\\), then \\(n=8\\) and the sample median is \\((4+5)/2=4.5\\).\n\nA set of percentiles dividing probability into \\(q\\) equal pieces is called the \\(q\\)‚Äìquantiles.\n\nExample 2.5 The 4-quantiles are called quartiles. The first quartile is the 25th percentile, or the value that exceeds 1/4 of the population. The second quartile is the median. The third quartile is the 75th percentile.\nSometimes the definition is extended to the zeroth quartile, which is the minimum sample value, and the fourth quartile, which is the maximum sample value.\n\n\n\n\n\n\n\nDanger\n\n\n\nIf this all isn‚Äôt confusing enough yet, sometimes the word quantile is used to mean percentile. This is the case for the quantile method in pandas.\n\n\n\nDefinition 2.3 The interquartile range (IQR) is the difference between the 75th percentile and the 25th percentile.\n\nIQR is an indication of the spread of the values. For some distributions, the median and IQR might be a good substitute for the mean and standard deviation.\n\nExample 2.6 The describe method includes mean, standard deviation, and the quartiles.\n\nfrom numpy.random import default_rng\nimport pandas as pd\nrng = default_rng(19716)\ndf = pd.DataFrame( {\n    \"normal\" : rng.normal( size=(4000,) ),\n    \"uniform\" : rng.uniform( size=(4000,) )\n    } )\ndf.describe()\n\n\n\n\n\n\n\n\nnormal\nuniform\n\n\n\n\ncount\n4000.000000\n4000.000000\n\n\nmean\n-0.026801\n0.493338\n\n\nstd\n0.995135\n0.293177\n\n\nmin\n-3.361732\n0.000257\n\n\n25%\n-0.685307\n0.232339\n\n\n50%\n-0.017852\n0.486322\n\n\n75%\n0.633775\n0.750565\n\n\nmax\n3.115486\n0.999204"
  },
  {
    "objectID": "stats.html#distributions",
    "href": "stats.html#distributions",
    "title": "2¬† Descriptive statistics",
    "section": "2.2 Distributions",
    "text": "2.2 Distributions\nMean and dispersion (variance or STD) attempt to summarize quantitative data with a couple of numbers. At the other extreme, we can express the distribution of all values precisely using a function.\n\n2.2.1 CDF\n\nDefinition 2.4 The cumulative distribution function (CDF) of a population is the function \\[\nF(t) = \\text{fraction of the population that is less than or equal to $t$},\n\\] where \\(t\\) ranges over all possible values.\n\nNote that by its definition, \\(F\\) ranges between 0 and 1 (inclusive) and is a nondecreasing function.\n\nExample 2.7 If a population is \\(x_i=i\\) for \\(i=1,\\ldots,n\\), then \\(F(k)=k/n\\) at each \\(k=1,\\ldots,n\\). We could, however, also regard \\(F\\) as a function of a continuous variable \\(t\\), in which case \\[\nF(t) = \\frac{\\lfloor t \\rfloor}{n},\n\\] where \\(\\lfloor\\cdot\\rfloor\\) is the floor function that rounds leftward to the nearest integer. This produces a step function that looks like stairs going up from 0 to 1.\n\nExample¬†2.7 becomes interesting as a template for generalizing to infinite populations. If we take not \\(x_i=i\\) but \\(x_i=i/n\\) and then let \\(n\\to \\infty\\), then the graph of \\(F\\) converges to \\[\nF(t) = \\begin{cases}\n0, & t < 0, \\\\\nt,& 0 \\le t \\le 1, \\\\\n1,& t > 1.\n\\end{cases}\n\\tag{2.6}\\] While it doesn‚Äôt make sense to think about a fraction of the number of values in the infinite case, we can interpret \\(F(t)\\) as the *probability of observing a value less than or equal to the real number \\(t\\).\n\nDefinition 2.5 A uniform distribution gives an equal probability to every value. In particular, the uniform distribution over the interval \\([0,1]\\) has the CDF given in Equation¬†2.6.\n\n\n\n2.2.2 Empirical CDF\nGiven a sample of a population, we can always calculate the analog of a CDF from its values.\n\nDefinition 2.6 The empirical cumulative distribution function or ECDF of a sample is the function \\(\\hat{F}\\) whose value at \\(t\\) equals the proportion of the sample values that are less than or equal to \\(t\\).\n\n\nExample 2.8 Here is an experiment that producing the ECDF for a sample from the random number generator:\n\nfrom numpy.random import default_rng\nrng = default_rng(19716)\nx = rng.uniform( size=(100,) )\nsns.displot(x, kind=\"ecdf\");\n\n\n\n\nIf we take more samples, we expect to see a curve closer to the theoretical CDF, \\(F(t)=t\\):\n\nx = rng.uniform( size=(4000,) )\nsns.displot(x, kind=\"ecdf\");\n\n\n\n\n\n\n\n2.2.3 PDF\nBy definition, we know that if \\(a<b\\), \\(\\hat{F}(b) - \\hat{F}(a)\\) is the number of observations in the half-open interval \\((a,b]\\). This leads into the next definition.\n\nDefinition 2.7 Select the ordered values \\(t_1 < t_2 < \\cdots < t_m\\), called edges, and define bins as the intervals \\[\nB_k = (t_k,t_{k+1}], \\qquad k=0,\\ldots,m,\n\\] where we adopt the convention that \\(t_0=-\\infty\\) and \\(t_{m+1}=\\infty\\). Let \\(c_k\\) be the number of data values in \\(B_k\\). Then a histogram relative to the bins is the list of \\((B_0,c_0),\\ldots,(B_m,c_m)\\).\n\nThe default for a seaborn displot is to show a histogram.\n\nExample 2.9 Continuing with the uniform distribution over \\([0,1]\\):\n\nx = rng.uniform( size=(1000,) )\nsns.displot(x);\n\n\n\n\nWe can choose the number of bins to use, or give a vector of their edges:\n\nimport numpy as np\nsns.displot(x, bins=40);\n\n\n\n\n\nAgain something interesting happens in a limiting case. If we normalize the count in a bin by the length of that bin, we get \\[\n\\frac{c_k}{t_{k+1}-t_k} = \\frac{\\hat{F}(t_{k+1})-\\hat{F}(t_k)}{t_{k+1}-t_k}.\n\\tag{2.7}\\] If we let the number of observations tend to infinity, then \\(\\hat{F}\\) will converge to \\(F\\), and if we also let the number of bins go to infinity, then the fraction in Equation¬†2.7 converges to \\(F'(t_k)\\).\n\nDefinition 2.8 The probability density function or PDF of a distribution is the derivative of the CDF.\n\n\nExample 2.10 If we have many samples, then we can use a normalized histogram to give an approximation of the PDF:\n\nx = rng.uniform( size=(20000,) )\nsns.displot(x, bins=24, stat=\"density\");\n\n\n\n\nAlternatively, we can use process called kernel density estimation to plot a continuous estimate of the PDF:\n\nsns.displot(x, kind=\"kde\");\n\n\n\n\nIn this case we did not obtain a particularly good approximation of the true PDF. In part this is because kernel density estimation assumes that the PDF is continuous, but here it is 1 over \\([0,1]\\) and jumps down to 0 elsewhere.\n\n\n\n2.2.4 Mean and variance\nIt‚Äôs possible to compute the mean and variance (thus STD) of a distribution from its PDF: \\[\n\\begin{split}\n\\mu &= \\int x f(x) \\, dx \\\\\n\\sigma^2 &= \\int (x-\\mu)^2 f(x) \\, dx,\n\\end{split}\n\\] where the integrals are taken over the domain of \\(f\\).\n\nExample 2.11 The uniform distribution over \\([0,1]\\) has \\(f(x)=1\\) over that interval. Hence, \\[\n\\begin{split}\n\\mu &= \\int_0^1 x \\, dx = \\left[ \\frac{1}{2} x^2\\right]_0^1 = \\frac{1}{2}, \\\\\n\\sigma^2 &= \\int_0^1 \\left(x-\\tfrac{1}{2}\\right)^2 \\, dx = \\frac{1}{3} - \\frac{1}{2} + \\frac{1}{4} = \\frac{1}{12}.\n\\end{split}\n\\]\nLet‚Äôs check these results empirically:\n\nfrom numpy.random import default_rng\nimport numpy as np\n\nrng = default_rng(19716)\nx = rng.uniform( size=(2000,) )\nprint(f\"¬µ = {np.mean(x):.5f}, 12œÉ¬≤ = {12*np.var(x):.5f}\")\n\n¬µ = 0.50518, 12œÉ¬≤ = 1.00019\n\n\n\n\n\n2.2.5 Normal distribution\nNext to perhaps the uniform distribution, the following is the most widely used distribution of a random variable.\n\nDefinition 2.9 The normal distribution or Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is defined by the PDF \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{ -(x-\\mu)^2/(2\\sigma^2)}.\n\\tag{2.8}\\] The standard normal distribution uses \\(\\mu=0\\) and \\(\\sigma=1\\).\n\nFor data that are distributed normally, about 68% of the values lie within one standard deviation of the mean, and 95% lie within two standard deviations.\n\nExample 2.12 The normal method of a NumPy RNG simulates a standard normal distribution.\n\nrng = default_rng(19716)\nx = rng.normal( size=(10000,) )\nsns.displot(x, bins=np.linspace(-4,4,28), stat=\"probability\");\n\n\n\n\nWe can change the variance by multiplication by \\(\\sigma\\) and change the mean by adding \\(\\mu\\):\n\ndf = pd.DataFrame( {\"x\": x, \"3x-10\": 3*x-10} )\ndf.describe()\n\n\n\n\n\n\n\n\nx\n3x-10\n\n\n\n\ncount\n10000.000000\n10000.000000\n\n\nmean\n-0.012414\n-10.037242\n\n\nstd\n0.993568\n2.980704\n\n\nmin\n-3.436924\n-20.310773\n\n\n25%\n-0.672352\n-12.017056\n\n\n50%\n-0.008374\n-10.025122\n\n\n75%\n0.659730\n-8.020809\n\n\nmax\n4.044099\n2.132297\n\n\n\n\n\n\n\nThe KDE density estimator works pretty well for normally distributed data, except in the tails where there are few observations:\n\nsns.displot(data=df, x=\"3x-10\", kind=\"kde\");"
  },
  {
    "objectID": "stats.html#grouping-data",
    "href": "stats.html#grouping-data",
    "title": "2¬† Descriptive statistics",
    "section": "2.3 Grouping data",
    "text": "2.3 Grouping data\nSometimes we are interested in breaking down data by categorical values or other criteria. Both seaborn and pandas make this relatively straightforward.\nHere is the distribution of the mpg variable over the entire dataset:\n\nsns.displot(data=cars, x=\"mpg\", bins=20);\n\n\n\n\nWe now look at ways to group the samples within this dataset.\n\n2.3.1 Splitting\nWe can use categorical variables to define groups within the data set. Suppose we want to separate by the origin column:\n\ncars[\"origin\"].value_counts()\n\nusa       249\njapan      79\neurope     70\nName: origin, dtype: int64\n\n\nThe groupby method for a data frame splits the frame into groups based on categorical values in a designated column:\n\ncars.groupby([\"origin\"])[\"mpg\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\norigin\n\n\n\n\n\n\n\n\n\n\n\n\neurope\n70.0\n27.891429\n6.723930\n16.2\n24.0\n26.5\n30.65\n44.3\n\n\njapan\n79.0\n30.450633\n6.090048\n18.0\n25.7\n31.6\n34.05\n46.6\n\n\nusa\n249.0\n20.083534\n6.402892\n9.0\n15.0\n18.5\n24.00\n39.0\n\n\n\n\n\n\n\nBoth the median and the mean values are quite a bit lower for usa cars than for the other regions. This is also apparent when we plot the distributions individually using different colors:\n\nsns.displot(data=cars, x=\"mpg\", hue=\"origin\");\n\n\n\n\nThat graph might be hard to read because of the overlaps. We can instead plot the groups in separate columns in what is often called a facet plot:\n\nsns.displot(data=cars, \n    x=\"mpg\", \n    col=\"origin\", height=2.2\n    );\n\n\n\n\nIt‚Äôs now clear that the U.S.A. cars are more clustered on the left (smaller MPG) than are the Japanese and European cars.\nAnother way to visualize grouped data is with a box plot:\n\nsns.catplot(data=cars, \n    x=\"origin\", y=\"mpg\", \n    kind=\"box\"\n    );\n\n\n\n\nEach colored box shows the interquartile range, with the interior horizontal line showing the median. The whiskers and dots are explained in a later section. A related visualization is a violin plot:\n\nsns.catplot(data=cars, \n    x=\"mpg\", y=\"origin\", \n    kind=\"violin\"\n    );\n\n\n\n\nIn a violin plot, the inner lines show the same information as the box plot, with the thick part showing the IQR, while the sides of the ‚Äúviolins‚Äù are KDE estimates of the density functions.\nIt‚Äôs also possible to split using a quantitative variable. The cut method will put the values into bins that serve to define the groups:\n\ncuts = pd.cut( \n    cars[\"weight\"],         # series to cut by\n    range(1500, 5800, 1000)    # bin edges\n    )\n\ncars[\"cuts\"] = cuts\ncars.head(10)\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\nmpg_z\ncuts\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nusa\nchevrolet chevelle malibu\n-0.705551\n(3500, 4500]\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nusa\nbuick skylark 320\n-1.089379\n(3500, 4500]\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n-0.705551\n(2500, 3500]\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n-0.961437\n(2500, 3500]\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nusa\nford torino\n-0.833494\n(2500, 3500]\n\n\n5\n15.0\n8\n429.0\n198.0\n4341\n10.0\n70\nusa\nford galaxie 500\n-1.089379\n(3500, 4500]\n\n\n6\n14.0\n8\n454.0\n220.0\n4354\n9.0\n70\nusa\nchevrolet impala\n-1.217322\n(3500, 4500]\n\n\n7\n14.0\n8\n440.0\n215.0\n4312\n8.5\n70\nusa\nplymouth fury iii\n-1.217322\n(3500, 4500]\n\n\n8\n14.0\n8\n455.0\n225.0\n4425\n10.0\n70\nusa\npontiac catalina\n-1.217322\n(3500, 4500]\n\n\n9\n15.0\n8\n390.0\n190.0\n3850\n8.5\n70\nusa\namc ambassador dpl\n-1.089379\n(3500, 4500]\n\n\n\n\n\n\n\n\nsns.catplot(data=cars, \n    x=\"mpg\", y=\"cuts\", \n    kind=\"violin\"\n    );\n\n\n\n\n\n\n2.3.2 Aggregation\nGroups defined by groupby can then be passed through aggregators that reduce each grouped column to a single value. A list of the most common predefined aggregation functions is given in Table¬†2.1.\n\n\nTable¬†2.1: Aggregation functions. All ignore NaN values.\n\n\nmethod\neffect\n\n\n\n\ncount\nNumber of values in each group\n\n\nmean\nMean value in each group\n\n\nsum\nSum within each group\n\n\nstd, var\nStandard deviation/variance within groups\n\n\nmin, max\nMin or max within groups\n\n\ndescribe\nDescriptive statistics\n\n\nfirst, last\nFirst or last of group values\n\n\n\n\n\nby_weight = cars.groupby(cuts)\nby_weight[\"mpg\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nweight\n\n\n\n\n\n\n\n\n\n\n\n\n(1500, 2500]\n147.0\n30.631293\n5.864413\n18.0\n26.0\n30.5\n34.450\n46.6\n\n\n(2500, 3500]\n142.0\n22.547183\n4.754061\n13.0\n19.0\n22.0\n25.325\n38.0\n\n\n(3500, 4500]\n92.0\n15.688043\n2.760428\n10.0\n14.0\n15.0\n17.125\n26.6\n\n\n(4500, 5500]\n17.0\n12.411765\n1.622453\n9.0\n12.0\n13.0\n13.000\n16.0\n\n\n\n\n\n\n\nIf you want a more exotic operation, you can call agg with your own function:\n\ndef iqr(x):\n    q1,q3 = x.quantile( [.25, .75] )\n    return q3 - q1\n\nby_weight[\"mpg\"].agg(iqr)\n\nweight\n(1500, 2500]    8.450\n(2500, 3500]    6.325\n(3500, 4500]    3.125\n(4500, 5500]    1.000\nName: mpg, dtype: float64\n\n\n\n\n2.3.3 Transformation\nA transformation applies a function to each element of a column, producing a result of the same length that can be indexed the same way. This transformation can be applied group by group.\nFor example, we can standardize to z-scores within each group separately:\n\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\ncars[\"group_z\"] = by_weight[\"mpg\"].transform(standardize)\n\nsns.displot(data=cars, \n    x=\"group_z\", \n    col=\"origin\", height=2.3\n    );\n\n\n\n\nNote how this differs from computing z-scores based on global statistics:\n\ncars[\"global_z\"] = standardize( cars[\"mpg\"] )\n\nsns.displot(data=cars, \n    x=\"global_z\", \n    col=\"origin\", height=2.3\n    );\n\n\n\n\n\n\n2.3.4 Filtering\nTo apply a filter, provide a function that operates on a column and returns either True, meaning to keep the column, or False, meaning to reject it. This filter is applied groupwise.\nFor example, suppose we want to group cars by horsepower:\n\ncuts = pd.cut(cars[\"horsepower\"], range(40,220,20))\n\nby_hp = cars.groupby(cuts)\nby_hp.count()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\nmpg_z\ncuts\ngroup_z\nglobal_z\n\n\nhorsepower\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(40, 60]\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n\n\n(60, 80]\n99\n99\n99\n99\n99\n99\n99\n99\n99\n99\n99\n99\n99\n\n\n(80, 100]\n123\n123\n123\n123\n123\n123\n123\n123\n123\n123\n123\n123\n123\n\n\n(100, 120]\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n48\n\n\n(120, 140]\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n25\n\n\n(140, 160]\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n40\n\n\n(160, 180]\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n20\n\n\n(180, 200]\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n7\n\n\n\n\n\n\n\nSay we want to drop the cars belonging to groups having fewer than 30 members:\n\nhp_30 = by_hp.filter( lambda x: len(x) > 29 )\nhp_30.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\nmpg_z\ncuts\ngroup_z\nglobal_z\n\n\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n-0.705551\n(2500, 3500]\n-0.956484\n-0.705551\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n-0.961437\n(2500, 3500]\n-1.377177\n-0.961437\n\n\n11\n14.0\n8\n340.0\n160.0\n3609\n8.0\n70\nusa\nplymouth 'cuda 340\n-1.217322\n(3500, 4500]\n-0.611515\n-1.217322\n\n\n12\n15.0\n8\n400.0\n150.0\n3761\n9.5\n70\nusa\nchevrolet monte carlo\n-1.089379\n(3500, 4500]\n-0.249252\n-1.089379\n\n\n14\n24.0\n4\n113.0\n95.0\n2372\n15.0\n70\njapan\ntoyota corona mark ii\n0.062107\n(1500, 2500]\n-1.130768\n0.062107\n\n\n\n\n\n\n\nNotice that the result has been merged back into a single frame. If we want to work with the groups again, we have to apply the grouping anew.\n\ncuts = pd.cut(hp_30[\"horsepower\"], range(40,220,20))\nhp_30.groupby(cuts)[\"mpg\"].median()\n\nhorsepower\n(40, 60]        NaN\n(60, 80]      31.00\n(80, 100]     23.80\n(100, 120]    20.40\n(120, 140]      NaN\n(140, 160]    14.25\n(160, 180]      NaN\n(180, 200]      NaN\nName: mpg, dtype: float64\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThere is a balance to strike between a plot that is information-poor versus one that is too busy to read clearly. But you can probably fit more information comfortably than you have been accustomed to. Great data visualizations reward time spent by the reader to examine them. Edward Tufte has written several great books on this subject."
  },
  {
    "objectID": "stats.html#outliers",
    "href": "stats.html#outliers",
    "title": "2¬† Descriptive statistics",
    "section": "2.4 Outliers",
    "text": "2.4 Outliers\nInformally, an outlier is a data value that is considered to be far from typical. In some applications, such as detecting earthquakes or cancer, outliers are the cases of real interest. But we will be thinking of them as unwelcome values that might result from equipment failure, confounding effects, mistyping a value, using an extreme value to represent missing data, and so on. In such cases we want to minimize the effect of the outliers on the statistics.\nThere are various ways of deciding what ‚Äútypical‚Äù means, and there is no one-size recommendation for all applications.\n\n2.4.1 IQR\nLet‚Äôs look at another data set, based on an fMRI experiment.\n\nfmri = sns.load_dataset(\"fmri\")\nfmri.head()\n\n\n\n\n\n\n\n\nsubject\ntimepoint\nevent\nregion\nsignal\n\n\n\n\n0\ns13\n18\nstim\nparietal\n-0.017552\n\n\n1\ns5\n14\nstim\nparietal\n-0.080883\n\n\n2\ns12\n18\nstim\nparietal\n-0.081033\n\n\n3\ns11\n18\nstim\nparietal\n-0.046134\n\n\n4\ns10\n18\nstim\nparietal\n-0.037970\n\n\n\n\n\n\n\nWe want to focus on the signal column, splitting according to the event.\n\nfmri.groupby(\"event\")[\"signal\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nevent\n\n\n\n\n\n\n\n\n\n\n\n\ncue\n532.0\n-0.006669\n0.047752\n-0.181241\n-0.031122\n-0.008871\n0.015825\n0.215735\n\n\nstim\n532.0\n0.013748\n0.123179\n-0.255486\n-0.062378\n-0.022202\n0.058143\n0.564985\n\n\n\n\n\n\n\nHere is a box plot of the signal for these groups.\n\nsns.catplot(data=fmri,\n    x=\"event\", y=\"signal\",\n    kind=\"box\"\n    );\n\n\n\n\nThe dots lying outside the whiskers in the plot may be considered outliers. They are determined by the quartiles. Let \\(Q_1\\) and \\(Q_3\\) be the first and third quartiles (i.e., 25% and 75% percentiles), and let \\(I=Q_3-Q_1\\) be the interquartile range (IQR). Then \\(x\\) is an outlier value if\n\\[\nx < Q_1 - 1.5I \\text{ or } x > Q_3 + 1.5I.\n\\]\n\n\n2.4.2 Mean and STD\nFor normal distributions, values more than twice the standard deviation \\(\\sigma\\) from the mean might be declared to be outliers; this would exclude 5% of the values, on average. A less aggressive criterion is to allow a distance of \\(3\\sigma\\), which excludes only about 0.3% of the values. The IQR criterion above corresponds to about \\(2.7\\sigma\\) in the normal case.\nThe following plot shows the outlier cutoffs for 2000 samples from a normal distribution, using the criteria for 2œÉ (red), 3œÉ (blue), and 1.5 IQR (black).\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom numpy.random import default_rng\nrandn = default_rng(1).normal \n\nx = pd.Series(randn(size=2000))\nsns.displot(data=x,bins=30);\nm,s = x.mean(),x.std()\nplt.axvline(m-2*s,color='r')\nplt.axvline(m+2*s,color='r')\nplt.axvline(m-3*s,color='b')\nplt.axvline(m+3*s,color='b')\n\nq1,q3 = x.quantile([.25,.75])\nplt.axvline(q3+1.5*(q3-q1),color='k')\nplt.axvline(q1-1.5*(q3-q1),color='k');\n\n\n\n\n\nFor asymmetric distributions, or those with a heavy tail, these criteria might show greater differences.\n\n\n\n\n\n\nNote\n\n\n\nA criticism of classical statistics is that much of it is conditioned on the assumption of normally distributed variables. This assumption is often violated by real datasets, and quantities that depend on normality should be used judiciously.\n\n\n\n\n2.4.3 Removing outliers\nIt is well known that the mean is more sensitive to outliers than the median is.\n\nExample 2.13 The values \\(1,2,3,4,5\\) have a mean and median both equal to 3. If we change the largest value to be a lot larger, say \\(1,2,3,4,1000\\), then the mean changes to 202. But the median is still 3!\n\nLet‚Äôs use IQR to remove outliers from the fmri data set. We do this by creating a Boolean-valued series indicating which rows of the frame represent outliers within their group.\n\ndef is_outlier(x):\n    Q1,Q3 = x.quantile([.25,.75])\n    I = Q3-Q1\n    return (x < Q1-1.5*I) |  (x > Q3+1.5*I)\n\nouts = fmri.groupby(\"event\")[\"signal\"].transform(is_outlier)\nfmri[outs][\"event\"].value_counts()\n\nstim    40\ncue     26\nName: event, dtype: int64\n\n\nYou can see above that there are 66 outliers. To negate the outlier indicator, we can use ~outs as a row selector.\n\ncleaned = fmri[~outs]\n\nThe median values are barely affected by the omission of the outliers.\n\nprint( \"medians with outliers:\" )\nprint( fmri.groupby(\"event\")[\"signal\"].median() )\nprint( \"\\nmedians without outliers:\" )\nprint( cleaned.groupby(\"event\")[\"signal\"].median() )\n\nmedians with outliers:\nevent\ncue    -0.008871\nstim   -0.022202\nName: signal, dtype: float64\n\nmedians without outliers:\nevent\ncue    -0.009006\nstim   -0.028068\nName: signal, dtype: float64\n\n\nThe means show much greater change.\n\nprint( \"means with outliers:\" )\nprint( fmri.groupby(\"event\")[\"signal\"].mean() )\nprint( \"\\nmeans without outliers:\" )\nprint( cleaned.groupby(\"event\")[\"signal\"].mean() )\n\nmeans with outliers:\nevent\ncue    -0.006669\nstim    0.013748\nName: signal, dtype: float64\n\nmeans without outliers:\nevent\ncue    -0.008243\nstim   -0.010245\nName: signal, dtype: float64\n\n\nFor the stim case in particular, the mean value changes by almost 200%, including a sign change. (Relative to the standard deviation, it‚Äôs closer to a 20% change.)"
  },
  {
    "objectID": "stats.html#correlation",
    "href": "stats.html#correlation",
    "title": "2¬† Descriptive statistics",
    "section": "2.5 Correlation",
    "text": "2.5 Correlation\n\nThere are often observations that we believe to be linked, either because one influences the other, or both are influenced by some other factor. That is, we say the quantities are correlated.\nThere are several ways to measure correlation, but it‚Äôs good practice to look at the data before jumping to the numbers.\n\n2.5.1 Relational plots\nWe can use relplot to make a scatter plot of two different columns in a frame:\n\nsns.relplot(data=cars, \n    x=\"model_year\", y=\"mpg\"\n    );\n\n\n\n\nLike the other plot types above, we can use color, column, marker size, etc. to separate the dots into groups.\nIf we want to emphasize a trend, we can instead plot the average value at each \\(x\\) with error bars:\n\nsns.relplot(data=cars, \n    x=\"model_year\", y=\"mpg\",\n    kind=\"line\", errorbar=\"sd\"\n    );\n\n\n\n\nThe error ribbon above is drawn at one standard deviation around the mean.\nIn order to see multiple pairwise scatter plots, we can use pairplot in seaborn:\n\ncolumns = [ \"mpg\", \"horsepower\", \"displacement\", \"origin\" ]\nsns.pairplot(data= cars[columns],\n    hue=\"origin\", height=2\n    );\n\n\n\n\nThe panels along the diagonal show each quantitative variable‚Äôs PDF. The other panels show scatter plots putting one pair at a time of the variables on the coordinate axes.\n\n\n2.5.2 Covariance\n\nDefinition 2.10 Suppose we have two series of observations, \\([x_i]\\) and \\([y_i]\\), representing observations of random quantities \\(X\\) and \\(Y\\) having means \\(\\mu_X\\) and \\(\\mu_Y\\). Their covariance is defined as \\[\n\\Cov(X,Y) = \\frac{1}{n} \\sum_{i=1}^n (x_i-\\mu_X)(y_i-\\mu_Y).\n\\]\n\nNote that the values \\(x_i-\\mu_X\\) and \\(y_i-\\mu_Y\\) are deviations from the means. It follows from the definitions that \\[\n\\begin{split}\n    \\Cov(X,X) &= \\sigma_X^2, \\\\\n    \\Cov(Y,Y) &= \\sigma_Y^2,\n\\end{split}\n\\] i.e., self-covariance is simply variance.\nCovariance is not easy to interpret. Its units are the products of the units of the two variables, and it is sensitive to rescaling the variables (e.g., grams versus kilograms).\n\n\n2.5.3 Pearson coefficient\nWe can remove the dependence on units and scale by applying the covariance to standardized scores for both variables. The following is the best-known measure of correlation.\n\nDefinition 2.11 For the populations of \\(X\\) and \\(Y\\), the Pearson correlation coefficient is \\[\n\\begin{split}\n    \\rho(X,Y) &= \\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{x_i-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y_i-\\mu_Y}{\\sigma_Y}\\right) \\\\\n    & = \\frac{\\Cov(X,Y)}{\\sigma_X\\sigma_Y},\n\\end{split}\n\\tag{2.9}\\] where \\(\\sigma_X^2\\) and \\(\\sigma_Y^2\\) are the population variances of \\(X\\) and \\(Y\\).\nFor samples from the two populations, we use \\[\nr_{xy} =  \\frac{\\sum_{i=1}^n (x_i-\\bar{x}) (y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\,\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})^2}},\n\\tag{2.10}\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are sample means.\n\nBoth \\(\\rho_{XY}\\) and \\(r_{xy}\\) are between \\(-1\\) and \\(1\\), with the endpoints indicating perfect correlation (inverse or direct).\nAn equivalent formula for \\(r_{xy}\\) is \\[\nr_{xy} =  \\frac{1}{n-1} \\sum_{i=1}^n \\left(\\frac{x_i-\\bar{x}}{s_x}\\right)\\, \\left(\\frac{y_i-\\bar{y}}{s_y}\\right),\n\\tag{2.11}\\] where the quantities in parentheses are z-scores.\n\nExample 2.14 We might reasonably expect horsepower and miles per gallon to be inversely correlated:\n\nsns.relplot( data=cars,\n    x=\"horsepower\",y=\"mpg\"\n    );\n\n\n\n\nCovariance allows us to confirm the relationship:\n\ncars[ [\"horsepower\", \"mpg\"] ].cov()\n\n\n\n\n\n\n\n\nhorsepower\nmpg\n\n\n\n\nhorsepower\n1481.569393\n-233.857926\n\n\nmpg\n-233.857926\n61.089611\n\n\n\n\n\n\n\nBut should these numbers considered big? The Pearson coefficient is more helpful:\n\ncars[ [\"horsepower\", \"mpg\"] ].corr()\n\n\n\n\n\n\n\n\nhorsepower\nmpg\n\n\n\n\nhorsepower\n1.000000\n-0.778427\n\n\nmpg\n-0.778427\n1.000000\n\n\n\n\n\n\n\nThe value of about \\(-0.79\\) suggests that knowing one of the values would allow us to predict the other one rather well using a best-fit straight line (more on that in a future chapter).\n\nAs usual when dealing with means, however, the Pearson coefficient can be sensitive to outlier values.\n\nExample 2.15 The Pearson coefficient of any variable with itself is 1. But let‚Äôs correlate two series that differ in only one element: \\(0,1,2,\\ldots,19\\), and the same sequence but with the fifth value replaced by \\(-100\\):\n\nx = pd.Series( range(20) )\ny = x.copy()\ny[4] = -100\nx.corr(y)\n\n0.43636501543147005\n\n\nDespite the change being in a single value, over half of the predictive power was lost.\n\n\n\n2.5.4 Spearman coefficient\nThe Spearman coefficient is one way to lessen the impact of outliers when measuring correlation. The idea is that the values are used only in their orderings.\n\nDefinition 2.12 If \\(x_1,\\ldots,x_n\\) is a series of observations, let their sorted ordering be \\[\nx_{s_1},x_{s_2},\\ldots,x_{s_n}.\n\\] Then \\(s_1,s_2,\\ldots,s_n\\) is the rank series of \\(\\bfx\\).\n\n\nDefinition 2.13 The Spearman coefficient of two series of equal length is the Pearson coefficient of their rank series.\n\n\nExample 2.16 Returning to Example¬†2.15, we find the Spearman coefficient is barely affected by the single outlier:\n\nx = pd.Series( range(20) )\ny = x.copy()\ny[4] = -100\nx.corr(y,\"spearman\")\n\n0.9849624060150375\n\n\nIt‚Äôs trivial in this case to produce the two rank series by hand:\n\ns = pd.Series( range(1,21) )    # already sorted\nt = s.copy()\nt[:5] = [2,3,4,5,1]     # modified sort ordering\n\nt.corr(s)\n\n0.9849624060150375\n\n\nAs long as y[4] is negative, it doesn‚Äôt matter what its particular value is:\n\ny[4] = -1000000\nx.corr(y,\"spearman\")\n\n0.9849624060150375\n\n\n\nSince real data almost always features outlying or anomalous values, it‚Äôs important to think about the robustness of the statistics you choose.\n\n\n2.5.5 Categorical correlation\nAn ordinal variable, such as the days of the week, is often straightforward to quantify as integers. But a nominal variable poses a different challenge.\n\nExample 2.17 Grouped histograms suggest an association between country of origin and MPG:\n\nsns.displot(data=cars, kind=\"kde\",\n    x=\"mpg\", hue=\"origin\");\n\n\n\n\nHow can we quantify the association? The first step is to convert the origin column into dummy variables:\n\ndum = pd.get_dummies(cars, columns=[\"origin\"])\ndum.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\nname\nmpg_z\ncuts\ngroup_z\nglobal_z\norigin_europe\norigin_japan\norigin_usa\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nchevrolet chevelle malibu\n-0.705551\n(3500, 4500]\n0.837535\n-0.705551\n0\n0\n1\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nbuick skylark 320\n-1.089379\n(3500, 4500]\n-0.249252\n-1.089379\n0\n0\n1\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nplymouth satellite\n-0.705551\n(2500, 3500]\n-0.956484\n-0.705551\n0\n0\n1\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\namc rebel sst\n-0.961437\n(2500, 3500]\n-1.377177\n-0.961437\n0\n0\n1\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nford torino\n-0.833494\n(2500, 3500]\n-1.166831\n-0.833494\n0\n0\n1\n\n\n\n\n\n\n\nThe original origin column has been replaced by three binary indicator columns. Now we can look for correlations between them and mpg:\n\ncolumns = [\n    \"mpg\",\n    \"origin_europe\",\n    \"origin_japan\",\n    \"origin_usa\"\n    ]\ndum[columns].corr()\n\n\n\n\n\n\n\n\nmpg\norigin_europe\norigin_japan\norigin_usa\n\n\n\n\nmpg\n1.000000\n0.259022\n0.442174\n-0.568192\n\n\norigin_europe\n0.259022\n1.000000\n-0.229895\n-0.597198\n\n\norigin_japan\n0.442174\n-0.229895\n1.000000\n-0.643317\n\n\norigin_usa\n-0.568192\n-0.597198\n-0.643317\n1.000000\n\n\n\n\n\n\n\nAs you can see from the above, europe and japan are positively associated with mpg, while usa is inversely associated with mpg."
  },
  {
    "objectID": "stats.html#cautionary-tales",
    "href": "stats.html#cautionary-tales",
    "title": "2¬† Descriptive statistics",
    "section": "2.6 Cautionary tales",
    "text": "2.6 Cautionary tales\nAttaching theorem-supported numbers to real data feels precise and infallible. The theorems do what they say, of course‚Äîthey‚Äôre theorems‚Äîbut our intuition can be a little too ready to attach significance to the numbers, causing misconceptions or mistakes. Proper visualizations can help us see through such issues.\n\n2.6.1 The datasaurus\nThe Datasaurus Dozen is a collection of datasets that highlights the perils of putting blind trust into summary statistics. The Datasaurus is a set of 142 points making a handsome portrait:\n\ndozen = pd.read_csv(\"_datasets/DatasaurusDozen.tsv\", delimiter=\"\\t\")\nsns.relplot(data=dozen[dozen[\"dataset\"]==\"dino\"], x=\"x\", y=\"y\");\n\n\n\n\nHowever, there are 12 other datasets that all have roughly the same mean and variance for \\(x\\) and \\(y\\), and the same correlations between them:\n\nby_set = dozen.groupby(\"dataset\")\nby_set.mean()\n\n\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\naway\n54.266100\n47.834721\n\n\nbullseye\n54.268730\n47.830823\n\n\ncircle\n54.267320\n47.837717\n\n\ndino\n54.263273\n47.832253\n\n\ndots\n54.260303\n47.839829\n\n\nh_lines\n54.261442\n47.830252\n\n\nhigh_lines\n54.268805\n47.835450\n\n\nslant_down\n54.267849\n47.835896\n\n\nslant_up\n54.265882\n47.831496\n\n\nstar\n54.267341\n47.839545\n\n\nv_lines\n54.269927\n47.836988\n\n\nwide_lines\n54.266916\n47.831602\n\n\nx_shape\n54.260150\n47.839717\n\n\n\n\n\n\n\n\nby_set.std()\n\n\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\naway\n16.769825\n26.939743\n\n\nbullseye\n16.769239\n26.935727\n\n\ncircle\n16.760013\n26.930036\n\n\ndino\n16.765142\n26.935403\n\n\ndots\n16.767735\n26.930192\n\n\nh_lines\n16.765898\n26.939876\n\n\nhigh_lines\n16.766704\n26.939998\n\n\nslant_down\n16.766759\n26.936105\n\n\nslant_up\n16.768853\n26.938608\n\n\nstar\n16.768959\n26.930275\n\n\nv_lines\n16.769959\n26.937684\n\n\nwide_lines\n16.770000\n26.937902\n\n\nx_shape\n16.769958\n26.930002\n\n\n\n\n\n\n\n\nby_set.corr()\n\n\n\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\n\naway\nx\n1.000000\n-0.064128\n\n\ny\n-0.064128\n1.000000\n\n\nbullseye\nx\n1.000000\n-0.068586\n\n\ny\n-0.068586\n1.000000\n\n\ncircle\nx\n1.000000\n-0.068343\n\n\ny\n-0.068343\n1.000000\n\n\ndino\nx\n1.000000\n-0.064472\n\n\ny\n-0.064472\n1.000000\n\n\ndots\nx\n1.000000\n-0.060341\n\n\ny\n-0.060341\n1.000000\n\n\nh_lines\nx\n1.000000\n-0.061715\n\n\ny\n-0.061715\n1.000000\n\n\nhigh_lines\nx\n1.000000\n-0.068504\n\n\ny\n-0.068504\n1.000000\n\n\nslant_down\nx\n1.000000\n-0.068980\n\n\ny\n-0.068980\n1.000000\n\n\nslant_up\nx\n1.000000\n-0.068609\n\n\ny\n-0.068609\n1.000000\n\n\nstar\nx\n1.000000\n-0.062961\n\n\ny\n-0.062961\n1.000000\n\n\nv_lines\nx\n1.000000\n-0.069446\n\n\ny\n-0.069446\n1.000000\n\n\nwide_lines\nx\n1.000000\n-0.066575\n\n\ny\n-0.066575\n1.000000\n\n\nx_shape\nx\n1.000000\n-0.065583\n\n\ny\n-0.065583\n1.000000\n\n\n\n\n\n\n\nHowever, a plot reveals that these sets are, to put it mildly, quite distinct:\n\nothers = dozen[ dozen[\"dataset\"] != \"dino\" ]\nsns.relplot(data=others,\n    x=\"x\", y=\"y\",\n    col=\"dataset\", col_wrap=3, height=2.2\n    );\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways plot your data.\n\n\nAlways. Plot. Your. Data!\n\n\n2.6.2 Simpson‚Äôs paradox\nThe penguin dataset contains a common paradox (well, a counterintuitive phenomenon, anyway). Two of the variables show a fairly strong negative correlation:\n\npenguins = sns.load_dataset(\"penguins\")\ncolumns = [ \"body_mass_g\", \"bill_depth_mm\" ]\npenguins[columns].corr()\n\n\n\n\n\n\n\n\nbody_mass_g\nbill_depth_mm\n\n\n\n\nbody_mass_g\n1.000000\n-0.471916\n\n\nbill_depth_mm\n-0.471916\n1.000000\n\n\n\n\n\n\n\nBut something surprising happens if we compute correlations after grouping by species.\n\npenguins.groupby(\"species\")[columns].corr()\n\n\n\n\n\n\n\n\n\nbody_mass_g\nbill_depth_mm\n\n\nspecies\n\n\n\n\n\n\n\nAdelie\nbody_mass_g\n1.000000\n0.576138\n\n\nbill_depth_mm\n0.576138\n1.000000\n\n\nChinstrap\nbody_mass_g\n1.000000\n0.604498\n\n\nbill_depth_mm\n0.604498\n1.000000\n\n\nGentoo\nbody_mass_g\n1.000000\n0.719085\n\n\nbill_depth_mm\n0.719085\n1.000000\n\n\n\n\n\n\n\nWithin each individual species, the correlation between the variables is strongly positive!\nThis is an example of Simpson‚Äôs paradox. The reason for it can be seen from a scatter plot:\n\nsns.relplot(data=penguins,\n    x=columns[0], y=columns[1], \n    hue=\"species\"\n    );\n\n\n\n\nWithin each color, the positive association is clear. But what dominates the combination of all three species is the large difference between Gentoo and the others. Because the Gentoo are both larger and have shallower bills, the dominant relationship is negative.\nAs often happens in statistics, the precise framing of the question can strongly affect its answer. This can lead to honest mistakes by the naive as well as outright deception by the unscrupulous."
  },
  {
    "objectID": "stats.html#exercises",
    "href": "stats.html#exercises",
    "title": "2¬† Descriptive statistics",
    "section": "Exercises",
    "text": "Exercises\nFor these exercises, you may of course use computer help to work on a problem, but your answer should be self-contained without reference to computer output (unless stated otherwise).\n\nExercise 2.1 Suppose that \\(n>2\\), and let \\(x_i=0\\) for \\(i=1,\\ldots,n-1\\), and \\(x_n=M\\) for a positive number \\(M\\).\n\nFind the sample mean.\nFind the sample median. (You will have to describe multiple cases.)\nFind the corrected sample variance \\(s_{n-1}^2\\).\nFind the sample z-scores of the \\(x_i\\) in terms of \\(M\\) and \\(n\\).\n\n\n\nExercise 2.2 Suppose the samples \\(x_1,\\ldots,x_n\\) have the z-scores \\(z_1,\\ldots,z_n\\).\n(a) Show that \\(\\displaystyle \\sum_{i=1}^n z_i = 0.\\)\n(b) Show that \\(\\displaystyle \\sum_{i=1}^n z_i^2 = n-1.\\)\n\n\nExercise 2.3 For the sample set in Exercise¬†2.1, find a value \\(N\\) such that if \\(n>N\\), there is at least one outlier according to the 2œÉ criterion.\n\n\nExercise 2.4 Define a population by\n\\[\nx_i = \\begin{cases}\n1, & 1 \\le i \\le 11, \\\\\n2, & 12 \\le i \\le 14,\\\\\n4, & 15 \\le i \\le 22, \\\\\n6, & 23 \\le i \\le 32.\n\\end{cases}\n\\]\n(a) Find the median of the population.\n(b) Which of the following values are outliers according to the 1.5 IQR criterion?\n\\[\n-10,-5,0,5,10,15,20\n\\]\n\n\nExercise 2.5 Suppose that a population has values \\(x_1,x_2,\\ldots,x_n\\). Define the function\n\\[\nr_2(x) = \\sum_{i=1}^n (x_i-x)^2.\n\\]\nShow using calculus that \\(r_2\\) is minimized at \\(x=\\mu\\), the population mean.\n\n\nExercise 2.6 Suppose that \\(n=2k+1\\) and a population has values \\(x_1,x_2,\\ldots,x_{n}\\) in sorted order, so that the median is equal to \\(x_k\\). Define the function\n\\[\nr_1(x) = \\sum_{i=1}^n |x_i - x|.\n\\]\n(This function is called the total absolute deviation of \\(x\\) from the population.) Show that \\(r_1\\) has a global minimum at \\(x=x_k\\) by way of the following steps.\n(a) Explain why the derivative of \\(r_1\\) is undefined at every \\(x_i\\). Consequently, all of the \\(x_i\\) are critical points of \\(r_1\\).\n(b) Determine \\(r_1'\\) within each interval \\((-\\infty,x_1),\\, (x_1,x_2),\\, (x_2,x_3),\\) and so on. Explain why this shows that there cannot be any additional critical points to consider. (Note: you can replace the absolute values with a piecewise definition of \\(r_1\\), where the formula for the pieces changes as you cross over each \\(x_i\\).)\n(c) By considering the \\(r_1'\\) values between the \\(x_i\\), explain why it must be that\n\\[\nr_1(x_1) > r_1(x_2) > \\cdots > r_1(x_k) < r_1(x_{k+1}) < \\cdots < r_1(x_n).\n\\]\n\n\nExercise 2.7 Prove that two sample sets have a Pearson correlation coefficient equal to 1 if they have identical z-scores. (Hint: See Exercise¬†2.2.)\n\n\nExercise 2.8 Suppose that two sample sets satisfy \\(y_i=-x_i\\) for all \\(i\\). Prove that the Pearson correlation coefficient between \\(x\\) and \\(y\\) equals \\(-1\\)."
  },
  {
    "objectID": "classification.html#classification-basics",
    "href": "classification.html#classification-basics",
    "title": "3¬† Classification",
    "section": "3.1 Classification basics",
    "text": "3.1 Classification basics\nA single training example or sample is characterized by a feature vector \\(\\bfx\\) of \\(d\\) real numbers and a label \\(y\\) drawn from a finite set \\(L\\). If \\(L\\) has only two members (say, ‚Äútrue‚Äù and ‚Äúfalse‚Äù), we have a binary classification problem; otherwise, we have a multiclass problem.\nWhen we have \\(n\\) training samples, it‚Äôs natural to collect them into columns of a feature matrix \\(\\bfX\\) with \\(n\\) rows and \\(d\\) columns. Using subscripts to represent the indexes of the matrix, we can write\n\\[\n\\bfX = \\begin{bmatrix}\nX_{11} & X_{12} & \\cdots & X_{1d} \\\\\nX_{21} & X_{22} & \\cdots & X_{2d} \\\\\n\\vdots & \\vdots && \\vdots \\\\\nX_{n1} & X_{n2} & \\cdots & X_{nd}\n\\end{bmatrix}.\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nA 2D array or matrix has elements that are addressed by two subscripts. These are always given in order row, column.\nIn math, we usually start the row and column indexes at 1, but Python starts them at 0.\n\n\nEach row of the feature matrix is a single feature vector, while each column is the value for a single feature over the entire training set.\n\nExample 3.1 Suppose we want to train an algorithm to predict whether a basketball shot will score. For one shot, we might collect three coordinates to represent the launch point, three to represent the launch velocity, and three to represent the initial angular rotation (axis and magnitude). Thus each shot will require a feature vector of length 9. A collection of 200 sample shots would be encoded as a \\(200\\times 9\\) feature matrix.\n\nWe can also collect the associated training labels into the label vector\n\\[\n\\bfy = \\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nIn linear algebra, the default shape for a vector is usually as a single column. In Python, a vector doesn‚Äôt exactly have a row or column orientation, though when it matters, a row shape is usually preferred.\n\n\nEach component \\(y_i\\) of the label vector is drawn from the label set \\(L\\).\n\n\n3.1.1 Encoding qualitative data\nWe have defined the features as numerical values. What should we do with qualitative data? We could arbitrarily assign numbers to possible values, such as ‚Äú0=red‚Äù, ‚Äú1=blue‚Äù, ‚Äú2=yellow,‚Äù and so on. But this is not ideal: most of the time, we would not want to say that yellow is twice as far from red as it is from blue!\nA better strategy is to use the one-hot or dummy encoding. If a particular feature can take on \\(k\\) unique values, then we introduce \\(k\\) new features indicating which value is present. (We can use \\(k-1\\) dummy features if we interpret all-zeros to mean the \\(k\\)th possibility.)\n\n\n3.1.2 Walkthrough\nThe scikit-learn package sklearn is a collection of well-known machine learning algorithms and tools. It includes a few classic example datasets. We will load one derived from automatic recognition of handwritten digits.\n\nfrom sklearn import datasets \nds = datasets.load_digits()        # loads a well-known dataset\nX, dig = ds[\"data\"], ds[\"target\"]      # assign feature matrix and label vector\nprint(\"The feature matrix has shape\", X.shape)\nprint(\"The label vector has shape\", dig.shape)\nn, d = X.shape\nprint(\"there are\", d, \"features and\", n, \"samples\")\n\nThe feature matrix has shape (1797, 64)\nThe label vector has shape (1797,)\nthere are 64 features and 1797 samples\n\n\nIt happens that the 64 features are the pixel grayscale values from an \\(8\\times 8\\) bitmap of a handwritten digit. The labels are the integer values 0 through 9, indicating the true value of the digit.\nLet‚Äôs consider the binary problem, ‚Äúis this digit a 6?‚Äù That implies the following label vector:\n\ny = (dig == 6)\nprint(\"Number of sixes in training set:\", sum(y))\n\nNumber of sixes in training set: 181\n\n\nThe process of training a classifier is called fitting. We first have to import a particular classifier type, then create an instance of that type. Here, we choose one that we will study in a future section:\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=20)    # specification of the model\n\nNow we can fit the learner to the training data:\n\nknn.fit(X, y)                                 # training of the model\n\nKNeighborsClassifier(n_neighbors=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=20)\n\n\nAt this point, the classifier object knn has figured out what it needs from the training data. It has methods we can now call to make predictions and evaluate the quality of the results.\nEach new prediction is for a query vector with 64 features. In practice, we can use a list in place of a vector for the query.\n\nquery = [20]*d    # list with d copies of 20  \n\nThe predict method of the classifier allows specifying multiple query vectors as rows of an array. In fact, it expects a 2D array in all cases, even if there is just one row.\n\nXq = [ query ]    # 2D array with a single row\n\nThe result of the prediction will be a vector of labels, one per row of the query.\n\n# Get vector of predictions:\nknn.predict(Xq)  \n\narray([False])\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe predict method requires a vector or list of query vectors or lists and it outputs a vector of classes. This is true even if there is just a single query.\n\n\nAt the moment, we don‚Äôt have any realistic query data at hand other than the training data. But we can investigate the question of how well the classifier does on that data, simply by using the feature matrix as the query:\n\n# Get vector of predictions for the training set:\nyhat = knn.predict(X)    \n\nNow we simply count up the number of correctly predicted labels and divide by the total number of samples to get the accuracy of the classifier.\n\nacc = sum(yhat == y) / n    # fraction of correct predictions\nprint(f\"accuracy is {acc:.1%}\")\n\naccuracy is 99.9%\n\n\nNot surprisingly, sklearn has functions for doing this measurement in fewer steps. The metrics module has functions that can compare true labels with predictions. In addition, each classifier object has a score method that allows you to skip finding the predictions vector yourself.\n\nfrom sklearn.metrics import accuracy_score\n\n# Compare original labels to predictions:\nacc = accuracy_score(y, yhat)    \nprint(f\"accuracy score is {acc:.1%}\")\n\n# Same result, if we don't want to keep the predicted values around:\nacc = knn.score(X, y)    \nprint(f\"knn score is {acc:.1%}\")\n\naccuracy score is 99.9%\nknn score is 99.9%\n\n\nDoes this mean that the classifier is a good one? The raw number looks great, but that question is more subtle than you would expect."
  },
  {
    "objectID": "classification.html#classifier-performance",
    "href": "classification.html#classifier-performance",
    "title": "3¬† Classification",
    "section": "3.2 Classifier performance",
    "text": "3.2 Classifier performance\nLet‚Äôs return to the (previously cleaned) loan applications dataset.\n\nimport pandas as pd\nloans = pd.read_csv(\"_datasets/loan_clean.csv\")\nloans.head()\n\n\n\n\n\n\n\n\nloan_amnt\nint_rate\ninstallment\nannual_inc\ndti\ndelinq_2yrs\ndelinq_amnt\npercent_funded\n\n\n\n\n0\n5000\n10.65\n162.87\n24000.0\n27.65\n0\n0\n100.0\n\n\n1\n2500\n15.27\n59.83\n30000.0\n1.00\n0\n0\n100.0\n\n\n2\n2400\n15.96\n84.33\n12252.0\n8.72\n0\n0\n100.0\n\n\n3\n10000\n13.49\n339.31\n49200.0\n20.00\n0\n0\n100.0\n\n\n4\n3000\n12.69\n67.79\n80000.0\n17.94\n0\n0\n100.0\n\n\n\n\n\n\n\nWe create a binary classification problem by labelling whether each loan was at least 95% funded. The other columns will form the features for the predictions.\n\nX = loans.drop(\"percent_funded\", axis=1)\ny = loans[\"percent_funded\"] > 95\n\n\n\n\n\n\n\nTip\n\n\n\nScikit-learn works as seamlessly with pandas data frames as it does with numerical arrays. In an application, it‚Äôs often worthwhile to refer to quantities by memorable names, rather than relying on numerical indexes into a matrix.\n\n\n\n3.2.1 Train‚Äìtest paradigm\nIt seems desirable for a classifier to work well on the samples it was trained on. But we probably want to do more than that.\n\nDefinition 3.1 The performance of a predictor on previously unseen data is known as the generalization of the predictor.\n\nIn order to gauge generalization, we hold back some of the labeled data from training and use it only to test the performance. An sklearn helper function called train_test_split allows us to split off 20% of the data to use for testing. It‚Äôs usually recommended to shuffle the order of the samples before the split, and in order to make the results reproducible, we give a specific random seed to the RNG used for the shuffle.\n\nfrom sklearn.model_selection import train_test_split\n\nX_tr, X_te, y_tr, y_te = train_test_split(X, y,\n  test_size=0.2,\n  shuffle=True,\n  random_state=3\n)\n\nprint(\"There are\", X_tr.shape[0], \"training samples.\")\nprint(\"There are\", X_te.shape[0], \"testing samples.\")\n\nThere are 31773 training samples.\nThere are 7944 testing samples.\n\n\nWe can check that the test and train labels have similar characteristics:\n\nimport pandas as pd\nprint(\"labels in the training set:\")\nprint( pd.Series(y_tr).describe() )\n\nprint(\"\\nlabels in the testing set:\")\nprint( pd.Series(y_te).describe() )\n\nlabels in the training set:\ncount     31773\nunique        2\ntop        True\nfreq      30351\nName: percent_funded, dtype: object\n\nlabels in the testing set:\ncount     7944\nunique       2\ntop       True\nfreq      7575\nName: percent_funded, dtype: object\n\n\nNow we train on the training data‚Ä¶\n\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_tr, y_tr)    # fit only to train set\n\nKNeighborsClassifier(n_neighbors=9)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=9)\n\n\n‚Ä¶and test on the rest.\n\nacc = knn.score(X_te, y_te)    # score only on test set\nprint(f\"test accuracy is {acc:.1%}\")\n\ntest accuracy is 95.6%\n\n\nThis seems like a high accuracy, perhaps. But consider that the vast majority of loans were funded:\n\nfunded = sum(y)\nprint(f\"{funded/len(y):.1%} were funded\")\n\n95.5% were funded\n\n\nTherefore, an algorithm that simply ‚Äúpredicts‚Äù funding every single loan could do as well as the trained classifier!\n\nfrom sklearn import metrics\ngenerous = [True]*len(y_te)\nacc = metrics.accuracy_score(y_te, generous)\nprint(f\"fund-them-all accuracy is {acc:.1%}\")\n\nfund-them-all accuracy is 95.4%\n\n\nIn this context, our trained classifier is not impressive at all. We need a metric other than accuracy.\n\n\n3.2.2 Binary classifiers\nA binary classifier is one that produces just two unique labels, which we call ‚Äúyes‚Äù and ‚Äúno‚Äù here. To fully understand the performance of a binary classifier, we have to account for four cases:\n\nTrue positives (TP): Predicts ‚Äúyes‚Äù, actually is ‚Äúyes‚Äù\nFalse positives (FP): Predicts ‚Äúyes‚Äù, actually is ‚Äúno‚Äù\nTrue negatives (TN): Predicts ‚Äúno‚Äù, actually is ‚Äúno‚Äù\nFalse negatives (FN): Predicts ‚Äúno‚Äù, actually is ‚Äúyes‚Äù\n\nThe four cases correspond to a 2√ó2 table according to the states of the prediction and ground truth, which is the accepted correct value (i.e., the given label). The table can be filled with counts or percentages of tested instances, to create a confusion matrix, as illustrated in Figure¬†3.1.\n\n\n\nFigure¬†3.1: Confusion matrix\n\n\nsklearn makes it straightforward to compute a confusion matrix:\n\nyhat = knn.predict(X_te)\nC = metrics.confusion_matrix(y_te, yhat, labels=[True,False])\nlbl = [\"fund\", \"reject\"]\nmetrics.ConfusionMatrixDisplay(C, display_labels=lbl).plot();\n\n\n\n\n\n\n\n\n\n\nDanger\n\n\n\nIt‚Äôs advisable to call confusion_matrix with the labels argument, even though it is optional, in order to have control over the ordering within the matrix. In particular, False < True, so the default is to count the upper left corner of the matrix as ‚Äútrue negatives,‚Äù assuming that False represents a negative result.\n\n\nHence, there are 7570 true positives. Therefore, the accuracy is\n\\[\n\\text{accuracy} = \\frac{\\TP + \\TN}{\\TP + \\FP + \\TN + \\FN} = \\frac{7570}{7944} \\approx 0.95292\n\\]\ni.e., 95.3%. However, there are four other useful quantities defined by putting a ‚Äúnumber correct‚Äù value in the numerator and the sum of a confusion matrix row or column in the denominator:\n\\[\n\\begin{split}\n\\text{recall (aka sensitivity)} &= \\frac{\\TP}{\\TP + \\FN} \\\\[2mm]\n\\text{specificity} &= \\frac{\\TN}{\\TN + \\FP} \\\\[2mm]\n\\text{precision} &= \\frac{\\TP}{\\TP + \\FP} \\\\[2mm]\n\\text{negative predictive value (NPV)} &= \\frac{\\TN}{\\TN + \\FN}\n\\end{split}\n\\]\nIn words, these metrics answer the following questions:\n\nrecall How often are actual ‚Äúyes‚Äù cases predicted correctly?\nspecificity How often are actual ‚Äúno‚Äù cases predicted correctly?\nprecision How often are the ‚Äúyes‚Äù predictions correct?\nNPV How often are the ‚Äúno‚Äù predictions correct?\n\nFor our loan classifier, here are the scores:\n\nTP,FN,FP,TN = C.ravel()    # grab the 4 values in the confusion matrix\nprint(f\"recall = {TP/(TP+FN):.1%}\")\nprint(f\"specificity = {TN/(TN+FP):.1%}\")\nprint(f\"precision = {TP/(TP+FP):.1%}\")\nprint(f\"NPV = {TN/(TN+FN):.1%}\")\n\nrecall = 99.9%\nspecificity = 7.0%\nprecision = 95.7%\nNPV = 83.9%\n\n\nThe high recall rate means that few who ought to get a loan will go away disappointed. However, the low specificity would be concerning to those providing the funds, because almost all of those who should be rejected will be approved by the classifier.\nIn sklearn.metrics there are functions to compute recall and precision without reference to the confusion matrix. You must put the ground-truth labels before the predicted labels, and you should also specify which of the labels corresponds to a ‚Äúpositive‚Äù result. Swapping the ‚Äúpositive‚Äù role effectively swaps recall with specificity, and precision with NPV.\n\nfor pos in [True,False]:\n  print(\"With\", pos, \"as positive:\")\n  s = metrics.recall_score(y_te, yhat, pos_label=pos)\n  print(f\"    recall is {s:.3f}\")\n  s = metrics.precision_score(y_te, yhat, pos_label=pos) \n  print(f\"    precision is {s:.3f}\")\n  print()\n\nWith True as positive:\n\n\n    recall is 0.999\n    precision is 0.957\n\nWith False as positive:\n\n\n    recall is 0.070\n    precision is 0.839\n\n\n\nThere are several ways to combine the measures above into a single value. None is universally best, because different applications emphasize different aspects of performance. One of the most popular is the F‚ÇÅ score, which is the harmonic mean of the precision and the recall:\n\\[\nF_1 = \\left[ \\frac{1}{2} \\left(\\frac{\\TP + \\FN}{\\TP} + \\frac{\\TP+\\FP}{\\TP} \\right)  \\right]^{-1} = \\frac{2\\TP}{2\\TP+\\FN+\\FP}.\n\\]\nThis score varies between 0 (poor) and 1 (ideal). If one of the quantities is much smaller than the other, their harmonic mean will be close to the small value. Thus, F‚ÇÅ score punishes a classifier if either recall or precision is poor.\nAnother composite score is balanced accuracy, which is the arithmetic mean of recall and specificity. It also ranges from 0 to 1, with 1 meaning perfect accuracy.\n\nprint( \"F1:\", metrics.f1_score(y_te, yhat) )\nprint( \"Balanced:\", metrics.balanced_accuracy_score(y_te, yhat) )\n\nF1: 0.9775309917355371\nBalanced: 0.5349003193002227\n\n\nThe loan classifier trained above has excellent recall, respectable precision, and terrible specificity, resulting in a good F‚ÇÅ score and a low balanced accuracy score.\n\nExample 3.2 If \\(k\\) of the \\(n\\) testing samples were funded loans, then the fund-them-all loan classifier has\n\\[\n\\TP = k,\\, \\TN = 0,\\, \\FP = n-k,\\, \\FN = 0.\n\\]\nIts F‚ÇÅ score is thus\n\\[\n\\frac{2\\TP}{2\\TP+\\FN+\\FP} = \\frac{2k}{2k+n-k} = \\frac{2k}{k+n}.\n\\]\nIf the fraction of funded samples in the test set is \\(k/n=a\\), then the accuracy of this classifier is \\(a\\). Its F‚ÇÅ score is \\(2a/(1+a)\\), which is larger than \\(a\\) unless \\(a=1\\). That‚Äôs because the true positives greatly outweigh the other confusion matrix values.\nThe balanced accuracy is\n\\[\n\\frac{1}{2} \\left(\\frac{\\TP}{\\TP+\\FN} + \\frac{\\TN}{\\TN+\\FP} \\right)  = \\frac{1}{2},\n\\]\nindependently of \\(a\\). This quantity is sensitive to the low specificity.\n\n\n\n3.2.3 Multiclass classifiers\nWhen there are more than two unique possible labels, these metrics can be applied using the one-vs-rest paradigm. For \\(K\\) unique labels, this paradigm poses \\(K\\) binary questions: ‚ÄúIs it in class 1, or not?‚Äù, ‚ÄúIs it in class 2, or not?‚Äù, etc. The confusion matrix becomes \\(K\\times K\\).\nIt‚Äôs easiest to see how this works by an example. We will load a dataset on the characteristics of cars and use quantitative factors to predict the region of origin.\n\nimport seaborn as sns\ncars = sns.load_dataset(\"mpg\").dropna()\ncars.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nusa\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nusa\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nusa\nford torino\n\n\n\n\n\n\n\n\nfeatures = [\"cylinders\", \"horsepower\", \"weight\", \"acceleration\", \"mpg\"]\nX = cars[features]\ny = pd.Categorical(cars[\"origin\"])\nprint(X.shape[0], \"samples and\", X.shape[1], \"features\")\n\n392 samples and 5 features\n\n\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n  X, y, \n  test_size=0.2, \n  shuffle=True, \n  random_state=1\n)\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(X_tr, y_tr)\nyhat = knn.predict(X_te)\nprint(f\"accuracy is {metrics.accuracy_score(y_te, yhat):.1%}\")\n\naccuracy is 77.2%\n\n\n\nlabels = y.categories\nC = metrics.confusion_matrix(y_te, yhat, labels=labels)\nmetrics.ConfusionMatrixDisplay(C, display_labels=labels).plot();\n\n\n\n\nFrom the confusion matrix above we can see that, for example, out of 54 predictions of ‚Äúusa‚Äù on the test set, there are 8 total false positives, in the sense that the actual labels were otherwise.\nWe also get \\(K\\) versions of the metrics like accuracy, recall, F‚ÇÅ score, and so on. We can get all the individual precision scores, say, automatically:\n\nprec = metrics.precision_score(y_te, yhat, average=None)\nfor (i,p) in enumerate(prec): \n    print(f\"{labels[i]}: {p:.1%}\")\n\neurope: 62.5%\njapan: 58.8%\nusa: 85.2%\n\n\nTo get a composite precision score, we have to specify an averaging method. The \"macro\" option simply takes the mean of the vector above.\n\nmac = metrics.precision_score(y_te, yhat, average=\"macro\")\nprint(mac)\n\n0.6883623819898329\n\n\nThere are other ways to perform the averaging, depending on whether poorly represented classes should be weighted more weakly than others."
  },
  {
    "objectID": "classification.html#decision-trees",
    "href": "classification.html#decision-trees",
    "title": "3¬† Classification",
    "section": "3.3 Decision trees",
    "text": "3.3 Decision trees\nA decision tree is much like playing ‚ÄúTwenty Questions.‚Äù A question is asked, and the answer reduces the possible results, leading to a new question. CART (Classification And Regression Tree) is a popular method for systematizing the idea.\nGiven feature vectors \\(\\bfx_1,\\ldots,\\bfx_n\\) with labels \\(y_1,\\ldots,y_n\\), the immediate goal is to partition the samples into subsets whose labels are as uniform as possible. The process is then repeated recursively on the subsets. Defining a measurement of label uniformity is a key step.\n\n3.3.1 Gini impurity\nLet \\(S\\) be a subset of the samples, given as a list of indices into the original set. Suppose there are \\(K\\) unique labels, which we denote \\(1,2,\\ldots,K\\). Define\n\\[\np_k = \\frac{1}{ |S| } \\sum_{i\\in S} \\mathbb{1}_k(y_i),\n\\]\nwhere \\(|S|\\) is the number of elements in \\(S\\) and \\(\\mathbb{1}_k\\) is the indicator function\n\\[\n\\mathbb{1}_k(t) = \\begin{cases}\n  1, & \\text{if } t=k, \\\\\n  0, & \\text{otherwise.}\n  \\end{cases}\n\\]\nIn words, \\(p_k\\) is the proportion of samples in \\(S\\) that have label \\(k\\). Then the Gini impurity is defined as\n\\[\nH(S) = \\sum_{k=1}^K p_k(1-p_k).\n\\]\nIf one of the \\(p_k\\) is 1, then the others are all zero and \\(H(S)=0\\). This is considered optimal. At the other extreme, if \\(p_k=1/K\\) for all \\(k\\), then\n\\[\nH(S) = \\sum_{k=1}^K \\frac{1}{K} \\left(1 - \\frac{1}{K} \\right) = K\\cdot \\frac{1}{K}\\cdot\\frac{K-1}{K} = \\frac{K-1}{K} < 1.\n\\]\n\nExample 3.3 Suppose a set \\(S\\) has \\(n\\) members with label 1, 1 member with label 2, and 1 member with label 3. What is the Gini impurity of \\(S\\)?\n\nSolution. We have \\(p_1=n/(n+2)\\), \\(p_2=p_3=1/(n+2)\\). Hence\n\\[\n\\begin{split}\n    H(S) &= \\frac{n}{n+2}\\left( 1 - \\frac{n}{n+2} \\right) + 2 \\frac{1}{n+2}\\left( 1 - \\frac{1}{n+2} \\right) \\\\\n    &= \\frac{n}{n+2}\\frac{2}{n+2} + \\frac{2}{n+2}\\frac{n+1}{n+2} \\\\\n    &= \\frac{4n+2}{(n+2)^2}.\n\\end{split}\n\\]\nThis value is 1/2 for \\(n=0\\) and approaches zero as \\(n\\to\\infty\\).\n\n\n\n\n3.3.2 Partitioning\nNow we can describe the partition process. If \\(j\\) is a dimension (feature) number and \\(\\theta\\) is a numerical threshold, then the sample set can be partitioned into complementary sets \\(S_L\\), in which \\(x_j \\le \\theta\\), and \\(S_R\\), in which \\(x_j > \\theta\\). Define the total impurity of the partition to be\n\\[\nQ(j,\\theta) = \\lvert S\\rvert\\, H(S) + \\lvert T \\rvert \\, H(T).\n\\]\nChoose the \\((j,\\theta)\\) that minimize \\(Q\\), and then recursively partition \\(S\\) and \\(T\\).\n\nExample 3.4 Suppose the 1D real samples \\(x_i=i\\) for \\(i=0,1,2,3\\) have labels A,B,A,B. What is the optimal partition?\n\nSolution. There are three ways to partition them.\n\n\\(S=\\{0\\}\\), \\(T=\\{1,2,3\\}\\). We have \\(H(S)=0\\) and \\(H(T)=(2/3)(1/3)+(1/3)(2/3)=4/9\\). Hence the total impurity for this partition is \\((1)(0) + (3)(4/9) = 4/3\\).\n\\(S=\\{0,1\\}\\), \\(T=\\{2,3\\}\\). Then \\(H(S)=H(T)=2(1/2)(1/2)=1/2\\), and the total impurity is \\((2)(1/2)+(2)(1/2)=2\\).\n\\(S=\\{0,1,2\\}\\), \\(T=\\{3\\}\\). This arrangement is the same as the first case with \\(A‚ÜîB\\).\n\nThe best partition threshold is \\(x\\le 0\\) (or \\(x\\le 2\\), which is equivalent).\n\n\n\n\n3.3.3 Toy example\nWe first create a toy dataset with 20 random points, with two subsets of 10 that are shifted left/right a bit.\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom numpy.random import default_rng\n\nrng = default_rng(1)\nx1 = rng.random((10,2))\nx1[:,0] -= 0.25\nx2 = rng.random((10,2))\nx2[:,0] += 0.25\nX = np.vstack((x1,x2))\ny = np.hstack(([1]*10,[2]*10))\n\nimport seaborn as sns\ndf = pd.DataFrame( {\"x‚ÇÅ\":X[:,0], \"x‚ÇÇ\":X[:,1], \"y\":y} )\nsns.scatterplot(data=df, x=\"x‚ÇÅ\", y=\"x‚ÇÇ\", hue=\"y\");\n\n\n\n\n\nNow we create a decision tree for these samples.\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nt = DecisionTreeClassifier(max_depth=3)\nt.fit(X,y)\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(18,11), dpi=160)\nplot_tree(t, feature_names=[\"x‚ÇÅ\", \"x‚ÇÇ\"]);\n\n\n\n\nThe root of the tree (at the top) shows that the best split was found at the vertical line \\(x_1=0.644\\). To the right of that line is a Gini value of zero: 8 samples, all with label 2. Thus, any future prediction by this tree will immediately return label 2 if the first feature of the input exceeds 0.644. Otherwise, it moves to the left child node and tests whether the second feature is greater than \\(0.96\\). This splits along a horizontal line, above which there is a single sample with label 2. And so on.\nNotice that the bottom right node has a nonzero Gini impurity. This node could be partitioned, but the classifier was constrained to stop at a depth of 3. If a prediction ends up here, then the classifier returns label 1, which is the most likely outcome.\nBecause we can follow the decision tree‚Äôs logic step by step, we say it is highly interpretable. The transparency of the prediction algorithm is an attractive aspect of decision trees, although this advantage can weaken as the numbers of features and observations increase.\n\n\n3.3.4 Penguin data\nWe return to the penguins. There is no need to standardize the columns for a decision tree, because each feature is considered on its own.\n\nimport pandas as pd\npen = sns.load_dataset(\"penguins\")\npen = pen.dropna()\nfeatures = [\n  \"bill_length_mm\",\n  \"bill_depth_mm\",\n  \"flipper_length_mm\",\n  \"body_mass_g\"\n]\nX = pen[features]\ny = pen[\"species\"]\n\nWe get some interesting information from looking at the top levels of a decision tree trained on the full dataset.\n\ndt = DecisionTreeClassifier(max_depth=4)\ndt.fit(X, y)\n\nfrom matplotlib.pyplot import figure\nfigure(figsize=(18,11), dpi=160)\nplot_tree(dt, max_depth=2, feature_names=features);\n\n\n\n\nThe most determinative feature for identifying the species is the flipper length. If it exceeds 206.5 mm, then the penguin is rather likely to be a Gentoo.\nWe can measure the relative importance of each feature by comparing their total contributions to reducing the Gini index. This is known as Gini importance.\n\npd.Series(dt.feature_importances_, index=features)\n\nbill_length_mm       0.367310\nbill_depth_mm        0.089690\nflipper_length_mm    0.529362\nbody_mass_g          0.013638\ndtype: float64\n\n\nFlipper length alone accounts for about half of the resolving power of the tree, followed in importance by the bill length. The other measurements apparently have little discriminative value.\nIn order to assess the effectiveness of the tree, we use the train‚Äìtest paradigm.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n  X, y,\n  test_size=0.2,\n  shuffle=True,\n  random_state=0\n)\ndt.fit(X_tr, y_tr)\n\nyhat = dt.predict(X_te)\nprint( confusion_matrix(y_te, yhat) )\nprint( classification_report(y_te, yhat) )\n\n[[39  0  0]\n [ 2  8  0]\n [ 0  0 18]]\n              precision    recall  f1-score   support\n\n      Adelie       0.95      1.00      0.97        39\n   Chinstrap       1.00      0.80      0.89        10\n      Gentoo       1.00      1.00      1.00        18\n\n    accuracy                           0.97        67\n   macro avg       0.98      0.93      0.95        67\nweighted avg       0.97      0.97      0.97        67\n\n\n\nThe performance is quite good, although the Chinstrap case is hindered by the relatively low number of training examples:\n\ny_tr.value_counts()\n\nAdelie       107\nGentoo       101\nChinstrap     58\nName: species, dtype: int64\n\n\n\n\n3.3.5 Limitations\nDecision trees depend sensitively on the sample locations. A tree trained on one data subset may not do well on a new set. A small change can completely rewrite large parts of the tree, which gives a caveat about interpretation. Also, the partition algorithm, which is greedy by doing the best thing at the moment, does not necessarily find a globally optimal tree, or even a nearby one."
  },
  {
    "objectID": "classification.html#nearest-neighbors",
    "href": "classification.html#nearest-neighbors",
    "title": "3¬† Classification",
    "section": "3.4 Nearest neighbors",
    "text": "3.4 Nearest neighbors\nOur first learning algorithm is conceptually simple: Given a new point to classify, survey the nearest known examples and choose the most frequently occurring class. This is called the \\(k\\) nearest neighbors (KNN) algorithm, where \\(k\\) is the number of neighboring examples to survey.\n\n3.4.1 Norms\nThe existence of ‚Äúclosest‚Äù examples means that we need to define a notion of distance in spaces of any dimension. Let \\(\\real^d\\) be the space of vectors with \\(d\\) real components, and let \\(\\bfzero\\) be the vector of all zeros.\n\nDefinition 3.2 A norm is a function \\(\\norm{\\bfx}\\) on \\(\\real^d\\) that satisfies the following properties:\n\\[\n\\begin{split}\n\\norm{\\bfzero} &= 0,  \\\\\n\\norm{\\bfx} &> 0 \\text{ if $\\bfx$ is a nonzero vector}, \\\\\n\\norm{c\\bfx} &= |c| \\, \\norm{x} \\text{ for any real number $c$}, \\\\\n\\norm{\\bfx + \\bfy } &\\le \\norm{\\bfx} + \\norm{\\bfy}\n\\end{split}\n\\]\n\nThe last inequality above is called the triangle inequality. It turns out that these four characteristics are all we expect from a function that behaves like a distance.\nOn the number line (i.e., \\(\\real^1\\)), the distance between two values is just the absolute value of their difference, \\(\\abs{x-y}\\). In \\(\\real^d\\), the distance between two vectors is the norm of their difference, \\(\\norm{ \\bfx - \\bfy }\\).\nThere are three commonly used norms:\n\nThe 2-norm or Euclidean norm: \\[\n\\twonorm{\\bfx} = \\bigl(x_1^2 + x_2^2 + \\cdots + x_d^2\\bigr)^{1/2}.\n\\]\nThe 1-norm or Manhattan norm: \\[\n\\onenorm{\\bfx} = |x_1| + |x_2| + \\cdots + |x_d|.\n\\]\nThe \\(\\infty\\)-norm or max norm: \\[\\infnorm{\\bfx} = \\max_i \\abs{x_i}.\\]\n\nThe Euclidean norm generalizes ordinary geometric distance in \\(\\real^2\\) and \\(\\real^3\\) and is usually considered the default. One of its most important features is that \\(\\twonorm{\\bfx}^2\\) is a differentiable function of the components of \\(\\bfx\\).\n\n\n\n\n\n\nNote\n\n\n\nWhen \\(\\norm{\\,}\\) is used with no subscript, it‚Äôs usually meant to be the 2-norm, but sometimes it means a generic, unspecified norm.\n\n\n\n\n3.4.2 Algorithm\nAs data, we are given labeled examples \\(\\bfx_1,\\ldots,\\bfx_n\\) in \\(\\real^d\\). Given a new query vector \\(\\bfx\\), find the \\(k\\) labeled vectors closest to \\(\\bfx\\) and choose the most frequently occurring label among them. Ties can be broken randomly.\nKNN divides up the feature space into domains that are dominated by nearby instances. The boundaries between those domains, called decision boundaries, can be fairly complicated, though, as shown in the animation below.\n\nImplementation of KNN is straightforward for small data sets, but requires care to get reasonable execution efficiency for large sets.\n\n\n3.4.3 KNN in sklearn\nLet‚Äôs revisit the penguins. We use dropna to drop any rows with missing values.\n\nimport seaborn as sns\nimport pandas as pd\npenguins = sns.load_dataset(\"penguins\")\npenguins = penguins.dropna()\npenguins.head(6)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nFemale\n\n\n\n\n\n\n\nThe data set has four quantitative columns that we use as features, and the species name is the label.\n\nfeatures = [\n  \"bill_length_mm\",\n  \"bill_depth_mm\",\n  \"flipper_length_mm\",\n  \"body_mass_g\"\n]\nX = penguins[features]\ny = penguins[\"species\"]\n\nEach type of classifier has to be imported before its first use in a session. (Importing more than once does no harm.)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\nWe can manually find the neighbors of a new vector. However, we have to make the query in the form of a data frame, since that is how the training data was provided. Here we make a query frame for values very close to the ones in the first row of the data.\n\nvals = [39, 19, 180, 3750]\nquery = pd.DataFrame([vals], columns=features)\ndist, idx = knn.kneighbors(query)\nidx[0]\n\narray([  0, 143,  53, 100, 153])\n\n\nThe result above indicates that the first sample (index 0) was the closest, followed by four others. We can look up the labels of these points:\n\ny[ idx[0] ]\n\n0         Adelie\n143       Adelie\n53        Adelie\n100       Adelie\n153    Chinstrap\nName: species, dtype: object\n\n\nBy a vote of 4‚Äì1, then, the classifier should choose Adelie as the result at this location.\n\nknn.predict(query)\n\narray(['Adelie'], dtype=object)\n\n\nNote that points can be outvoted by their neighbors. In other words, the classifier won‚Äôt necessarily be correct on every training sample. For example:\n\nprint(\"Predicted:\")\nprint( knn.predict(X.iloc[:5,:]) )\nprint()\nprint(\"Data:\")\nprint( y.iloc[:5].values )\n\nPredicted:\n['Adelie' 'Adelie' 'Chinstrap' 'Adelie' 'Chinstrap']\n\nData:\n['Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie']\n\n\nNext, we split into training and test sets to gauge the performance of the classifier. The classification_report function creates a summary of some of the important metrics.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nX_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2)\nknn.fit(X_tr,y_tr)\n\nyhat = knn.predict(X_te)\nprint(confusion_matrix(y_te,yhat))\nprint(classification_report(y_te,yhat))\n\n[[19  1  1]\n [10  4  1]\n [ 3  0 28]]\n              precision    recall  f1-score   support\n\n      Adelie       0.59      0.90      0.72        21\n   Chinstrap       0.80      0.27      0.40        15\n      Gentoo       0.93      0.90      0.92        31\n\n    accuracy                           0.76        67\n   macro avg       0.78      0.69      0.68        67\nweighted avg       0.80      0.76      0.74        67\n\n\n\nThe default norm in the KNN learner is the 2-norm. To use the 1-norm instead, add metric=\"manhattan\" to the classifier construction call.\n\n\n3.4.4 Standardization\nThe values in the columns of the penguin frame are scaled quite differently. In particular, the values in the body mass column are more than 20x larger than the other columns on average:\n\nX.mean()\n\nbill_length_mm         43.992793\nbill_depth_mm          17.164865\nflipper_length_mm     200.966967\nbody_mass_g          4207.057057\ndtype: float64\n\n\nConsequently, the mass feature will dominate the distance calculations. To remedy this issue, we can transform the data into z-scores:\n\nZ = X.transform( lambda x: (x - x.mean()) / x.std() )\n\nIn this instance, standardization makes performance dramatically better:\n\nZ_tr, Z_te, y_tr, y_te = train_test_split(Z,y,test_size=0.2)\nknn.fit(Z_tr,y_tr)\n\nyhat = knn.predict(Z_te)\nprint(confusion_matrix(y_te,yhat))\nprint(classification_report(y_te,yhat))\n\n[[32  0  0]\n [ 0 11  0]\n [ 0  0 24]]\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        32\n   Chinstrap       1.00      1.00      1.00        11\n      Gentoo       1.00      1.00      1.00        24\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n\n\n\n3.4.5 Pipelines\nOne nuisance of the standardization step above is that it must be performed again for any new query vector that comes along. This means we need to keep track of the mean and std of the original training set.\nScikit-learn allows us to create a pipeline that automates the transformation. Pipelines make it easy to chain together data transformations followed by a learner. The composite object acts much like a regular learner.\nAs you might guess, standardization of data is so common that it is predefined:\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler   # converts to z-scores\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n  X, y, \n  test_size=0.2,\n  shuffle=True,\n  random_state=0\n)\n\nknn = KNeighborsClassifier(n_neighbors=5)\npipe = make_pipeline(StandardScaler(), knn)\n\npipe.fit(X_tr, y_tr)\npipe.score(X_te, y_te)\n\n0.9701492537313433\n\n\nWe can look under the hood of the pipeline. For example, we can see that the mean and variance of each of the original data columns is stored in the first part of the pipeline:\n\nprint( pipe[0].mean_ )\nprint( pipe[0].var_ )\n\n[  44.49774436   17.13496241  201.80827068 4253.28947368]\n[2.86849573e+01 4.06520620e+00 1.88493315e+02 6.27702995e+05]"
  },
  {
    "objectID": "classification.html#sec-class-quant",
    "href": "classification.html#sec-class-quant",
    "title": "3¬† Classification",
    "section": "3.5 Quantifying votes",
    "text": "3.5 Quantifying votes\nBoth kNN and decision trees base classification on a voting procedure‚Äîfor kNN, the \\(k\\) nearest neighbors cast votes, and for a decision tree, the values at a leaf cast votes. So far, we have interpreted the voting results in a winner-takes-all sense, i.e., the class with the most votes wins. But that interpretation discards a lot of potentially valuable information.\n\nDefinition 3.3 Let \\(\\bfx\\) be a query vector in a vote-based classification method. The probability vector \\(\\hat{p}(\\bfx)\\) is the vector of vote fraction received by each class.\n\n\nExample 3.5 Suppose we have trained a kNN classifier with \\(k=10\\) for data with three classes, called A, B, and C, and that the votes at the testing points are as follows:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n9\n0\n1\n\n\n1\n5\n3\n2\n\n\n2\n6\n1\n3\n\n\n3\n2\n0\n8\n\n\n4\n4\n5\n1\n\n\n\n\n\n\n\nThe values of \\(\\hat{p}\\) over the test set form a \\(5\\times 3\\) matrix: \n\np_hat = np.array( [\n    [0.9, 0, 0.1],\n    [0.5, 0.3, 0.2],\n    [0.6, 0.1, 0.3],\n    [0.2, 0, 0.8],\n    [0.4, 0.5, 0.1]\n    ] )\n\n\nIt‚Äôs natural to interpret \\(\\hat{p}\\) as predicting the probability of each label at any query point, since the values are nonnegative and sum to 100%. Given \\(\\hat{p}\\), we can still output a predicted class; it‚Äôs just that the information we get is upstream in the process.\n\nExample 3.6 Consider the penguin species classification problem:\n\npenguins = sns.load_dataset(\"penguins\").dropna()\n# Select only numeric columns for features:\nX = penguins.loc[:, penguins.dtypes==\"float64\"]  \ny = penguins[\"species\"].astype(\"category\")\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.2, \n    shuffle=True, random_state=5\n    )\n\nWe can train a kNN classifier and then retrieve the probabilities via predict_proba:\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_tr, y_tr)\np_hat = knn.predict_proba(X_te)\np_hat[:6,:]\n\narray([[0.8, 0.2, 0. ],\n       [0.8, 0.2, 0. ],\n       [0. , 0. , 1. ],\n       [0. , 0. , 1. ],\n       [0.8, 0.2, 0. ],\n       [0.6, 0.4, 0. ]])\n\n\nFrom the output above we see that, for example, while the third and fourth test cases led to unanimous votes for Gentoo, the sixth case is deemed Adelie in a 3‚Äì2 squeaker (or is it a squawker?):\n\nyhat = knn.predict(X_te)\nyhat[:6]\n\narray(['Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Adelie', 'Adelie'],\n      dtype=object)\n\n\n\n\n3.5.1 ROC curve\nHaving values between the classes means that we can fine-tune how we decide to assign them.\n\nDefinition 3.4 Let \\(\\theta\\) be a number in the interval \\([0,1]\\). We say that a class \\(T\\) hits at level \\(\\theta\\) at a query point if the fraction of votes that \\(T\\) receives at that point is at least \\(\\theta\\).\n\n\nExample 3.7 Continuing with the data in Example¬†3.5, we find that at \\(\\theta=0\\), everything always hits:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n\n\n3\n1\n1\n1\n\n\n4\n1\n1\n1\n\n\n\n\n\n\n\nAt \\(\\theta=0.05\\), say, we lose all the cases where no votes were received:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n0\n1\n\n\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n\n\n3\n1\n0\n1\n\n\n4\n1\n1\n1\n\n\n\n\n\n\n\nAt \\(\\theta=0.15\\), we have also lost all those receiving 1 out of 10 votes:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n1\n0\n1\n\n\n3\n1\n0\n1\n\n\n4\n1\n1\n0\n\n\n\n\n\n\n\nBy the time we get to \\(\\theta=0.7\\), there are only two hits left:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n\n\n2\n0\n0\n0\n\n\n3\n0\n0\n1\n\n\n4\n0\n0\n0\n\n\n\n\n\n\n\n\nThe probability vector \\(\\hat{p}(\\bfx)\\) holds the largest possible \\(\\theta\\) values for which each class hits at \\(\\bfx\\). Looking at it another way, \\(\\theta=0\\) represents maximum credulity‚Äîeverybody‚Äôs a winner!‚Äîwhile \\(\\theta=1\\) represents maximum skepticism‚Äîunanimous winners only.\nThe ROC curve, or receiver operator characteristic curve, is a way to visualize the hits as a function of \\(\\theta\\) over a fixed testing set. The name is a little misleading, since the multiclass case requires multiple curves. The idea is to tally, at each value of \\(\\theta\\), all the hits within each class that represent true positives and false positives.\n\n\n\n\n\n\nNote\n\n\n\nThe name of the ROC curve is a throwback to the early days of radar, when the idea was first developed.\n\n\n\nExample 3.8 We continue with the data from Example¬†3.5, but now we add ground truth to the queries:\n\n\n\n\n\n\n\n\n\nA\nB\nC\ntruth\n\n\n\n\n0\n9\n0\n1\nA\n\n\n1\n5\n3\n2\nB\n\n\n2\n6\n1\n3\nA\n\n\n3\n2\n0\n8\nC\n\n\n4\n4\n5\n1\nA\n\n\n\n\n\n\n\nLet‚Äôs look at class A. At \\(\\theta=0.05\\), class A hits in every case, giving TP=3 and FP=2. At \\(\\theta=0.25\\), the fourth query drops out; we still have TP=3, but now FP=1. Here is the table of all the unique values of TP and FP that we can achieve as \\(\\theta\\) varies between 0 and 1:\n\n\n\n\n\n\n\n\n\ntheta\nFP\nTP\n\n\n\n\n0\n0.05\n2\n3\n\n\n1\n0.25\n1\n3\n\n\n2\n0.45\n1\n2\n\n\n3\n0.55\n0\n2\n\n\n4\n0.65\n0\n1\n\n\n5\n0.95\n0\n0\n\n\n\n\n\n\n\nIn order to make a graph, we convert the raw TP and FP numbers to rates. Since there are 2 positive and 3 negative over the entire test set, we can represent the rows above as the points \\[\n\\left(\\tfrac{2}{2},\\tfrac{3}{3}\\right), \\, \\left(\\tfrac{1}{2},\\tfrac{3}{3}\\right), \\, \\left(\\tfrac{1}{2},\\tfrac{2}{3}\\right), \\, \\left(\\tfrac{0}{2},\\tfrac{2}{3}\\right), \\, \\left(\\tfrac{0}{2},\\tfrac{1}{3}\\right)\\, \\left(\\tfrac{0}{2},\\tfrac{0}{3}\\right).\n\\] The ROC curve for class A is just connect-the-dots for these points:\n\ndata = pd.DataFrame({\"FP rate\": [1,1/2,1/2,0,0,0], \"TP rate\": [1,1,2/3,2/3,1/3,0]})\nsns.relplot(data=data, \n    x=\"FP rate\", y=\"TP rate\", \n    kind=\"line\", estimator=None\n    );\n\n\n\n\n\nUnsurprisingly, sklearn can compute the points defining the ROC curve automatically, which greatly simplifies drawing them.\n\nExample 3.9 Continuing Example¬†3.6, we will plot ROC curves for the three species in the penguin data:\n\nfrom sklearn.metrics import roc_curve\n\np_hat = knn.predict_proba(X_te)\nresults = []\nfor i, label in enumerate(knn.classes_):\n    actual = (y_te==label)\n    fp, tp, theta = roc_curve(actual,p_hat[:,i])\n    results.extend( [(label,fp,tp) for fp,tp in zip(fp,tp)] )\nroc = pd.DataFrame( results, columns=[\"label\",\"FP rate\",\"TP rate\"] )\nroc\n\n\n\n\n\n\n\n\nlabel\nFP rate\nTP rate\n\n\n\n\n0\nAdelie\n0.000000\n0.000000\n\n\n1\nAdelie\n0.027027\n0.233333\n\n\n2\nAdelie\n0.054054\n0.566667\n\n\n3\nAdelie\n0.216216\n0.900000\n\n\n4\nAdelie\n0.405405\n0.966667\n\n\n5\nAdelie\n0.459459\n1.000000\n\n\n6\nAdelie\n1.000000\n1.000000\n\n\n7\nChinstrap\n0.000000\n0.000000\n\n\n8\nChinstrap\n0.019231\n0.333333\n\n\n9\nChinstrap\n0.153846\n0.733333\n\n\n10\nChinstrap\n0.423077\n0.933333\n\n\n11\nChinstrap\n1.000000\n1.000000\n\n\n12\nGentoo\n0.000000\n0.000000\n\n\n13\nGentoo\n0.000000\n0.909091\n\n\n14\nGentoo\n0.022222\n0.909091\n\n\n15\nGentoo\n0.022222\n1.000000\n\n\n16\nGentoo\n0.088889\n1.000000\n\n\n17\nGentoo\n0.200000\n1.000000\n\n\n18\nGentoo\n1.000000\n1.000000\n\n\n\n\n\n\n\nThe table above holds all of the key points on the ROC curves:\n\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None\n    );\n\n\n\n\nEach curve starts in the lower left corner and ends at the upper right corner. The ideal situation is in the top left corner of the plot, corresponding to perfect recall and specificity. All of the curves explicitly show the tradeoff between recall and specificity as the decision threshold is varied. The Gentoo curve comes closest to the ideal.\nIf we weight neighbors‚Äô votes inversely to their distances from the query point, then the thresholds aren‚Äôt restricted to multiples of \\(\\tfrac{1}{5}\\):\n\nknnw = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\nknnw.fit(X_tr, y_tr)\np_hat = knnw.predict_proba(X_te)\n\nresults = []\nfor i, label in enumerate(knn.classes_):\n    actual = (y_te==label)\n    fp, tp, theta = roc_curve(actual,p_hat[:,i])\n    results.extend( [(label,fp,tp) for fp,tp in zip(fp,tp)] )\nroc = pd.DataFrame( results, columns=[\"label\",\"FP rate\",\"TP rate\"] )\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None\n    );\n\n\n\n\n\n\n\n3.5.2 AUC\nROC curves lead to another classification performance metric known as area under ROC curve (AUC). Its name tells you exactly what it is, and it ranges between 0 (bad) and 1 (ideal). Unlike the other classification metrics we have encountered, AUC tries to account not just for the result of the classification at a single threshold, but over the full range from credulous to skeptical. You might think of it as grading with partial credit.\n\nExample 3.10 The AUC metric allows us to compare the standard and weighted kNN classifiers from Example¬†3.9. Note that the function for computing them, roc_auc_score, requires a keyword argument when there are more than two classes, to specify ‚Äúone vs.¬†rest‚Äù (our usual) or ‚Äúone vs.¬†one‚Äù matchups.\n\nfrom sklearn.metrics import roc_auc_score\ns = roc_auc_score(\n    y_te, knn.predict_proba(X_te), \n    multi_class=\"ovr\", average=None\n    )\n\nsw = roc_auc_score(\n  y_te, knnw.predict_proba(X_te), \n  multi_class=\"ovr\", average=None\n  )\n\npd.DataFrame(\n    {\"standard\": s, \"weighted\": sw},\n    index=knn.classes_\n    )\n\n\n\n\n\n\n\n\nstandard\nweighted\n\n\n\n\nAdelie\n0.903153\n0.935586\n\n\nChinstrap\n0.857051\n0.883333\n\n\nGentoo\n0.997980\n0.998990\n\n\n\n\n\n\n\nBased on the above scores, the weighted classifier seems to be better at identifying all three species."
  },
  {
    "objectID": "classification.html#exercises",
    "href": "classification.html#exercises",
    "title": "3¬† Classification",
    "section": "Exercises",
    "text": "Exercises\nFor these exercises, you may use computer help to work on a problem, but your answer should be self-contained without reference to computer output (unless stated otherwise).\n\nExercise 3.1 Here is a confusion matrix for a classifier of meme dankness.\n\nConsidering dank to be the positive outcome, calculate the (a) recall, (b) precision, (c) specificity, (d) accuracy, and (e) \\(F_1\\) score of the classifier.\n\n\nExercise 3.2 Here is a confusion matrix for a classifier of ice cream flavors.\n\n(a) Calculate the recall rate for chocolate.\n(b) Find the precision for vanilla.\n(c) Find the accuracy for strawberry.\n\n\nExercise 3.3 Find the Gini impurity of this set: \\[ \\{ A, B, B, C, C, C \\}.\\]\n\n\nExercise 3.4 Given \\(x_i=i\\) for \\(i=0,\\ldots,5\\), with labels \\[\ny_1=y_5=y_6=A, \\quad y_2=y_3=y_4=B,\n\\] find an optimal partition threshold using Gini impurity.\n\n\nExercise 3.5 Carefully sketch the set of all points in \\(\\real^2\\) whose 1-norm distance from the origin equals 1. This is a Manhattan unit circle.\n\n\nExercise 3.6 Three points in the plane lie at the vertices of an equilateral triangle. One is labeled A and the other two are B. Carefully sketch the decision boundary for \\(k\\)-nearest neighbors with \\(k=1\\), using (a) the 2-norm and (b) the \\(\\infty\\)-norm.\n\n\nExercise 3.7 Define 8 points on an ellipse by \\(x_k=a\\cos(\\theta_k)\\) and \\(y_k=b\\sin(\\theta_k)\\), where \\(a\\) and \\(b\\) are positive and \\[\n\\theta_k \\in \\left\\{ \\frac{\\pi}{4}, \\frac{\\pi}{2}, \\frac{3\\pi}{4}, \\ldots, 2\\pi \\right\\}.\n\\] Let \\(u_1,\\ldots,u_8\\) and \\(v_1,\\ldots,v_8\\) be the z-scores of the \\(x_k\\) and the \\(y_k\\), respectively. Show that the points \\((u_k,v_k)\\) all lie on a circle centered at the origin for all \\(k=1,\\ldots,8\\). (By extension, standardizing points into z-scores is sometimes called sphereing them.)"
  },
  {
    "objectID": "selection.html#sec-select-learning-curves",
    "href": "selection.html#sec-select-learning-curves",
    "title": "4¬† Model selection",
    "section": "4.1 Bias‚Äìvariance tradeoff",
    "text": "4.1 Bias‚Äìvariance tradeoff\nWhen we train a classifier, we use a particular set of training data. In a different parallel universe, we might have been handed a different training set drawn from the same overall population. While we might be optimistic and hope for the best-case scenario of a training set that is perfectly representative, it‚Äôs more prudent to consider what‚Äôs best in the average case.\n\n4.1.1 Learner bias\nSuppose that \\(f(x)\\) is a perfect labeller, i.e., a function with 100% accuracy over an entire population. For simplicity, we can imagine that \\(f\\) is a binary classifier, i.e., \\(f(x) \\in \\{0,1\\}\\), although this assumption is not essential.\nLet \\(\\hat{f}(x)\\) denote a classification function obtained after training. It depends on the particular training set we used. Suppose there are \\(N\\) total possible training sets, leading to labelling functions \\[\n\\hat{f}_1(x),\\hat{f}_2(x),\\dots,\\hat{f}_N(x).\n\\] Then we define the expected value of the classifier as the average over all training sets: \\[\n\\E{\\hat{f}(x)} = \\frac{1}{N} \\sum_{i=1}^N \\hat{f_i}(x).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nExcept on toy problems, we don‚Äôt know how to calculate this average. This is more of a thought experiment. But we will simulate the idea later on.\n\n\nThe term expected doesn‚Äôt mean that we anticipate getting this answer for our particular \\(\\hat{f}\\). It‚Äôs just what we would get by averaging over all parallel universes that received unique training sets.\nWe can apply the expectation operator \\(\\mathbb{E}\\) to any function of \\(x\\). In particular, the expected error in our own universe‚Äôs prediction is \\[\n\\begin{split}\n    \\E{f(x) - \\hat{f}(x)} &= \\frac{1}{N} \\sum_{i=1}^N \\left( f(x) - \\hat{f_i}(x) \\right) \\\\\n  &= \\frac{1}{N} \\left( \\sum_{i=1}^N  f(x)  \\right) - \\frac{1}{N}\\left( \\sum_{i=1}^N \\hat{f_i}(x) \\right) \\\\\n  &= f(x) - \\E{\\hat{f}(x)}.\n\\end{split}\n\\] We will set \\(y=f(x)\\) as the true label and \\(\\hat{y}=\\E{\\hat{f}(x)}\\) as the expected prediction. The quantity above, \\(y-\\hat{y}\\), is called the bias of the classifier. Bias depends on the particular algorithm and its hyperparameters. Each architecture can reproduce a function of limited complexity.\n\n\n4.1.2 Variance\nIt might seem as though the only important goal is to minimize the bias. To see why this is not the case, imagine that you are playing a ring toss game at a carnival. You have an array of sticks on a grid laid out horizontally in front of you, and the goal is to toss rings so that they land surrounding any one of the sticks.\nOne strategy is to aim for the middle of the grid, because missing in any direction still gives you a chance to score. This is like aiming directly for the mean position. However, it‚Äôs likely that you have more consistency with shorter throws than longer ones: the farther away your aiming point is, the more variation there will be in the landing spots. Hence, it might be superior to aim at a closer stick. Even though throws that are too short may have no chance at scoring, you will do better due to the decreased spread of your throws.\nIn essence, you have only a finite‚Äîprobably small‚Äînumber of throws to make, so the performance of the average case isn‚Äôt the only consideration. Reducing variance can improve your odds of getting close, even if the average case is mediocre.\nTo see this tradeoff mathematically, we can compute the variance of the predicted labels at any \\(x\\): \\[\n\\begin{split}\n    \\E{(y - \\hat{y})^2} &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{f_i}(x) \\right)^2 \\\\   \n    &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{y} + \\hat{y} - \\hat{f_i}(x) \\right)^2  \\\\\n    &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{y} \\right)^2 + \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right)^2 \\\\\n  & \\qquad - 2 \\left( y - \\hat{y} \\right) \\cdot \\frac{1}{N}\\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right).  \n\\end{split}\n\\] Now we find something interesting: \\[\n\\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right) =\n\\hat{y} - \\frac{1}{N} \\sum_{i=1}^N \\hat{f}_i(x) = 0,\n\\] by the definition of \\(\\hat{y}\\). So overall, \\[\n\\begin{split}\n    \\E{(y - \\hat{y})^2} &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{y} \\right)^2 + \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right)^2 \\\\\n  &= (y-\\hat{y})^2 + \\E{\\left(\\hat{y} - \\hat{f}(x)\\right)^2}\n\\end{split}\n\\] The first term is the squared bias. The second is the variance of the learning method. In words, the variance of the learning process has two contributions:\n\nBias\n\nHow close is the average prediction to the ground truth? Variance\n\n\nHow close to the average prediction is any one prediction likely to be?\n\n\nWhy would these two factors be in opposition? When a learning method has the capacity to capture complex behavior, it potentially has a low bias. However, that same capacity means that the learner will fit itself very well to each individual training set, which increases the potential for variance over the whole collection of training sets.\nThis tension is known as the bias‚Äìvariance tradeoff. Perhaps we can view this tradeoff as a special case of Occam‚Äôs Razor: it‚Äôs best to choose the least complex method necessary to reach a particular level of explanatory power.\n\n\n4.1.3 Learning curves\nWe can illustrate the tradeoff between bias and variance by running an artificial experiment with different sizes for the training datasets.\n\nExample 4.2 We will use a subset of a realistic data set used to predict the dominant type of tree in patches of forest. We train a decision tree classifier with fixed depth throughout. (Don‚Äôt confuse the forest data for the tree classifier, haha.)\n\nforest = datasets.fetch_covtype()\nX = forest[\"data\"][:250000,:8]   # 250,000 samples, 8 dimensions\ny = forest[\"target\"][:250000]\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.05, \n    shuffle=True, random_state=0\n)\n\nalln = range(200, 4001, 200)       # sizes of the training subsets\nresults = []                       # for tracking results\ntree = DecisionTreeClassifier(max_depth=4) \nfor n in alln:             # iterate over training set sizes\n    for i in range(50):    # iterate over training sets\n        X_tr, y_tr = shuffle(X_tr, y_tr, random_state=i)\n        XX, yy = X_tr[:n,:], y_tr[:n]       # training subset of size n\n        tree.fit(XX, yy)\n        results.append( (\"train\", n, 1-tree.score(XX,yy)) )\n        results.append( (\"test\", n, 1-tree.score(X_te, y_te)) )\n\ncols = [ \"kind\", \"training set size\", \"error\" ]\nresults = pd.DataFrame(results, columns=cols)\nsns.relplot(data=results, \n    x=cols[1], y=cols[2], \n    kind=\"line\", errorbar=\"sd\", hue=cols[0]\n);\n\n\n\n\nThe plot above shows learning curves. The solid line is the mean result over all trials, and the ribbon has a width of one standard deviation. For a small training set, the tree has more than enough resolving power, and the result is severe overfitting, as seen by the large gap between testing and training. As the size of the training set grows, however, variance decreases and the two error measurements come together.\nNote that the curves seem to approach a horizontal asymptote at a nonzero level of error. This level indicates an unavoidable bias for this tree size, no matter how much of the data we throw at it. As a simple analogy, think about approximating curves in the plane by a parabola. You will be able to do a perfect job for linear and quadratic functions, but if you approximate a cosine curve, you can‚Äôt get it exactly correct no matter how much information you have about it.\n\nWhen you see a large gap between training and test errors, you should suspect that the learner will not generalize well. Ideally, you could bring more data to the table, perhaps by artificially augmenting the training examples. If not, you might as well decrease the resolving power of your learner, because the excess power is likely to make things no better, and maybe worse."
  },
  {
    "objectID": "selection.html#overfitting",
    "href": "selection.html#overfitting",
    "title": "4¬† Model selection",
    "section": "4.2 Overfitting",
    "text": "4.2 Overfitting\nOne important factor we have not yet considered is noise in the training data‚Äîthat is, erroneous values. If a learner responds too adeptly to isolated wrong values, it will also respond incorrectly to other nearby inputs. This situation is known as overfitting.\n\n\n\n4.2.1 Overfitting in kNN\nConsider a kNN classifier with \\(k=1\\). The class assigned to each value is just that of the nearest training example, making for a piecewise constant labelling. Let‚Äôs see how this plays out in about as simple a classification problem as we can come up with: a single feature, with the class being the sign of the feature‚Äôs value. (We arbitrarily assign zero to have class \\(+1\\).)\nUsing \\(k=1\\) produces fine results, as shown here for 4 different training sets of size 40:\n\n\n\n\n\nFigure¬†4.1: kNN with k=1 and perfect data\n\n\n\n\nNow suppose we use training sets that have just 3 mislabeled examples each. Here are some resulting classification functions:\n\n\n\n\n\nFigure¬†4.2: kNN with k=1 and noisy data\n\n\n\n\nEvery sample is its own nearest neighbor, so this classifier responds to noisy data by reproducing it perfectly, which interferes with the larger trend we actually want to capture. This is an extreme example of overfitting.\nNow let‚Äôs bump up to \\(k=3\\). The results are more like we want, even with noisy data:\n\n\n\n\n\nFigure¬†4.3: kNN with k=3 and noisy data\n\n\n\n\nThe voting mechanism of kNN allows the classifier to ignore isolated outliers. If we continue to \\(k=7\\), then the 3 outliers will never be able to outvote the correct values:\n\n\n\n\n\nFigure¬†4.4: kNN with k=7 and noisy data\n\n\n\n\nNote above that the decision boundary is still affected by the noisy values, so some of the more-borderline predictions are wrong, but clearer cases are always handled correctly.\n\n\n\n\n\n\nDanger\n\n\n\nThe lesson here is not simply that ‚Äúbigger \\(k\\) is better.‚Äù In the extreme case of \\(k=21\\) above, the classifier will predict the same value everywhere! If the true classification boundary were more complicated (i.e., if the classes switched back and forth at high frequency), using even \\(k=7\\) would be unable to capture many of the details.\n\n\n\n\n4.2.2 Overfitting in decision trees\nAs mentioned in Example¬†4.1, the depth of a decision tree correlates with its ability to parse the samples more finely. For \\(n=40\\) values, a tree of depth 6 is guaranteed to reproduce every sample value perfectly. With noisy data, we see clear signs of overfitting:\n\n\n\n\n\nFigure¬†4.5: Decision tree with depth=6 and noisy data\n\n\n\n\nUsing a shallower tree reduces the extent of overfitting:\n\n\n\n\n\nFigure¬†4.6: Decision tree with depth=3 and noisy data\n\n\n\n\nWe can eliminate the overfitting completely and get a single point as the decision boundary, although its location still might not be ideal:\n\n\n\n\n\nFigure¬†4.7: Decision tree with depth=2 and noisy data\n\n\n\n\n\n\n4.2.3 Overfitting and variance\nThe tendency to fit closely to training data also implies that the learner may have a good deal of variance in training (see Figure¬†4.2, and Figure¬†4.5, for example). Thus, overfitting is often associated with a large gap between training and testing variance, as observed in Section¬†4.1.\n\nExample 4.3 Returning to the forest data from Example¬†4.2, we try decision trees of maximum depth \\(r=12\\) on 100 random training subsets of size 5000:\n\n\nCode\nforest = datasets.fetch_covtype()\nX = forest[\"data\"][:50000,:8]\ny = (forest[\"target\"][:50000] == 1)\n\ndef experiment(learner, X, y, n):\n    X_tr, X_te, y_tr, y_te = train_test_split(\n        X, y,\n        test_size=0.2,\n        shuffle=True,\n        random_state=1\n    )\n    results = [] \n    for i in range(100):\n        X_tr, y_tr = shuffle(X_tr, y_tr, random_state=i)\n        XX, yy = X_tr[:n,:], y_tr[:n]\n        learner.fit(XX, yy) \n        err = 1 - balanced_accuracy_score(yy, learner.predict(XX))\n        results.append( (\"train\", err) )   # training error\n        err = 1 - balanced_accuracy_score(y_te, learner.predict(X_te))\n        results.append( (\"test\", err) )    # test error\n\n    results = pd.DataFrame( results, columns=[\"kind\", \"error\"] )\n    sns.displot(data=results, x=\"error\", hue=\"kind\", bins=20);\n\n\n\ntree = DecisionTreeClassifier(max_depth=12)\nexperiment(tree, X, y, 5000)\n\n\n\n\nFigure¬†4.8: ?(caption)\n\n\n\n\nSince \\(2^{12}=4096\\), this tree is probably overfit to the training data, and we also see the wide separation between training and testing that suggests the training does not generalize well. With a depth of \\(r=4\\), the training and testing results completely overlap:\n\ntree = DecisionTreeClassifier(max_depth=4)\nexperiment(tree, X, y, 5000)\n\n\n\n\nHowever, notice above that the testing error increased substantially from the overfit case.\n\nWe could say that the last tree in Example¬†4.3 is actually underfit to the data: the behavior of the data is probably too complex to be replicated well by such a shallow tree. We have encountered the bias‚Äìvariance tradeoff again."
  },
  {
    "objectID": "selection.html#ensemble-methods",
    "href": "selection.html#ensemble-methods",
    "title": "4¬† Model selection",
    "section": "4.3 Ensemble methods",
    "text": "4.3 Ensemble methods\nWhen a relatively expressive learning model is used, overfitting and strong dependence on the training set are possible. One meta-strategy for reducing training variance without decreasing the model expressiveness is to use an ensemble method.\nThe idea of an ensemble is that averaging over many different training sets will reduce the variance that comes from overfitting. It‚Äôs much like trying to simulate the computation we used in the theory of expected values. The most common way to construct the training sets is called bootstrap aggregation, or bagging for short, in which samples are drawn randomly from the original training set. (Usually this is done with replacement, which means that some samples might be selected multiple times.)\nSklearn has a BaggingClassifier that automates the process of generating an ensemble from just one basic type of estimator.\n\nExample 4.4 Here is a dataset collected from images of dried beans:\n\nbeans = pd.read_excel(\"_datasets/Dry_Bean_Dataset.xlsx\")\nX = beans.drop(\"Class\", axis=1)\nX.head()\n\n\n\n\n\n\n\n\nArea\nPerimeter\nMajorAxisLength\nMinorAxisLength\nAspectRation\nEccentricity\nConvexArea\nEquivDiameter\nExtent\nSolidity\nroundness\nCompactness\nShapeFactor1\nShapeFactor2\nShapeFactor3\nShapeFactor4\n\n\n\n\n0\n28395\n610.291\n208.178117\n173.888747\n1.197191\n0.549812\n28715\n190.141097\n0.763923\n0.988856\n0.958027\n0.913358\n0.007332\n0.003147\n0.834222\n0.998724\n\n\n1\n28734\n638.018\n200.524796\n182.734419\n1.097356\n0.411785\n29172\n191.272750\n0.783968\n0.984986\n0.887034\n0.953861\n0.006979\n0.003564\n0.909851\n0.998430\n\n\n2\n29380\n624.110\n212.826130\n175.931143\n1.209713\n0.562727\n29690\n193.410904\n0.778113\n0.989559\n0.947849\n0.908774\n0.007244\n0.003048\n0.825871\n0.999066\n\n\n3\n30008\n645.884\n210.557999\n182.516516\n1.153638\n0.498616\n30724\n195.467062\n0.782681\n0.976696\n0.903936\n0.928329\n0.007017\n0.003215\n0.861794\n0.994199\n\n\n4\n30140\n620.134\n201.847882\n190.279279\n1.060798\n0.333680\n30417\n195.896503\n0.773098\n0.990893\n0.984877\n0.970516\n0.006697\n0.003665\n0.941900\n0.999166\n\n\n\n\n\n\n\nAlthough the dataset has data on 7 classes of beans, we will simplify our output by making it a one-vs-rest problem for just one class:\n\ny = beans[\"Class\"] == \"DERMASON\"\n\nHere is the confusion matrix we get from training a single kNN classifier on this dataset:\n\n\nCode\nX_tr, X_te, y_tr, y_te = train_test_split(\n  X, y,\n  test_size=0.2,\n  shuffle=True,\n  random_state=1\n)\n\npipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\n\npipe.fit(X_tr, y_tr)\nyhat = pipe.predict(X_te)\n\nprint( confusion_matrix(y_te, yhat, labels=[True,False]) )\n\n\n[[ 650   55]\n [  46 1972]]\n\n\nHere, we create an ensemble with 100 such classifiers, each trained on a different subset of size 40% of the size of the original training set:\n\nfrom sklearn.ensemble import BaggingClassifier\n\nensemble = BaggingClassifier( \n    pipe, \n    max_samples=0.75,\n    n_estimators=100,\n    random_state=0\n    )\n\nensemble.fit(X_tr, y_tr)\nyhat = ensemble.predict(X.iloc[:1,:])\nprint(\"prediction on first sample is\", \n    bool(yhat[0])\n    )\n\nprediction on first sample is False\n\n\nThe estimators_ field of the ensemble object is a list of the individual trained classifiers. With a little work, we can manually tally up the number that vote True on a query:\n\nquery = X.to_numpy()[:1,:]   # must use an array\nyy = [ model.predict(query)[0] for model in ensemble.estimators_ ]\nprint(f\"{sum(yy) / len(yy):.0%} vote for True\")\n\n26% vote for True\n\n\nSince only 26% vote True, the prediction of the ensemble is False, as printed out above. Over the testing set, we find some improvement in the confusion matrix:\n\nyhat = ensemble.predict(X_te)\nprint( confusion_matrix(y_te, yhat, labels=[True,False]) )\n\n[[ 654   51]\n [  41 1977]]\n\n\n\n\nExample 4.5 Let‚Äôs return to the experiment of Example¬†4.3, where we found that a decision tree of depth \\(r=12\\) was badly overfit. Now we use an ensemble of 50 such trees:\n\n\nCode\nforest = datasets.fetch_covtype()\nX = forest[\"data\"][:50000,:8]\ny = (forest[\"target\"][:50000] == 1)\n\ntree = DecisionTreeClassifier(max_depth=12)\nensemble = BaggingClassifier( \n    tree,               # model used in each estimator\n    max_samples=0.25,   # fraction of data to use for each\n    n_estimators=50,\n    random_state=0,\n    n_jobs=-1            # use processes in parallel\n    )\nexperiment(ensemble, X, y, 5000)\n\n\n\n\n\nCompared to the earlier experiment (see Figure¬†4.8), the separation between training and testing was greatly reduced, although seemingly at a small cost to the bias.\n\n\n\n\n\n\n\nNote\n\n\n\nAn ensemble of decision trees is known as a random forest. We could have used a RandomForestClassifier to accomplish the bagged decision tree ensemble in Example¬†4.5.\n\n\nIn addition to training on random subsets of the data, a bagging classifier can use random subsets of features (i.e., dimensions). The purpose again is to increase the diversity of the individual estimators in order to make the ensemble more robust.\nEnsembles can be constructed for any individual model type. Their chief disadvantage is the need to repeat the fitting process multiple times, although this can be mitigated by computing the fits in parallel. For random forests in particular, we also lose the potential for interpreting the decision process the way we can for an individual tree."
  },
  {
    "objectID": "selection.html#validation",
    "href": "selection.html#validation",
    "title": "4¬† Model selection",
    "section": "4.4 Validation",
    "text": "4.4 Validation\nWe now return to the opening questions of this chapter: how should we determine optimal hyperparameters and algorithms?\nIt‚Äôs tempting to compute test scores over a range of hyperparameter choices and simply choose the cast that scores best. That amounts to inspecting the graphs and values in the examples above and choosing the best outcomes. However, if we base hyperparameter optimization on a fixed test set, then we are effectively learning from that set! The hyperparameters might become too tuned‚Äîi.e., overfit‚Äîto our particular choice of the test set.\nTo avoid this pitfall, we can split the data into three subsets for training, validation, and testing. The validation set is used to tune hyperparameters. Once training is performed at values determined to be best on validation, the test set is used to assess the generalization of the optimized learner.\nUnfortunately, a fixed three-way split of the data further reduces the amount of data available for training, which we often want to avoid.\n\n4.4.1 Cross-validation\nIn cross-validation, each learner is trained multiple times using unique training and validation sets drawn from the same pool. The most common version is \\(k\\)-fold cross-validation:\n\nDivide the original data into training and testing sets.\nFurther divide the training data set into \\(k\\) roughly equal parts called folds.\nTrain a learner using folds \\(2,3,\\ldots,k\\) and validate on the cases in fold 1. Then train another learner on folds \\(1,3,\\ldots,k\\) and validate against the cases in fold 2. Continue until each fold has served once for validation.\nSelect the hyperparameters producing the best validation score and retrain on the entire training set.\nAssess performance using the test set.\n\nA variation is stratified \\(k\\)-fold, in which the division in step 2 is constrained so that the relative membership of each class is the same in every fold as it is in the full training set. This is advisable when one or more classes is scarce and might otherwise become underrepresented in some folds.\n\nExample 4.6 Here is how 16 elements can be split into 4 folds:\n\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=4, shuffle=True, random_state=0)\nfor train,test in kf.split(range(16)): \n    print(\"train:\", train, \", test:\", test)\n\ntrain: [ 0  2  3  4  5  7 10 11 12 13 14 15] , test: [1 6 8 9]\ntrain: [ 0  1  3  5  6  7  8  9 10 11 12 15] , test: [ 2  4 13 14]\ntrain: [ 0  1  2  3  4  5  6  8  9 12 13 14] , test: [ 7 10 11 15]\ntrain: [ 1  2  4  6  7  8  9 10 11 13 14 15] , test: [ 0  3  5 12]\n\n\n\n\nExample 4.7 Let‚Äôs apply cross-validation to the beans dataset.\n\nbeans = pd.read_excel(\"_datasets/Dry_Bean_Dataset.xlsx\")\nX = beans.drop(\"Class\", axis=1)\ny = beans[\"Class\"]\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.15, \n    shuffle=True, random_state=0\n    )\n\nA round of 6-fold cross-validation on a standardized kNN classifier looks like the following:\n\nfrom sklearn.model_selection import cross_validate\n\nknn = KNeighborsClassifier(n_neighbors=5)\nlearner = make_pipeline(StandardScaler(), knn)\n\nkf = KFold(n_splits=6, shuffle=True, random_state=0)\nscores = cross_validate(\n    learner, \n    X_tr, y_tr, \n    cv=kf,\n    scoring=\"balanced_accuracy\"\n    )\n\nprint(\"Validation scores:\")\nprint( scores[\"test_score\"] )\n\nValidation scores:\n[0.93393194 0.93762617 0.923854   0.93617109 0.93037398 0.9326016 ]\n\n\nThe low variance across the folds that we see above is reassurance that they are representative. Conversely, if the scores were spread more widely, we would be concerned that there was too much dependence on the training set, which might indicate overfitting.\n\n\n\n4.4.2 Hyperparameter tuning\nIf we perform cross-validations as we vary a hyperparameter, we get a validation curve.\n\nExample 4.8 Here is a validation curve for the maximum depth of a decision tree classifier on the beans data:\n\nfrom sklearn.model_selection import StratifiedKFold\n\ndepths = range(4, 16, 1)\nkf = StratifiedKFold(n_splits=8, shuffle=True, random_state=2)\nresults = []    # for keeping results\nfor d in depths:\n    tree = DecisionTreeClassifier(max_depth=d, random_state=1)\n    cv = cross_validate(tree, \n        X_tr, y_tr, \n        cv=kf, \n        scoring=\"balanced_accuracy\",\n        n_jobs=-1\n        )\n    for err in 1 - cv[\"test_score\"]:\n      results.append( (d, err) )\n\nresults = pd.DataFrame(results, columns=[\"depth\", \"error\"] )\nsns.relplot(data=results, \n    x=\"depth\", y=\"error\", \n    kind=\"line\", errorbar=\"sd\"\n    );\n\n\n\n\nInitially the error decreases because the shallowest decision trees are underfit. The minimum error is at max depth 9, after which overfitting seems to take over:\n\nresults.groupby(\"depth\").mean()\n\n\n\n\n\n\n\n\nerror\n\n\ndepth\n\n\n\n\n\n4\n0.203600\n\n\n5\n0.115527\n\n\n6\n0.098991\n\n\n7\n0.090455\n\n\n8\n0.086932\n\n\n9\n0.081718\n\n\n10\n0.082619\n\n\n11\n0.085970\n\n\n12\n0.084984\n\n\n13\n0.086531\n\n\n14\n0.085893\n\n\n15\n0.087356\n\n\n\n\n\n\n\nWe can now train this optimal classifier on the entire training set and measure performance on the reserved testing data:\n\ntree = DecisionTreeClassifier(max_depth=9, random_state=1)\ntree.fit(X_tr, y_tr)\nyhat = tree.predict(X_te)\nprint( \"score is\", balanced_accuracy_score(y_te, yhat) )\n\nscore is 0.9191394340897692\n\n\n\n\n4.4.2.1 Grid search\nWhen there is a single hyperparameter in play, the validation curve is useful way to optimize it. When multiple hyperparameters are available, it‚Äôs common to perform a grid search, in which we try cross-validated fitting using every specified combination of parameter values.\n\nExample 4.9 Let‚Äôs work with a dataset on breast cancer detection:\n\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer(as_frame=True)[\"frame\"]\nX = cancer.drop(\"target\", axis=1)\ny = cancer[\"target\"]\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y, \n    test_size=0.15, \n    shuffle=True, random_state=2\n    )\nX_te.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n528\n13.940\n13.17\n90.31\n594.2\n0.12480\n0.09755\n0.10100\n0.066150\n0.1976\n0.06457\n...\n14.62\n15.38\n94.52\n653.3\n0.1394\n0.1364\n0.15590\n0.1015\n0.2160\n0.07253\n\n\n291\n14.960\n19.10\n97.03\n687.3\n0.08992\n0.09823\n0.05940\n0.048190\n0.1879\n0.05852\n...\n16.25\n26.19\n109.10\n809.8\n0.1313\n0.3030\n0.18040\n0.1489\n0.2962\n0.08472\n\n\n467\n9.668\n18.10\n61.06\n286.3\n0.08311\n0.05428\n0.01479\n0.005769\n0.1680\n0.06412\n...\n11.15\n24.62\n71.11\n380.2\n0.1388\n0.1255\n0.06409\n0.0250\n0.3057\n0.07875\n\n\n108\n22.270\n19.67\n152.80\n1509.0\n0.13260\n0.27680\n0.42640\n0.182300\n0.2556\n0.07039\n...\n28.40\n28.01\n206.80\n2360.0\n0.1701\n0.6997\n0.96080\n0.2910\n0.4055\n0.09789\n\n\n340\n14.420\n16.54\n94.15\n641.2\n0.09751\n0.11390\n0.08007\n0.042230\n0.1912\n0.06412\n...\n16.67\n21.51\n111.40\n862.1\n0.1294\n0.3371\n0.37550\n0.1414\n0.3053\n0.08764\n\n\n\n\n5 rows √ó 30 columns\n\n\n\nWe start by trying decision tree classifiers in which we vary the maximum depth as well as some other options.\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = { \"criterion\":[\"gini\", \"entropy\"], \n         \"max_depth\":range(2, 15), \n         \"min_impurity_decrease\":np.arange(0,0.01,0.002) }\nlearner = DecisionTreeClassifier(random_state=1)\nkf = KFold(n_splits=4, shuffle=True, random_state=0)\n\ngrid_dt = GridSearchCV(learner, grid, \n    scoring=\"f1\", \n    cv=kf,\n    n_jobs=-1\n    )\ngrid_dt.fit(X_tr, y_tr)\n\nprint(\"Best parameters:\")\nprint(grid_dt.best_params_)\nprint()\nprint(\"Best score:\")\nprint(grid_dt.best_score_)\n\nBest parameters:\n{'criterion': 'gini', 'max_depth': 5, 'min_impurity_decrease': 0.0}\n\nBest score:\n0.9582328658156416\n\n\nNext, we do the same search over kNN classifiers. We always use standardization as a preprocessor; note how the syntax of the grid search is adapted:\n\ngrid = { \"kneighborsclassifier__metric\":[\"euclidean\", \"manhattan\"], \n         \"kneighborsclassifier__n_neighbors\":range(1, 20), \n         \"kneighborsclassifier__weights\":[\"uniform\", \"distance\"] }\nlearner = make_pipeline(StandardScaler(), KNeighborsClassifier())\ngrid_knn = GridSearchCV(learner, grid, \n    scoring=\"f1\", \n    cv=kf,\n    n_jobs=-1\n    )\ngrid_knn.fit(X_tr, y_tr)\ngrid_knn.best_params_, grid_knn.best_score_\n\n({'kneighborsclassifier__metric': 'euclidean',\n  'kneighborsclassifier__n_neighbors': 8,\n  'kneighborsclassifier__weights': 'uniform'},\n 0.9749108434555516)\n\n\nEach fitted grid search object is itself a classifier that was trained on the full training set at the optimal hyperparameters:\n\nscore = lambda cl, X, y: f1_score( y, cl.predict(X) )\n\nprint(\"best tree f1 score:\",score(grid_dt, X_te, y_te))\nprint(\"best knn f1 score:\",score(grid_knn, X_te, y_te))\n\nbest tree f1 score: 0.9306930693069307\nbest knn f1 score: 0.9814814814814815\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt may be instructive to rerun the competition above using different random seeds. The meaningfulness of the results is limited by their sensitivity to such choices. Don‚Äôt let floating-point values give you a false feeling of precision!\n\n\n\n\n\n4.4.2.2 Alternatives to grid search\nGrid search is a brute-force approach. It is embarrassingly parallel, meaning that different processors can work on different locations on the grid at the same time. But it is usually too slow for large training sets, or when the search space has more than two dimensions. In such cases you can try searching over crude versions of the grid, perhaps with just part of the training data, and gradually narrow the search while using all the data. When desperate, one may try a randomized search and to guide the process with experience and intuition."
  },
  {
    "objectID": "regression.html#sec-regression-linear",
    "href": "regression.html#sec-regression-linear",
    "title": "5¬† Regression",
    "section": "5.1 Linear regression",
    "text": "5.1 Linear regression\nYou have likely previously encountered the most basic form of regression: fitting a straight line to data points \\((x_i,y_i)\\) in the \\(xy\\)-plane. In linear regression, we have a one-dimensional feature \\(x\\) and assume a relation\n\\[\ny \\approx \\hat{f}(x) = ax + b.\n\\]\nWe also define a loss function or misfit function that adds up how far predictions are from the data. The standard choice is a sum of squared differences between the predictions and the true values:\n\\[\nL(a,b) = \\sum_{i=1}^n (\\hat{f}(x_i)-y_i)^2 = \\sum_{i=1}^n (a x_i + b - y_i)^2.\n\\]\nThe loss can be minimized using a little multidimensional calculus. Momentarily suppose that \\(b\\) is held fixed and take a derivative with respect to \\(a\\):\n\\[\n\\pp{L}{a} = \\sum_{i=1}^n 2x_i(a x_i + b - y_i) = 2 a \\left(\\sum_{i=1}^n x_i^2\\right) + 2b\\left(\\sum_{i=1}^n x_i\\right) - 2\\sum_{i=1}^n x_i y_i.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe symbol \\(\\pp{}{}\\) is called a partial derivative and is defined just as described here: differentiate in one variable while all others are temporarily held constant.\n\n\nSimilarly, if we hold \\(a\\) fixed and differentiate with respect to \\(b\\), then\n\\[\n\\pp{L}{b} = \\sum_{i=1}^n 2(a x_i + b - y_i) = 2 a \\left(\\sum_{i=1}^n x_i\\right) + 2bn - 2 \\sum_{i=1}^n y_i.\n\\]\nSetting both derivatives to zero creates a system of two linear equations to be solved for \\(a\\) and \\(b\\): \\[\n\\begin{split}\n    a \\left(\\sum_{i=1}^n x_i^2\\right) + b\\left(\\sum_{i=1}^n x_i\\right) &= \\sum_{i=1}^n x_i y_i, \\\\\n    a \\left(\\sum_{i=1}^n x_i\\right) + b n &= \\sum_{i=1}^n y_i.\n\\end{split}\n\\tag{5.1}\\]\n\nExample 5.1 Suppose we want to find the linear regressor of the points \\((-1,0)\\), \\((0,2)\\), \\((1,3)\\). We need to calculate a few sums:\n\\[\n\\begin{split}\n\\sum_{i=1}^n x_i^2 = 1+0+1=2, \\qquad & \\sum_{i=1}^n x_i = -1+0+1=0, \\\\\n\\sum_{i=1}^n x_iy_i = 0+0+3=3, \\qquad & \\sum_{i=1}^n y_i = 0+2+3=5.\n\\end{split}\n\\]\nNote that \\(n=3\\). Therefore we must solve\n\\[\n\\begin{split}\n2a + 0b &= 3, \\\\\n0a + 3b &= 5.\n\\end{split}\n\\]\nThe regression function is \\(\\hat{f}(x)=\\tfrac{3}{2} x + \\tfrac{5}{3}\\).\n\n\n5.1.1 Linear algebra\nBefore moving on, we want to adopt a vector-oriented description of the process. If we define \\[\n\\bfe = [1,1,\\ldots,1] \\in \\real^n,\n\\] that is, \\(\\bfe\\) as a vector of \\(n\\) ones, then \\[\nL(a,b) =  \\twonorm{a\\, \\bfx + b \\,\\bfe - \\bfy}^2,\n\\] Minimizing \\(L\\) over all values of \\(a\\) and \\(b\\) is called the least squares problem. (More specifically, this setup is called simple least squares or ordinary least squares.)\nWe can write out the equations for \\(a\\) and \\(b\\) using another important idea from linear algebra. ::::{#def-regression-inner-product} Given any \\(d\\)-dimensional real-values vectors \\(\\bfu\\) and \\(\\bfv\\), their inner product is \\[\n\\bfu^T \\bfv = \\sum_{i=1}^d u_i v_i = u_1v_1 + u_2v_2 + \\cdots + u_d v_d.\n\\tag{5.2}\\] ::::\n\n\n\n\n\n\nNote\n\n\n\nInner product is a term from linear algebra. In physics and vector calculus with \\(d=2\\) or \\(d=3\\), the same thing is often called a dot product and written as \\(\\bfu \\cdot \\bfv\\).\n\n\nThere is an important link between the dot product and the 2-norm: \\[\n\\bfu^T \\bfu = \\sum_{i=1}^d u_i^2 = \\twonorm{\\bfu}^2.\n\\tag{5.3}\\]\nThe equations in Equation¬†5.1 may now be written as \\[\n\\begin{split}\n    a \\left(\\bfx^T \\bfx\\right) + b \\left(\\bfx^T\\bfe\\right) &= \\bfx^T\\bfy, \\\\\n    a \\left(\\bfe^T \\bfx\\right) + b \\left(\\bfe^T\\bfe\\right) &= \\bfe^T\\bfy.\n\\end{split}\n\\tag{5.4}\\]\n\n\n5.1.2 Performance metrics\nWe need to establish ways to measure regression performance. Unlike with binary classification, in regression it‚Äôs not just a matter of right and wrong answers‚Äîthe amount of wrongness matters, too.\nIn this section, we will use \\(x_i\\) for \\(i=1,\\ldots,n\\) to mean the training set features, \\(y_i\\) to mean the corresponding training values, and \\(\\hat{y}_i\\) to mean the values predicted on the training set by the regressor.\n\nDefinition 5.2 The residuals of the regression are \\[\ny_i - \\hat{y}_i, \\qquad i=1,\\ldots,n.\n\\tag{5.5}\\] We can express them as the vector \\(\\bfy-\\hat{\\bfy}\\).\n\nA quirk of linear regression is that it‚Äôs an older idea than most of machine learning, and it‚Äôs often presented as though the training and testing sets are identical. We give the following definitions in terms of \\(\\bfy\\) and \\(\\hat{\\bfy}\\) from the training data. They can also be calculated for a set of values and predictions obtained from a separate testing set, though a few of the properties don‚Äôt translate in that case.\n\n\n\n\n\n\n\nDanger\n\n\n\nThe terms error and residual are frequently used interchangeably and inconsistently. We try to follow the most common practices here, even though the names can be confusing if you think about them too hard.\n\n\n\nDefinition 5.3 The mean squared error (MSE) is \\[\n\\text{MSE} = \\frac{1}{m} \\sum_{i=1}^m \\, \\left( y_i - \\hat{y}_i \\right)^2 = \\frac{1}{m} \\twonorm{\\bfy - \\hat{\\bfy}}^2.\n\\] The mean absolute error (MAE) is \\[\n\\text{MAE} = \\frac{1}{m} \\sum_{i=1}^m \\abs{y_i - \\hat{y}_i }= \\frac{1}{m} \\onenorm{\\bfy - \\hat{\\bfy}}.\n\\]\n\nMAE is less sensitive than MSE to large outliers. Both quantities are dimensional and therefore depend on how the variables are scaled, but at least the units of MAE are the same as of the data.\n\nDefinition 5.4 The coefficient of determination (CoD) is denoted \\(R^2\\) and defined as \\[\nR^2 = 1 - \\frac{\\displaystyle\\sum_{i=1}^n \\,\\left(y_i - \\hat{y}_i \\right)^2}{\\displaystyle\\sum_{i=1}^n \\, \\left(y_i - \\bar{y}\\right)^2},\n\\] where \\(\\bar{y}\\) is the mean of \\(y_1,\\ldots,y_n\\).\n\nImportant things to know about the coefficient of determination:\n\nThe CoD is dimensionless and therefore independent of scaling.\nIf the \\(\\hat{y}_i\\) are found from a linear regression, then \\(R^2\\) is the square of the Pearson correlation coefficient between \\(\\bfy\\) and \\(\\hat{\\bfy}\\).\nIf \\(\\hat{y}_i=y_i\\) for all \\(i\\) (i.e., perfect predictions), then \\(R^2=1\\).\nIf \\(\\hat{y}_i=\\bar{y}\\) for all \\(i\\), then \\(R^2=0\\).\n\nThe notation is highly unfortunate, though, because for other regression methods, \\(R^2\\) can actually be negative! Such a result indicates that the predictor is doing worse than just predicting the mean value every time.\n\nExample 5.2 Let‚Äôs find the coefficient of determination for the fit in Example¬†5.1, where we found the regressor \\(\\hat{f}(x)=\\tfrac{3}{2} x + \\tfrac{5}{3}\\). Now \\(\\bar{y} = \\frac{1}{3}(0+2+3)=\\frac{5}{3}\\), and\n\\[\n\\begin{split}\n    \\sum_{i=1}^m \\sum_{i=1}^n \\,\\left(y_i - \\hat{y}_i \\right)^2 &= \\left(0-\\tfrac{1}{6}\\right)^2 + \\left(2-\\tfrac{5}{3}\\right)^2 + \\left(3-\\tfrac{19}{6}\\right)^2 = \\frac{1}{6}, \\\\\n    \\sum_{i=1}^m \\left(y_i - \\bar{y}\\right)^2 &= \\left(0-\\tfrac{5}{3}\\right)^2 + \\left(2-\\tfrac{5}{3}\\right)^2 + \\left(3-\\tfrac{5}{3}\\right)^2 = \\frac{14}{3}.\n\\end{split}\n\\]\nThis yields \\(R^2 = 1 - (1/6)(3/14) = 27/28\\).\nIf we instead use the arbitrary regressor \\(\\hat{f}(x)=x\\), then\n\\[\n\\begin{split}\n    \\sum_{i=1}^m \\sum_{i=1}^n \\,\\left(y_i - \\hat{y}_i \\right)^2 &= \\left(0+1\\right)^2 + \\left(2-0\\right)^2 + \\left(3-1\\right)^2 = 9, \\\\\n    \\sum_{i=1}^m \\left(y_i - \\bar{y}\\right)^2 &= \\frac{14}{3}.\n\\end{split}\n\\]\nThis yields \\(R^2 = 1 - (9)(3/14) = -13/14\\). Since the result is negative, we would be better off always predicting \\(5/3\\).\n\n\n\n5.1.3 Case study: Arctic ice\nLet‚Äôs import data about the extent of sea ice in the Arctic circle, collected monthly since 1979.\n\nice = pd.read_csv(\"_datasets/sea-ice.csv\")\n# Simplify column names:\nice.columns = [s.strip() for s in ice.columns]   \nice.head()\n\n\n\n\n\n\n\n\nyear\nmo\ndata-type\nregion\nextent\narea\n\n\n\n\n0\n1979\n1\nGoddard\nN\n15.41\n12.41\n\n\n1\n1980\n1\nGoddard\nN\n14.86\n11.94\n\n\n2\n1981\n1\nGoddard\nN\n14.91\n11.91\n\n\n3\n1982\n1\nGoddard\nN\n15.18\n12.19\n\n\n4\n1983\n1\nGoddard\nN\n14.94\n12.01\n\n\n\n\n\n\n\nA quick plot reveals something odd-looking.\n\nsns.relplot(data=ice, x=\"mo\", y=\"extent\");\n\n\n\n\nEverything in the plot is dominated by two large negative values. These probably represent missing or unreliable data, so we remove those rows.\n\nice = ice[ ice[\"extent\"]>0 ].copy()\nsns.relplot(data=ice, x=\"mo\", y=\"extent\");\n\n\n\n\nEach dot represents one measurement. As you would expect, the extent of ice rises in the winter and falls in summer.\n\nbymonth = ice.groupby(\"mo\")\nbymonth[\"extent\"].mean()\n\nmo\n1     14.214762\n2     15.100233\n3     15.256977\n4     14.525581\n5     13.117442\n6     11.539767\n7      9.097907\n8      6.793256\n9      5.993488\n10     7.887907\n11    10.458182\n12    12.664419\nName: extent, dtype: float64\n\n\nWhile the effect of the seasonal variation somewhat cancels out over time when fitting a line, it‚Äôs preferable to remove this obvious trend before the fit takes place. We will add a column that measures the relative change from the mean in each month, i.e., \\((x-\\bar{x})/\\bar{x}\\) within each group.\n\nrecenter = lambda x: x/x.mean() - 1\nice[\"detrended\"] = bymonth[\"extent\"].transform(recenter)\nsns.relplot(data=ice, x=\"mo\", y=\"detrended\");\n\n\n\n\nAn lmplot in seaborn will show the best-fit line.\n\nsns.lmplot(data=ice, x=\"year\", y=\"detrended\");\n\n\n\n\nHowever, keep Simpson‚Äôs paradox in mind. The previous plot showed considerably more variance in the warm months. How do the fits look for the data within each month?\n\nsns.lmplot(data=ice,\n    x=\"year\", y=\"detrended\",\n    col=\"mo\", col_wrap=3, height=2\n    );\n\n\n\n\nWhile the correlation is negative in each month, the effect size is clearly larger in the summer.\nWe can get numerical information about a regression line by using a LinearRegression() in sklearn. We will focus on the data for August.\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n\naug = ice[\"mo\"]==8\n# We need a frame, not a series, so use a vector for columns for X: \nX = ice.loc[aug, [\"year\"] ]  \ny = ice.loc[aug, \"detrended\"]\nlm.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nWe can get the slope and \\(y\\)-intercept of the regression line from the learner‚Äôs properties. (Calculated parameters tend to have underscores at the ends of their names in sklearn.)\n\n(lm.coef_, lm.intercept_)\n\n(array([-0.01103658]), 22.073164930307787)\n\n\nThe slope indicates average decrease over time. Here, we assess the performance on the training set. Both the MSE and mean absolute error are small relative to dispersion within the values themselves:\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nyhat = lm.predict(X)\nmse = mean_squared_error(y, yhat)\nmae = mean_absolute_error(y, yhat)\n\nprint(f\"MSE: {mse:.2e}, compared to variance {y.var():.2e}\")\nprint(f\"MAE: {mae:.2e}, compared to standard deviation {y.std():.2e}\")\n\nMSE: 4.01e-03, compared to variance 2.33e-02\nMAE: 4.93e-02, compared to standard deviation 1.53e-01\n\n\nThe score method of the regressor object computes the coefficient of determination:\n\nR2 = lm.score(X, y)\nprint(\"R-squared:\", R2)\n\nR-squared: 0.8237357450183893\n\n\nAn \\(R^2\\) value this close to 1 would usually be considered a sign of a good fit, although we have not tested for generalization to new data."
  },
  {
    "objectID": "regression.html#multilinear-and-polynomial-regression",
    "href": "regression.html#multilinear-and-polynomial-regression",
    "title": "5¬† Regression",
    "section": "5.2 Multilinear and polynomial regression",
    "text": "5.2 Multilinear and polynomial regression\nWe can extend linear regression to \\(d\\) predictor variables \\(x_1,\\ldots,x_d\\):\n\\[\n\\begin{split}\n    y \\approx \\hat{f}(\\bfx) &= b + w_1 x_1 + w_2x_2 + \\cdots w_d x_d.\n\\end{split}\n\\]\nWe can drop the intercept term \\(b\\) from the discussion, because we could always define an additional constant predictor variable \\(x_{d+1}=1\\) and get the same effect.\n\nDefinition 5.5 Multilinear regression is the approximation \\[\ny \\approx \\hat{f}(\\bfx) = w_1 x_1 + w_2x_2 + \\cdots w_d x_d = \\bfw^T\\bfx,\n\\] for a constant vector \\(\\bfw\\) known as the weight vector.\n\n\n\n\n\n\n\nNote\n\n\n\nMultilinear regression is also simply called linear regression in many contexts.\n\n\nAs before, we find the unknown weight vector \\(\\bfw\\) by minimizing a loss function. To create the least-squares loss function, we use \\(\\bfx_i\\) to denote the \\(i\\)th row of an \\(n\\times d\\) feature matrix \\(\\bfX\\). Then\n\\[\nL(\\bfw) = \\sum_{i=1}^n (y_i - \\hat{f}(\\bfx_i))^2 = \\sum_{i=1}^n (y_i - \\bfx_i^T\\bfw)^2.\n\\]\nWe introduce a shorthand notation that is actually the backbone of linear algebra.\n\nDefinition 5.6 Given an \\(n\\times d\\) matrix \\(\\bfX\\) with rows \\(\\bfx_1,\\ldots,\\bfx_n\\) and a \\(d\\)-vector \\(\\bfw\\), the product \\(\\bfX\\bfw\\) is defined by \\[\n\\bfX \\bfw =\n\\begin{bmatrix}\n    \\bfx_1^T\\bfw \\\\ \\bfx_2^T\\bfw \\\\ \\vdots \\\\ \\bfx_n^T\\bfw\n\\end{bmatrix}.\n\\]\n\nWe now have the compact expression\n\\[\nL(\\bfw) = \\twonorm{\\bfX \\bfw- \\bfy}^2.\n\\tag{5.6}\\]\nAs in the \\(d=1\\) case, minimizing the loss is equivalent to solving a linear system of equations known as the normal equations for \\(\\bfw\\). We do not present them here.\n\n5.2.1 Case study: Advertising and sales\nHere we load data about advertising spending on different media in many markets:\n\nads = pd.read_csv(\"_datasets/advertising.csv\")\nads.head(8)\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n5\n8.7\n48.9\n75.0\n7.2\n\n\n6\n57.5\n32.8\n23.5\n11.8\n\n\n7\n120.2\n19.6\n11.6\n13.2\n\n\n\n\n\n\n\nPairwise scatter plots yield some hints about what to expect from this dataset:\n\nsns.pairplot(data=ads, height=2);\n\n\n\n\nThe clearest association between Sales and spending is with TV. So we first try a univariate linear fit of sales against TV spending alone:\n\nX = ads[ [\"TV\"] ]    # has to be a frame, so [\"TV\"] not \"TV\"\ny = ads[\"Sales\"]\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X, y)\nprint(\"R^2 score:\", f\"{lm.score(X, y):.4f}\")\nprint(\"Regression weight:\", lm.coef_)\n\nR^2 score: 0.8122\nRegression weight: [0.05546477]\n\n\nThe coefficient of determination is already quite good. Since we are going to do multiple fits with different features, we write a function that does the grunt work:\n\ndef regress(lm, data, y, features):\n    X = data[features]\n    lm.fit(X, y)\n    R2 = lm.score(X,y)\n    print(\"R^2 score:\", f\"{R2:.5f}\")\n    print(\"Regression weights:\")\n    print( pd.Series(lm.coef_, index=features) )\n    return None\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you run the same lines of code over and over with only a slight change at the beginning, it‚Äôs advisable to put that code into a function. It makes the overall code shorter and easier to understand and change.\n\n\nNext we try folding in Newspaper as well:\n\nregress(lm, ads, y, [\"TV\", \"Newspaper\"])\n\nR^2 score: 0.82364\nRegression weights:\nTV           0.055091\nNewspaper    0.026021\ndtype: float64\n\n\nThe additional feature had very little effect on the quality of fit. We go on to fit using all three features:\n\nregress(lm, ads, y, [\"TV\", \"Newspaper\", \"Radio\"])\n\nR^2 score: 0.90259\nRegression weights:\nTV           0.054446\nNewspaper    0.000336\nRadio        0.107001\ndtype: float64\n\n\nJudging by the weights of the model, it‚Äôs even clearer now that we can explain Sales very well without contributions from Newspaper. In order to reduce model variance, it would be reasonable to leave that column out. Doing so has a negligible effect:\n\nregress(lm, ads, y, [\"TV\", \"Radio\"])\n\nR^2 score: 0.90259\nRegression weights:\nTV       0.054449\nRadio    0.107175\ndtype: float64\n\n\nWhile we have a very good \\(R^2\\) score now, we can try to improve it. We can add an additional feature that is the product of TV and Radio, representing the possibility that these media reinforce one another‚Äôs effects:\n\nX = ads[ [\"Radio\", \"TV\"] ].copy()\nX[\"Radio*TV\"] = X[\"Radio\"]*X[\"TV\"]\nregress(lm, X, y, X.columns)\n\nR^2 score: 0.91404\nRegression weights:\nRadio       0.042270\nTV          0.043578\nRadio*TV    0.000443\ndtype: float64\n\n\n\n\n\n\n\n\nTip\n\n\n\nIn order to modify a frame, it has to be an independent copy, not just a subset of another frame.\n\n\nWe did see some increase in the \\(R^2\\) score, and therefore the combination of both types of spending does have a positive effect on Sales.\n\n\n\n\n\n\nDanger\n\n\n\nWe have to be careful interpreting the magnitudes of regression coefficients. These are sensitive to the scales of the features. For example, distances expressed in meters would have a coefficient that is 1000 times larger than the same distances expressed in kilometers.\nComparisons of coefficients are more meaningful if the features are first standardized.\n\n\nInterpreting linear regression is a major topic in statistics. There are tests that can lend more precision and rigor to the brief discussion above.\n\n\n5.2.2 Polynomial regression\nA special case of multilinear regression is when there is initially a single predictor variable \\(t\\), and then we define\n\\[\nx_1 = t^0, \\, x_2 = t^1, \\ldots, x_d = t^{d-1}.\n\\]\nThis makes the regressive approximation into\n\\[\ny \\approx w_1 + w_2 t + \\cdots + w_d t^{d-1},\n\\]\nwhich is a polynomial of degree \\(d-1\\). This allows representation of data that depends on \\(t\\) in ways more complicated than a straight line. However, it can lead to overfitting if taken too far.\n\n\n5.2.3 Case study: Fuel efficiency\nWe return to the data set regarding the fuel efficiency of cars:\n\ncars = sns.load_dataset(\"mpg\").dropna()\ncars.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nusa\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nusa\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nusa\nford torino\n\n\n\n\n\n\n\nAs we would expect, horsepower and miles per gallon are negatively correlated. However, the relationship is not well captured by a straight line:\n\nsns.lmplot(data=cars, x=\"horsepower\", y=\"mpg\");\n\n\n\n\nA cubic polynomial produces a much more plausible fit, especially on the right half of the plot:\n\nsns.lmplot(data=cars, x=\"horsepower\", y=\"mpg\", order=3);\n\n\n\n\nIn order to produce the cubic fit within sklearn, we use the PolynomialFeatures preprocessor in a pipeline. If the original predictor variable is \\(t\\), then the preprocessor will create features for \\(1\\), \\(t\\), \\(t^2\\), and \\(t^3\\). (Since the constant feature is added in, we don‚Äôt need to fit the intercept with the linear regressor.)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX = cars[ [\"horsepower\"] ]\ny = cars[\"mpg\"]\nlm = LinearRegression(fit_intercept=False)\ncubic = make_pipeline(PolynomialFeatures(degree=3), lm)\ncubic.fit(X, y)\n\nquery = pd.DataFrame([200], columns=X.columns)\nprint(\"prediction at hp=200:\", cubic.predict(query))\n\nprediction at hp=200: [12.90220247]\n\n\nThe prediction above is consistent with the earlier figure.\nWe can get the coefficients of the cubic polynomial from the trained regressor:\n\ncubic[1].coef_\n\narray([ 6.06847849e+01, -5.68850128e-01,  2.07901126e-03, -2.14662591e-06])\n\n\nThe coefficients go in order of increasing degree.\nIf a cubic polynomial can fit better than a line, it‚Äôs plausible that increasing the degree more will lead to even better fits. In fact, the training error can only go down, because a lower-degree polynomial case is a subset of a higher-degree case.\nTo explore the effect of degree, we split into train and test sets:\n\nfrom sklearn.metrics import mean_squared_error\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.2, random_state=0\n    )\n\nfor deg in range(2,11):\n    poly = make_pipeline(PolynomialFeatures(degree=deg), lm)\n    poly.fit(X_tr, y_tr)\n    MSE = mean_squared_error(y_te, poly.predict(X_te))\n    print(f\"MSE for degree {deg:2}: {MSE:.3f}\")\n\nMSE for degree  2: 16.013\nMSE for degree  3: 15.911\nMSE for degree  4: 15.819\nMSE for degree  5: 15.653\nMSE for degree  6: 15.649\nMSE for degree  7: 15.593\nMSE for degree  8: 18.177\nMSE for degree  9: 28.510\nMSE for degree 10: 55.261\n\n\nThe results above are a clear example of overfitting and the bias‚Äìvariance tradeoff. A plot of the degree-10 fit shows that the polynomial becomes more oscillatory:\n\nsns.lmplot(data=cars, x=\"horsepower\", y=\"mpg\", order=10);\n\n\n\n\nIn the above plot, note the widening of the confidence intervals near the ends of the domain, indicating increased variance in the predictions.\nNext, we keep more of the original data features and pursue a multilinear fit. We chain it with a StandardScaler so that all columns have equal mean and scale:\n\ndef fitcars(model, cars, features):\n    X = cars[features]\n    X_tr, X_te, y_tr, y_te = train_test_split(\n        X, y,\n        test_size=0.2, random_state=0\n        )\n    model.fit(X_tr, y_tr)\n    MSE = mean_squared_error(y_te, model.predict(X_te))\n    print(f\"MSE: {MSE:.3f}\")\n    return None\n\nfeatures = [\"horsepower\", \"displacement\", \"cylinders\", \"weight\"]\nlm = LinearRegression(fit_intercept=True)\npipe = make_pipeline(StandardScaler(), lm)\nfitcars(pipe, cars, features)\n\nMSE: 18.624\n\n\nThe fit here is actually a little worse than the low-degree fits based on horsepower alone. However, by comparing the coefficients of the individual features, some interesting information emerges:\n\npd.Series(pipe[1].coef_, index=features)\n\nhorsepower     -1.587584\ndisplacement    0.191193\ncylinders      -0.594598\nweight         -4.771222\ndtype: float64\n\n\nWe now have a hypothesis that weight is the most significant negative factor for MPG, and by a wide margin.\nFinally, we can combine the use of multiple features and higher degree:\n\npipe = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(degree=2),\n    lm\n    )\nfitcars(pipe, cars, features)\n\nMSE: 14.794\n\n\nThis is our best regression fit so far, by a mile."
  },
  {
    "objectID": "regression.html#regularization",
    "href": "regression.html#regularization",
    "title": "5¬† Regression",
    "section": "5.3 Regularization",
    "text": "5.3 Regularization\nAs a general term, regularization refers to modifying something that is difficult to compute accurately with something more tractable. For learning models, regularization is a common way to combat overfitting.\nSuppose we had an \\(\\real^{n\\times 4}\\) feature matrix in which the features are identical; that is, the predictor variables satisfy \\(x_1=x_2=x_3=x_4\\), and suppose the target \\(y\\) also equals \\(x_1\\). Clearly, we get a perfect regression if we use\n\\[\ny = 1x_1 + 0x_2 + 0x_3 + 0x_4.\n\\]\nBut an equally good regression is\n\\[\ny = \\frac{1}{4}x_1 + \\frac{1}{4}x_2 + \\frac{1}{4}x_3 + \\frac{1}{4}x_4.\n\\]\nFor that matter, so is\n\\[\ny = 1000x_1 - 500x_2 - 500x_3 + 1x_4.\n\\]\nA problem with more than one valid solution is called ill-posed. If we made tiny changes to the predictor variables in this thought experiment, the problem would technically be well-posed, but there would be a wide range of solutions that were very nearly correct, in which case the problem is said to be ill-conditioned; for practical purposes, it remains just as difficult.\nThe poor conditioning can be regularized away by modifying the least-squares loss function to penalize complexity in the model, in the form of excessively large regression coefficients. The common choices are ridge regression,\n\\[\nL(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\twonorm{\\bfw}^2,\n\\]\nand LASSO,\n\\[\nL(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\onenorm{\\bfw}.\n\\]\nAs \\(\\alpha\\to 0\\), both forms revert to the usual least-squares loss, but as \\(\\alpha \\to \\infty\\), the optimization becomes increasingly concerned with prioritizing a small result for \\(\\bfw\\).\nWhile ridge regression is an easier function to minimize quickly, LASSO has an interesting advantage, as illustrated in this figure.\n\nLASSO tends to produce sparse results, meaning that some of the regression coefficients are zero or negligible. These zeros indicate predictor variables that have minor predictive value, which can be valuable information in itself. Moreover, when regression is run without these variables, there may be little effect on the bias, but a reduction in variance.\n\n5.3.1 Case study: Diabetes\nWe‚Äôll apply regularized regression to data collected about the progression of diabetes:\n\ndiabetes = datasets.load_diabetes(as_frame=True)[\"frame\"]\ndiabetes.head(10)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n5\n-0.092695\n-0.044642\n-0.040696\n-0.019442\n-0.068991\n-0.079288\n0.041277\n-0.076395\n-0.041176\n-0.096346\n97.0\n\n\n6\n-0.045472\n0.050680\n-0.047163\n-0.015999\n-0.040096\n-0.024800\n0.000779\n-0.039493\n-0.062917\n-0.038357\n138.0\n\n\n7\n0.063504\n0.050680\n-0.001895\n0.066629\n0.090620\n0.108914\n0.022869\n0.017703\n-0.035816\n0.003064\n63.0\n\n\n8\n0.041708\n0.050680\n0.061696\n-0.040099\n-0.013953\n0.006202\n-0.028674\n-0.002592\n-0.014960\n0.011349\n110.0\n\n\n9\n-0.070900\n-0.044642\n0.039062\n-0.033213\n-0.012577\n-0.034508\n-0.024993\n-0.002592\n0.067737\n-0.013504\n310.0\n\n\n\n\n\n\n\nThe features in this dataset were standardized, making it easy to compare the magnitudes of the regression coefficients.\nFirst, we look at basic linear regression on all ten predictive features in the data:\n\nX = diabetes.drop(\"target\", axis=1)\ny = diabetes[\"target\"]\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.2, random_state=2\n    )\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_tr, y_tr)\nprint(f\"linear model CoD score: {lm.score(X_te, y_te):.4f}\")\n\nlinear model CoD score: 0.4399\n\n\nFirst, we find that ridge regression can improve the score a bit:\n\nfrom sklearn.linear_model import Ridge\n\nrr = Ridge(alpha=0.5)\nrr.fit(X_tr, y_tr)\nprint(f\"ridge CoD score: {rr.score(X_te, y_te):.4f}\")\n\nridge CoD score: 0.4411\n\n\nRidge regularization added a penalty for the 2-norm of the regression coefficients vector. Accordingly, the regularized solution has smaller coefficients:\n\nfrom numpy.linalg import norm\nprint(f\"2-norm of unregularized coefficients: {norm(lm.coef_):.1f}\")\nprint(f\"2-norm of ridge coefficients: {norm(rr.coef_):.1f}\")\n\n2-norm of unregularized coefficients: 1525.2\n2-norm of ridge coefficients: 605.9\n\n\nAs we continue to increase the regularization parameter, the method becomes increasingly obsessed with keeping the coefficient vector small and pays ever less attention to the data:\n\nfor alpha in [0.25, 0.5, 1, 2]:\n    rr = Ridge(alpha=alpha)    # more regularization\n    rr.fit(X_tr, y_tr)\n    print(f\"alpha = {alpha:.2f}\")\n    print(f\"2-norm of coefficient vector: {norm(rr.coef_):.1f}\")\n    print(f\"ridge regression CoD score: {rr.score(X_te, y_te):.4f}\")\n    print()\n\nalpha = 0.25\n2-norm of coefficient vector: 711.7\nridge regression CoD score: 0.4527\n\nalpha = 0.50\n2-norm of coefficient vector: 605.9\nridge regression CoD score: 0.4411\n\nalpha = 1.00\n2-norm of coefficient vector: 480.8\nridge regression CoD score: 0.4078\n\nalpha = 2.00\n2-norm of coefficient vector: 353.5\nridge regression CoD score: 0.3478\n\n\n\nLASSO penalizes the 1-norm of the coefficient vector. Here‚Äôs a LASSO regression fit:\n\nfrom sklearn.linear_model import Lasso\nlass = Lasso(alpha=0.1)\nlass.fit(X_tr, y_tr)\nR2 = lass.score(X_te, y_te)\nprint(f\"LASSO model CoD score: {R2:.4f}\")\n\nLASSO model CoD score: 0.4335\n\n\nA validation curve suggests modest gains in the \\(R^2\\) score as the regularization parameter is varied:\n\nkf = KFold(n_splits=4, shuffle=True, random_state=0)\nalpha = np.linspace(0,0.1,80)[1:]  # exclude alpha=0\n\n_,scores = validation_curve(\n    lass,\n    X_tr, y_tr,\n    cv=kf,\n    n_jobs=-1,\n    param_name=\"alpha\", param_range=alpha\n    )\n\nsns.relplot(x=alpha, y=np.mean(scores, axis=1) );\n\n\n\n\nMoreover, while ridge regression still used all of the features, LASSO put zero weight on three of them:\n\nlass = Lasso(alpha=0.05)\nlass.fit(X_tr, y_tr)\npd.DataFrame( {\n    \"feature\":X.columns,\n    \"ridge\":rr.coef_,\n    \"LASSO\":lass.coef_\n    } )\n\n\n\n\n\n\n\n\nfeature\nridge\nLASSO\n\n\n\n\n0\nage\n43.113029\n-0.000000\n\n\n1\nsex\n-23.953301\n-155.276227\n\n\n2\nbmi\n199.535945\n529.173009\n\n\n3\nbp\n144.586873\n313.419043\n\n\n4\ns1\n25.977923\n-132.507438\n\n\n5\ns2\n2.751708\n-0.000000\n\n\n6\ns3\n-106.337626\n-165.167100\n\n\n7\ns4\n89.526889\n0.000000\n\n\n8\ns5\n185.660175\n580.262391\n\n\n9\ns6\n85.576399\n30.557703\n\n\n\n\n\n\n\nWe can rank the relative importance of the features by ordering them in terms of decreasing coefficient magnitude:\n\n# Get the permutation that sorts values in increasing order.\norder = np.argsort( np.abs(lass.coef_) )  \norder = order[::-1]    # reverse the order\npd.Series( order, index=X.columns )\n\nage    8\nsex    2\nbmi    3\nbp     6\ns1     1\ns2     4\ns3     9\ns4     7\ns5     5\ns6     0\ndtype: int64\n\n\nThe last three features were dropped by LASSO:\n\nzeroed = X.columns[order[-3:]]\nprint(zeroed)\n\nIndex(['s4', 's2', 'age'], dtype='object')\n\n\nNow we can drop these features from the dataset:\n\nX_tr_reduced = X_tr.drop(zeroed, axis=1)\nX_te_reduced = X_te.drop(zeroed, axis=1)\n\nReturning to a fit with no regularization, we find that little is lost by using the reduced feature set:\n\nprint(f\"original linear model score: {lm.score(X_te,y_te):.4f}\")\nlm.fit(X_tr_reduced, y_tr)\nR2 = lm.score(X_te_reduced, y_te)\nprint(f\"reduced linear model score: {R2:.4f}\")\n\noriginal linear model score: 0.4399\nreduced linear model score: 0.4388"
  },
  {
    "objectID": "regression.html#nonlinear-regression",
    "href": "regression.html#nonlinear-regression",
    "title": "5¬† Regression",
    "section": "5.4 Nonlinear regression",
    "text": "5.4 Nonlinear regression\nMultilinear regression limits the representation of the dataset to a function of the form \\[\n\\hat{f}(\\bfx) = \\bfw^T \\bfx.\n\\] This is a linear function, meaning that two key properties are satisfied. For all possible vectors \\(\\bfu,\\bfv\\) and numbers \\(c\\),\n\n\\(\\hat{f}(\\bfu + \\bfv) = \\hat{f}(\\bfu) + \\hat{f}(\\bfv)\\),\n\\(\\hat{f}(c\\bfu) = c \\hat{f}(\\bfu)\\).\n\nThese properties are the essence of what makes a function easy to manipulate, solve for, and analyze. For our particular \\(\\hat{f}\\), they follow easily from how the inner product is defined. For example, \\[\n\\hat{f}(c\\bfu) = (c\\bfu)^T\\bfw = \\sum_{i=1}^d (cu_i) w_i = c \\sum_{i=1}^d u_i w_i = c(\\bfu^T\\bfw) = c \\hat{f}(\\bfu).\n\\]\nOne benefit of the linear approach is that the dependence of the weight vector \\(\\bfw\\) on the regressed data is also linear, which makes solving for it relatively straightforward.\nAs the simplest type of multidimensional function, linear relationships are a good first resort. Furthermore, we can augment the features with powers in order to get polynomial relationships. However, that approach becomes infeasible for more than 2 or 3 dimensions, because the number of polynomial terms needed explodes. While there is a way around this restriction known as the kernel trick, that‚Äôs beyond our mathematical scope here.\nAlternatively, we can resort to fully nonlinear regression methods. Two of them come from generalizations of our staple classifiers.\n\n5.4.1 Nearest neighbors\nTo use kNN for regression, we find the \\(k\\) nearest examples as with classification, but replace voting on classes with the mean or median of the neighboring values. A simple example confirms that the resulting approximation is not linear.\n\nExample 5.3 Suppose we have just two samples with one-dimensional features: \\(x_1=0\\) and \\(x_2=2\\), and let the corresponding sample values be \\(y_1=0\\) and \\(y_2=1\\). Using kNN with \\(k=1\\), the resulting approximation \\(\\hat{f}(x)\\) is \\[\n\\hat{f}(x) =\n\\begin{cases}\n    0, & x < 1, \\\\\n    \\tfrac{1}{2}, & x=1, \\\\  \n    1, & x > 1.\n\\end{cases}\n\\] (Convince yourself that the result is the same whether the mean or the median is used.) Thus, for instance, \\(\\hat{f}(1.2)=1\\), while \\(2\\hat{f}(0.6) = 0\\), which is not equal to \\(\\hat{f}(2 \\cdot 0.6)\\).\n\nkNN regression can produce a function that conforms itself to the training data much more closely than a linear regressor does. This can both decrease bias and increase variance, especially for small values of \\(k\\). As illustrated in the following video, increasing \\(k\\) flattens out the approximation, decreasing variance while increasing bias.\n\nAs with classification, we can choose the norm to use and whether to weight the neighbors equally or by inverse distance. As a reminder, it is usually advisable to work with z-scores for the features rather than raw data.\n\nExample 5.4 We return again to the dataset of cars and their fuel efficiency. A linear regression on four quantitative features is only OK:\n\ncars = sns.load_dataset(\"mpg\").dropna()\nfeatures = [\"displacement\", \"horsepower\", \"weight\", \"acceleration\"]\nX = cars[features]\ny = cars[\"mpg\"]\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.2, random_state=0\n    )\n\nlm = LinearRegression()\nlm.fit(X_tr, y_tr)\nprint(f\"linear model CoD: {lm.score(X_te, y_te):.4f}\")\n\nlinear model CoD: 0.6928\n\n\nNext we try a kNN regressor, doing a grid search to find good hyperparameters:\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nkf = KFold(n_splits=6, shuffle=True, random_state=1)\ngrid = {\n    \"kneighborsregressor__n_neighbors\": range(2, 25),\n    \"kneighborsregressor__weights\": [\"uniform\", \"distance\"] \n    }\nknn = make_pipeline( StandardScaler(), KNeighborsRegressor() )\noptim = GridSearchCV(\n    knn,\n    grid, \n    cv=kf, \n    n_jobs=-1\n    )\noptim.fit(X_tr, y_tr)\n\nprint(f\"best kNN CoD: {optim.score(X_te, y_te):.4f}\")\n\nbest kNN CoD: 0.7439\n\n\nAs you can see above, we got some improvement over the linear regressor.\n\n\n\n5.4.2 Decision tree\nRecall that a decision tree recursively divides the examples into subsets. As with kNN, we can replace taking a classification vote over a leaf subset with taking a mean or median of the values. But the method of determining splits needs to be changed as well.\nInstead of using a measure of subset impurities to determine the best split, the split is chosen to cause the greatest reduction in dispersion within the two subsets. The most common choices for the dispersion measure \\(H\\) are:\n\nIf using the mean of subset values, then let \\(H\\) be standard deviation.\nIf using the median of subset values, then let \\(H\\) be mean absolute deviation (MAD), defined as \\[\n\\text{MAD} = \\frac{1}{m} \\sum_{i=1}^m | t_i - t_\\text{med} |\n\\] for any list of values \\(t_1,\\ldots,t_m\\) and \\(t_\\text{med}\\) equal to the median value.\n\nAs with classification, a proposal to split into subsets \\(S\\) and \\(T\\) is assigned the weighted score \\[\nQ = |S| H(S) + |T| H(T).\n\\] The split location is chosen to minimize \\(Q\\).\n\nExample 5.5 Suppose we are given the observations \\(x_i=i\\), \\(i=1,\\ldots,4\\), where \\(y_1=2\\), \\(y_2=-1\\), \\(y_3=1\\), \\(y_4=0\\). Let‚Äôs find the decision tree regressor using medians/MAD.\nThe original value set has median \\(\\frac{1}{2}\\) and gets a weighted dispersion of \\(\\frac{5}{2}(3+3+1+1)=20\\). There are three ways to split the data, depending on where the partition falls in relation to the \\(x_i\\):\n\n\\(S=\\{2\\},\\,T=\\{-1,1,0\\}\\) \\[\n\\begin{split}\nQ &= 1\\left[ \\frac{1}{1} |2-2|  \\right] +  3 \\left[ \\frac{1}{3}\\bigl( | -1-0 | + |1-0| + |0-0|  \\bigr)  \\right]\\\\\n&=  0 + 2 = 2.\n\\end{split}\n\\]\n\\(S=\\{2,-1\\},\\, T=\\{1,0\\}\\) \\[\n\\begin{split}\n  Q &= 2\\left[ \\frac{1}{2}\\bigl( \\left| 2-\\tfrac{1}{2} \\right| + \\left| -1-\\tfrac{1}{2} \\right| \\bigr)  \\right] +  2 \\left[ \\frac{1}{2}\\bigl( \\left|1-\\tfrac{1}{2} \\right| + \\left|0-\\tfrac{1}{2} \\right|  \\bigr)  \\right]\\\\\n  &=  3 + 1 = 4.\n\\end{split}\n\\]\n\\(S=\\{2,-1,1\\},\\, T=\\{0\\}\\) \\[\n\\begin{split}\n  Q &= 3\\left[ \\frac{1}{3}\\bigl( \\left| 2-1 \\right| + \\left| -1-1 \\right|+ |1-1| \\bigr)  \\right] +  1 \\left[ \\frac{1}{1} \\left|0-0 \\right|  \\right]\\\\\n  &=  3 + 0 = 3.\n\\end{split}\n\\]\n\nThus, the first split above produces the smallest total dispersion.\n\nTo predict a value for a query \\(x\\), we follow the tree until ending at a leaf, where we use the mean (if dispersion is STD) or median (if dispersion is MAD) of the examples in the leaf.\n\nExample 5.6 Here is some simple 2D data:\n\n\nCode\nrng = default_rng(1)\nx1 = rng.random((10,2))\nx1[:,0] -= 0.25\nx2 = rng.random((10,2))\nx2[:,0] += 0.25\nX = np.vstack((x1,x2))\ny = np.exp(X[:,0]-2*X[:,1]**2+X[:,0]*X[:,1])\n\ndf = pd.DataFrame({\"x‚ÇÅ\":X[:,0],\"x‚ÇÇ\":X[:,1],\"y\":y})\nsns.scatterplot(data=df,x=\"x‚ÇÅ\",y=\"x‚ÇÇ\",hue=\"y\");\n\n\n\n\n\nThe default in sklearn is to use STD as the dispersion measure (called squared_error in sklearn). Here is a shallow tree fitted to the data:\n\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\ndtree = DecisionTreeRegressor(max_depth=2)\ndtree.fit(X, y)\n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n\n\nCode\nfrom matplotlib.pyplot import figure\nfigure(figsize=(17,10), dpi=160)\nplot_tree(dtree,feature_names=[\"x‚ÇÅ\",\"x‚ÇÇ\"]);\n\n\n\n\n\nAll of the original samples end up in one of the four leaves. We can find out the tree node number that each sample ends up at using apply:\n\nleaf = dtree.apply(X)\nprint(leaf)\n\n[3 3 2 5 2 2 3 2 2 2 5 6 5 5 3 5 6 6 2 5]\n\n\nFrom the above we deduce that the leaves are the nodes numbered 2, 3, 5, and 6. With some pandas grouping, we can find out the mean value for the samples within each of these:\n\nleaves = pd.DataFrame( {\"y\": y, \"leaf\": leaf} )\nleaves.groupby(\"leaf\")[\"y\"].mean()\n\nleaf\n2    0.911328\n3    0.270725\n5    2.378427\n6    1.003786\nName: y, dtype: float64\n\n\nAll values of the regressor will be one of the four values above. This is exactly what is done internally by the predict method of the regressor:\n\nprint( dtree.predict(X) )\n\n[0.27072468 0.27072468 0.91132782 2.37842709 0.91132782 0.91132782\n 0.27072468 0.91132782 0.91132782 0.91132782 2.37842709 1.00378567\n 2.37842709 2.37842709 0.27072468 2.37842709 1.00378567 1.00378567\n 0.91132782 2.37842709]\n\n\n\n\nExample 5.7 Continuing with the data from Example¬†5.4, we find that we can do even better with a random forest of decision tree regressors:\n\nX = cars[features]\ny = cars[\"mpg\"]\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.2, random_state=0\n    )\n\nfrom sklearn.ensemble import RandomForestRegressor\n\ngrid = {\n    \"max_depth\": range(3, 8),\n    \"max_samples\": np.arange(0.2, 0.6, 0.1),\n    }\nknn = RandomForestRegressor(n_estimators=60)\noptim = GridSearchCV(\n    knn,\n    grid, \n    cv=kf, \n    n_jobs=-1\n    )\noptim.fit(X_tr, y_tr)\n\nprint(f\"best forest CoD: {optim.score(X_te, y_te):.4f}\")\n\nbest forest CoD: 0.7751"
  },
  {
    "objectID": "regression.html#logistic-regression",
    "href": "regression.html#logistic-regression",
    "title": "5¬† Regression",
    "section": "5.5 Logistic regression",
    "text": "5.5 Logistic regression\nSometimes a regressed value is subject to certain known bounds or other conditions. A major example is probability, which has to be between 0 and 1 (inclusive).\nA linear regressor, \\(\\hat{f}(\\bfx) = \\bfw^T \\bfx\\) for a constant vector \\(\\bfw\\), typically ranges over all of \\((-\\infty,\\infty)\\). In order to get a result that must lie within \\([0,1]\\), we can transform its output using the logistic function, defined as\n\\[\n\\sigma(x) = \\frac{1}{1+e^{-x}}.\n\\]\nThe logistic function has the real line as its domain and takes the form of a smoothed step increasing from 0 to 1:\n\n\n\n\n\nGiven samples of a probability variable \\(p(\\bfx)\\), the regression task is to find a weight vector \\(\\bfw\\) so that \\[\np \\approx \\sigma(\\bfx^T\\bfw).\n\\] The result is known as logistic regression. A common way to use logistic regression is for binary classification. Suppose we have training samples \\((\\bfx_i, y_i)\\), \\(i=1,\\ldots,n\\), where for each \\(i\\) either \\(y_i=0\\) or \\(y_i=1\\). The resulting approximation to \\(p\\) at some query \\(\\bfx\\) can then be interpreted as the probability of observing a 1 at \\(\\bfx\\).\nIn order to fully specify the regressor, we need to specify a loss function to be optimized.\n\n5.5.1 Loss function\nDefining \\(\\hat{p}_i = \\sigma(\\bfx_i^T\\bfw)\\) at all the training points, a straightforward loss function would be \\[\n\\sum_{i=1}^n \\left( \\hat{p}_i - y_i \\right)^2.\n\\] For binary classification, however, it‚Äôs more common to use the cross-entropy loss function \\[\nL(\\bfw) = -\\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i) \\right].\n\\tag{5.7}\\] (The logarithms in Equation¬†5.7 can be in any base, since that choice only affects \\(L\\) by a constant factor.) In cross-entropy loss, sample \\(i\\) contributes \\[\n-\\log(1-\\hat{p}_i)\n\\] if \\(y_i=0\\), which becomes infinite as \\(\\hat{p}_i\\to 1^-\\), and \\[\n-\\log(\\hat{p}_i)\n\\] if \\(y_i=1\\), which becomes infinite as \\(\\hat{p}_i\\to 0^+\\). In words, there is a steep penalty for being almost completely wrong about an observation.\nLogistic regression does have a major disadvantage compared to linear regression: the minimization of loss does not lead to a linear problem for the weight vector \\(\\bfw\\). The difference in practice is usually not a concern, though.\n\n\n5.5.2 Regularization\nAs with other forms of regression, the loss function may be regularized using the ridge or LASSO penalty. The standard formulation is\n\\[\n\\widetilde{L}(\\bfw) = C \\, L(\\bfw) + \\norm{\\bfw},\n\\]\nwhere \\(C\\) is a positive hyperparameter and the vector norm is either the 2-norm (ridge) or 1-norm (LASSO).\n\n\n\n\n\n\nImportant\n\n\n\nThe parameter \\(C\\) functions like the inverse of the regularization parameter \\(\\alpha\\) we used in the linear regressor. It‚Äôs just a different convention chosen historically. As \\(C\\) decreases, the regularization strength increases.\n\n\n\n\n5.5.3 Case study: Personal spam filter\nWe will try logistic regression for a simple spam filter. The data set is based on work and personal emails for one individual. The features are calculated word and character frequencies, as well as the appearance of capital letters.\n\nspam = pd.read_csv(\"_datasets/spambase.csv\")\nspam.head()\n\n\n\n\n\n\n\n\nword_freq_make\nword_freq_address\nword_freq_all\nword_freq_3d\nword_freq_our\nword_freq_over\nword_freq_remove\nword_freq_internet\nword_freq_order\nword_freq_mail\n...\nchar_freq_%3B\nchar_freq_%28\nchar_freq_%5B\nchar_freq_%21\nchar_freq_%24\nchar_freq_%23\ncapital_run_length_average\ncapital_run_length_longest\ncapital_run_length_total\nclass\n\n\n\n\n0\n0.00\n0.64\n0.64\n0.0\n0.32\n0.00\n0.00\n0.00\n0.00\n0.00\n...\n0.00\n0.000\n0.0\n0.778\n0.000\n0.000\n3.756\n61\n278\n1\n\n\n1\n0.21\n0.28\n0.50\n0.0\n0.14\n0.28\n0.21\n0.07\n0.00\n0.94\n...\n0.00\n0.132\n0.0\n0.372\n0.180\n0.048\n5.114\n101\n1028\n1\n\n\n2\n0.06\n0.00\n0.71\n0.0\n1.23\n0.19\n0.19\n0.12\n0.64\n0.25\n...\n0.01\n0.143\n0.0\n0.276\n0.184\n0.010\n9.821\n485\n2259\n1\n\n\n3\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n...\n0.00\n0.137\n0.0\n0.137\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n4\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n...\n0.00\n0.135\n0.0\n0.135\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n\n\n5 rows √ó 58 columns\n\n\n\nWe create a feature matrix and label vector, and split into train/test sets:\n\nX = spam.drop(\"class\", axis=\"columns\")\ny = spam[\"class\"]\n\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.2,\n    shuffle=True, random_state=1\n    )\n\nWhen using norm-based regularization, it‚Äôs good practice to standardize the variables, so we will use scaling pipelines. First we use a large value of \\(C\\) to emphasize the regressive loss over the regularization penalty:\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogr = LogisticRegression(C=100, solver=\"liblinear\")\npipe = make_pipeline(StandardScaler(), logr)\npipe.fit(X_tr, y_tr)\nprint(\"accuracy:\", pipe.score(X_te, y_te))\n\naccuracy: 0.9337676438653637\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe use of the solver keyword is optional, but the solver used above seems to be far faster and more reliable for small datasets than the default.\n\n\nLet‚Äôs look at the most extreme regression coefficients, associating them with the feature names and then sorting the results:\n\ncoef = pd.Series(logr.coef_[0], index=X.columns).sort_values()\nprint(\"most hammy features:\")\nprint(coef[:4])\nprint()\nprint(\"most spammy features:\")\nprint(coef[-4:])\n\nmost hammy features:\nword_freq_george    -24.055810\nword_freq_cs         -8.573920\nword_freq_hp         -3.512677\nword_freq_meeting    -1.940989\ndtype: float64\n\nmost spammy features:\nchar_freq_%23                 1.155189\nchar_freq_%24                 1.262551\ncapital_run_length_longest    1.974990\nword_freq_3d                  2.112905\ndtype: float64\n\n\nThe word ‚Äúgeorge‚Äù is a strong counter-indicator for spam; remember that this data set comes from an individual‚Äôs inbox. Its presence makes the inner product \\(\\bfx^T\\bfw\\) more negative, which drives the logistic function closer to 0. Conversely, the presence of consecutive capital letters increases the inner product and pushes the probability closer to 1.\nThe ultimate predictions by the regressor are all either 0 or 1. But we can also see the forecasted probabilities before thresholding:\n\nprint(\"predicted classes:\")\nprint( pipe.predict(X_tr.iloc[:5,:]) )\nprint(\"\\nprobabilities:\")\nprint( pipe.predict_proba(X_tr.iloc[:5,:]) )\n\npredicted classes:\n[0 0 0 0 0]\n\nprobabilities:\n[[0.53768185 0.46231815]\n [0.99694715 0.00305285]\n [0.63975941 0.36024059]\n [0.99634195 0.00365805]\n [0.93740669 0.06259331]]\n\n\nThe probabilities might be useful for making decisions based on the results. For example, the first instance above was much less certain about the classification than the second. A more skeptical threshold greater than \\(0.54\\) would change the class to 1. As in Section¬†3.5, the probability matrix can be used to create an ROC curve showing the tradeoffs over all thresholds.\nFor a validation-based selection of the best regularization parameter value, we can use LogisticRegressionCV, which is a convenience method for a grid search. You can specify which values of \\(C\\) to search over, or just say how many, as we do here:\n\nfrom sklearn.linear_model import LogisticRegressionCV\n\nlogr = LogisticRegressionCV(\n    Cs=40,    # 40 automatically chosen values of C\n    cv=5, \n    solver=\"liblinear\", \n    n_jobs=-1, random_state=0\n    )\npipe = make_pipeline(StandardScaler(), logr)\npipe.fit(X_tr, y_tr)\n\nprint(f\"best C value: {logr.C_[0]:.3g}\")\nprint(f\"accuracy score: {pipe.score(X_te,y_te):.5f}\")\n\nbest C value: 21.5\naccuracy score: 0.93485\n\n\n\n\n5.5.4 Multiclass case\nWhen there are more than two unique labels possible, logistic regression can be extended through the one-vs-rest (OVR) paradigm we have used previously.\nGiven \\(K\\) classes, there are \\(K\\) binary regressors fit for the outcomes ‚Äúclass 1/not class 1,‚Äù ‚Äúclass 2/not class 2,‚Äù and so on, giving \\(K\\) different coefficient vectors, \\(\\bfw_k\\). Now for a query point \\(\\bfx\\), we can predict probabilities for it being in each class:\n\\[\n\\hat{q}_{k}(\\bfx) = \\sigma(\\bfx^T \\bfw_k), \\qquad k=1,\\ldots,K.\n\\]\nSince the \\(K\\) OVR regressors are done independently, there is no reason to think these probabilities will sum to 1 over all the classes. So we must normalize them:\n\\[\n\\hat{p}_{k}(bfx) = \\frac{\\hat{q}_{k}(\\bfx)}{\\sum_{k=1}^K \\hat{q}_{k}(\\bfx)}.\n\\]\nComputed over a testing set, we get a matrix of probabilities. Each of the rows gives the class probabilities at a single query point, and each of the \\(K\\) columns gives the probability of one class at all the points.\n\nExample 5.8 As a multiclass example, we use a data set about gas sensors recording values over long periods of time:\n\ngas = pd.read_csv(\"_datasets/gas_drift.csv\")\ny = gas[\"Class\"]\nX = gas.drop(\"Class\", axis=\"columns\")\nX_tr, X_te, y_tr, y_te = train_test_split(\n    X, y,\n    test_size=0.2,\n    shuffle=True, random_state=1\n    )\n\nlogr = LogisticRegression(solver=\"liblinear\")\npipe = make_pipeline(StandardScaler(), logr)\npipe.fit(X_tr, y_tr)\nprint(\"accuracy score:\", pipe.score(X_te, y_te))\n\naccuracy score: 0.98705966930266\n\n\nWe can now look at probabilistic predictions for each class:\n\np_hat = pipe.predict_proba(X_te)\ncols = [\"Class \"+str(i) for i in range(1,7)]\npd.DataFrame(p_hat, columns=cols).head(10)\n\n\n\n\n\n\n\n\nClass 1\nClass 2\nClass 3\nClass 4\nClass 5\nClass 6\n\n\n\n\n0\n0.000154\n5.324116e-06\n0.020485\n4.207072e-03\n0.003196\n9.719529e-01\n\n\n1\n0.000004\n9.999198e-01\n0.000074\n6.812414e-07\n0.000002\n5.869686e-17\n\n\n2\n0.008624\n3.847762e-03\n0.000017\n1.282243e-03\n0.001295\n9.849346e-01\n\n\n3\n0.237558\n5.503335e-08\n0.000020\n7.222371e-01\n0.000054\n4.013053e-02\n\n\n4\n0.016611\n3.643991e-02\n0.010126\n2.032083e-01\n0.032053\n7.015607e-01\n\n\n5\n0.002096\n9.976914e-01\n0.000163\n4.242972e-05\n0.000007\n7.264576e-11\n\n\n6\n0.000212\n2.805245e-05\n0.003276\n5.669049e-04\n0.017867\n9.780503e-01\n\n\n7\n0.080161\n7.056994e-08\n0.000025\n8.963731e-01\n0.000091\n2.334934e-02\n\n\n8\n0.002059\n9.640239e-06\n0.000399\n7.548506e-02\n0.002079\n9.199678e-01\n\n\n9\n0.979774\n1.129101e-03\n0.000001\n1.136161e-02\n0.000003\n7.730162e-03\n\n\n\n\n\n\n\nThis allows us to see that the ROC curves are nearly perfect:\n\nresults = []\nfor i, label in enumerate(pipe.classes_):\n    actual = (y_te==label)\n    fp, tp, theta = roc_curve(actual, p_hat[:,i])\n    results.extend( [ (label,fp,tp) for fp,tp in zip(fp,tp) ] )\n\nroc = pd.DataFrame( results, columns=[\"label\", \"FP rate\", \"TP rate\"] )\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None\n    );\n\n\n\n\nBased on the ROC curves, we could choose a high decision threshold to cut down on false positives without losing many true positives."
  },
  {
    "objectID": "regression.html#exercises",
    "href": "regression.html#exercises",
    "title": "5¬† Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5.1 Suppose that the distinct plane points \\((x_i,y_i)\\) for \\(i=1,\\ldots,n\\) are to be fit using a linear function without intercept, \\(\\hat{f}(x)=\\alpha x\\). Use calculus to find a formula for the value of \\(\\alpha\\) that minimizes the sum of squared residuals, \\[ r = \\sum_{i=1}^n (f(x_i)-y_i)^2. \\]\n\n\nExercise 5.2 Suppose that \\(x_1=-2\\), \\(x_2=1\\), and \\(x_3=2\\). Define \\(\\alpha\\) as in Exercise¬†5.1, and define the predicted values \\(\\hat{y}_k=\\alpha x_k\\) for \\(k=1,2,3\\). Express each \\(\\hat{y}_k\\) as a combination of the three values \\(y_1\\), \\(y_2\\), and \\(y_3\\), which remain arbitrary. (This is a special case of a general fact about linear regression: each prediction is a linear combination of the training values.)\n\n\nExercise 5.3 Using the formulas derived in Section¬†5.1, show that the point \\((\\bar{x},\\bar{y})\\) always lies on the linear regression line. (Hint: You only have to show that \\(f(\\bar{x}) = \\bar{y}\\). This can be done without first solving for \\(a\\) and \\(b\\), which is a bit tedious to write out.)\n\n\nExercise 5.4 Suppose that values \\(y_i\\) for \\(i=1,\\ldots,n\\) are to be fit to features \\((u_i,v_i)\\) using a multilinear function \\(f(u,v)=\\alpha u + \\beta v\\). Define the sum of squared residuals \\[\nr = \\sum_{i=1}^n (f(u_i,v_i)-y_i)^2.\n\\] Show that by holding \\(\\alpha\\) is constant and taking a derivative with respect to \\(\\beta\\), and then holding \\(\\beta\\) constant and taking a derivative with respect to \\(\\alpha\\), at the minimum residual we must have \\[\n\\begin{split}\n\\left(\\sum u_i^2 \\right) \\alpha + \\left(\\sum u_i v_i \\right) \\beta &= \\sum u_i y_i, \\\\\n\\left(\\sum u_i v_i \\right) \\alpha + \\left(\\sum v_i^2 \\right) \\beta &= \\sum v_i y_i.\n\\end{split}\n\\]\n\n::::{#exr-regression-regular-no-intercept} Repeat Exercise¬†5.1, but using the regularized residual \\[\n\\tilde{r} = C \\alpha^2 + \\sum_{i=1}^n (f(x_i)-y_i)^2.\n\\]\n\nExercise 5.5 Repeat Exercise¬†5.4, but using the regularized residual \\[\n\\tilde{r} = C (\\alpha^2 + \\beta^2) + \\sum_{i=1}^n (f(u_i,v_i)-y_i)^2.\n\\]\n\n\nExercise 5.6 Given the data set \\((x_i,y_i)=\\{(0,-1),(1,1),(2,3),(3,0),(4,3)\\}\\), find the MAD-based \\(Q\\) score for the following hypothetical decision tree splits.\n(a) \\(x \\le 0.5\\quad\\)\n(b) \\(x \\le 1.5\\quad\\)\n(c) \\(x \\le 2.5\\quad\\)\n(d) \\(x \\le 3.5\\)\n\n\nExercise 5.7 Here are values on an integer lattice.\n\nLet \\(\\hat{f}(x_1,x_2)\\) be a kNN regressor with \\(k=4\\), Euclidean metric, and mean averaging. Carefully sketch a one-dimensional plot of \\(\\hat{f}\\) along the given line.\n(a) \\(\\hat{f}(1.2,t)\\) for \\(2\\le t \\le 2\\)\n(b) \\(\\hat{f}(t,-0.75)\\) for \\(2\\le t \\le 2\\)\n(c) \\(\\hat{f}(t,1.6)\\) for \\(2\\le t \\le 2\\)\n(d) \\(\\hat{f}(-0.25,t)\\) for \\(2\\le t \\le 2\\)\n\n\nExercise 5.8 Here are blue/orange labels on an integer lattice.\n\nLet \\(\\hat{f}(x_1,x_2)\\) be a kNN probabilistic classifier with \\(k=4\\), Euclidean metric, and mean averaging. Carefully sketch a one-dimensional plot of the probability of the blue class along the given line.\n(a) \\(\\hat{f}(1.2,t)\\) for \\(2\\le t \\le 2\\)\n(b) \\(\\hat{f}(t,-0.75)\\) for \\(2\\le t \\le 2\\)\n(c) \\(\\hat{f}(t,1.6)\\) for \\(2\\le t \\le 2\\)\n(d) \\(\\hat{f}(-0.25,t)\\) for \\(2\\le t \\le 2\\)\n\n\nExercise 5.9 Here are some label values and probabilistic predicted categories for them.\n\\[\n\\begin{split}\n    y: &\\quad [0,0,1,1] \\\\\n    \\hat{p}: &\\quad [\\tfrac{1}{4},0,\\tfrac{1}{2},1]\n\\end{split}\n\\]\nUsing base-2 logarithms, calculate the cross-entropy loss for these predictions.\n\n\nExercise 5.10 Let \\(\\bfx=[-1,0,1]\\) and \\(\\bfy=[0,1,0]\\). This is to be fit to a probabilistic predictor \\(\\hat{p}(x) = \\sigma(a x)\\) for parameter \\(a\\).\n(a) Show that the cross-entropy loss function \\(L(a)\\), using natural logarithms, satisfies\n\\[\nL'(a) = \\frac{e^a-1}{e^a+1}.\n\\]\n(b) Explain why part (a) implies that \\(a=0\\) is the global minimizer of the loss \\(L\\).\n(c) Using the result of part (b), simplify the optimum predictor function \\(\\hat{p}\\).\n\n\nExercise 5.11 Let \\(\\bfx=[-1,1]\\) and \\(\\bfy=[0,1]\\). This is to be fit to a probabilistic predictor \\(\\hat{p}(x) = \\sigma(a x)\\) for parameter \\(a\\). Without regularization, the best fit takes \\(a\\to\\infty\\), which makes the predictor become infinitely steep at \\(x=0\\). To combat this behavior, let \\(L\\) be the cross-entropy loss function with LASSO penalty, i.e.,\n\\[\nL(a) = \\ln[1-\\hat{p}(-1)] - \\ln[\\hat{p}(1)] + C |a|,\n\\]\nfor a positive regularization constant \\(C\\).\n(a) Show that \\(L'\\) is never zero for \\(a<0\\).\n(b) Show that if \\(0<C<1\\), then \\(L'\\) has a zero at\n\\[\na=\\ln\\left(\\frac{2}{C}-1\\right).\n\\]\nAssume that this value minimizes \\(L\\).\n(c) Show that the minimizer above is a decreasing function of \\(C\\). (Therefore, increasing \\(C\\) makes the predictor less steep as a function of \\(x\\).)"
  },
  {
    "objectID": "clustering.html#similarity-and-distance",
    "href": "clustering.html#similarity-and-distance",
    "title": "6¬† Clustering",
    "section": "6.1 Similarity and distance",
    "text": "6.1 Similarity and distance\nGiven an \\(n\\times d\\) feature matrix, we want to define disjoint subsets of the \\(\\bfx_i\\) such that the samples within a subset, or cluster, are more similar to one another than they are to members of other clusters.\nThe first decision we have to make is how to measure similarity. When a distance metric is available, we consider similarity to be inversely related to distance. For example, if we have defined a distance function between pairs of vectors as \\(\\dist(\\bfx,\\bfy)\\), then we could define similarity as\n\\[\n\\simil(\\bfx,\\bfy) = \\exp \\left[ - \\frac{\\dist(\\bfx,\\bfy)^2}{2\\sigma^2}  \\right].\n\\]\nThus a distance of zero implies a similarity of 1, while the similarity tends to zero as distance increases. The scaling parameter \\(\\sigma\\) controls the rate of decrease; for instance, when the distance is \\(\\sigma\\), the similarity is \\(e^{-1/2}\\approx 0.6\\).\nThere are ways to define similarity without making use of a distance, but we won‚Äôt be using them.\n\n6.1.1 Distance metrics\n\nDefinition 6.1 A distance metric is a function dist on pairs of vectors that satisfies the following properties for all vectors:\n\n\\(\\dist(\\bfx,\\bfy)=0\\) if and only if \\(\\bfx=\\bfy\\),\n\\(\\dist(\\bfx,\\bfy) = \\dist(\\bfy,\\bfx)\\), and\n\\(\\dist(\\bfx,\\bfy) \\le \\dist(\\bfx,\\bfz) + \\dist(\\bfz,\\bfy)\\), known as the triangle inequality.\n\n\nThese are considered the essential axioms of a distance metric. From them, you can also deduce that the distance function is always nonnegative.\n\n\n\n\n\n\nDanger\n\n\n\nThe term distance metric isn‚Äôt always used carefully to mean a function satisfying the three axioms, however, and some applications use a metric that does not satisfy the triangle inequality.\n\n\nWe already have the distance metric defined by\n\\[\n\\dist(\\bfx,\\bfy) = \\norm{\\bfx-\\bfy}\n\\]\nfor any vector norm.\nAnother proper distance metric is angular distance. Generalizing from 2D and 3D vector geometry, we define the angle \\(\\theta\\) between vectors \\(\\bfu\\) and \\(\\bfv\\) in \\(\\real^d\\) by\n\\[\n\\cos(\\theta) = \\frac{\\mathbf{u}^T\\mathbf{v}}{\\twonorm{\\mathbf{u}} \\, \\twonorm{\\mathbf{v}}}.\n\\tag{6.1}\\]\nThen the quantity \\(\\theta/\\pi\\) is a distance metric. Because arccos is a relatively expensive computational operation, though, it‚Äôs common to use cosine similarity, defined as \\(\\cos(\\theta)\\), and the related cosine distance \\(\\tfrac{1}{2}[1-\\cos(\\theta)]\\), even though the latter does not satisfy the triangle inequality.\nCategorical variables can be included in distance metrics. An ordinal variable is easily converted to equally spaced numerical values, which then may get a standard treatment. Nominal features are often compared using Hamming distance, which is just the total number of features that have different values in the two vectors.\n\n\n6.1.2 Probability distributions\n\nDefinition 6.2 A discrete probability distribution is a vector \\(\\bfx\\) whose components are nonnegative and satisfying \\(\\onenorm{\\bfx}=1\\).\n\nSuch a vector can be interpreted as frequencies or probabilities of observing different classes, for example. We already encountered one way to measure the dissimilarity of two probability distributions, the cross-entropy:\n\\[\n\\operatorname{CE}(\\bfx,\\bfy) = -\\sum_{i=1}^d x_i \\log(y_i).\n\\]\nA related measure is the Kullback‚ÄìLeibler (KL) divergence or relative entropy,\n\\[\n\\operatorname{KL}(\\bfx,\\bfy) = \\sum_{i=1}^d x_i \\log\\left( \\frac{x_i}{y_i} \\right).\n\\]\nWhenever \\(0\\cdot \\log(0)\\) is encountered in the CE or KL definitions, it equals zero, in accordance with its limiting value from calculus.\nNeither cross-entropy nor KL divergence are symmetric in their arguments. But there is a related value called information radius, defined as\n\\[\n\\operatorname{IR}(\\bfu,\\bfv) = \\frac{1}{2} \\left[ \\operatorname{KL}(\\bfu,\\bfz) + \\operatorname{KL}(\\bfv,\\bfz) \\right]\n\\]\nwhere \\(\\bfz=(\\bfu+\\bfv)/2\\). Typically one uses a base-2 logarithm, in which case IR ranges between 0 and 1. The square root of IR is a distance metric.\n\nExample 6.1 Let \\(\\bfu=\\frac{1}{4}[1,3]\\) and \\(\\bfv=\\frac{1}{4}[3,1]\\). Then \\(\\bfz=(\\bfu+\\bfv)/2=[\\tfrac{1}{2},\\tfrac{1}{2}]\\), and\n\\[\n\\begin{split}\n    \\operatorname{KL}(\\bfu,\\bfz)  &= \\tfrac{1}{4} \\cdot \\log \\left( \\frac{1/4}{1/2} \\right) + \\tfrac{3}{4} \\cdot \\log \\left( \\frac{3/4}{1/2} \\right) \\\\ &= \\tfrac{1}{4} \\cdot \\log \\left( \\frac{1}{2} \\right) + \\tfrac{3}{4} \\cdot \\log \\left( \\frac{3}{2} \\right) = -\\tfrac{1}{4} + \\tfrac{3}{4} (\\log(3)-1),\\\\\n    \\operatorname{KL}(\\bfv,\\bfz)  &= \\tfrac{3}{4} (\\log(3)-1) -\\tfrac{1}{4},\\\\\n    \\operatorname{IR}(\\bfu,\\bfu)  &= \\tfrac{3}{4} (\\log(3)-1) -\\tfrac{1}{4} \\approx 0.1887.\n\\end{split}\n\\]\n\n\n\n6.1.3 Distance matrix\n\nDefinition 6.3 Given the feature vectors \\(\\bfx_1,\\ldots,\\bfx_n\\), the pairwise distances between them are collected in the \\(n\\times n\\) distance matrix\n\\[\nD_{ij} = \\text{dist}(\\bfx_i,\\bfx_j).\n\\]\n\nNote that \\(D_{ii}=0\\) and \\(D_{ji}=D_{ij}\\). Many clustering algorithms allow supplying \\(\\mathbf{D}\\) in lieu of the feature vectors.\nOne can analogously define a similarity matrix using the Gaussian kernel. An advantage of similarity is that small values can be rounded down to zero. This has little effect on the results, but can create big gains in execution time and memory usage.\n\nExample 6.2 The distance matrix of our bullseye dataset has some interesting structure:\n\nfrom sklearn.metrics import pairwise_distances\nX = bullseye_data()[[\"x1\", \"x2\"]]\nD2 = pairwise_distances(X, metric=\"euclidean\")   # use 2-norm metric\nax = sns.heatmap(D2)\nax.set_aspect(1);\n\n\n\n\nBecause we set up three geometrically distinct groups of points, the distances of pairs within and between groups are fairly homogeneous. The lower-right corner, for example, shows that points in the outermost ring tend to be separated by the greatest distance.\nIn the 1-norm, the stripes dataset is also a little interesting:\n\nX = stripes_data()[[\"x1\", \"x2\"]]\nD1 = pairwise_distances(X, metric=\"manhattan\")   # use 1-norm metric\nax = sns.heatmap(D1)\nax.set_aspect(1);\n\n\n\n\nPoints in different stripes are always separated by at least the inter-stripe distance, while points within the same stripe have a range of possible distances.\n\n\n\n6.1.4 Distance in high dimensions\nHigh-dimensional space does not conform to some intuitions formed by our experiences in 2D and 3D.\nFor example, consider the unit hyperball \\(\\twonorm{\\bfx}\\le 1\\) in \\(d\\) dimensions. We‚Äôll take it as given that scaling a \\(d\\)-dimensional object by a number \\(r\\) will scale the volume by \\(r^d\\). Then for any \\(r<1\\), the fraction of the unit hyperball‚Äôs volume lying outside the smaller hyperball of fixed radius \\(r\\) is \\(1-r^d\\), which approaches \\(1\\) as \\(d\\to \\infty\\). That is, if we choose points randomly within a hyperball, almost all of them will be near the outer boundary.\nThe volume of the unit hyperball also vanishes as \\(d\\to \\infty\\). This is because the inequality\n\\[\nx_1^2 + x_2^2 + \\cdots + x_d^2 \\le 1,\n\\]\nwhere each \\(x_i\\) is chosen randomly in \\([-1,1]\\), becomes ever harder to satisfy as the number of terms in the sum grows, and the relative occurrence of such points is increasingly rare.\nThere are other, similar mathematical results demonstrating the unexpectedness of distances in high-dimensional space. These go under the colorful name curse of dimensionality, and the advice given in response to them is sometimes stated flatly as, ‚ÄúDon‚Äôt use distance metrics in high-dimensional space.‚Äù\nBut that advice is easy to overstate. The curse is essentially about randomly chosen points, and it is correct that dimensions of noisy or irrelevant features will make many learning algorithms less effective. But if features carry useful information, adding them usually makes matters better, not worse."
  },
  {
    "objectID": "clustering.html#performance-measures",
    "href": "clustering.html#performance-measures",
    "title": "6¬† Clustering",
    "section": "6.2 Performance measures",
    "text": "6.2 Performance measures\nBefore we start generating clusterings, we need to decide how we will evaluate them. Recall that a clustering is simply a partitioning of the sample points into disjoint subsets. If a classification of the samples is available, then it automatically implies a clustering: divide the samples into subsets determined by class membership.\nWe will use some nonstandard terminology that makes the definitions a bit easier to state and read.\n\nDefinition 6.4 We say that two sample points in a clustering are buddies if they are in the same cluster, and strangers otherwise.\n\n\n6.2.1 Rand index and ARI\nIf a trusted or reference clustering is available, then we can compare it to any other clustering result. This allows us to use classification datasets as proving grounds for clustering, although the problems of classification and clustering have different goals (separation versus similarity).\nLet \\(b\\) be the number of pairs that are buddies in both clusterings, and let \\(s\\) be the number of pairs that are strangers in both clusterings. Noting that there are \\(\\binom{n}{2}\\) distinct pairs of \\(n\\) sample points, we define the Rand index by\n\\[\n\\text{RI} = \\frac{b+s}{\\binom{n}{2}}.\n\\]\nOne way to interpret the Rand index is through binary classification: if we define a positive result on a pair of samples to mean ‚Äúin the same cluster‚Äù and a negative result to mean ‚Äúin different clusters‚Äù, then the Rand index is the accuracy of the classifier over all pairs of samples.\n\nExample 6.3 Suppose that samples \\(x_1,x_2,x_4\\) are classified as blue, and \\(x_3,x_5\\) are classified as red. Let‚Äôs compute the Rand index relative to the reference classification for the clustering \\(A=\\{x_1,x_2\\}\\) and \\(B=\\{x_3,x_4,x_5\\}\\).\nHere is a table showing which pairs of samples are buddies in both clusterings (indicated as TP), strangers in both (TN), or neither (F).\n\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\\(x_4\\)\n\\(x_5\\)\n\n\n\n\n\\(x_1\\)\n\nTP\nTN\nF\nTN\n\n\n\\(x_2\\)\n\n\nTN\nF\nTN\n\n\n\\(x_3\\)\n\n\n\nF\nTP\n\n\n\\(x_4\\)\n\n\n\n\nF\n\n\n\\(x_5\\)\n\n\n\n\n\n\n\n\nHence the Rand index is 6/10 = 0.6.\n\nThe Rand index has some attractive features:\n\nIt is symmetric in the two clusterings; it doesn‚Äôt matter which is considered the reference.\nThere is no need to find a correspondence between the clusters in the two clusterings. In fact, the clusterings need not even have the same number of clusters.\nThe value is between 0 (complete disagreement) and 1 (complete agreement).\n\nA weakness of the Rand index is that it can be fairly close to 1 even for a random clustering. The adjusted Rand index is\n\\[\n\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\text{max}(\\text{RI})-E[\\text{RI}]},\n\\]\nwhere the mean and max operations are taken over all possible clusterings. (These values can be worked out exactly by combinatorics.) The value can be negative. An ARI of 0 indicates no better agreement than a random clustering, and an ARI of 1 is complete agreement.\n\n\n6.2.2 Silhouettes\nIf no reference clustering is available, then we must use an intrinsic measurement to assess quality. Suppose \\(\\bfx_i\\) is a sample point. Let \\(\\bar{b}_i\\) be the mean distance between \\(\\bfx_i\\) and its buddies, and let \\(\\bar{r}_i\\) be the mean distance between \\(\\bfx_i\\) and the members of the nearest cluster of strangers. Then the silhouette value of \\(\\bfx_i\\) is\n\\[\ns_i = \\frac{\\bar{r}_i-\\bar{b}_i}{\\max\\{\\bar{r}_i,\\bar{b}_i\\}}.\n\\]\nThis value is between \\(-1\\) (bad) and \\(1\\) (good) for every sample point. A silhouette score is derived by taking a mean of the silhouette values, either per cluster or overall depending on the usage.\n\nExample 6.4 Suppose that two clusters in one dimension are defined as \\(A=\\{-4,-1,1\\}\\) and \\(B=\\{2,6\\}\\). Find the silhouette values of all the samples, the silhouette scores of the clusters, and the overall silhouette score.\n\n\n\n\n\n\n\n\n\n\\(x_i\\)\n\\(\\bar{b}_i\\)\n\\(\\bar{r}_i\\)\n\\(s_i\\)\n\n\n\n\n\\(-4\\)\n\\(\\frac{3+5}{2}\\)\n\\(\\frac{6+10}{2}\\)\n\\(\\frac{8-4}{8}=\\frac{1}{2}\\)\n\n\n\\(-1\\)\n\\(\\frac{3+2}{2}\\)\n\\(\\frac{3+7}{2}\\)\n\\(\\frac{5-2.5}{5}=\\frac{1}{2}\\)\n\n\n\\(1\\)\n\\(\\frac{5+2}{2}\\)\n\\(\\frac{1+5}{2}\\)\n\\(\\frac{3-3.5}{3.5}=-\\frac{1}{7}\\)\n\n\n\\(2\\)\n\\(\\frac{4}{1}\\)\n\\(\\frac{6+3+1}{3}\\)\n\\(\\frac{(10/3)-4}{4}=-\\frac{1}{6}\\)\n\n\n\\(6\\)\n\\(\\frac{4}{1}\\)\n\\(\\frac{10+7+5}{3}\\)\n\\(\\frac{(22/3)-4}{(22/3)}=\\frac{5}{11}\\)\n\n\n\nThe silhouette score of cluster \\(A\\) is\n\\[\n\\frac{1}{3}\\left( \\frac{1}{2} + \\frac{1}{2} - \\frac{1}{7} \\right) \\approx 0.286,\n\\]\nand of cluster \\(B\\) is\n\\[\n\\frac{1}{2}\\left( \\frac{5}{11} - \\frac{1}{6} \\right) \\approx 0.144.\n\\]\nThe overall score is the mean of the five values in the last column, which is about \\(0.229\\).\n\nThe silhouette score is fairly easy to understand and use. However, it relies on distances and tends to favor convex, compact clusters.\n\nExample 6.5 Let‚Äôs use the predefined cluster assignments in our blobs dataset. We will add a column to the data frame that records the silhouette score for each point:\n\nfrom sklearn.metrics import silhouette_samples\n\nblobs = blobs_data()\nX = blobs.drop(\"class\", axis=1)\n\nblobs[\"sil\"] = silhouette_samples(X, blobs[\"class\"])\nsns.relplot(data=blobs,\n    x=\"x1\", y=\"x2\",\n    hue=\"class\",size=\"sil\"\n    );\n\n\n\n\nIn the plot above, the size of each dot shows its silhouette coefficient. Those points which don‚Äôt belong comfortably with their cluster have negative scores and the smallest dots. We can find the average score in each cluster through a grouped mean:\n\nblobs.groupby(\"class\")[\"sil\"].mean()\n\nclass\n0    0.815086\n1    0.641591\n2    0.582871\nName: sil, dtype: float64\n\n\nThese values are ordered as we would expect. Now let‚Äôs create another clustering based on the quadrants of the plane:\n\ndef quad(x,y):\n    if x > 0:\n        if y > 0: return 1\n        else: return 4\n    else:\n        if y > 0: return 2\n        else: return 3\n\nblobs[\"quadrant\"] = [quad(x,y) for (x,y) in zip(blobs.x1, blobs.x2)]\nblobs[\"sil\"] = silhouette_samples(X, blobs[\"quadrant\"])\nsns.relplot(data=blobs, \n    x=\"x1\", y=\"x2\",\n    hue=\"quadrant\",size=\"sil\"\n    );\n\n\n\n\n\nblobs.groupby(\"quadrant\")[\"sil\"].mean()\n\nquadrant\n1    0.654031\n2    0.816362\n3    0.357247\n4    0.095618\nName: sil, dtype: float64\n\n\nEven though the original clustering had three classes, and there are four quadrants, we can still compare them by adjusted Rand index:\n\nfrom sklearn.metrics import adjusted_rand_score\n\nadjusted_rand_score(blobs[\"class\"], blobs[\"quadrant\"])\n\n0.904092765401111\n\n\nNot surprisingly, they are seen as fairly similar.\n\n\nExample 6.6 sklearn has a well-known dataset that contains labeled handwritten digits. Let‚Äôs extract the examples for just the numerals 4, 5, and 6:\n\ndigits = datasets.load_digits(as_frame=True)[\"frame\"]\nkeep = digits[\"target\"].isin([4,5,6])\ndigits = digits[keep]\nX = digits.drop(\"target\", axis=1)\ny = digits.target\ny.value_counts()\n\n5    182\n4    181\n6    181\nName: target, dtype: int64\n\n\nWe can visualize the raw data. Here are some of the 6s:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_digits(X):\n    fig, axes = plt.subplots(4,4)\n    for i in range(4):\n        for j in range(4):\n            row = j + 4*i\n            A = np.reshape(np.array(X.iloc[row,:]),(8,8))\n            sns.heatmap(A,ax=axes[i,j],square=True,cmap=\"gray\",cbar=False)\n            axes[i,j].axis(False)\n    return None\n\nplot_digits(X[y==6])\n\n\n\n\n\nA clustering method won‚Äôt be able to learn from the ground truth labels. In order to set expectations, we should see how well the originally labels cluster the samples. Here are the mean silhouette scores for the clusters.\n\ndigits[\"sil\"] = silhouette_samples(X,y)\ndigits.groupby(\"target\")[\"sil\"].mean()\n\ntarget\n4    0.194477\n5    0.225677\n6    0.327088\nName: sil, dtype: float64\n\n\nAs usual, means can tell us only so much. A look at the distributions of the values reveals more details:\n\nsns.catplot(data=digits,\n    x=\"target\", y=\"sil\",\n    kind=\"violin\"\n    );\n\n\n\n\nThe values are mostly positive, which indicates nearly all of the samples for a digit are at least somewhat closer to each other than to the other samples. The 6s are the most distinct. The existence of values close to and below zero suggest that a clustering algorithm might reconstruct the classification to some extent, but the ground truth may represent something more than geometric distances in feature space.\n\n\n\n\n\n\n\nImportant\n\n\n\nThe universe doesn‚Äôt owe you a clustering. Not all phenomena are amenable to clustering in whatever features you happen to choose.\n\n\nWhile classification just requires us to separate different classes of examples, clustering is more specific and more demanding: examples in a cluster need to be more like each other, or the ‚Äúaverage‚Äù cluster member, than they are like members of other clusters. We should expect that edge cases, even within the training data, will look ambiguous."
  },
  {
    "objectID": "clustering.html#k-means",
    "href": "clustering.html#k-means",
    "title": "6¬† Clustering",
    "section": "6.3 k-means",
    "text": "6.3 k-means\nThe \\(k\\)-means algorithm is one of the best-known and most widely used clustering methods, although it has some serious limitations and drawbacks.\nGiven a sample matrix \\(\\bfX\\) with \\(n\\) rows \\(\\bfx_i\\), the algorithm divides the sample points into disjoint sets \\(C_1,\\ldots,C_k\\), where \\(k\\) is a preselected hyperparameter. Cluster \\(j\\) has a centroid \\(\\bfmu_j\\), which is the mean of the points in \\(C_j\\). Define the inertia of \\(C_j\\) as\n\\[\nI_j = \\sum_{\\bfx\\in C_j} \\norm{ \\bfx - \\bfmu_j }_2^2.\n\\]\nThe goal of the algorithm is to choose the clusters in order to minimize the total inertia,\n\\[\nI = \\sum_{j=1}^k I_j.\n\\]\n\n\nExample 6.7 Let \\(k=2\\). Given the values \\(-3,-2,-1,2,5,7\\), we might cluster \\(\\{-3,-2,-1\\}\\) and \\(\\{2,5,7\\}\\). The total inertia is then\n\\[\n\\left[  (-3+2)^2 + (-2+2)^2 + (-1+2)^2   \\right]  + \\left[  \\bigl(2-\\tfrac{14}{3}\\bigr)^2 + \\bigl(5-\\tfrac{14}{3}\\bigr)^2 + \\bigl(7-\\tfrac{14}{3}\\bigr)^2   \\right] = 2 + \\frac{124}{9} = 15.78.\n\\]\nIf we instead cluster as \\(\\{-3,-2,-1,2\\}\\) and \\(\\{5,7\\}\\), then the total inertia is\n\\[\n\\left[  (-3+1)^2 + (-2+1)^2 + (-1+1)^2  + (2+1)^2 \\right]  + \\left[   (5-6)^2 + (7-6)^2   \\right] = 14 + 2 = 16.\n\\]\n\nFinding the minimum inertia among all possible \\(k\\)-clusterings is an infeasible problem to solve exactly at any practical size. Instead, the approach is to iteratively improve from a starting clustering.\n\n6.3.1 Lloyd‚Äôs algorithm\nThe standard method is known as Lloyd‚Äôs algorithm. Starting with values for the \\(k\\) centroids, there is an iteration consisting of two steps:\n\nAssignment Each sample point is assigned to the cluster whose centroid is the nearest. (Ties are broken randomly.)\nUpdate Recalculate the centroids based on the cluster assignments:\n\n\\[\n\\bfmu_j^+ = \\frac{1}{|C_j|} \\sum_{\\bfx\\in C_j} \\bfx.\n\\]\nThe algorithm stops when the assignment step does not change any of the clusters. In practice, this almost always happens quickly. Here is a demonstration:\n\nWhile Lloyd‚Äôs algorithm will find a local minimum of total inertia, in the sense that small changes cannot decrease it, there is no guarantee of converging to the global minimum.\n\n\n6.3.2 Practical issues\n\nInitialization. The performance of \\(k\\)-means depends a great deal on the initial set of centroids. Traditionally, the centroids were chosen as random members of the sample set, but better/more reliable heuristics, such as \\(k\\)-means++, have since become more dominant.\nMultiple runs. All the initialization methods include an element of randomness, and since the Lloyd algorithm usually converges quickly, it is often run with multiple instances of the initialization, and the run with the lowest inertia is kept.\nSelection of \\(k\\). The algorithm treats \\(k\\) as a hyperparameter. Occam‚Äôs Razor dictates preferring smaller values to large ones. There are many suggestions on how to find the choice that gives the most ‚Äúbang for the buck.‚Äù\nDistance metric. The Lloyd algorithm often fails to converge for norms other than the 2-norm, and must be modified if another norm is preferred.\nShape effects. Because of the dependence on the norm, the inertia criterion disfavors long, skinny clusters and clusters of unequal dispersion. Basically, it wants to find spherical blobs (as defined by the metric) of roughly equal size.\n\n\nExample 6.8 Let‚Äôs generate some test blobs:\nWe start \\(k\\)-means with \\(k=2\\) clusters, not presupposing prior knowledge of how the samples were created.\n\nfrom sklearn.cluster import KMeans\n\nX = blobs_data()[[\"x1\", \"x2\"]]\nkm2 = KMeans(n_clusters=2, n_init=\"auto\")\nkm2.fit(X)\n\nKMeans(n_clusters=2, n_init='auto')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=2, n_init='auto')\n\n\nThe fitted clustering object can tell how many iterations were required, and what the final inertia and cluster centroids are:\n\nprint(\"k=2 took\",km2.n_iter_,\"iterations\")\nprint(f\"final inertia: {km2.inertia_:.5g}\")\nprint(\"cluster centroids:\")\nprint(km2.cluster_centers_)\n\nk=2 took 4 iterations\nfinal inertia: 919.12\ncluster centroids:\n[[ 0.03501811  2.21111415]\n [ 1.22650329 -2.88445195]]\n\n\nThere is a predict method that can make cluster assignments for arbitrary points in feature space. In k-means, this just tells you which centroid is closest, i.e., the cluster membership:\n\nkm2.predict([ [-2,-1], [1,2] ])\n\narray([1, 0], dtype=int32)\n\n\nFor the training samples we don‚Äôt need to call predict. Every fitted clustering object has a labels_ property that lists the of cluster index values. We can use those labels to compute silhouette scores:\n\ndef report(clustering):\n    blobs[\"cluster\"] = clustering.labels_\n    blobs[\"sil\"] = silhouette_samples(X, blobs[\"cluster\"])\n    print(f\"inertia: {clustering.inertia_:.5g}\")\n    print(f\"overall silhouette score: {blobs['sil'].mean():.5g}\")\n    sns.catplot(data=blobs,\n        x=\"cluster\", y=\"sil\",\n        kind=\"violin\", height=3.5\n        );\n    sns.relplot(data=blobs,\n        x=\"x1\", y=\"x2\",\n        hue=\"cluster\", size=blobs[\"sil\"], height=3.5\n        );\n    return blobs \n\nreport(km2);\n\ninertia: 919.12\noverall silhouette score: 0.48147\n\n\n\n\n\n\n\n\nIt‚Äôs clear in both plots that cluster 0 is more tightly packed than cluster 1. Let‚Äôs repeat the computation for \\(k=3\\) clusters:\n\nkm3 = KMeans(n_clusters=3, n_init=\"auto\")\nkm3.fit(X)\nreport(km3);\n\ninertia: 203.3\noverall silhouette score: 0.69987\n\n\n\n\n\n\n\n\nThis result shows a modest reduction in silhouette scores for original good cluster, but improvement for the problematic one.\nMoving on to \\(k=4\\) clusters shows clear degradation of the silhouette scores:\n\nkm4 = KMeans(n_clusters=4, n_init=\"auto\")\nkm4.fit(X)\nreport(km4);\n\ninertia: 166.02\noverall silhouette score: 0.60839\n\n\n\n\n\n\n\n\nBased on silhouette scores, then, we would probably stop at \\(k=3\\) clusters.\n\n\nExample 6.9 K-means is expecting to find roughly spherical clusters. When the data do not conform to that model, it tends to perform poorly:\n\nstripes = stripes_data()\nX = stripes[[\"x1\", \"x2\"]]\nresults = pd.DataFrame()\nfor k in [2,3,4]:\n    km = KMeans(n_clusters=k, n_init=\"auto\")\n    km.fit(X)\n    stripes[\"cluster\"] = km.labels_\n    stripes[\"k\"] = k\n    results = pd.concat( (results, stripes) )\n    \nsns.relplot(data=results,\n    x=\"x1\", y=\"x2\",\n    hue=\"cluster\", col=\"k\", height=3.5\n    );\n\n\n\n\nIt‚Äôs not a bad idea to standardize the data. But that‚Äôs no panacea:\n\nresults = pd.DataFrame()\nfor k in [2,3,4]:\n    km = make_pipeline(StandardScaler(), KMeans(n_clusters=k, n_init=\"auto\"))\n    km.fit(X)\n    stripes[\"cluster\"] = km[1].labels_\n    stripes[\"k\"] = k\n    results = pd.concat( (results, stripes) )\n    \nsns.relplot(data=results,\n    x=\"x1\", y=\"x2\",\n    hue=\"cluster\", col=\"k\", height=3.5\n    );\n\n\n\n\nClustering is hard!\n\n\nExample 6.10 We return to the handwriting recognition dataset. Again we keep only the samples labeled 4, 5, or 6:\n\ndigits = datasets.load_digits(as_frame=True)[\"frame\"]\nkeep = digits[\"target\"].isin([4,5,6])\ndigits = digits[keep]\n\nX = digits.drop(\"target\",axis=\"columns\")\ny = digits[\"target\"]\n\nWe fit 3 clusters to the feature matrix:\n\nkm = KMeans(n_clusters=3)\nkm.fit(X)\ndigits[\"kmeans3\"] = km.labels_\ndigits[[\"target\", \"kmeans3\"]].head(9)\n\n\n\n\n\n\n\n\ntarget\nkmeans3\n\n\n\n\n4\n4\n1\n\n\n5\n5\n0\n\n\n6\n6\n2\n\n\n14\n4\n1\n\n\n15\n5\n0\n\n\n16\n6\n2\n\n\n24\n4\n1\n\n\n25\n5\n0\n\n\n26\n6\n2\n\n\n\n\n\n\n\nThe adjusted Rand index suggests that we have reproduced the classification very well:\n\nARI = adjusted_rand_score(y, digits[\"kmeans3\"])\nprint(f\"ARI: {ARI:.4f}\")\n\nARI: 0.9618\n\n\nHowever, that conclusion benefits from our prior knowledge. What if we did not know how many clusters to look for? Let‚Äôs look over a range of \\(k\\) values, recording the final total inertia and the mean silhouette score for each\n\nfrom sklearn.metrics import silhouette_score\nresults = []\nfor k in range(2,8):\n    km = KMeans(n_clusters=k, random_state=0)\n    km.fit(X)\n\n    sil = silhouette_score(X, km.labels_)\n    results.append( [k, sil] )\n\npd.DataFrame(results, columns=[\"k\", \"mean silhouette\"])\n\n\n\n\n\n\n\n\nk\nmean silhouette\n\n\n\n\n0\n2\n0.226800\n\n\n1\n3\n0.251904\n\n\n2\n4\n0.245855\n\n\n3\n5\n0.235199\n\n\n4\n6\n0.188324\n\n\n5\n7\n0.170556\n\n\n\n\n\n\n\nThe silhouette score is maximized at \\(k=3\\), which could be considered a reason to choose 3 clusters. While the score for 4 clusters is fairly close, we should prefer the less complex model."
  },
  {
    "objectID": "clustering.html#hierarchical-clustering",
    "href": "clustering.html#hierarchical-clustering",
    "title": "6¬† Clustering",
    "section": "6.4 Hierarchical clustering",
    "text": "6.4 Hierarchical clustering\nThe idea behind hierarchical clustering is to organize all the sample points into a tree structure called a dendrogram. At the root of the tree is the entire sample set, while each leaf of the tree is a single sample vector. Groups of similar samples are connected as nearby relatives in the tree, with less-similar groups located as more distant relatives.\nDendrograms can be found by starting with the root and recursively splitting, or by starting at the leaves and recursively merging. We will describe the latter approach, known as agglomerative clustering.\nThe algorithm begins with \\(n\\) singleton clusters, i.e., \\(C_i=\\{\\bfx_i\\}\\). Then, the similarity or distance between each pair of clusters is determined. The pair with the minimum distance is merged, and the process repeats.\nCommon ways to define the distance between two clusters \\(C_i\\) and \\(C_j\\) are:\n\n\nsingle linkage\n\n(also called minimum linkage) \\[\n\\displaystyle \\min_{\\bfx\\in C_i,\\,\\bfy\\in C_j} \\{ \\norm{\\bfx-\\bfy } \\}\n\\tag{6.2}\\]\n\n\n\ncomplete linkage\n\n(also called maximum linkage) \\[\n\\displaystyle \\max_{\\bfx\\in C_i,\\,\\bfy\\in C_j} \\{ \\norm{\\bfx-\\bfy} \\}\n\\tag{6.3}\\]\n\n\n\naverage linkage\n\n\\[\n\\displaystyle \\frac{1}{|C_i|\\,|C_j|} \\sum_{\\bfx\\in C_i,\\,\\bfy\\in C_j} \\norm{ \\bfx-\\bfy }\n\\tag{6.4}\\]\n\n\n\nWard linkage\n\nThe increase in inertia resulting from merging \\(C_i\\) and \\(C_j\\), equal to\n\n\n\n\\[\n\\frac{ |C_i|\\,|C_j| }{|C_i| + |C_j|} \\norm{\\bfmu_i - \\bfmu_j}_2^2,\n\\tag{6.5}\\]\nwhere \\(\\bfmu_i\\) and \\(\\bfmu_j\\) are the centroids of \\(C_i\\) and \\(C_j\\).\nAgglomerative clustering with Ward linkage amounts to trying to minimize the increase of inertia with each merger. In that sense, it has the same objective as \\(k\\)-means, but it is usually not as successful at minimizing inertia.\nSingle linkage only pays attention to the gaps between clusters, not the size or spread of them. Complete linkage, on the other hand, wants to keep clusters packed tightly together. Average linkage is a compromise between these extremes. All three of these options can work with a distance matrix in lieu of the original feature matrix.\n\nExample 6.11 Given clusters \\(C_1=\\{-3,-2,-1\\}\\) and \\(C_2=\\{3,4,5\\}\\), we find the different linkages between them:\nWard. The centroids of the clusters are \\(-2\\) and \\(4\\). So the linkage is\n\\[\n\\frac{3\\cdot 3}{3+3} \\, 6^2 = 54.\n\\]\nSingle. The pairwise distances between members of \\(C_1\\) and \\(C_2\\) form a \\(3\\times 3\\) matrix:\n\n\n\n\n-3\n-2\n-1\n\n\n\n\n3\n6\n5\n4\n\n\n4\n7\n6\n5\n\n\n5\n8\n7\n6\n\n\n\nThe single linkage is therefore 4.\nComplete. The maximum of the matrix above is 8.\nAverage. The average value of the matrix entries is \\(54/9\\), which is 6.\n\n\nExample 6.12 Let‚Äôs use 5 sample points in the plane, and agglomerate them by single linkage. The pairwise_distances function converts sample points into a distance matrix:\n\nX = np.array( [[-2,-1] ,[2,-2], [1,0.5], [0,2], [-1,1]] )\nD = pairwise_distances(X, metric=\"euclidean\")\nD\n\narray([[0.        , 4.12310563, 3.35410197, 3.60555128, 2.23606798],\n       [4.12310563, 0.        , 2.6925824 , 4.47213595, 4.24264069],\n       [3.35410197, 2.6925824 , 0.        , 1.80277564, 2.06155281],\n       [3.60555128, 4.47213595, 1.80277564, 0.        , 1.41421356],\n       [2.23606798, 4.24264069, 2.06155281, 1.41421356, 0.        ]])\n\n\nThe minimum value in the upper triangle of the distance matrix is in row 3, column 4 (starting index at 0). So our first merge results in the cluster \\(C_1=\\{\\bfx_3,\\bfx_4\\}\\). The next-smallest entry in the upper triangle is at position \\((2,3)\\), so we want to merge those samples together next, resulting in\n\\[\nC_1=\\{\\bfx_2,\\bfx_3,\\bfx_4\\},\\, C_2 = \\{\\bfx_0\\},\\, C_3=\\{\\bfx_1\\}.\n\\]\nThe next-smallest element in the matrix is at \\((2,4)\\), but those points are already merged, so we move on to position \\((0,4)\\). Now we have\n\\[\nC_1=\\{\\bfx_0,\\bfx_2,\\bfx_3,\\bfx_4\\},\\, C_2 = \\{\\bfx_1\\}.\n\\]\nThe final merge is to combine these.\nThe entire dendrogram can be visualized with seaborn:\n\nsns.clustermap(X, \n    col_cluster=False,\n    dendrogram_ratio=(.75,.15)\n    );\n\n\n\n\nThe horizontal position in the dendrogram above indicates the linkage strength. Note on the right that the ordering of the samples has been changed (so that the lines won‚Äôt cross each other). The two colored columns show a heatmap of the two features of the sample points. Working from right to left, we see the merger of samples 3 and 4, which are then merged with sample 2, etc.\nIn effect, we get an entire family of clusterings by stopping at any linkage value we want. If we chose to stop at value 2.5, for instance, we would have two clusters of size 4 and 1. Or, if we predetermine that we want \\(k\\) clusters, we can stop after \\(n-k\\) merge steps.\n\n\nExample 6.13 We define a function that allows us to run all three linkages for a dataset:\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ndef run_experiment(data):\n    results = pd.DataFrame()\n    for linkage in [\"single\", \"complete\", \"ward\"]:\n        agg = AgglomerativeClustering(n_clusters=3, linkage=linkage)\n        agg.fit( data[[\"x1\", \"x2\"]] )\n        data[\"cluster\"] = agg.labels_\n        data[\"linkage\"] = linkage\n        results = pd.concat( (results, data) )\n    return results\n\nWe first try the blobs seen previously:\n\n\nCode\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(\n    n_samples=[60, 50, 40],\n    centers=[ [-2,3], [3.5,1.5], [1,-3] ],\n    cluster_std=[0.5, 0.9, 1.2],\n    random_state=19716\n    )\nblobs = pd.DataFrame( {\"x1\": X[:,0], \"x2\": X[:,1], \"class\": y} )\nblobs.head()\n\n\n\n\n\n\n\n\n\nx1\nx2\nclass\n\n\n\n\n0\n4.436817\n1.681397\n1\n\n\n1\n3.193103\n1.379978\n1\n\n\n2\n1.400972\n-2.840545\n2\n\n\n3\n-2.147649\n2.716864\n0\n\n\n4\n0.438409\n-3.074790\n2\n\n\n\n\n\n\n\n\nresults = run_experiment(blobs)\nsns.relplot(data=results,\n        x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", height=4\n        )\n\n<seaborn.axisgrid.FacetGrid at 0x1257ce550>\n\n\n\n\n\nAs you can see, the simple linkage was confused by the two blobs that nearly run together.\nNext, we try data points lying in three distinct stripes:\n\nstripes = stripes_data()\nresults = run_experiment(stripes)\nsns.relplot(data=results,\n        x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", height=4\n        )\n\n<seaborn.axisgrid.FacetGrid at 0x125104f50>\n\n\n\n\n\nBoth the complete and Ward linkages are committed to finding compact, roughly spherical clusters. They group together points across stripes rather than clusters extending lengthwise. The single linkage has more flexibility.\nFinally, we try the most demanding test, points that are arranged in rings:\n\nbullseye = bullseye_data()\nresults = run_experiment(bullseye)\np = sns.relplot(data=results,\n        x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", height=4\n        )\np.set(aspect=1);\n\n\n\n\nThe single linkage is the only one with enough geometric flexibility to cluster the rings properly. However, it‚Äôs a delicate situation, and it can be sensitive to individual samples. Here, we add just one point to the bullseye picture and get a big change:\n\nbullseye = pd.concat( ( bullseye, pd.DataFrame({\"x1\": [0], \"x2\": [2.25]}) ) )\nresults = run_experiment(bullseye)\np = sns.relplot(data=results,\n        x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", height=4\n        )\np.set(aspect=1);\n\n\n\n\n\n\n6.4.1 Case study: Penguins\nLet‚Äôs try agglomerative clustering to discover the species of the penguins. First, let‚Äôs recall how many of each species we have.\n\npenguins = sns.load_dataset(\"penguins\").dropna()\nfeatures = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nX = penguins[features]\npenguins[\"species\"].value_counts()\n\nAdelie       146\nGentoo       119\nChinstrap     68\nName: species, dtype: int64\n\n\nOur first attempt is single linkage. Because 2-norm distances are involved, we will use standardization in a pipeline with the clustering method. After fitting, the labels_ property of the cluster object is a vector of cluster assignments.\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nsingle = AgglomerativeClustering(n_clusters=3, linkage=\"single\")\npipe = make_pipeline(StandardScaler(),single)\npipe.fit(X)\npenguins[\"single\"] = single.labels_       # cluster assignments\npenguins.loc[::24,[\"species\", \"single\"]]   # print out some rows\n\n\n\n\n\n\n\n\nspecies\nsingle\n\n\n\n\n0\nAdelie\n0\n\n\n29\nAdelie\n0\n\n\n54\nAdelie\n0\n\n\n78\nAdelie\n0\n\n\n102\nAdelie\n0\n\n\n126\nAdelie\n0\n\n\n150\nAdelie\n0\n\n\n174\nChinstrap\n0\n\n\n198\nChinstrap\n0\n\n\n222\nGentoo\n2\n\n\n247\nGentoo\n2\n\n\n271\nGentoo\n2\n\n\n296\nGentoo\n2\n\n\n320\nGentoo\n2\n\n\n\n\n\n\n\nIt seems that Gentoo is associated with cluster number 2, but the situation with the other species is less clear. Here are the value counts:\n\nprint(\"single linkage results:\")\npenguins[\"single\"].value_counts()\n\nsingle linkage results:\n\n\n0    213\n2    119\n1      1\nName: single, dtype: int64\n\n\nAs we saw with the toy datasets in Example¬†6.13, the single linkage is susceptible to declaring one isolated point to be a cluster, while grouping together other points we would like to separate. Here is the ARI for this clustering, compared to the true classification:\n\nfrom sklearn.metrics import adjusted_rand_score\nARI = adjusted_rand_score(penguins[\"species\"],penguins[\"single\"])\nprint(f\"single linkage ARI: {ARI:.4f}\")\n\nsingle linkage ARI: 0.6506\n\n\nNow let‚Äôs try it with Ward linkage (the default):\n\nward = AgglomerativeClustering(n_clusters=2, linkage=\"ward\")\npipe = make_pipeline(StandardScaler(), ward)\npipe.fit(X)\npenguins[\"ward\"] = ward.labels_\n\nprint(\"Ward linkage results:\")\nprint(penguins[\"ward\"].value_counts())\n\nWard linkage results:\n0    214\n1    119\nName: ward, dtype: int64\n\n\nThis result looks more promising. The ARI confirms that hunch:\n\nARI = adjusted_rand_score(penguins[\"species\"], penguins[\"ward\"])\nprint(f\"Ward linkage ARI: {ARI:.4f}\")\n\nWard linkage ARI: 0.6486\n\n\nIf we guess at the likely correspondence between the cluster numbers and the different species, then we can find the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\ny = penguins[\"species\"]\n# Convert cluster numbers into labels:\ny_hat = penguins[\"ward\"].replace({1:\"Adelie\",0:\"Gentoo\",2:\"Chinstrap\"}) \n\nConfusionMatrixDisplay(confusion_matrix(y,y_hat),display_labels=y.unique()).plot();"
  },
  {
    "objectID": "clustering.html#exercises",
    "href": "clustering.html#exercises",
    "title": "6¬† Clustering",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 6.1 Using only the three axioms of a distance metric, prove that \\(\\dist(\\bfx,\\bfy) \\ge 0\\) for all vectors \\(\\bfx\\) and \\(\\bfy\\). (Hint: apply the triangle inequality to go from \\(\\bfx\\) to \\(\\bfy\\) and back again.)\n\n\nExercise 6.2 Prove that the angular distance between any nonzero vector and itself is zero.\n\n\nExercise 6.3 Find a counterexample showing that cosine distance does not satisfy the triangle inequality. (Hint: it‚Äôs enough to consider some simple vectors in two dimensions.)\n\n\nExercise 6.4 Let \\(c\\) be a positive number, and consider the 12 sample points \\(\\{(\\pm c,\\pm j): j=1,2,3\\}\\). One way to cluster the sample points, which we designate as clustering \\(\\alpha\\), is to split according to the sign of \\(x_1\\). Another way, which we designate as clustering \\(\\beta\\), is to split according to the sign of \\(x_2\\). Compute the inertia of both clusterings. For which values of \\(c\\), if any, does clustering \\(\\alpha\\) have less inertia than clustering \\(\\beta\\)?\n\n\nExercise 6.5 Here is a distance matrix for points \\(\\bfx_1,\\ldots,\\bfx_5\\).\n\\[\n\\left[\n\\begin{array}{ccccc}\n0 & 2 & 4 & 5 & 6 \\\\\n2 & 0 & 2 & 3 & 4 \\\\\n4 & 2 & 0 & 1 & 2 \\\\\n5 & 3 & 1 & 0 & 1 \\\\\n6 & 4 & 2 & 1 & 0 \\\\\n\\end{array}\n\\right]\n\\]\nCompute the average linkage between the clusters with index sets \\(C_1=\\{1,3\\}\\) and \\(C_2=\\{2,4,5\\}\\).\n\n\nExercise 6.6 Perform by hand an agglomerative clustering for the values \\(2,4,5,8,12\\) using single linkage. This means finding the four merge steps needed to convert five singleton clusters into one global cluster."
  },
  {
    "objectID": "networks.html#graphs",
    "href": "networks.html#graphs",
    "title": "7¬† Networks",
    "section": "7.1 Graphs",
    "text": "7.1 Graphs\nIn mathematics, a network is represented as a graph. A graph is a collection of nodes (also called vertices) and edges that connect pairs of nodes. A basic distinction in graph theory is between an undirected graph, in which the edge \\((a,b)\\) is identical to \\((b,a)\\), and a directed graph or digraph, in which \\((a,b)\\) and \\((b,a)\\) are different potential edges. In either type of graph, each edge might be labeled with a numerical value, which results in a weighted graph.\nUndirected, unweighted graphs will give us plenty to handle, and we will not seek to go beyond them. We also will not consider graphs that allow a node to link to itself.\n\n7.1.1 NetworkX\nWe will use the NetworkX package to work with graphs.\n\nimport networkx as nx\n\nOne way to create a graph is from a list of edges.\n\nstar = nx.Graph( [ (1,2),(1,3),(1,4),(1,5),(1,6) ] )\nnx.draw(star, with_labels=True, node_color=\"lightblue\")\n\n\n\n\nAnother way to create a graph is to give the start and end nodes of the edges as columns in a data frame.\n\nnetwork = pd.DataFrame( {'from': [1,2,3,4,5,6], 'to': [2,3,4,5,6,1]} )\nprint(network)\nH = nx.from_pandas_edgelist(network, 'from', 'to')\nnx.draw(H, with_labels=True, node_color=\"lightblue\")\n\n   from  to\n0     1   2\n1     2   3\n2     3   4\n3     4   5\n4     5   6\n5     6   1\n\n\n\n\n\nWe can conversely deconstruct a graph object into its nodes and edges. The results have special types that may need to be converted into sets, lists, or other objects.\n\nprint(\"Nodes as a list:\")\nprint( list(star.nodes) )\nprint(\"\\nNodes as an Index:\")\nprint( pd.Index(star.nodes) )\n\nNodes as a list:\n[1, 2, 3, 4, 5, 6]\n\nNodes as an Index:\nInt64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n\n\nIt‚Äôs also easy to find out which nodes are adjacent to a given node, i.e., connected to it by an edge. The result is that node‚Äôs list of neighbors.\n\nprint( \"Neighbors of node 3 in graph H:\", list(H[3]) )\n\nNeighbors of node 3 in graph H: [2, 4]\n\n\n\n\n7.1.2 Common graph types\nThere are functions that generate different well-studied types of graphs. The first graph constructed above is a star graph, and the graph H above is a cycle graph.\n\nnx.draw(nx.cycle_graph(9))\n\n\n\n\nA cross between the star and the cycle is a wheel graph.\n\nnx.draw(nx.wheel_graph(9))\n\n\n\n\nA complete graph is one that has every possible edge.\n\nK5 = nx.complete_graph(5)\nprint(\"5 nodes,\", nx.number_of_edges(K5), \"edges\")\nnx.draw(K5)\n\n5 nodes, 10 edges\n\n\n\n\n\nIn a graph on \\(n\\) nodes, there are\n\\[\n\\binom{n}{2} = \\frac{n!}{(n-2)!2!} = \\frac{n(n-1)}{2}\n\\]\nunique pairs of distinct nodes. Hence, there are \\(\\binom{n}{2}\\) edges in the undirected complete graph on \\(n\\) nodes.\nA lattice graph has a regular structure, like graph paper.\n\nlat = nx.grid_graph( (5,4) )\nprint(lat.number_of_nodes(), \"nodes,\", lat.number_of_edges(), \"edges\")\nnx.draw(lat, node_size=100)\n\n20 nodes, 31 edges\n\n\n\n\n\nIn an \\(m\\times n\\) lattice graph, there are \\(m-1\\) edges in one direction repeated \\(n\\) times, plus \\(n-1\\) edges in the other direction, repeated \\(m\\) times. Thus there are\n\\[\n(m-1)n + (n-1)m = 2mn-(m+n)\n\\]\nedges altogether.\nThere are different ways to draw a particular graph in the plane, as determined by the positions of the nodes. The default is to imagine that the edges are springs pulling on the nodes. But there are alternatives that may be useful at times.\n\nnx.draw_circular(lat)\n\n\n\n\nAs you can see, it‚Äôs not easy to tell how similar two graphs are by comparing renderings of them.\n\n\n7.1.3 Adjacency matrix\nEvery graph can be associated with an adjacency matrix. Suppose the nodes are numbered from \\(0\\) to \\(n-1\\). The adjacency matrix is \\(n\\times n\\) and has a 1 at position \\((i,j)\\) if node \\(i\\) and node \\(j\\) are adjacent, and a 0 otherwise.\n\nA = nx.adjacency_matrix(star)\nA\n\n<6x6 sparse matrix of type '<class 'numpy.int64'>'\n    with 10 stored elements in Compressed Sparse Row format>\n\n\nThe matrix A is not stored in the format we have been used to. In a large network we would expect most of its entries to be zero, so it makes more sense to store it as a sparse matrix, where we keep track of only the nonzero entries.\n\nprint(A)\n\n  (0, 1)    1\n  (0, 2)    1\n  (0, 3)    1\n  (0, 4)    1\n  (0, 5)    1\n  (1, 0)    1\n  (2, 0)    1\n  (3, 0)    1\n  (4, 0)    1\n  (5, 0)    1\n\n\nWe can easily convert A to a standard array, if it is not too large to fit in memory.\n\nA.toarray()\n\narray([[0, 1, 1, 1, 1, 1],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0]])\n\n\nIn an undirected graph, we have \\(A_{ij}=A_{ji}\\) everywhere, and we say that \\(A\\) is symmetric.\n\n\n7.1.4 Importing networks\nThere are many ways to read graphs from (and write them to) files. For example, here is a friend network among Twitch users.\n\ntwitch = nx.read_edgelist(\"musae_edges.csv\", delimiter=',', nodetype=int)\n\nThe file just imported has a pair of nodes representing one edge on each line. The nodes can have any names at all; by default they are interpreted as strings, which we overrode above to get integer node labels.\n\nprint(\"Twitch network has\", \n    twitch.number_of_nodes(), \n    \"nodes and\",\n    twitch.number_of_edges(),\n    \"edges\"\n    )\n\nTwitch network has 7126 nodes and 35324 edges\n\n\nThis graph is difficult to draw in its entirety. We can zoom in on a subset by selecting a node and its ego graph, which includes its neighbors along with all edges between the captured nodes.\n\nego = nx.ego_graph(twitch, 400)\nnx.draw(ego, with_labels=True, node_size=800, node_color=\"yellow\")\n\n\n\n\nNotice that the nodes of the ego network have the same labels as they did in the graph that it was taken from. We can widen the ego graph to include the ego graphs of all the neighbors:\n\nbig_ego = nx.ego_graph(twitch, 400, radius=2)\nprint(big_ego.number_of_nodes(), \"nodes and\", \n    big_ego.number_of_edges(), \"edges\")\n\npos = nx.spring_layout(big_ego, iterations=60)\nnx.draw(big_ego, \n    pos=pos, width=0.2, node_size=10, node_color=\"purple\")\n\n528 nodes and 1567 edges\n\n\n\n\n\nThe reason for the two-step process in making the drawing above is that computing the node positions via springs takes a hidden computational iteration. By calling that iteration explicitly, we were able to stop it early and save time.\n\n\n7.1.5 Degree and average degree\nThe degree of a node is the number of edges that have the node as an endpoint. Equivalently, it is the number of nodes in its ego graph, minus the original node itself. The average degree of a graph is the mean of the degrees of all of its nodes.\nThe degree property of a graph gives a dictionary-style object of all nodes with their degrees.\n\nego.degree\n\nDegreeView({897: 2, 400: 9, 5394: 2, 3379: 3, 4406: 1, 5079: 1, 6136: 2, 5049: 1, 6107: 1, 639: 2})\n\n\nThe result here can be a bit awkward to work with; it‚Äôs actually a generator of a list, rather than the list itself. (This ‚Äúlazy‚Äù attitude is useful when dealing with very large networks.) So, for instance, we can collect it into a list of ordered tuples:\n\nlist(ego.degree)\n\n[(897, 2),\n (400, 9),\n (5394, 2),\n (3379, 3),\n (4406, 1),\n (5079, 1),\n (6136, 2),\n (5049, 1),\n (6107, 1),\n (639, 2)]\n\n\nIt can be convenient to use a series or frame to keep track of quantities like degree that are associated with nodes.\n\nnodes = pd.Index(ego.nodes)\ndegrees = pd.Series(dict(ego.degree), index=nodes)\nprint(\"average degree of ego graph:\", degrees.mean())\n\naverage degree of ego graph: 2.4\n\n\nThere‚Äôs a much easier way to compute this particular quantity, however. If we sum the degrees of all the nodes in a graph, we must get twice the number of edges in the graph. For \\(n\\) nodes and \\(e\\) edges, the average degree is therefore \\(2m/n\\).\n\ndef average_degree(g):\n    return 2*g.number_of_edges() / g.number_of_nodes()\n\nprint(\"average degree of Twitch network:\", average_degree(twitch))\n\naverage degree of Twitch network: 9.914117316867808\n\n\n\n\n7.1.6 Random graphs\nOne way of understanding a real-world network is by comparing it to ones that are constructed randomly, but according to relatively simple rules. The idea is that if the real network behaves similarly to members of some random family, then perhaps it is constructed according to similar principles.\nAn Erd≈ës-R√©nyi graph (ER graph) includes each individual possible edge with a fixed probability \\(p\\). That is, if you have a weighted coin that comes up heads (100p)% of the time, then you toss the coin for each possible pair of vertices and include their edge if it is heads.\n\nn,p = 50,0.08\nER = nx.erdos_renyi_graph(n,p,seed=2)\nprint(ER.number_of_nodes(),\"nodes,\",ER.number_of_edges(),\"edges\")\nnx.draw_circular(ER,node_size=50,edge_color=\"gray\")\n\n50 nodes, 91 edges\n\n\n\n\n\nSince there are \\(\\binom{n}{2}\\) unique pairs among \\(n\\) nodes, the mean number of edges in an ER graph is\n\\[\np\\binom{n}{2} = \\frac{pn(n-1)}{2}.\n\\]\nThis fact is usually stated in terms of the average node degree, \\(\\bar{k}\\):\n\\[\nE[\\bar{k}] = \\frac{1}{n} pn(n-1) = p(n-1).\n\\]\nThere are two senses of ‚Äúaverage‚Äù going on here: in each graph instance, you find the average degree, then you take the average (expectation, \\(E[\\cdot]\\)) over all random instances. Here is the distribution of \\(\\bar{k}\\) over 10000 instances when its expected value is \\(4.0\\):\n\nn,p = 41,0.1\nkbar = []\nfor iter in range(10000):\n    ER = nx.erdos_renyi_graph(n,p,seed=iter+1001)\n    kbar.append(average_degree(ER))\n\nsns.displot(x=kbar,bins=16);"
  },
  {
    "objectID": "networks.html#clustering",
    "href": "networks.html#clustering",
    "title": "7¬† Networks",
    "section": "7.2 Clustering",
    "text": "7.2 Clustering\n\n\n\n\n\n\nNote\n\n\n\nThe term clustering has a meaning for network analysis that has virtually nothing to do with clustering of numerical data.\n\n\nIn your social networks, your friends are probably more likely to be friends with each other than pure randomness would imply. There are various ways to quantify this precisely, but one of the easiest is the local clustering coefficient, defined for a node \\(i\\) as\n\\[\nC(i) = \\frac{ 2 T(i) }{d_i(d_i-1)}.\n\\]\nIn this formula, \\(d_i\\) is the degree of the node and \\(T(i)\\) is the number of edges between node \\(i\\)‚Äôs neighbors. If \\(d_i=0\\) or \\(d_i=1\\), we set \\(C(i)=0\\).\nEquivalently, \\(T(i)\\) is the number of triangles in the graph that pass through node \\(i\\). Because the subgraph of the neighbors has\n\\[\n\\binom{d_i}{2}\n\\]\npossible edges, the value of \\(C(i)\\) is between 0 and 1.\nHere is a wheel graph to help us explore a bit:\n\nW = nx.wheel_graph(7)\nnx.draw(W,with_labels=True,node_color=\"lightblue\")\n\n\n\n\n\nExample 7.1 Let‚Äôs find the clustering coefficient for each node in the wheel graph drawn above.\nNode 0 in this graph is adjacent to 6 other nodes, and there are 6 triangles passing through it. Thus, its clustering coefficient is\n\\[\nC(0) = \\frac{6}{6 \\cdot 5 / 2} = \\frac{2}{5}.\n\\]\nEvery other node has 3 friends and 2 triangles, so they each have\n\\[\nC(i) = \\frac{2}{3 \\cdot 2 / 2} = \\frac{2}{3}, \\quad i\\neq 0.\n\\]\n\nIn NetworkX, we can manually count the number of edges among neighbors of node 0 by examining the ego subgraph.\n\n    nbrhood = W.subgraph(W[0])  # does not include node 0 itself\n    print(nbrhood.number_of_edges(), \"edges among neighbors of node 0\")\n    nx.draw(nbrhood, with_labels=True, node_color=\"pink\")\n\n6 edges among neighbors of node 0\n\n\n\n\n\nMore directly, the clustering function in NetworkX computes \\(C(i)\\) for any single node, or for all the nodes in a graph.\n\nprint(\"node 0 clustering =\", nx.clustering(W,0))\nprint(\"\\nclustering at each node:\")\nprint( pd.Series(nx.clustering(W), index=W.nodes) )\n\nnode 0 clustering = 0.4\n\nclustering at each node:\n0    0.400000\n1    0.666667\n2    0.666667\n3    0.666667\n4    0.666667\n5    0.666667\n6    0.666667\ndtype: float64\n\n\nIn addition, the average_clustering function will take the average over all nodes of the local clustering values.\n\nprint(\"average clustering =\", nx.average_clustering(W))\n\naverage clustering = 0.6285714285714284\n\n\n\nExample 7.2 Let‚Äôs compute average clustering within multiple ER random graphs.\n\nn,p = 121,1/20\nresults = []\nfor iter in range(400):\n    ER = nx.erdos_renyi_graph(n, p, seed=iter+5000)\n    results.append( nx.average_clustering(ER) )\n\nsns.displot(x=results);\n\n\n\n\nThe distribution above can‚Äôt be normal, because there are hard bounds at 0 and 1, but it looks similar to a normal distribution. The peak is at the value of \\(p\\) used in the simulation, which is not a coincidence.\n\nTheorem 7.1 The expected value of the average clustering in ER graphs of type \\((n,p)\\) is \\(p\\).\n\nA formal proof of this theorem is largely superfluous; considering that each edge in the graph has a probability \\(p\\) of inclusion, that is also the expected fraction of edges that appear within the neighborhood subgraph of any node.\n\n\nExample 7.3 Let‚Äôs examine clustering within the Twitch network.\n\ntwitch = nx.read_edgelist(\"musae_edges.csv\", delimiter=',', nodetype=int)\nn,e = twitch.number_of_nodes(), twitch.number_of_edges()\nkbar = 2*e/n\nprint(n, \"nodes and\", e, \"edges\")\nprint(f\"average degree is {kbar:.3f}\")\n\n7126 nodes and 35324 edges\naverage degree is 9.914\n\n\nComputing the distances between all pairs of nodes in this graph would take a rather long time, so we will estimate the average distance by sampling.\n\ncluster = pd.Series(nx.clustering(twitch),index=twitch.nodes)\nsns.displot(data=cluster);\n\n\n\n\nThe average clustering coefficient is\n\nprint( \"average Twitch clustering:\", cluster.mean() )\n\naverage Twitch clustering: 0.1309282190147198\n\n\nHow does this value compare to an ER graph? If we set the number of nodes and average degree to be the same, then the expected average clustering for ER graphs is \\(p=\\bar{k}/(n-1)\\):\n\nprint( \"average equivalent ER clustering:\", kbar/(n-1) )\n\naverage equivalent ER clustering: 0.0013914550620165345\n\n\nThis is too small by a factor of 100! Clearly, the Twitch graph is not equivalent to a random graph in the sense of ER. From a sociological perspective, of course, this is a ‚Äúno duh‚Äù conclusion.\n\n\n7.2.1 Watts‚ÄìStrogatz graphs\nA Watts‚ÄìStrogatz graph (WS graph) tries to model the small-world phenomenon. A WS graph has three parameters: \\(n\\), an even integer \\(k\\), and a probability \\(q\\).\nImagine \\(n\\) nodes arranged in a circle. Connect each node with an edge to each of its \\(k/2\\) left neighbors and \\(k/2\\) right neighbors. Now we ‚Äúrewire‚Äù some of the edges by visiting each node \\(i\\) in turn. For each edge from \\(i\\) to a neighbor, with probability \\(q\\) replace it with an edge between \\(i\\) and a node chosen at random from all the nodes \\(i\\) is not currently connected to. The idea is to start with tight-knit, overlapping communities, and randomly toss in some far-flung links.\n\nWS = nx.watts_strogatz_graph(40, 6, 0.15, seed=1)\nnx.draw_circular(WS, node_size=100)\n\n\n\n\nBy the nature of the construction, the initial state of the network (before the rewiring phase) is highly clustered. Thus, if \\(q\\) is close to zero, the final graph will retain much of this initial clustering.\n\nn, k = 60, 6\nresults = []\nseed = 0\nfor q in np.arange(0.05, 1.05, 0.05):\n    for iter in range(50):\n        WS = nx.watts_strogatz_graph(n, k, q, seed=seed)\n        results.append( (q, nx.average_clustering(WS)) )\n        seed += 1\n        \nresults = pd.DataFrame( results, columns=[\"q\", \"mean clustering\"] )\n\nprint(\"Mean clustering in WS graphs on 60 nodes:\")\nsns.relplot(data=results,\n    x=\"q\", y=\"mean clustering\",\n    kind=\"line\"\n    );\n\nMean clustering in WS graphs on 60 nodes:\n\n\n\n\n\nLet‚Äôs scale the experiment above up to the size of the Twitch network. Conveniently, the average degree is nearly 10, which is the value we will use in the WS construction. To save computation time, we will use just one WS realization at each value of \\(q\\).\n\nseed = 99999\nn, k = twitch.number_of_nodes(), 10\nfor q in np.arange(0.15, 0.61, 0.05):\n    WS = nx.watts_strogatz_graph(n, k, q, seed=seed)\n    print(f\"q = {q:.2f}, avg WS clustering = {nx.average_clustering(WS):.4f}\")\n    seed += 1\n\nq = 0.15, avg WS clustering = 0.4143\nq = 0.20, avg WS clustering = 0.3418\nq = 0.25, avg WS clustering = 0.2870\n\n\nq = 0.30, avg WS clustering = 0.2326\nq = 0.35, avg WS clustering = 0.1809\nq = 0.40, avg WS clustering = 0.1470\n\n\nq = 0.45, avg WS clustering = 0.1112\nq = 0.50, avg WS clustering = 0.0851\n\n\nq = 0.55, avg WS clustering = 0.0627\nq = 0.60, avg WS clustering = 0.0424\n\n\nThe mean clustering resembles the value of 0.131 for the Twitch network at around \\(q=0.42\\), which we verify using more realizations:\n\nseed = 999\nn,k,q = twitch.number_of_nodes(),10,0.42\ncbar = []\nfor iter in range(10):\n    WS = nx.watts_strogatz_graph(n, k, q, seed=seed)\n    cbar.append( nx.average_clustering(WS) )\n    seed += 10\nprint( \"avg WS clustering at q = 0.42:\", np.mean(cbar) )\n\navg WS clustering at q = 0.42: 0.13177740621327544\n\n\nThe WS construction gives a plausible way to reconstruct the clustering observed in the Twitch network. However, there are other graph properties left to examine."
  },
  {
    "objectID": "networks.html#distance",
    "href": "networks.html#distance",
    "title": "7¬† Networks",
    "section": "7.3 Distance",
    "text": "7.3 Distance\nThe small-world phenomenon is, broadly speaking, the observation that any two people in a group can be connected by a surprisingly short path of acquaintances. This concept appears, for instance, in the Bacon number game, where actors are nodes, appearing in the same movie creates an edge between them, and one tries to find the distance between Kevin Bacon and some other designated actor.\nThe distance between two nodes in a connected graph is the number of edges in the shortest path between them. For example, in a complete graph, the distance between any pair of distinct nodes is 1, since all possible pairs are connected by an edge.\n\nK5 = nx.complete_graph(5)\ndist = pd.Series(nx.shortest_path_length(K5,0), index=K5.nodes)\nprint(\"Distance from node 0:\", dist)\n\nDistance from node 0: 0    0\n1    1\n2    1\n3    1\n4    1\ndtype: int64\n\n\nThe maximum distance over all pairs of nodes in a graph is called its diameter. Since this value depends on an extreme outlier in the distribution of distances, we often preferr to use the average distance as a measure of how difficult it is to connect two randomly selected nodes.\nFor example, here is a wheel graph:\n\nW = nx.wheel_graph(7)\nnx.draw(W, with_labels=True, node_color=\"lightblue\")\n\n\n\n\nNo node is more than two hops away from another (if the first hop is to node 0), so the diameter of this graph is 2. The average distance is somewhat smaller. This graph is so small that we can easily find the entire matrix of pairwise distances. The matrix is symmetric, so it‚Äôs only necessary to compute its upper triangle.\n\nnodes = list(W.nodes)\nn = len(nodes)\nD = np.zeros( (n,n), dtype=int )\nfor i in range(n):\n    for j in range(i+1,n):\n        D[i,j] = nx.shortest_path_length(W, nodes[i], nodes[j]) \n\nprint(D)\n\n[[0 1 1 1 1 1 1]\n [0 0 1 2 2 2 1]\n [0 0 0 1 2 2 2]\n [0 0 0 0 1 2 2]\n [0 0 0 0 0 1 2]\n [0 0 0 0 0 0 1]\n [0 0 0 0 0 0 0]]\n\n\nTo get the average distance, we can sum over all the entries and divide by \\(\\binom{n}{2}\\):\n\nprint( \"average distance:\", 2*D.sum() / (n*(n-1)) )\n\naverage distance: 1.4285714285714286\n\n\nThere is a convenience function for computing this average. (It becomes slow as \\(n\\) grows, though.)\n\nprint( \"average distance:\", nx.average_shortest_path_length(W) )\n\naverage distance: 1.4285714285714286\n\n\n\n7.3.1 ER graphs\nIf we want to compute distances within ER random graphs, we quickly run into a problem: an ER graph may not have a path between every pair of nodes:\n\nn, p = 101, 1/25\nER = nx.erdos_renyi_graph(n, p, seed=0)\nnx.draw(ER, node_size=50)\n\n\n\n\nWe say that such a graph is not connected. When no path exists between two nodes, the distance between them is either undefined or infinite. NetworkX will give an error if we try to compute the average distance in a disconnected graph:\n\nnx.average_shortest_path_length(ER)\n\nNetworkXError: Graph is not connected.\n\n\nOne way to cope with this eventuality is to decompose the graph into connected components, a disjoint separation of the nodes into connected subgraphs. We can use nx.connected_components to get node sets for each component.\n\n[ len(cc) for cc in nx.connected_components(ER) ]\n\n[100, 1]\n\n\nThe result above tells us that removing the lone unconnected node in the ER graph leaves us with a connected component. We can always get the largest component with the following idiom:\n\nER_sub = ER.subgraph( max(nx.connected_components(ER), key=len) )\nprint(ER_sub.number_of_nodes(), \"nodes in largest component\")\n\n100 nodes in largest component\n\n\nNow the average path length is a valid computation.\n\nnx.average_shortest_path_length(ER_sub)\n\n3.3082828282828283\n\n\nLet‚Äôs use this method to examine average distances within ER graphs of a fixed type.\n\nn,p = 121,1/20\ndbar = []\nfor iter in range(100):\n    ER = nx.erdos_renyi_graph(n, p, seed=iter+5000)\n    ER_sub = ER.subgraph( max(nx.connected_components(ER), key=len) )\n    dbar.append( nx.average_shortest_path_length(ER_sub) )\n\nprint(\"average distance in the big component of ER graphs:\")\nsns.displot(x=dbar, bins=13);\n\naverage distance in the big component of ER graphs:\n\n\n\n\n\nThe chances are good, therefore, that any message could be passed along in three hops or fewer (within the big component). In fact, theory states that as \\(n\\to\\infty\\), the mean distance in ER graphs is expected to be approximately\n\\[\n\\frac{\\ln(n)}{\\ln(\\bar{k})}.\n\\tag{7.1}\\]\nFor \\(n=121\\) and \\(\\bar{k}=6\\) as in the experiment above, this value is about 2.68.\n\n\n7.3.2 Watts‚ÄìStrogatz graphs\nThe Watts‚ÄìStrogatz model was originally proposed to demonstrate small-world networks. The initial ring-lattice structure of the construction exhibits both large clustering and large mean distance:\n\nG = nx.watts_strogatz_graph(400, 6, 0)  # q=0 ==> initial ring lattice\nC0 = nx.average_clustering(G)\nL0 = nx.average_shortest_path_length(G)\nprint(f\"Ring lattice has average clustering {C0:.4f}\")\nprint(f\"and average shortest path length {L0:.2f}\")\n\nRing lattice has average clustering 0.6000\nand average shortest path length 33.75\n\n\nAt the other extreme of \\(p=1\\), we get an ER random graph, which (at equivalent parameters) has small clustering and small average distance. The most interesting aspect of WS graphs is the transition between these extremes as \\(p\\) varies.\n\ncbar, dbar, logq = [],[],[]\nfor lq in np.arange(-3.5, 0.01, 0.25):\n    for iter in range(8):\n        G = nx.watts_strogatz_graph(400, 6, 10**lq, seed=975+iter)\n        cbar.append( nx.average_clustering(G) / C0 )\n        dbar.append( nx.average_shortest_path_length(G) / L0 )\n        logq.append(lq)\n\n\nresults = pd.DataFrame( {\"log10(q)\":logq, \"avg clustering\":cbar, \"avg distance\":dbar} )  \nsns.relplot(data=pd.melt(results, id_vars=\"log10(q)\"),\n            x=\"log10(q)\", y=\"value\",\n            hue=\"variable\", kind=\"line\"\n            );\n\n\n\n\nThe horizontal axis above is \\(\\log_{10}(q)\\), and the vertical axis shows the average clustering and shortest path length normalized by their values at \\(q=0\\). Watts and Strogatz raised awareness of the fact that for quite small values of \\(q\\), i.e., relatively few nonlocal connections, there are networks with a large clustering coefficient and small average distance.\n\n\n7.3.3 Twitch network\nLet‚Äôs consider distances within the Twitch network.\n\ntwitch = nx.read_edgelist(\"musae_edges.csv\", delimiter=',', nodetype=int)\nn, e = twitch.number_of_nodes(), twitch.number_of_edges()\nkbar = 2*e/n\nprint(n, \"nodes and\", e, \"edges\")\nprint(f\"average degree is {kbar:.3f}\")\n\n7126 nodes and 35324 edges\naverage degree is 9.914\n\n\nComputing the distances between all pairs of nodes in this graph would take a rather long time, so we will sample some pairs randomly.\n\nrng = default_rng(1)\n\n# Compute the distance between a random pair of distinct nodes:\ndef pairdist(G):\n    n = nx.number_of_nodes(G)\n    i = j = rng.integers(0,n)\n    while i==j: j=rng.integers(0,n)   # get distinct nodes\n    return nx.shortest_path_length(G,source=i,target=j)\n\ndistances = [ pairdist(twitch) for _ in range(50000) ]\nprint(\"Pairwise distances in Twitch graph:\")\nsns.displot(x=distances, discrete=True)\nprint( \"estimated mean =\", np.mean(distances) )\n\nPairwise distances in Twitch graph:\nestimated mean = 3.67376\n\n\n\n\n\nLet‚Äôs compare these results to ER graphs with the same size and average degree, i.e., with \\(p=\\bar{k}/(n-1)\\). The theoretical estimate from above gives\n\nprint( \"Comparable ER graphs expected mean distance:\", np.log(n) / np.log(kbar) )\n\nComparable ER graphs expected mean distance: 3.8673326382368893\n\n\nThe Twitch network has a slightly smaller value than this, but the numbers are comparable. However, remember that the ER graphs have a negligible clustering coefficient.\nNext we explore Watts‚ÄìStrogatz graphs with the same \\(n\\) as the Twitch network and \\(k=10\\) to get a similar average degree.\n\nresults = []\nseed = 44044\nn, k = twitch.number_of_nodes(), 10\nfor q in np.arange(0.1, 0.76, 0.05):\n    for iter in range(10):\n        WS = nx.watts_strogatz_graph(n, k, q, seed=seed)\n        dbar = sum(pairdist(WS) for _ in range(60))/60\n        results.append( (q,dbar) )\n        seed += 7\n\nresults = pd.DataFrame( results, columns=[\"q\", \"avg distance\"] )\nprint(\"Pairwise distances in WS graphs:\")\nsns.relplot(data=results, x=\"q\", y=\"avg distance\", kind=\"line\");\n\nPairwise distances in WS graphs:\n\n\n\n\n\nThe decrease with \\(q\\) is less pronounced that it was for the smaller WS graphs above. In the previous section, we found that \\(q=0.42\\) reproduces the same average clustering as in the Twitch network. That corresponds to a mean distance of about 4.5, which is a bit above the observed Twitch mean distance of 3.87, but not dramatically so. Thus, the Watts-Strogatz model could still be considered a plausible one for the Twitch network. In the next section, though, we will see that it misses badly in at least one important aspect."
  },
  {
    "objectID": "networks.html#degree-distributions",
    "href": "networks.html#degree-distributions",
    "title": "7¬† Networks",
    "section": "7.4 Degree distributions",
    "text": "7.4 Degree distributions\nAs we know, means of distributions do not always tell the entire story. For example, the distribution of the degrees of all the nodes in our Twitch network has some surprising features.\n\ntwitch = nx.read_edgelist(\"musae_edges.csv\", delimiter=',', nodetype=int)\ntwitch_degrees = pd.Series( dict(twitch.degree), index=twitch.nodes )\ntwitch_degrees.describe()\n\ncount    7126.000000\nmean        9.914117\nstd        22.190263\nmin         1.000000\n25%         2.000000\n50%         5.000000\n75%        11.000000\nmax       720.000000\ndtype: float64\n\n\nObserve above that that there is a significant disparity between the mean and median values of the degree distribution, and that the standard deviation is much larger than the mean. A histogram plot confirms that the degree distribution is widely dispersed:\n\nprint(\"Twitch network degree distribution:\")\nsns.displot(twitch_degrees);\n\nTwitch network degree distribution:\n\n\n\n\n\nA few nodes in the network have hundreds of friends:\n\nfriend_counts = twitch_degrees.value_counts()  # histogram heights\nfriend_counts.sort_index(ascending=False).head(10)\n\n720    1\n691    1\n465    1\n378    1\n352    1\n336    1\n316    1\n278    1\n272    1\n254    1\ndtype: int64\n\n\nThese ‚Äúgregarious nodes‚Äù or hubs are characteristic of many social and other real-world networks.\nWe can compare the above distribution to that in a collection of ER graphs with the same size and expected average degree.\n\nn, e = twitch.number_of_nodes(), twitch.number_of_edges()\nkbar = 2*e/n\np = kbar/(n-1)\ndegrees = []\nfor iter in range(3):\n    ER = nx.erdos_renyi_graph(n, p, seed=111+iter)\n    degrees.extend( [ER.degree(i) for i in ER.nodes] )\n\nprint(\"ER graphs degree distribution:\")\nsns.displot(degrees, discrete=True);\n\nER graphs degree distribution:\n\n\n\n\n\nTheory proves that the plot above converges to a binomial distribution. This is yet another indicator that the ER model does not explain the Twitch network well. A WS graph has a similar distribution:\n\nk,q = 10, 0.42\ndegrees = []\nfor iter in range(3):\n    WS = nx.watts_strogatz_graph(n, k, q, seed=222+iter)\n    degrees.extend( [WS.degree(i) for i in WS.nodes] )\n\nprint(\"WS graphs degree distribution:\")\nsns.displot(degrees, discrete=True);\n\nWS graphs degree distribution:\n\n\n\n\n\n\n7.4.1 Power-law distribution\nThe behavior of the Twitch degree distribution gets very interesting when the axes are transformed to use log scales:\n\nhist = sns.displot(data=twitch_degrees, log_scale=True)\nhist.axes[0,0].set_yscale(\"log\")\n\n\n\n\nFor degrees between 10 and several hundred, the counts lie nearly on a straight line. That is, if \\(x\\) is degree and \\(y\\) is the node count at that degree, then\n\\[\n\\log(y) \\approx  - a\\cdot \\log(x) + b,\n\\]\ni.e.,\n\\[\ny \\approx B x^{-a},\n\\]\nfor some \\(a > 0\\). This relationship is known as a power law. Many social networks seem to follow a power-law distribution of node degrees, to some extent. (The precise extent is a subject of hot debate.)\nNote that the decay of \\(x^{-a}\\) to zero as \\(x\\to\\infty\\) is much slower than, say, the normal distribution‚Äôs \\(e^{-x^2/2}\\), or even just an exponential \\(e^{-cx}\\). This last comparison is how a heavy-tailed distribution is usually defined.\nWe can get a fair estimate of the constants \\(B\\) and \\(a\\) in the power law by doing a least-squares fit on the logs of \\(x\\) and \\(y\\). First, we need the counts:\n\ny = twitch_degrees.value_counts()\ncounts = pd.DataFrame( {\"degree\": y.index, \"count\": y.values} )\ncounts = counts[ (counts[\"degree\"] > 10) & (counts[\"degree\"] < 200) ];\ncounts.head(6)\n\n\n\n\n\n\n\n\ndegree\ncount\n\n\n\n\n10\n11\n193\n\n\n11\n12\n155\n\n\n12\n13\n131\n\n\n13\n14\n122\n\n\n14\n15\n103\n\n\n15\n17\n83\n\n\n\n\n\n\n\nNow we will get additional columns by log transformations. (Note: the np.log function is the natural logarithm.)\n\nlogcounts = counts.transform(np.log)\n\nNow we use sklearn for a linear regression.\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(logcounts[[\"degree\"]], logcounts[\"count\"])\nlm.coef_[0], lm.intercept_\n\n(-1.9941272617745713, 9.7094067609447)\n\n\nThe first value, which is both the slope of the line and the exponent of \\(x\\) in the power law, is the most interesting part. It estimates that the degree counts vary as \\(Bx^{-2.1}\\) over a wide range of degrees.\n\n\n7.4.2 Barab√°si‚ÄìAlbert graphs\nA random Barab√°si‚ÄìAlbert graph (BA graph) is constructed by starting with a small seed network and connecting one node at a time with \\(m\\) new edges to it. Edges are added randomly, but higher probability is given to connect to nodes that already have higher degree (i.e., are more ‚Äúpopular‚Äù), a concept known as preferential attachment. Because of this rule, there is a natural tendency to develop a few hubs of high degree.\n\nBA = nx.barabasi_albert_graph(100, 2, seed=0)\nBA_degrees = pd.Series( dict(BA.degree), index=BA.nodes )\nnx.draw(BA, node_size=8*BA_degrees, node_color=\"red\")\n\n\n\n\nWhen we match these graphs to the size and average degree of the Twitch network, a power-law distribution emerges. Since we add \\(m\\) edges (almost) \\(n\\) times, the expected average degree is \\(2mn/n=2m\\). Therefore, in the BA construction we want to choose\n\\[\nm \\approx \\frac{\\bar{k}}{2}.\n\\]\n\nm = round(kbar/2)\nBA = nx.barabasi_albert_graph(n, m, seed=5)\nBA_degrees = pd.Series( dict(BA.degree), index=BA.nodes )\nhist = sns.displot(data=BA_degrees, log_scale=True)\nhist.axes[0,0].set_yscale(\"log\")\n\n\n\n\nTheory predicts that the exponent of the power-law distribution in a BA graph is \\(-3\\).\n\ny = BA_degrees.value_counts()\ncounts = pd.DataFrame( {\"degree\":y.index, \"count\":y.values} )\ncounts = counts[ (counts[\"degree\"] > 5) & (counts[\"degree\"] < 80) ]\nlogcounts = counts.transform(np.log)\nlm.fit( logcounts[[\"degree\"]], logcounts[\"count\"] )\nprint( \"exponent of power law:\", lm.coef_[0] )\n\nexponent of power law: -2.873136852062997\n\n\nLet‚Äôs check distances and clustering, too. As a reminder, the mean distance in the Twitch network is approximately:\n\nfrom numpy.random import default_rng\nrng = default_rng(1)\n\ndef pairdist(G):\n    n = nx.number_of_nodes(G)\n    i = j = rng.integers(0, n)\n    while i==j: j=rng.integers(0, n)   # get distinct nodes\n    return nx.shortest_path_length(G, source=i, target=j)\n\nprint(\"Mean distance in Twitch graph:\",\n    sum(pairdist(twitch) for _ in range(4000)) / 4000 )\n\nMean distance in Twitch graph: 3.657\n\n\nNow we repeat that for some BA graphs.\n\ndbar = []\nseed = 911\nfor iter in range(10):\n    BA = nx.barabasi_albert_graph(n, m, seed=seed)\n    d = sum(pairdist(BA) for _ in range(200)) / 200\n    dbar.append(d)\n    seed += 1\n\nprint( \"Mean distance in BA graphs:\", np.mean(dbar) )\n\nMean distance in BA graphs: 3.5555\n\n\nNot bad! Now, let‚Äôs check the clustering. For Twitch, we have:\n\nprint( \"Mean clustering in Twitch graph:\", nx.average_clustering(twitch) )\n\nMean clustering in Twitch graph: 0.13092821901472096\n\n\nAnd for BA, we get\n\ncbar = []\nseed = 59\nfor iter in range(20):\n    BA = nx.barabasi_albert_graph(n, m, seed=seed)\n    cbar.append( nx.average_clustering(BA) )\n    seed += 1\n    \nprint( \"Mean clustering in BA graphs:\", np.mean(cbar) )\n\nMean clustering in BA graphs: 0.009219743245252128\n\n\nThe BA model is our closest approach so far, but it fails to produce the close-knit neighbor subgraphs that we find in the Twitch network and the WS model."
  },
  {
    "objectID": "networks.html#centrality",
    "href": "networks.html#centrality",
    "title": "7¬† Networks",
    "section": "7.5 Centrality",
    "text": "7.5 Centrality\nIn some applications we might want to know which nodes of a network are the most important. For instance, we might want to find influential members of a social network, or nodes that are critical for efficient connections within the network. These traits go under the general name of centrality.\nAn easy candidate for measuring the centrality of a node is its degree. Usually this is normalized by the number of nodes in the graph and called degree centrality. While it can yield useful infortmation in some networks, it is not always a reliable measuring stick. For example, consider the following Watts‚ÄìStrogatz graph:\n\nG = nx.watts_strogatz_graph(60, 2, .1, seed=6)\npos = nx.spring_layout(G,seed=1)\nstyle = dict(pos=pos, edge_color=\"gray\", node_color=\"pink\", with_labels=True)\nnx.draw(G, **style, node_size=120)\n\n\n\n\nThere is little variation in the degrees of the nodes. In fact, there are only 3 unique values of the degree centrality:\n\ncentrality = pd.DataFrame( {\"degree\":nx.degree_centrality(G)}, index=G.nodes )\nsns.displot(data=centrality, x=\"degree\");\n\n\n\n\nFrom the drawing of the graph, however, it‚Äôs clear that (for instance) nodes 3 and 6 do not have comparable roles, despite the fact that both have degree equal to 2.\n\n7.5.1 Betweenness centrality\nA different way to measure centrality is to use shortest paths between nodes. Let \\(\\sigma(i,j)\\) denote the number of shortest paths between nodes \\(i\\) and \\(j\\). This means that we count the number of unique ways to get between these nodes using the minimum possible number of edges. Let \\(\\sigma(i,j|k)\\) be the number of such paths that pass through node \\(k\\). Then, for a graph on \\(n\\) nodes, the betweenness centrality of node \\(k\\) is\n\\[\nc_B(k) = \\frac{1}{\\binom{n-1}{2}}\\, \\displaystyle\\sum_{\\substack{\\text{all pairs }i,j\\\\i\\neq k,\\,j\\neq k}} \\frac{\\sigma(i,j|k)}{\\sigma(i,j)}.\n\\]\nEach term in the sum is less than or equal to 1, and the number of terms in the sum is \\(\\binom{n-1}{2}\\), so \\(0\\le c_B \\le 1\\) for any node. The definition requires an expensive computation if the number of nodes is more than a few hundred, so the \\(\\sigma\\) values are often estimated by sampling.\n\nExample 7.4 We will find the betweenness centrality of the following barbell graph:\n\n\n\nBarbell graph\n\n\nLet‚Äôs begin with node 3, in the middle. Any path, and therefore any shortest path, between nodes 0, 1, or 2 and nodes 4, 5, or 6 must pass through node 3, so these pairings each contribute 1 to the sum. The shortest paths for pairs of nodes within the end triangles clearly do not pass through node 3. Hence\n\\[\nc_B(3) = \\frac{1}{15} \\cdot (3\\cdot 3) = \\frac{3}{5}.\n\\]\nNext, consider node 2. The shortest paths through this node are the ones that pair nodes 0 or 1 with nodes 3, 4, 5, or 6, so\n\\[\nc_B(2) = \\frac{1}{15} \\cdot (2\\cdot 4) = \\frac{8}{15}.\n\\]\nBy symmetry, we get the same value for node 4.\nAll the other nodes play no role in any shortest paths. For instance, any path passing through node 0 can be replaced with a shorter one that follows the edge between nodes 1 and 2. Hence \\(c_B\\) is zero on these nodes.\n\nThe betweenness_centrality function returns a dictionary with nodes as keys and \\(c_B\\) as values.\n\ncentrality[\"between\"] = pd.Series(nx.betweenness_centrality(G), index=G.nodes)\nsns.displot(data=centrality, x=\"between\");\n\n\n\n\nThe distribution above shows that few nodes have a relatively high betweenness score in our graph.\n\n\n7.5.2 Eigenvector centrality\nA different way of distinguishing nodes of high degree is to suppose that not all links are equally valuable. By analogy with ranking sports teams, where wins over good teams should count for more than wins over bad teams, we should assign more importance to nodes that link to other important nodes.\nWe can try to turn this idea into an algorithm as follows. Suppose we initially assign uniform centrality scores \\(x_1,\\ldots,x_n\\) to all of the nodes. Now we can update the scores by looking at the current scores for all the neighbors. Specifically, the new scores are\n\\[\nx_i^+ = \\sum_{j\\text{ adjacent to }i} x_j = \\sum_{j=1}^n A_{ij} x_j,\\quad i=1,\\ldots,n,\n\\]\nwhere \\(A_{ij}\\) are entries of the adjacency matrix. Once we have updated the scores, we can repeat the process to update them again, and so on. If the scores were to converge, in the sense that \\(x_i^+\\) approaches \\(x_i\\), then we would have a solution of the equation\n\\[\nx_i \\stackrel{?}{=} \\sum_{j=1}^n A_{ij} x_j, \\quad i=1,\\ldots,n.\n\\]\nIn fact, since the sums are all inner products across rows of \\(\\bfA\\), this is simply\n\\[\n\\bfx \\stackrel{?}{=} \\bfA \\bfx.\n\\]\nExcept for \\(\\bfx\\) equal to the zero vector, this equation does not have a solution in general. However, if we relax it just a bit, we get somewhere important. Instead of equality, let‚Äôs look for proportionality, i.e.,\n\\[\n\\lambda \\bfx = \\bfA \\bfx\n\\]\nfor a number \\(\\lambda\\). This is an eigenvalue equation, one of the fundamental problems in linear algebra.\n\n:label: example-centrality-eigenvector Consider the complete graph \\(K_3\\), which is just a triangle. Its adjacency matrix is\n\\[\n\\bfA = \\begin{bmatrix}\n0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0\n\\end{bmatrix}.\n\\]\nWe should hope that all three vertices are ranked equally. In fact, if we define \\(\\bfx=\\tfrac{1}{3}[1,1,1]\\), then\n\\[\n\\bfA \\bfx = \\bigl[\\tfrac{2}{3},\\tfrac{2}{3},\\tfrac{2}{3} \\bigr] = 2 \\bfx,\n\\]\nso that \\(\\lambda=2\\) is an eigenvalue to go with eigenvector \\(\\bfx\\). Note that any (nonzero) multiple of \\(\\bfx\\) would work just as well:\n\\[\n\\bfA (c \\bfx) =  \\bigl[\\tfrac{2}{3}c,\\tfrac{2}{3}c,\\tfrac{2}{3}c \\bigr] = 2 (c\\bfx),\n\\]\nso that \\(c\\bfx\\) is also an eigenvector. All that the eigenvector gives us, then, is relative centrality of the nodes, though it would be natural to normalize it so that its elements sum to 1.\n\nEvery \\(n\\times n\\) matrix has at least one nonzero solution to the eigenvalue equation, although complex numbers might be involved. For an adjacency matrix, the Perron‚ÄìFrobenius theorem guarantees a real solution for some \\(\\lambda > 0\\) and for which the \\(x_i\\) all have the same sign. That last property allows us to interpret the \\(x_i\\) as relative importance or centrality of the nodes. This is called eigenvector centrality.\nNetworkX has two functions for computing eigenvector centrality. Here we use the one that calls on numpy to solve the eigenvalue problem. As with betweenness centrality, the return value is a dictionary with nodes as the keys.\n\ncentrality[\"eigen\"] = pd.Series(nx.eigenvector_centrality_numpy(G),index=G.nodes)\nsns.displot(data=centrality,x=\"eigen\");\n\n\n\n\nYou can see above that eigenvector centrality distinguishes a small number of nodes in our example.\n\n\n7.5.3 Comparison\nWe can verify using correlation coefficients that while the three centrality measures are related, they are far from redundant:\n\ncentrality.corr()\n\n\n\n\n\n\n\n\ndegree\nbetween\neigen\n\n\n\n\ndegree\n1.000000\n0.630732\n0.601884\n\n\nbetween\n0.630732\n1.000000\n0.736732\n\n\neigen\n0.601884\n0.736732\n1.000000\n\n\n\n\n\n\n\nHere is how betweenness ranks the centrality of the nodes:\n\ncentrality.sort_values(by=\"between\", ascending=False).head(8)\n\n\n\n\n\n\n\n\ndegree\nbetween\neigen\n\n\n\n\n42\n0.050847\n0.595850\n0.405831\n\n\n23\n0.050847\n0.576856\n0.458056\n\n\n41\n0.033898\n0.385739\n0.232910\n\n\n40\n0.033898\n0.368206\n0.133669\n\n\n51\n0.050847\n0.363822\n0.392303\n\n\n39\n0.033898\n0.349503\n0.076714\n\n\n38\n0.033898\n0.329632\n0.044027\n\n\n22\n0.033898\n0.329632\n0.262883\n\n\n\n\n\n\n\nAs you can see, the top two are quite clear, and a drawing of the graph supports the case that they are central:\n\nnx.draw(G, node_size=500*centrality[\"between\"], **style)\n\n\n\n\nA weakness, though, is that there is are many secondary nodes whose values taper off only slowly as we enter the remote branches.\nHere is a ranking according to eigenvector centrality:\n\ncentrality.sort_values(by=\"eigen\", ascending=False).head(8)\n\n\n\n\n\n\n\n\ndegree\nbetween\neigen\n\n\n\n\n23\n0.050847\n0.576856\n0.458056\n\n\n42\n0.050847\n0.595850\n0.405831\n\n\n51\n0.050847\n0.363822\n0.392303\n\n\n22\n0.033898\n0.329632\n0.262883\n\n\n4\n0.033898\n0.311806\n0.249077\n\n\n41\n0.033898\n0.385739\n0.232910\n\n\n52\n0.033898\n0.134717\n0.225527\n\n\n50\n0.033898\n0.212741\n0.225125\n\n\n\n\n\n\n\nThis ranking has a clear top choice, followed by two that are nearly identical.\n\nnx.draw(G, node_size=800*centrality[\"eigen\"], **style)\n\n\n\n\nEigenvector centrality identifies a more compact and distinct center. Of course, these observations are all made for a single network, so be careful not to over-generalize!\n\n\n7.5.4 Power-law example\nLet‚Äôs take a look at centrality measures for a power-law graph of the same size. By construction, a BA graph has a hub-and-spoke structure.\n\nG = nx.barabasi_albert_graph(60, 1, seed=2)\nstyle[\"pos\"] = nx.spring_layout(G, seed=3)\nnx.draw(G, **style, node_size=120)\n\n\n\n\nDegree centrality certainly notices the gregarious node 0:\n\ncentrality = pd.DataFrame( {\"degree\":nx.degree_centrality(G)}, index=G.nodes )\nnx.draw(G, node_size=1000*centrality[\"degree\"], **style)\n\n\n\n\nHowever, as you see above, the secondary hubs do not stand out much. Betweenness centrality highlights them quite nicely here:\n\ncentrality[\"between\"] = pd.Series(nx.betweenness_centrality(G))\nnx.draw(G, node_size=600*centrality[\"between\"], **style)\n\n\n\n\nOn the other hand, eigenvector centrality puts a lot of emphasis on the friends of node 0, even the ones that are dead ends, at the expense of the secondary hubs:\n\ncentrality[\"eigen\"] = pd.Series( nx.eigenvector_centrality_numpy(G) )\nnx.draw(G, node_size=600*centrality[\"eigen\"], **style)\n\n\n\n\nThis undesirable aspect of eigenvector centrality can be fixed through an extra normalization by the node degree, so that the hub node divides its ‚Äúattention‚Äù into smaller parts. Such thinking leads to the PageRank algorithm, which is what put Google on the map for web searches.\n\n\n7.5.5 Friendship paradox\nA surprising fact about social networks is that on average, your friends have more friends than you do, a fact that is called the friendship paradox. Let \\(\\mathbf{d}\\) be an \\(n\\)-vector whose components are the degrees of the nodes in the network. On average, the number of ‚Äúfriends‚Äù (i.e., adjacent nodes) is the average degree, which is equal to\n\\[\n\\frac{\\onenorm{\\mathbf{d}}}{n}.\n\\]\nNow imagine that we create a list as follows: for each node \\(i\\), add to the list the number of friends of each of \\(i\\)‚Äôs friends. The mean value of this list is the average number of ‚Äúfriends of friends.‚Äù\nFor example, consider the following graph:\n\nL = nx.lollipop_graph(4, 1)\nnx.draw(L, with_labels=True, node_color=\"lightblue\")\n\n\n\n\nThe average degree is \\((3+3+3+4+1)/5=14/5\\). Here are the entries in our friends-of-friends list contributed by each node:\n\nNode 0: 3 (from node 1), 3 (from node 2), 4 (from node 3)\nNode 1: 3 (from node 0), 3 (from node 2), 4 (from node 3)\nNode 2: 3 (from node 0), 3 (from node 1), 4 (from node 3)\nNode 3: 3 (from node 0), 3 (from node 1), 3 (from node 2), 1 (from node 4)\nNode 4: 4 (from node 3)\n\nThe average value of this list, i.e., the average number of friends‚Äô friends, is \\(44/14=3.143\\), which is indeed larger than the average degree.\nThere is an easy way to calculate this value in general. Node \\(i\\) contributes \\(d_i\\) terms to the list, so the total number of terms is \\(\\onenorm{\\mathbf{d}}\\). We observe that node \\(i\\) appears \\(d_i\\) times in the list, each time contributing the value \\(d_i\\), so the sum of the entire list must be\n\\[\n\\sum_{i=1}^n d_i^2 = \\twonorm{\\mathbf{d}}^2 = \\mathbf{d}^T \\mathbf{d}.\n\\]\nHence the mathematical statement of the friendship paradox is\n\\[\n\\frac{\\onenorm{\\mathbf{d}}}{n} \\le \\frac{\\mathbf{d}^T \\mathbf{d}}{\\onenorm{\\mathbf{d}}}.\n\\tag{7.2}\\]\nYou are asked to prove this inequality in the exercises. Here is a verification for the BA graph above:\n\nn = G.number_of_nodes()\nd = pd.Series(dict(G.degree), index=G.nodes)\ndbar = d.mean()\ndbar_friends = np.dot(d,d) / d.sum()\n\nprint(dbar, \"is less than\", dbar_friends)\n\n1.9666666666666666 is less than 4.389830508474576\n\n\nThe friendship paradox generalizes to eigenvector centrality: the average centrality of all nodes is less than the average of the centrality of all nodes‚Äô friends. The mathematical statement is \\[\n\\frac{\\onenorm{\\mathbf{x}}}{n} \\le \\frac{\\mathbf{x}^T \\mathbf{d}}{\\onenorm{\\mathbf{d}}},\n\\tag{7.3}\\] where \\(\\bfx\\) is the eigenvector defining centrality of the nodes.\n\nx = centrality[\"eigen\"]\nxbar = x.mean()\nxbar_friends = np.dot(x,d) / sum(d)\nprint(xbar, \"is less than\", xbar_friends)\n\n0.07583936265862018 is less than 0.15936510918420194\n\n\nIn fact, the friendship paradox inequality for any vector \\(\\bfx\\) is equivalent to \\(\\bfx\\) having nonnegative correlation with the degree vector."
  },
  {
    "objectID": "networks.html#communities",
    "href": "networks.html#communities",
    "title": "7¬† Networks",
    "section": "7.6 Communities",
    "text": "7.6 Communities\nIn applications, one may want to identify communities within a network. There are many ways to define this concept precisely. We will choose a random-walk model.\nImagine that a bunny sits on node \\(i\\). In one second, the bunny hops to one of \\(i\\)‚Äôs neighbors, chosen randomly. In the next second, the bunny hops to another node chosen randomly from the neighbors of the one it is sitting on, etc. This is a random walk on the nodes of the graph.\nNow imagine that we place another bunny on node \\(i\\) and track its path as it hops around the graph. Then we place another bunny, etc., so that we have an ensemble of walks. We can now reason about the probability of the location of the walk after any number of hops. Initially, the probability of node \\(i\\) is 100%. If \\(i\\) has \\(m\\) neighbors, then each of them will have probability \\(1/m\\) after one hop, and all the other nodes (including \\(i\\) itself) have zero probability.\nLet‚Äôs keep track of the probabilities for this simple wheel graph:\n\nG = nx.wheel_graph(5)\nnx.draw(G, node_size=300, with_labels=True, node_color=\"yellow\")\n\n\n\n\nWe start at node 4. This corresponds to the probability vector\n\\[\n\\bfp = [0,0,0,0,1].\n\\]\nOn the first hop, we are equally likely to visit each of the nodes 0, 1, or 3. This implies the probability distribution\n\\[\n\\mathbf{q} = \\left[\\tfrac{1}{3},\\tfrac{1}{3},0,\\tfrac{1}{3},0\\right].\n\\]\nLet‚Äôs now find the probability of standing on node 0 after the next hop. The two possible histories are 4-1-0 and 4-3-0, with total probability\n\\[\n\\underbrace{\\frac{1}{3}}_{\\text{to 1}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 0}} + \\underbrace{\\frac{1}{3}}_{\\text{to 3}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 0}} = \\frac{2}{9}.\n\\]\nWhat about node 2 after two hops? The viable paths are 4-0-2, 4-1-2, and 4-3-2. Keeping in mind that node 0 has 4 neighbors, we get\n\\[\n\\underbrace{\\frac{1}{3}}_{\\text{to 0}} \\cdot \\underbrace{\\frac{1}{4}}_{\\text{to 2}} + \\underbrace{\\frac{1}{3}}_{\\text{to 1}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 2}} + \\underbrace{\\frac{1}{3}}_{\\text{to 3}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 2}}= \\frac{11}{36}.\n\\]\nThis quantity is actually an inner product between the vector \\(\\mathbf{q}\\) (probabilities of the prior location) and\n\\[\n\\bfw_2 = \\left[ \\tfrac{1}{4},\\, \\tfrac{1}{3},\\, 0,\\, \\tfrac{1}{3},\\, 0 \\right],\n\\]\nwhich encodes the chance of hopping directly to node 2 from anywhere. In fact, the entire next vector of probabilities is just\n\\[\n\\bigl[ \\bfw_1^T \\mathbf{q},\\, \\bfw_2^T \\mathbf{q},\\, \\bfw_3^T \\mathbf{q},\\, \\bfw_4^T \\mathbf{q},\\, \\bfw_5^T \\mathbf{q} \\bigr] = \\bfW \\mathbf{q},\n\\]\nwhere \\(\\bfW\\) is the \\(n\\times n\\) matrix whose rows are \\(\\bfw_1,\\bfw_2,\\ldots.\\) In terms of matrix-vector multiplications, we have the easy statement that the probability vectors after each hop are\n\\[\n\\bfp, \\bfW\\bfp, \\bfW(\\bfW\\bfp), \\ldots.\n\\]\nExplicitly, the matrix \\(\\bfW\\) is\n\\[\n\\bfW = \\begin{bmatrix}\n0 & \\tfrac{1}{3}  & \\tfrac{1}{3}  & \\tfrac{1}{3}  & \\tfrac{1}{3} \\\\\n\\tfrac{1}{4} & 0 & \\tfrac{1}{3}  & 0  & \\tfrac{1}{3} \\\\\n\\tfrac{1}{4} & \\tfrac{1}{3} & 0 & \\tfrac{1}{3} & 0 \\\\\n\\tfrac{1}{4} & 0 & \\tfrac{1}{3} & 0 & \\tfrac{1}{3} \\\\\n\\tfrac{1}{4} & \\tfrac{1}{3} & 0 & \\tfrac{1}{3} & 0\n\\end{bmatrix}.\n\\]\nThis has a lot of resemblance to the adjacency matrix\n\\[\n\\bfA = \\begin{bmatrix}\n0 & 1  & 1  & 1  & 1 \\\\\n1 & 0 & 1 & 0  & 1 \\\\\n1 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 1 & 0 & 1 & 0\n\\end{bmatrix}.\n\\]\nThe only difference is that each column has to be normalized by the number of options outgoing at that node, i.e., the degree of the node. Thus,\n\\[\nW_{ij} = \\frac{1}{\\operatorname{deg}(j)}\\,A_{ij}.\n\\]\n\n7.6.1 Simulating the random walk\nLet‚Äôs do a simulation for a more interesting graph:\n\nWS = nx.watts_strogatz_graph(40, 4, 0.04, seed=11)\npos = nx.spring_layout(WS, k=0.25, seed=1, iterations=200)\nstyle = dict(pos=pos, with_labels=True, node_color=\"pink\", edge_color=\"gray\")\n\nnx.draw(WS, node_size=240, **style)\n\n\n\n\nFirst, we construct the random-walk matrix \\(\\bfW\\).\n\nn = WS.number_of_nodes()\nA = nx.adjacency_matrix(WS).astype(float)\ndegree = [ WS.degree[i] for i in WS.nodes ] \n\nW = A.copy()\nfor j in range(n):\n    W[:,j] /= degree[j]\n\nsns.heatmap(W.toarray()).set_aspect(1);\n\n\n\n\nWe set up a probability vector to start at node 0, and then use W.dot to compute the first hop. The result is to end up at 5 other nodes with equal probability:\n\ninit = 33\np = np.zeros(n)\np[init] = 1\np = W.dot(p)\nsz = 3000*p\nprint( \"Total probability after 1 hop:\", p.sum() )\nnx.draw(WS, node_size=sz, **style)\n\nTotal probability after 1 hop: 1.0\n\n\n\n\n\nAfter the next hop, there will again be a substantial probability of being at node 33. But we could also be at some second-generation nodes as well.\n\np = W.dot(p)\nprint( \"Total probability after 2 hops:\", p.sum() )\nnx.draw(WS, node_size=3000*p, **style)\n\nTotal probability after 2 hops: 1.0\n\n\n\n\n\nWe‚Äôll take 3 more hops. That lets us penetrate a little into the distant nodes.\n\nfor k in range(3):\n    p = W.dot(p)\nprint( \"Total probability after 5 hops:\", p.sum() )\nnx.draw(WS, node_size=3000*p, **style)\n\nTotal probability after 5 hops: 1.0\n\n\n\n\n\nIn the long run, the probabilities even out, as long as the graph is connected.\n\nfor k in range(200):\n    p = W.dot(p)\nnx.draw(WS, node_size=3000*p, **style)\n\n\n\n\n\n\n7.6.2 Label propagation\nThe random walk brings us to a type of algorithm known as label propagation. We start off by ‚Äúlabelling‚Äù one or several nodes whose community we want to identify. This is equivalent to initializing the probability vector \\(\\bfp\\). Then, we take a running total over the entire history of the random walk:\n\\[\n\\hat{\\bfx} = \\lambda \\bfp_1  + \\lambda^2 \\bfp_2 +  \\lambda^3 \\bfp_3 + \\cdots,\n\\]\nwhere \\(0 < \\lambda < 1\\) is a damping parameter, and\n\\[\n\\bfp_1 = \\bfW \\bfp, \\, \\bfp_2 = \\bfW \\bfp_1, \\, \\bfp_3 = \\bfW \\bfp_2,\\, \\ldots.\n\\]\n\nIn practice, we terminate the sum once \\(\\lambda^k\\) is sufficiently small. The resulting \\(\\hat{\\bfx}\\) can be normalized to a probability distribution,\n\\[\n\\bfx = \\frac{\\hat{\\bfx}}{\\norm{\\hat{\\bfx}}_1}.\n\\]\nThe value \\(x_i\\) can be interpreted as the probability of membership in the community.\nLet‚Äôs try looking for a community of node 0 in the WS graph above.\n\np = np.zeros(n)\np[init] = 1\nlam = 0.8\n\nWe will compute \\(\\bfx\\) by accumulating terms in a loop. Note that there is no need to keep track of the entire history of random-walk probabilities; we just use one generation at a time.\n\nx = np.zeros(n)\nmult = 1\nfor k in range(200):\n    p = W.dot(p)\n    mult *= lam\n    x += mult*p\n\nx /= np.sum(x)  # normalize to probability distribution\n\nThe probabilities tend to be distributed logarithmically:\nIn the following rendering, any node \\(i\\) with a value of \\(x_i < 10^{-2}\\) gets a node size of 0. (You can ignore the warning below. It happens because we have negative node sizes.)\n\nx[x<0.01] = 0\nstyle[\"node_color\"] = \"lightblue\"\nnx.draw(WS, node_size=4000*x, **style)\n\n\n\n\nThe parameter \\(\\lambda\\) controls how quickly the random-walk process is faded out. A smaller value puts more weight on the early iterations, generally localizing the community more strictly.\n\np = np.zeros(n)\np[init] = 1\nlam = 0.4\nx = np.zeros(n)\nmult = 1\nfor k in range(200):\n    p = W.dot(p)\n    mult *= lam\n    x += mult*p\n\nx /= np.sum(x)  \n\n\nx[x<0.01] = 0\nnx.draw(WS, node_size=4000*x, **style)\n\n\n\n\nIn practice, we could define a threshold cutoff on the probabilities, or set a community size and take the highest-ranking nodes. Then a new node could be selected and a community identified for it in the subgraph without the first community, etc.\nA more sophisticated version of the label propagation algorithm (and many other community detection methods) is offered in a special module.\n\nfrom networkx.algorithms.community import label_propagation_communities\ncomm = label_propagation_communities(WS)\n[ print(c) for c in comm ];\n\n{0, 1, 2, 3, 39}\n{4, 5, 6, 7, 8, 29, 30, 31}\n{9, 10, 11}\n{12, 13, 14, 15}\n{16, 17, 18, 19, 20}\n{32, 33, 34, 35, 21}\n{22, 23, 24, 25, 26, 27, 28}\n{36, 37, 38}\n\n\n\ncolor = np.array( [\"lightblue\",\"pink\",\"yellow\",\"lightgreen\",\"purple\",\"orange\",\"red\",\"lightgray\"] )\ncolor_index = [0]*n\nfor i,S in enumerate(comm):\n    for k in S:\n        color_index[k] = i\nnx.draw( WS, node_size=100, pos=pos, node_color=color[color_index] )"
  },
  {
    "objectID": "networks.html#exercises",
    "href": "networks.html#exercises",
    "title": "7¬† Networks",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 For each graph, give the number of nodes, the number of edges, and the average degree.\n(a) The complete graph \\(K_6\\).\n(b) \n(c) \n\n\nExercise 7.2 Give the adjacency matrix for the graphs in Exercise¬†7.1 (parts (a) and (b) only).\n\n\nExercise 7.3 For the graph below, draw the ego graph of (a) node 4 and (b) node 8.\n\n\n\n\n\n\n\nExercise 7.4 To construct an Erd≈ës-R√©nyi graph on 25 nodes with expected average degree 8, what should the edge inclusion probability \\(p\\) be?\n\n\nExercise 7.5 Find the diameters of the graphs in Exercise¬†7.1.\n\n\nExercise 7.6 Suppose that \\(\\bfA\\) is the adjacency matrix of an undirected graph on \\(n\\) nodes. Let \\(\\boldsymbol{1}\\) be the \\(n\\)-vector whose components all equal 1, and let \\[\n\\mathbf{d} = \\bfA \\boldsymbol{1}.\n\\] Explain why \\(\\mathbf{d}\\) is the vector whose components are the degrees of the nodes.\n\n\nExercise 7.7 Find (a) the clustering coefficient and (b) the betweenness centrality for each node in the following graph:\n\n\n\n\n\n\n\nExercise 7.8 A star graph with \\(n\\) nodes and \\(n-1\\) edges has a central node that has an edge to each other node. In terms of \\(n\\), find (a) the clustering coefficient and (b) the betweenness centrality of the central node of the star graph.\n\n\nExercise 7.9 The Watts‚ÄìStrogatz construction starts with a ring lattice in which the nodes are arranged in a circle and each is connected to its \\(k\\) nearest neighbors (i.e., \\(k/2\\) on each side). Show that the clustering coefficient of an arbitrary node in the ring lattice is \\[\n\\frac{3(k-2)}{4(k-1)}.\n\\]\n(Hint: Count up all the edges between the neighbors on one side of the node of interest, then all the edges between neighbors on the other side, and finally, the edges going from a neighbor on one side to a neighbor on the other side. It might be easier to work with \\(m=k/2\\) and then eliminate \\(m\\) at the end.)\n\n\nExercise 7.10 Recall that the complete graph \\(K_n\\) contains every possible edge on \\(n\\) nodes. Prove that the vector \\(\\bfx=[1,1,\\ldots,1]\\) is an eigenvector of the adjacency matrix of \\(K_n\\). (Therefore, the eigenvector centrality is uniform over the nodes.)\n\n\nExercise 7.11 Prove that for the star graph on \\(n\\) nodes as described in Exercise 8, the vector \\[\n\\bfx = \\bigl[ \\sqrt{n-1},1,1,\\ldots,1 \\bigr]\n\\] is an eigenvector of the adjacency matrix, where the central node corresponds to the first element of the vector.\n\n\nExercise 7.12 Prove the friendship paradox, i.e., inequality Equation¬†7.2. (Hint: Start with Equation¬†6.1 using \\(\\bfu=\\mathbf{d}\\) and \\(\\bfv\\) equal to a vector of all ones. Convert from equality to inequality to get rid of the angle \\(\\theta\\). Simplify the inner product, square both sides, and show that it can be rearranged into Equation¬†7.2.)"
  }
]