[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Science 1",
    "section": "",
    "text": "Preface\nThis book covers material for Math 219, Data Science 1, at the University of Delaware.\nThis site was made with Quarto."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "Data Science 1",
    "section": "Prerequisites",
    "text": "Prerequisites\nPrior experience with single-variable calculus (basic differentiation and integration) and with base Python are expected."
  },
  {
    "objectID": "index.html#fine-print",
    "href": "index.html#fine-print",
    "title": "Data Science 1",
    "section": "Fine print",
    "text": "Fine print\nSee the license file for rights information. (I hold the copyright and reserve all rights.)\nTo my knowledge and best ability, the information presented is accurate. However, I cannot guarantee that it is free of errors or that it will work for your application.\nThe book is provided free of charge. But if you find this book useful, and you are not a Math 219 student at Delaware, please consider showing your appreciation by buying me a coffee."
  },
  {
    "objectID": "examples.html#list-of-examples",
    "href": "examples.html#list-of-examples",
    "title": "List of examples",
    "section": "List of examples",
    "text": "List of examples\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Chap\n        \n         \n          Description\n        \n         \n          Link\n        \n         \n          Type\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nDescription\n\n\nType\n\n\nChap\n\n\nLink\n\n\n\n\n\n\nQuantitative data\n\n\n✍️\n\n\n1\n\n\nExample 1.1\n\n\n\n\nFloating-point numbers\n\n\n💻\n\n\n1\n\n\nExample 1.2\n\n\n\n\nInfinity and NaN\n\n\n💻\n\n\n1\n\n\nExample 1.3\n\n\n\n\nDates and times in NumPy\n\n\n💻\n\n\n1\n\n\nExample 1.4\n\n\n\n\nVectors in Python\n\n\n💻\n\n\n1\n\n\nExample 1.5\n\n\n\n\nAccessing vector elements\n\n\n💻\n\n\n1\n\n\nExample 1.6\n\n\n\n\nSlicing vectors\n\n\n💻\n\n\n1\n\n\nExample 1.7\n\n\n\n\nArrays in Python\n\n\n💻\n\n\n1\n\n\nExample 1.8\n\n\n\n\nIndexing arrays\n\n\n💻\n\n\n1\n\n\nExample 1.9\n\n\n\n\nArray reductions\n\n\n💻\n\n\n1\n\n\nExample 1.10\n\n\n\n\nOrdinal data\n\n\n✍️\n\n\n1\n\n\nExample 1.11\n\n\n\n\nDummy variables\n\n\n✍️\n\n\n1\n\n\nExample 1.12\n\n\n\n\nSeries and data frames\n\n\n✍️\n\n\n1\n\n\nExample 1.13\n\n\n\n\nSeries and frames in pandas\n\n\n💻\n\n\n1\n\n\nExample 1.14\n\n\n\n\nCreating a data frame\n\n\n💻\n\n\n1\n\n\nExample 1.15\n\n\n\n\nOrdinal data\n\n\n💻\n\n\n1\n\n\nExample 1.16\n\n\n\n\nCategorical data in pandas\n\n\n💻\n\n\n1\n\n\nExample 1.17\n\n\n\n\nImporting data with pandas\n\n\n💻\n\n\n1\n\n\nExample 1.18\n\n\n\n\niloc for rows in a frame\n\n\n💻\n\n\n1\n\n\nExample 1.19\n\n\n\n\nloc for rows in a frame\n\n\n💻\n\n\n1\n\n\nExample 1.20\n\n\n\n\nSelecting rows in a frame\n\n\n💻\n\n\n1\n\n\nExample 1.21\n\n\n\n\nCleaning a data frame\n\n\n💻\n\n\n1\n\n\nExample 1.22\n\n\n\n\nHandling missing values in pandas\n\n\n💻\n\n\n1\n\n\nExample 1.23\n\n\n\n\nStandard deviation by hand\n\n\n✍️\n\n\n2\n\n\nExample 2.1\n\n\n\n\nZ scores\n\n\n✍️, 💻\n\n\n2\n\n\nExample 2.2\n\n\n\n\nZ scores in Python\n\n\n💻\n\n\n2\n\n\nExample 2.3\n\n\n\n\nSample variance\n\n\n✍️\n\n\n2\n\n\nExample 2.4\n\n\n\n\nMedian\n\n\n✍️\n\n\n2\n\n\nExample 2.5\n\n\n\n\nQuantiles\n\n\n💻\n\n\n2\n\n\nExample 2.6\n\n\n\n\nQuartiles and IQR\n\n\n💻\n\n\n2\n\n\nExample 2.7\n\n\n\n\nCDF of a uniform distribution\n\n\n✍️\n\n\n2\n\n\nExample 2.8\n\n\n\n\nUniform random numbers in NumPy\n\n\n💻\n\n\n2\n\n\nExample 2.9\n\n\n\n\nEmpirical CDF\n\n\n💻\n\n\n2\n\n\nExample 2.10\n\n\n\n\nHistogram of a uniform distribution\n\n\n💻\n\n\n2\n\n\nExample 2.11\n\n\n\n\nPDF of a uniform distribution\n\n\n💻\n\n\n2\n\n\nExample 2.12\n\n\n\n\nMean of a uniform distribution\n\n\n✍️, 💻\n\n\n2\n\n\nExample 2.13\n\n\n\n\nNormal distribution in NumPy\n\n\n💻\n\n\n2\n\n\nExample 2.14\n\n\n\n\nFacet plots\n\n\n💻\n\n\n2\n\n\nExample 2.15\n\n\n\n\nBox and violin plots\n\n\n💻\n\n\n2\n\n\nExample 2.16\n\n\n\n\nAggregating data in groups\n\n\n💻\n\n\n2\n\n\nExample 2.17\n\n\n\n\nTransforming data in groups\n\n\n💻\n\n\n2\n\n\nExample 2.18\n\n\n\n\nFiltering data in groups\n\n\n💻\n\n\n2\n\n\nExample 2.19\n\n\n\n\nOutliers for mean and median\n\n\n✍️\n\n\n2\n\n\nExample 2.20\n\n\n\n\nInterquartile range\n\n\n💻\n\n\n2\n\n\nExample 2.21\n\n\n\n\nOutliers\n\n\n💻\n\n\n2\n\n\nExample 2.22\n\n\n\n\nRelational plots\n\n\n💻\n\n\n2\n\n\nExample 2.23\n\n\n\n\nPearson correlation coefficient\n\n\n💻\n\n\n2\n\n\nExample 2.24\n\n\n\n\nOutliers and the Pearson coefficient\n\n\n💻\n\n\n2\n\n\nExample 2.25\n\n\n\n\nSpearman correlation coefficient\n\n\n💻\n\n\n2\n\n\nExample 2.26\n\n\n\n\nCategorical correlation\n\n\n💻\n\n\n2\n\n\nExample 2.27\n\n\n\n\nThe Datasaurus\n\n\n💻\n\n\n2\n\n\nExample 2.28\n\n\n\n\nCorrelation vs. dependence\n\n\n💻\n\n\n2\n\n\nExample 2.29\n\n\n\n\nSimpson’s paradox\n\n\n💻\n\n\n2\n\n\nExample 2.30\n\n\n\n\nFeature matrix\n\n\n✍️\n\n\n3\n\n\nExample 3.1\n\n\n\n\nClassifier performance metrics\n\n\n✍️\n\n\n3\n\n\nExample 3.2\n\n\n\n\nBinary classifier metrics\n\n\n💻\n\n\n3\n\n\nExample 3.3\n\n\n\n\nCombined metrics\n\n\n✍️\n\n\n3\n\n\nExample 3.4\n\n\n\n\nMulticlass metrics\n\n\n💻\n\n\n3\n\n\nExample 3.5\n\n\n\n\nGini impurity\n\n\n✍️\n\n\n3\n\n\nExample 3.6\n\n\n\n\nTree partitioning\n\n\n✍️\n\n\n3\n\n\nExample 3.7\n\n\n\n\nInspecting a decision tree\n\n\n💻\n\n\n3\n\n\nExample 3.8\n\n\n\n\nTree classifier for the penguins dataset\n\n\n💻\n\n\n3\n\n\nExample 3.9\n\n\n\n\nCalculating distance\n\n\n✍️\n\n\n3\n\n\nExample 3.10\n\n\n\n\nkNN classifier\n\n\n✍️\n\n\n3\n\n\nExample 3.11\n\n\n\n\nkNN classifier for the penguins dataset\n\n\n💻\n\n\n3\n\n\nExample 3.12\n\n\n\n\nPipeline for standardized columns\n\n\n💻\n\n\n3\n\n\nExample 3.13\n\n\n\n\nProbabilistic classifier\n\n\n✍️, 💻\n\n\n3\n\n\nExample 3.14\n\n\n\n\nProbabilistic classifier for the penguins dataset\n\n\n💻\n\n\n3\n\n\nExample 3.15\n\n\n\n\nVarying decision threshold\n\n\n💻\n\n\n3\n\n\nExample 3.16\n\n\n\n\nROC curve, quasi-manually\n\n\n💻\n\n\n3\n\n\nExample 3.17\n\n\n\n\nROC curve for the penguins dataset\n\n\n💻\n\n\n3\n\n\nExample 3.18\n\n\n\n\nArea under ROC curve\n\n\n💻\n\n\n3\n\n\nExample 3.19\n\n\n\n\nHyperparameters\n\n\n✍️\n\n\n4\n\n\nExample 4.1\n\n\n\n\nLearning curves\n\n\n💻\n\n\n4\n\n\nExample 4.2\n\n\n\n\nOverfitting\n\n\n💻\n\n\n4\n\n\nExample 4.3\n\n\n\n\nBagging ensemble classifier\n\n\n💻\n\n\n4\n\n\nExample 4.4\n\n\n\n\nBagging ensemble (better)\n\n\n💻\n\n\n4\n\n\nExample 4.5\n\n\n\n\nRandom forest classifier\n\n\n💻\n\n\n4\n\n\nExample 4.6\n\n\n\n\nFolds for validation\n\n\n✍️\n\n\n4\n\n\nExample 4.7\n\n\n\n\nCross-validation\n\n\n💻\n\n\n4\n\n\nExample 4.8\n\n\n\n\nValidation curve\n\n\n💻\n\n\n4\n\n\nExample 4.9\n\n\n\n\nCross-validation grid search\n\n\n💻\n\n\n4\n\n\nExample 4.10\n\n\n\n\nLinear regression\n\n\n✍️\n\n\n5\n\n\nExample 5.13\n\n\n\n\nInner product\n\n\n✍️\n\n\n5\n\n\nExample 5.2\n\n\n\n\nMean squared and mean absolute error\n\n\n✍️\n\n\n5\n\n\nExample 5.3\n\n\n\n\nCoefficient of determination\n\n\n✍️\n\n\n5\n\n\nExample 5.4\n\n\n\n\nLinear regression for sea ice dataset\n\n\n💻\n\n\n5\n\n\nExample 5.5\n\n\n\n\nMatrix times vector\n\n\n✍️\n\n\n5\n\n\nExample 5.6\n\n\n\n\nMultilinear regression for MPG dataset\n\n\n💻\n\n\n5\n\n\nExample 5.7\n\n\n\n\nMultilinear regression for sales dataset\n\n\n💻\n\n\n5\n\n\nExample 5.8\n\n\n\n\nPolynomial regression for the MPG dataset\n\n\n💻\n\n\n5\n\n\nExample 5.9\n\n\n\n\nOverfitting in polynomial regression\n\n\n💻\n\n\n5\n\n\nExample 5.10\n\n\n\n\nRidge regression for diabetes dataset\n\n\n💻\n\n\n5\n\n\nExample 5.11\n\n\n\n\nLASSO regression for diabetes dataset\n\n\n💻\n\n\n5\n\n\nExample 5.12\n\n\n\n\nProving linearity\n\n\n✍️\n\n\n5\n\n\nExample 5.13\n\n\n\n\nProving nonlinearity\n\n\n✍️\n\n\n5\n\n\nExample 5.14\n\n\n\n\nNonlinearity of kNN regression\n\n\n✍️\n\n\n5\n\n\nExample 5.15\n\n\n\n\nkNN regression for MPG dataset\n\n\n💻\n\n\n5\n\n\nExample 5.16\n\n\n\n\nCreating a decision tree regressor\n\n\n✍️\n\n\n5\n\n\nExample 5.17\n\n\n\n\nTree regression on a small dataset\n\n\n💻\n\n\n5\n\n\nExample 5.18\n\n\n\n\nRandom forest for regression\n\n\n💻\n\n\n5\n\n\nExample 5.19\n\n\n\n\nCross-entropy\n\n\n✍️\n\n\n5\n\n\nExample 5.20\n\n\n\n\nLogistic regression as a spam filter\n\n\n💻\n\n\n5\n\n\nExample 5.21\n\n\n\n\nRegularization for the spam filter regressor\n\n\n💻\n\n\n5\n\n\nExample 5.22\n\n\n\n\nMulticlass logistic regression\n\n\n✍️\n\n\n5\n\n\nExample 5.23\n\n\n\n\nMulticlass logistic regression for forest cover dataset\n\n\n💻\n\n\n5\n\n\nExample 5.24\n\n\n\n\nDistance matrix\n\n\n✍️\n\n\n6\n\n\nExample 6.1\n\n\n\n\nUsage of pairwise_distances\n\n\n💻\n\n\n6\n\n\nExample 6.2\n\n\n\n\nAngular distance\n\n\n💻\n\n\n6\n\n\nExample 6.3\n\n\n\n\nRand index by hand\n\n\n✍️\n\n\n6\n\n\nExample 6.4\n\n\n\n\nAdjusted Rand index\n\n\n💻\n\n\n6\n\n\nExample 6.5\n\n\n\n\nSilhouette values\n\n\n✍️\n\n\n6\n\n\nExample 6.6\n\n\n\n\nSilhouette values calculation\n\n\n💻\n\n\n6\n\n\nExample 6.7\n\n\n\n\nSilhouettes as performance metric\n\n\n💻\n\n\n6\n\n\nExample 6.8\n\n\n\n\nLimitation of silhouettes\n\n\n💻\n\n\n6\n\n\nExample 6.9\n\n\n\n\nInertia\n\n\n✍️\n\n\n6\n\n\nExample 6.10\n\n\n\n\nk-means on blobs dataset\n\n\n💻\n\n\n6\n\n\nExample 6.11\n\n\n\n\nk-means on stripes dataset\n\n\n💻\n\n\n6\n\n\nExample 6.12\n\n\n\n\nk-means on digits dataset\n\n\n💻\n\n\n6\n\n\nExample 6.13\n\n\n\n\nLinkage\n\n\n✍️\n\n\n6\n\n\nExample 6.14\n\n\n\n\nAgglomerative for toy example\n\n\n💻\n\n\n6\n\n\nExample 6.15\n\n\n\n\nComparing linkages on simple datasets\n\n\n💻\n\n\n6\n\n\nExample 6.16\n\n\n\n\nAgglomerative for penguins dataset\n\n\n💻\n\n\n6\n\n\nExample 6.17\n\n\n\n\nConstructing networks/graphs\n\n\n💻\n\n\n7\n\n\nExample 7.1\n\n\n\n\nImporting networks\n\n\n💻\n\n\n7\n\n\nExample 7.2\n\n\n\n\nNeighbors of a node\n\n\n💻\n\n\n7\n\n\nExample 7.3\n\n\n\n\nEgo graph\n\n\n💻\n\n\n7\n\n\nExample 7.4\n\n\n\n\nAdjacency matrix\n\n\n💻\n\n\n7\n\n\nExample 7.5\n\n\n\n\nNode degrees\n\n\n💻\n\n\n7\n\n\nExample 7.6\n\n\n\n\nAverage degree\n\n\n💻\n\n\n7\n\n\nExample 7.7\n\n\n\n\nER graphs\n\n\n💻\n\n\n7\n\n\nExample 7.8\n\n\n\n\nWS graphs\n\n\n💻\n\n\n7\n\n\nExample 7.9\n\n\n\n\nClustering coefficient\n\n\n✍️\n\n\n7\n\n\nExample 7.10\n\n\n\n\nClustering in the Twitch network\n\n\n💻\n\n\n7\n\n\nExample 7.11\n\n\n\n\nClustering in ER graphs\n\n\n💻\n\n\n7\n\n\nExample 7.12\n\n\n\n\nClustering of WS graphs\n\n\n💻\n\n\n7\n\n\nExample 7.13\n\n\n\n\nDistances in a complete graph\n\n\n✍️\n\n\n7\n\n\nExample 7.14\n\n\n\n\nDistances in a wheel graph\n\n\n💻\n\n\n7\n\n\nExample 7.15\n\n\n\n\nConnectedness\n\n\n✍️\n\n\n7\n\n\nExample 7.16\n\n\n\n\nDistances in ER graphs\n\n\n💻\n\n\n7\n\n\nExample 7.17\n\n\n\n\nDegree distribution in the Twitch network\n\n\n💻\n\n\n7\n\n\nExample 7.18\n\n\n\n\nDegree distribution in the Twitch network\n\n\n💻\n\n\n7\n\n\nExample 7.19\n\n\n\n\nBarabási–Albert graphs\n\n\n💻\n\n\n7\n\n\nExample 7.20\n\n\n\n\nPower-law degrees in the Twitch network\n\n\n💻\n\n\n7\n\n\nExample 7.21\n\n\n\n\nDegree centrality\n\n\n💻\n\n\n7\n\n\nExample 7.22\n\n\n\n\nBetweenness centrality\n\n\n✍️\n\n\n7\n\n\nExample 7.23\n\n\n\n\nBetweenness centrality in BA graphs\n\n\n💻\n\n\n7\n\n\nExample 7.24\n\n\n\n\nBetweenness centrality in the Twitch network\n\n\n💻\n\n\n7\n\n\nExample 7.25\n\n\n\n\nEigenvectors\n\n\n✍️\n\n\n7\n\n\nExample 7.26\n\n\n\n\nEigenvector centrality\n\n\n✍️\n\n\n7\n\n\nExample 7.27\n\n\n\n\nComparison of centrality metrics\n\n\n💻\n\n\n7\n\n\nExample 7.28\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "resources.html#useful-guides",
    "href": "resources.html#useful-guides",
    "title": "Resources",
    "section": "Useful guides",
    "text": "Useful guides\n\npandas user guide, pandas cheat sheet, pandas long summary\nseaborn tutorial, seaborn cheat sheet\nscikit-learn user guide, sklearn cheat sheet\nnumpy cheat sheet"
  },
  {
    "objectID": "resources.html#data-sources",
    "href": "resources.html#data-sources",
    "title": "Resources",
    "section": "Data sources",
    "text": "Data sources\nHere are places around the web with data available for download.\n\nSearch engines\nThese point to a lot of other resources.\n\nGoogle Dataset Search\nRegistry of Open Data on AWS Access to datasets used by governments and researchers that happen to be stored on Amazon’s servers. Skewed toward large datasets.\n\n\n\nPackaged\nThese feature datasets that are essentially already packaged as CSV or Excel files, plus descriptions.\n\nFive Thirty-Eight Data used to support the site’s journalism, mainly in politics and sports.\nDelaware Open Data Publicly available data from the state government.\nKaggle Long-time host of data science competitions. The formal competitions are well-curated, but user contributions vary widely.\nUCI Machine Learning Repository Well-known source for datasets that have been used extensively in machine learning research, but also recent contributions.\nOpen ML Sort of abandoned years ago, but lots of eclectic datasets remain.\nIMDB Datasets Information about movies and TVs. (Big files!)\nStanford Network Analysis Project Datasets presented as networks.\n\n\n\nOpen-ended\nThese require you to navigate an interface to select data from a large pool. Typically, you can make selections, preview the dataset, and then download in CSV or Excel format.\n\nU.S. Census Bureau Tons of demographic data about the U.S.\nData.gov Home for all open U.S. government data.\nUNICEF Portal Worldwide data about child welfare.\nWorld Bank Focuses on economic and development data.\nWorld Health Organization Information on health and disease."
  },
  {
    "objectID": "resources.html#glossary",
    "href": "resources.html#glossary",
    "title": "Resources",
    "section": "Glossary",
    "text": "Glossary\nA much more exhaustive glossary can be found here.\n\nGit\n\nGit Protocol for maintaining the entire file history of a project, including all versions and author attributions.\nrepository Collection of files needed to record the history of a git project.\nGitHub Website that hosts git repositories created by private users, along with tools to help inspect and manage them.\ncommit Collection of particular changes to the repository made by an individual and given a message.\nstage Temporary designation of locally modified files to be added to the next commit.\nmerge Automatic union of non-conflicting commits from different sources.\nconflict Disagreement between repository versions that requires human intervention to resolve.\npush Sending one or more commits from a local repository to a remote repository.\npull Receiving and merging all commits from a remote repository that are unknown to the local repository.\n\n\n\nNotebooks\n\nnotebook Self-contained collection of text, math, code, output, and graphics.\nkernel Back-end that executes code from and returns output to the notebook.\ncell Atomic unit of a notebook that contains one or more lines of text or code.\nMarkdown Simplified syntax to put boldface, italics, and other formatting within text.\nTeX/LaTeX Language used to express mathematical notation within a notebook.\nJupyter Popular format and system for interactive editing, execution, and export of notebooks.\nJupyter Lab Layer over Jupyter notebook functionality to help manage notebooks and extensions.\n\n\n\nPython\n\npackage (or wheel) Collection of Python files distributed in a portable way to provide extra functionality.\nnumpy Package of essential tools for numerical computation.\nscipy Package of tools useful in scientific and engineering computation.\ndatabase Structured collection of data, usually with a formal interface for interaction with the data.\ndata frame Tabular representation of a data set analogous to a spreadsheet, in which columns are observable quantities and rows are different observations of the quantities.\npandas Package for working with data frames.\nmatplotlib Package providing plot capabilities, modeled on MATLAB.\nseaborn Package built on matplotlib and providing commands to simplify creating plots from data frames.\nscikit-learn Package that provides capabilities for machine learning using a variety of methods.\ntensorflow, keras, pytorch Best-known packages for machine learning using neural networks.\nAnaconda Bundle of Python, most of the popular Python packages, Jupyter, and other useful tools, all within an optional point-and-click interface.\n\n\n\nEditors/IDEs\n\nVS Code (recommended) Free all-purpose editor with many extensions for working closely with git, Github, Jupyter, and Python.\nJupyter Popular format and system for interactive editing, execution, and export of notebooks.\nJupyter Lab Layer over Jupyter notebook functionality to help manage notebooks and extensions.\nGoogle Colab Free cloud-based service for jumping into Jupyter notebooks without installing any software locally.\nSpyder Free development environment that somewhat resembles MATLAB.\nPyCharm Feature-rich freemium development environment for Python, geared toward large, complex projects.\nThonny Bare-bones development environment intended to prioritize beginners."
  },
  {
    "objectID": "starting.html#back-end",
    "href": "starting.html#back-end",
    "title": "Getting started",
    "section": "Back end",
    "text": "Back end\nHere are some pros and cons of the usual options:\n\n\n\nYour own machine\nCloud\n\n\n\n\nAvailable offline 😄\nUse any device 👍\n\n\nTotal control 😎/😕\nNo installations 😌\n\n\nChoose the font end\nBrowser only\n\n\nYours forever 😁\nPermanence is an illusion ⛄️\n\n\nYour hardware 💵\nYou get what you get ¯\\(ツ)/¯\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou want to use Python 3.x, not Python 2.x. These coexisted for a while during a dark time for Python. If you see guidance based on Python 2 now, it is either terribly outdated or the product of a disordered mind.\n\n\n\nRun on your own computer\nMy recommended option is to download Anaconda. It’s a big download and a long installation, but it comes with just about everything ready to go.\n\n\nRun on the cloud\nThere are many choices, but a popular one is Google Colab. It’s free and saves you from having to install anything. The hardware is basic but most likely fine for our purposes. You will need to download your results for assignment submissions."
  },
  {
    "objectID": "starting.html#front-end",
    "href": "starting.html#front-end",
    "title": "Getting started",
    "section": "Front end",
    "text": "Front end\nMuch of data science is expressed using notebooks, which we will use exclusively. Specifically, you will use Jupyter notebooks.\n\n\n\n\n\n\nTip\n\n\n\nThe name IPython refers to the direct ancestor of Jupyter. The older name continues to stick to a few of the tools, though.\n\n\nA notebook is a self-contained collection of text, math, code, output, and graphics grouped into cells. The front end manages the cells and communicates with a back end called the kernel.\nIf you are using Google Colab, then you are using a Jupyter notebook, though within an interface customized by Google.\n\nOn your own machine\nJupyter splits into “classic” Jupyter and the newer Jupyter Lab. Either is fine for this book, but there is no reason to prefer the older variant. Just start it up from the Anaconda dashboard, and it will open your web browser to the front-end server."
  },
  {
    "objectID": "starting.html#setting-up-vs-code",
    "href": "starting.html#setting-up-vs-code",
    "title": "Getting started",
    "section": "Setting up VS Code",
    "text": "Setting up VS Code\nIf you use the free Visual Studio Code editor, you can take advantage of its AI assistant.\n\nFirst install Anaconda Python. (Be patient! It’s a big download and a long installation.)\nDownload and install Visual Studio Code.\nStart VS Code and look for the “Extensions” icon on the left. (For me, it looks like 4 building blocks.) Search for “Jupyter” and install the extension by Microsoft.\nYou may also want to install the Python extension by Microsoft.\n\nA notebook file has the extension .ipynb. When you open or create such a file, you will see “Choose Kernel” button in the top right. The kernel is the background computing process that produces output for the notebook. If you click that, button, select “Python environments…” and then select the one called “base” or “anaconda3” or something similar.\n\n\n\n\n\n\nImportant\n\n\n\nYou may have other Python environments installed on your computer. Use the one associated with Anaconda to access the packages we will use in this course.\n\n\nOnce the kernel is selected for a notebook file, that selection will be remembered the next time you open the file."
  },
  {
    "objectID": "starting.html#tips-on-jupyter-success",
    "href": "starting.html#tips-on-jupyter-success",
    "title": "Getting started",
    "section": "Tips on Jupyter success",
    "text": "Tips on Jupyter success\n\n\n\n\n\n\nWarning\n\n\n\nThe order of cells that you see in a notebook is not necessarily the order in which they were executed.\n\n\nBy far the greatest source of confusion and subtle problems in a notebook is the freedom it gives you to execute the cells in whatever order you want. As you experiment and add and delete cells to try things out, you will reach a point at which the code on the screen is no longer a recipe for reaching the current state of your workspace.\n\n\n\n\n\n\nTip\n\n\n\nBefore submitting a notebook, it’s highly recommended to restart the kernel and run all cells in order, just to make sure that everything still works as seen on screen."
  },
  {
    "objectID": "genAI.html#getting-started-with-github-copilot",
    "href": "genAI.html#getting-started-with-github-copilot",
    "title": "Generative AI",
    "section": "Getting started with GitHub Copilot",
    "text": "Getting started with GitHub Copilot\n\nSign up for a free GitHub account.\nApply for GitHub Global Campus as a student. This requires uploading some validation of your student status, such as a student ID or bursar’s receipt. It may take some time for your status to be verified.\nDownload and install Visual Studio Code.\nInstall the GitHub Copilot and the GitHub Copilot chat extensions for Visual Studio Code.\nSign in to your GitHib account in Visual Studio Code. You won’t need the technical information on how to use Git or GitHub itself, but this is what unlocks access to GitHub Copilot."
  },
  {
    "objectID": "genAI.html#usage",
    "href": "genAI.html#usage",
    "title": "Generative AI",
    "section": "Usage",
    "text": "Usage\nThere are now several ways you can interact with Copilot:\n\nOpen or create a code file and start typing. You will soon start to see grayed-out suggestions from Copilot. You can accept these suggestions by pressing Tab or Enter. If you continue typing, the suggestions will change to match. Note that Copilot will use the open file as context for its suggestions, including comments and variable names.\nWithin your code file, you can type Ctrl+i, or Cmd+i on a Mac, to open the Copilot panel. You can type in the panel to ask questions or create a prompt for it to respond to.\nAlong the left side of the VS Code window are icons representing the extensions you have installed. The Copilot Chat icon is a pair of speech bubbles (though this can change). Clicking on this icon will open a chat window.\nYou can use Ctrl+Shift+P, or Cmd+Shift+P on a Mac, to open the command palette and type “Copilot” to see a list of commands you can use to interact with Copilot. For example, you can ask Copilot to generate a function definition or a class definition, or to complete a line of code."
  },
  {
    "objectID": "data.html#quantitative-data",
    "href": "data.html#quantitative-data",
    "title": "1  Representation of data",
    "section": "1.1 Quantitative data",
    "text": "1.1 Quantitative data\n\nDefinition 1.1 A quantitative value is one that is numerical and supports meaningful comparison and arithmetic operations.\n\nQuantitative data is further divided into continuous and discrete types. The difference is the same as between real numbers and integers.\n\nExample 1.1 Some continuous quantitative data sources:\n\nTemperature at noon at a given airport\nYour height\nVoltage across the terminals of a battery\n\nExamples of discrete quantitative data:\n\nThe number of shoes you own\nNumber of people at a restaurant table\nScore on an exam\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nSometimes there may be room for interpretation or context. For example, the retail price of a gallon of milk might be regarded as discrete data, since it technically represents a whole number of pennies. But in finance, transactions are regularly computed to much higher precision, so it might make more sense to interpret prices as continuous values. As a rule of thumb, if there are many possible values, or the values are the result of a measurement, the continuous interpretation is usually more appropriate.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNot all numerical values represent truly quantitative data. ZIP codes (postal codes) in the U.S. are 5-digit numbers, and while there is some logic to how they were assigned, there is no clearly meaningful interpretation of averaging them, for instance.\n\n\nFor both continuous and discrete quantities, it makes sense to order different values, compute averages of them, etc. (However, averages of discrete quantities are continuous.)\nMathematically, the real and integer number sets are infinite, but computers are finite machines. Integers are represented exactly within some range that is determined by how many binary bits are dedicated. The computational analog of real numbers are floating-point numbers, or more simply, floats. These are bounded in range as well as discretized. The details are complicated, but essentially, the floating-point numbers have about 16 significant decimal digits by default—which is virtually always far more precision than real data offers.\n\n\n\n\n\n\n\n\n\nExample 1.2 if you enter an integer, it is interpreted as an int:\n\n# This is an integer (int type)\nprint(3, \"is a\", type(3))\n\n3 is a &lt;class 'int'&gt;\n\n\nTo get the floating-point version, you add a decimal point:\n\n# This is a real number (float type)\nprint(3.0, \"is a\", type(3.0))\n\n3.0 is a &lt;class 'float'&gt;\n\n\nYou can also use scientific notation:\n\nprint(6.02e23, \"is Avogadro's number\")\nprint(6.62607e-34, \"is Planck's constant in Joule-seconds\")\n\n6.02e+23 is Avogadro's number\n6.62607e-34 is Planck's constant in Joule-seconds\n\n\nThere are also explicit converter functions. The conversion of an int to a float is generally exact:\n\n# Convert int to float (generally no change to numerical value)\nprint( \"float(3) creates\",float(3) )\n\nfloat(3) creates 3.0\n\n\nBut the conversion of int to float truncates the number, which can cause subtle bugs:\n\n# Truncate float to int\nprint( \"int(3.14) creates\", int(3.14) )\n\nint(3.14) creates 3\n\n\n\n\n\n\n\n\nWarning\n\n\n\nBe careful about comparing floating-point values.\n\n\nBecause operations on floats are not exact, results can be surprising:\n\nprint(0.2 - 0.1 == 0.1)    # as expected\nprint(3.1 - 3.0 == 0.1)    # uh-oh\n\nTrue\nFalse\n\n\nThe Numpy function isclose is a safer way to compare floats:\n\nimport numpy as np\nprint( np.isclose(3.1 - 3.0, 0.1) )\n\nTrue\n\n\n\n\n1.1.1 Inf and NaN\nThere are two additional quasi-numerical float values to be aware of as well.\n\n\n\n\n\n\nImportant\n\n\n\nFor numerical work in Python, the NumPy package indispensible. We will use it often, and it is also loaded and used by most other scientifically oriented packages.\n\n\nThe value inf in NumPy stands for infinity.\n\n\n\n\n\n\n\n\n\nExample 1.3 Infinity is greater than every number, and some arithmetic operations with infinity are well defined:\n\nimport numpy as np\nprint( \"(np.inf + 5) is\", np.inf + 5 )\nprint( \"(np.inf + np.inf) is\", np.inf + np.inf )\nprint( \"(5 - np.inf) is\", 5 - np.inf )\n\n(np.inf + 5) is inf\n(np.inf + np.inf) is inf\n(5 - np.inf) is -inf\n\n\nHowever, in calculus you learned that some expressions with infinity are considered to be undefined without additional information (e.g., L’Hôpital’s Rule):\n\nprint( \"np.inf / np.inf is\", np.inf / np.inf )\n\nnp.inf / np.inf is nan\n\n\nThe result nan above stands for Not a Number. It is the result of indeterminate arithmetic operations, like \\(\\infty/\\infty\\) and \\(\\infty - \\infty\\). It is also used sometimes as a placeholder for missing data.\n\n\n\n\n\n\nWarning\n\n\n\nBy definition, every operation that involves a NaN value results in a NaN.\n\n\nOne notorious trap of using NaN is that nan==nan is not True!\n\nprint( \"np.nan == np.nan is\", np.nan == np.nan )\n\nnp.nan == np.nan is False\n\n\nInstead, you should use np.isnan to check for NaN values:\n\nprint( \"np.isnan(np.nan) is\", np.isnan(np.nan) )\n\nnp.isnan(np.nan) is True\n\n\n\n\n\n1.1.2 Dates and times\nHandling times and dates can be tricky. Aside from headaches such as time zones and leap years, there are many different ways people and machines represent dates, and with varying amounts of precision. Python has its own inbuilt system for handling dates and times, but we will show the facilities provided by the NumPy package, which is more comprehensive.\nThere are two basic types:\n\nDefinition 1.2 A datetime is a representation of an instant in time. A time delta is a representation of a duration; i.e., a difference between two datetimes.\n\n\n\n\n\n\n\nImportant\n\n\n\nNative Python uses a datetime type, while NumPy uses datetime64.\n\n\n\nExample 1.4 You can use many natural date formats within strings:\n\nimport numpy as np\nnp.datetime64(\"2020-01-17\")    # YYYY-MM-DD \nnp.datetime64(\"1969-07-20T20:17\")    # YYYY-MM-DDThh:mm\n\nnumpy.datetime64('1969-07-20T20:17')\n\n\n\n# Current date and time, down to the second\nnp.datetime64(\"now\")\n\nnumpy.datetime64('2024-02-13T21:16:20')\n\n\nA time delta in NumPy indicates its units (i. e., its granularity).\n\nnp.datetime64(\"1969-07-20T20:17\") - np.datetime64(\"today\")\n\nnumpy.timedelta64(-28699423,'m')"
  },
  {
    "objectID": "data.html#vectors",
    "href": "data.html#vectors",
    "title": "1  Representation of data",
    "section": "1.2 Vectors",
    "text": "1.2 Vectors\nMost interesting phenomena are characterized and influenced by more than one factor. Collections of values therefore play a central role in data science. The workhorse type for collections in base Python is the list. However, we’re going to need some more powerful metaphors and tools as well, beginning with vectors.\n\nDefinition 1.3 A vector is a collection of values called elements, all of the same type, indexed by consecutive integers.\n\n\n\n\n\n\n\nImportant\n\n\n\nIn math, vector indexes usually begin with 1. In Python, they begin with 0.\n\n\nA vector with \\(n\\) elements is often referred to as an \\(n\\)-vector, and we say that \\(n\\) is the length of the vector. In math we often use \\(\\real^n\\) to denote the set of all \\(n\\)-vectors with real-valued elements.\n\n\n\n\n\n\n\n\n\nExample 1.5 The usual way to work with vectors in Python is through NumPy:\n\nimport numpy as np\n\nx = np.array( [1, 2, 3, 4, 5] )\nx\n\narray([1, 2, 3, 4, 5])\n\n\nA vector has a data type for its elements:\n\nx.dtype\n\ndtype('int64')\n\n\nAny float values in the vector cause the data type of the entire vector to be float:\n\ny = np.array( [1.0, 2, 3, 4, 5] )\ny.dtype\n\ndtype('float64')\n\n\nUse len to determine the length of a vector:\n\nlen(x)\n\n5\n\n\nYou can create a special type of vector called a range that has equally spaced elements:\n\nnp.arange(0, 5)\n\narray([0, 1, 2, 3, 4])\n\n\n\nnp.arange(1, 3, 0.5)\n\narray([1. , 1.5, 2. , 2.5])\n\n\nThe syntax here is (start,stop,step).\n\n\n\n\n\n\nCaution\n\n\n\nIn base Python and in NumPy, the last element of a range is omitted. This is guaranteed to cause confusion if you are used to just about any other computer language.\n\n\nThere are functions for creating vectors of ones and zeros:\n\nprint(np.ones(5))\nprint(np.zeros(4))\n\n[1. 1. 1. 1. 1.]\n[0. 0. 0. 0.]\n\n\n\n\n1.2.1 Element access\nThe elements of a vector can be accessed directly using square brackets notation.\n\n\n\n\n\n\n\n\n\nExample 1.6 Use square brackets to refer to an element of a vector, remembering that the first element has index 0:\n\nx[0]\n\n1\n\n\n\nx[4]\n\n5\n\n\nNegative values for the index are counted from the end. The last element of a vector always has index -1, and more-negative values move backward through the elements:\n\nx[-1]\n\n5\n\n\n\nx[-3]\n\n3\n\n\n\nx[-len(x)]\n\n1\n\n\nElement references can also be on the left side of an assignment:\n\nx[2] = -3\nx\n\narray([ 1,  2, -3,  4,  5])\n\n\nNote, however, that once the data type of a vector is set, it can’t be changed:\n\nx[0] = 1.234\nx    # float was truncated to int, without warning!!!\n\narray([ 1,  2, -3,  4,  5])\n\n\nYou can also use a list in square brackets to access multiple elements at once:\n\nx[ [0, 2, 4] ]\n\narray([ 1, -3,  5])\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe result of a list reference is a new vector, not a number.\n\n\nNote the difference here:\n\nx[0]\n\n1\n\n\n\nx[ [0] ]\n\narray([1])\n\n\n\n\n\n1.2.2 Slicing\nYou can also access multiple elements of a vector by using a list or range as the index. In the latter case, this is called slicing.\n\n\n\n\n\n\n\n\n\nExample 1.7 The syntax of a slice is start:stop.\n\n\n\n\n\n\nWarning\n\n\n\nAs with ranges in base Python, the last element of a slice range is omitted. This causes many headaches and bugs.\n\n\n\nx[0:3]\n\narray([ 1,  2, -3])\n\n\n\nx[-3:-1]\n\narray([-3,  4])\n\n\nIf you want to use a range that skips over elements, you can use a third argument in the form start:stop:step:\n\nx[0:2:5]\n\narray([1])\n\n\nWhen the start of the range is omitted, it means “from the beginning”, and when stop is omitted, it means “through to the end.” Hence, [:k] means “first \\(k\\) elements” and [-k:] means “last \\(k\\) elements”:\n\nx[:3]\n\narray([ 1,  2, -3])\n\n\n\nx[-3:]\n\narray([-3,  4,  5])\n\n\nAnd we also have this idiom:\n\nx[::-1]   # reverse the vector\n\narray([ 5,  4, -3,  2,  1])\n\n\nNumPy will happily allow you to reference invalid indexes. It will just return as much as is available without warning or error:\n\nx[:10]\n\narray([ 1,  2, -3,  4,  5])"
  },
  {
    "objectID": "data.html#arrays",
    "href": "data.html#arrays",
    "title": "1  Representation of data",
    "section": "1.3 Arrays",
    "text": "1.3 Arrays\nA vector is a special case of a more general construct.\n\nDefinition 1.4 An array is a collection of values called elements, all of the same type, indexed by one or more sets of consecutive integers. The number of indexes needed to specify a value is the dimension of the array.\n\n\n\n\n\n\n\nNote\n\n\n\nA dimension is called an axis in NumPy and related packages.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe term matrix is often used simply to mean a 2D array. Technically, though, a matrix should have only numerical values, and matrices obey certain properties that make them important mathematical objects. These properties and their consequences are studied in linear algebra.\n\n\nFor us, arrays will mostly be the results of importing or working on data. But it’s occasionally useful to know how to build them from scratch.\n\n\n\n\n\n\n\n\n\nExample 1.8 One way to construct an array is by a list comprehension:\n\nA = np.array([ [j-i for j in range(6)] for i in range(4) ])\nA\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [-1,  0,  1,  2,  3,  4],\n       [-2, -1,  0,  1,  2,  3],\n       [-3, -2, -1,  0,  1,  2]])\n\n\nThe shape of an array is what we would often call the size:\n\nA.shape\n\n(4, 6)\n\n\nThere is no difference between a vector and a 1D array:\n\nx.shape\n\n(5,)\n\n\nThere is also no difference between a 2D array and a vector of vectors giving the rows of the array.\n\nR = np.array( [ [1, 2, 3], [4, 5, 6] ])\nR\n\narray([[1, 2, 3],\n       [4, 5, 6]])\n\n\nHere are some other common ways to construct arrays.\n\nnp.ones(5)\n\narray([1., 1., 1., 1., 1.])\n\n\n\nnp.zeros( (3, 6) )\n\narray([[0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 0.]])\n\n\n\nnp.repeat(np.pi, 3)\n\narray([3.14159265, 3.14159265, 3.14159265])\n\n\nYou can also stack arrays vertically or horizontally to create new arrays.\n\nnp.hstack( ( np.ones((2, 2)), np.zeros((2, 3)) ) )\n\narray([[1., 1., 0., 0., 0.],\n       [1., 1., 0., 0., 0.]])\n\n\n\nnp.vstack( (range(5), range(5, 0, -1)) )\n\narray([[0, 1, 2, 3, 4],\n       [5, 4, 3, 2, 1]])\n\n\n\n\n1.3.1 Indexing and slicing\nOne way to think about a 2D array is a vector of vector rows of the same length. That idea is reflected in NumPy syntax.\n\n\n\n\n\n\n\n\n\nExample 1.9 We can use successive brackets to refer to an element, row then column:\n\nR[1][2]    # second row, third column\n\n6\n\n\nIt’s equivalent, and typically more convenient, to use a single bracket set with indexes separated by commas:\n\nR[1, 2]    # second row, third column\n\n6\n\n\nYou can use slicing ranges in each dimension individually.\n\nR[:1, -2:]    # first row, last two columns\n\narray([[2, 3]])\n\n\nThe result above is another 2D array. Note how this result is subtly different:\n\nR[0, -2:]\n\narray([2, 3])\n\n\nBecause we accessed an individual row, not a slice, the result is one dimension lower—a vector. Finally, a : in one slice position means to keep everything in that dimension.\n\nA[:, :2]    # all rows, first 2 columns\n\narray([[ 0,  1],\n       [-1,  0],\n       [-2, -1],\n       [-3, -2]])\n\n\n\n\n\n1.3.2 Reductions\nA common task is to reduce an array along one dimension, called an axis in numpy, resulting in an array of one less dimension.\n\n\n\n\n\n\n\n\n\nExample 1.10 The first dimension of an array is axis number 0, the second is axis number 1, etc.\n\nA\n\narray([[ 0,  1,  2,  3,  4,  5],\n       [-1,  0,  1,  2,  3,  4],\n       [-2, -1,  0,  1,  2,  3],\n       [-3, -2, -1,  0,  1,  2]])\n\n\n\nnp.sum(A, axis=0)    # sum along the rows\n\narray([-6, -2,  2,  6, 10, 14])\n\n\nEven when we sum along the column dimension, axis=1, the result is a vector that we would interpret as a row, not a column.\n\nnp.sum(A, axis=1)    # sum along the columns\n\narray([15,  9,  3, -3])\n\n\nIf you don’t specify an axis, the reduction occurs over all dimensions at once, resulting in a single number.\n\nnp.sum(A)\n\n24\n\n\n\nYou can also do reductions with np.max, np.min, np.mean, etc."
  },
  {
    "objectID": "data.html#qualitative-data",
    "href": "data.html#qualitative-data",
    "title": "1  Representation of data",
    "section": "1.4 Qualitative data",
    "text": "1.4 Qualitative data\nA qualitative value is one that is not quantitative. There are numerous types, but we will consider only a few of them here. Ultimately, we need to be able to represent qualitative data in a quantitative way for use in algorithms.\n\n1.4.1 Categorical\n\nDefinition 1.5 Categorical data has values drawn from a finite set \\(S\\) of categories. If the members of \\(S\\) support meaningful ordering comparisons, then the data is ordinal; otherwise, it is nominal.\n\n\nExample 1.11 Examples of ordinal categorical data:\n\nSeat classes on a commercial airplane (e.g., economy, business, first)\nLetters of the alphabet\n\nExamples of nominal categorical data:\n\nYes/No responses\nMarital status\nMake of a car\n\nThere are nuanced cases. For instance, letter grades are themselves ordinal categorical data. However, schools convert them to discrete quantitative data and then compute a continuous quantitative GPA.\n\nOne way to quantify ordinal categorical data is to assign integer values to the categories in a manner that preserves ordering. This approach can be suspect, though, when it comes to operations such as averaging or computing a distance between values.\n\n1.4.1.1 Dummy variables\nAnother means of quantifying categorical data is called dummy variables in classical statistics and one-hot encoding in much of machine learning. Suppose a variable \\(x\\) has values in a category set that has \\(m\\) members, i. e., \\(S = \\{c_1,\\ldots,c_m\\}\\). There are two ways to replace the values with dummy variables.\n\nIntroduce \\(m\\) variables \\(x_1,\\ldots,x_m\\), where for a given categorical value \\(v\\) we have\n\n\\[\nx_i = \\begin{cases} 1, & v = c_i, \\\\ 0, & v \\neq c_i. \\end{cases}\n\\]\n\nIntroduce only \\(m-1\\) variables, leaving out \\(x_m\\). If \\(x_1=x_2=\\ldots=x_{m-1}=0\\), then we know that the value is \\(c_m\\). (This variant is important in statistics because otherwise, \\(x_m\\) has to be correlated with the other \\(x_i\\).)\n\n\nExample 1.12 Suppose that the stooge variable can take the values Moe, Larry, Curly, or Shemp, and we have the dataset\n[Curly, Moe, Curly, Shemp]\nIf we use 4 dummy variables, the data would be replaced by the array\n[ [0, 0, 1, 0], [1, 0, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1] ]\nIf we use only 3 dummy variables, we would get\n[ [0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 0] ]\n\n\n\n\n1.4.2 Text\nText is a ubiquitous data source. One way to quantify text is to use a dictionary of interesting keywords \\(w_1,\\ldots,w_n\\). Given a collection of documents \\(d_1,\\ldots,d_m\\), we can define an \\(m\\times n\\) document–term matrix \\(T\\) by letting \\(T_{ij}\\) be the number of times term \\(j\\) appears in document \\(i\\).\n\n\n1.4.3 Images\nThe most straightforward way to represent an image is as a 3D array of values representing intensities representing of red, green and blue in each pixel. Sometimes it might be preferable to represent the image by a vector of statistics about these values, or by presence or absence of detected objects, etc."
  },
  {
    "objectID": "data.html#series-and-frames",
    "href": "data.html#series-and-frames",
    "title": "1  Representation of data",
    "section": "1.5 Series and frames",
    "text": "1.5 Series and frames\nThe most popular Python package for manipulating and analyzing data is pandas. We will use the paradigm it presents, which is fairly well understood throughout data science.\n\nDefinition 1.6 A series is a vector that is indexed by a finite ordered set. A data frame is a collection of series that all share the same index set.\n\nWe can conceptualize a series as a vector plus an index list, and a data frame as a 2D array with index sets for the rows and the columns. In that sense, they are simply syntactic sugar. However, since people are much better at remembering the meaning of words than arbitrarily assigned integers, data frames serve to prevent errors and misunderstandings.\n\nExample 1.13 Some data that can be viewed as series:\n\nThe length, width, and height of a box can be expressed as a 3-vector of positive real numbers. If we index the vector by the names of the measurements, it becomes a series.\nThe number of steps taken by an individual over the course of a week can be expressed as a 7-vector of nonnegative integers. We could index it by the integers 1–7, or in a series by the names of the days of the week.\nThe bid prices of a stock at the end of each trading day can be represented as a time series, in which the index is drawn from timestamps.\nThe scores of gymnasts on multiple apparatus types can be represented as a data frame whose rows are indexed by the names of the gymnasts and whole columns are indexed by the names of the apparatuses.\n\n\n\nExample 1.14 Here is a pandas series for the wavelengths of light corresponding to rainbow colors:\n\nimport pandas as pd\n\nwavelength = pd.Series( \n  [400, 470, 520, 580, 610, 710],    # values\n  index=[\"violet\", \"blue\", \"green\", \"yellow\", \"orange\", \"red\"],\n  name=\"wavelength\"\n  )\n\nwavelength\n\nviolet    400\nblue      470\ngreen     520\nyellow    580\norange    610\nred       710\nName: wavelength, dtype: int64\n\n\nWe can use an index value (one of the colors) to access a value in the series:\n\nwavelength[\"blue\"]\n\n470\n\n\nIf we access multiple values, we get a series that is a subset of the original:\n\nwavelength[ [\"violet\", \"red\"] ]\n\nviolet    400\nred       710\nName: wavelength, dtype: int64\n\n\nWe can also use the iloc property to access the underlying vector by NumPy slicing:\n\nwavelength.iloc[:4]\n\nviolet    400\nblue      470\ngreen     520\nyellow    580\nName: wavelength, dtype: int64\n\n\nHere is a series of NFL teams based on the same index:\n\nteam = pd.Series(\n  [\"Vikings\", \"Bills\", \"Eagles\", \"Chargers\", \"Bengals\", \"Cardinals\"],\n  index = wavelength.index,\n  name=\"team\"\n  )\n\nteam[\"green\"]\n\n'Eagles'\n\n\nNow we can create a data frame using these two series as columns:\n\nrainbow = pd.DataFrame( {\"wavelength\": wavelength, \"team name\": team} )\nrainbow\n\n\n\n\n\n\n\n\nwavelength\nteam name\n\n\n\n\nviolet\n400\nVikings\n\n\nblue\n470\nBills\n\n\ngreen\n520\nEagles\n\n\nyellow\n580\nChargers\n\n\norange\n610\nBengals\n\n\nred\n710\nCardinals\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nCurly braces { } are used to construct a dictionary in Python.\n\n\nWe can access a single column using simple bracket notation:\n\nrainbow[\"team name\"]\n\nviolet      Vikings\nblue          Bills\ngreen        Eagles\nyellow     Chargers\norange      Bengals\nred       Cardinals\nName: team name, dtype: object\n\n\nWe can add a column after the fact by using a bracket access on the left side of the assignment:\n\nrainbow[\"flower\"] = [\n    \"Lobelia\", \n    \"Cornflower\", \n    \"Bells-of-Ireland\", \n    \"Daffodil\",\n    \"Butterfly weed\",\n    \"Rose\"\n    ]\n\nrainbow\n\n\n\n\n\n\n\n\nwavelength\nteam name\nflower\n\n\n\n\nviolet\n400\nVikings\nLobelia\n\n\nblue\n470\nBills\nCornflower\n\n\ngreen\n520\nEagles\nBells-of-Ireland\n\n\nyellow\n580\nChargers\nDaffodil\n\n\norange\n610\nBengals\nButterfly weed\n\n\nred\n710\nCardinals\nRose\n\n\n\n\n\n\n\nWe can access a row by using brackets with the loc property of the frame, getting a series indexed by the column names of the frame:\n\nrainbow.loc[\"orange\"]\n\nwavelength               610\nteam name            Bengals\nflower        Butterfly weed\nName: orange, dtype: object\n\n\nWe are also free to strip away the index and get an ordinary array:\n\nrainbow.loc[\"red\"].to_numpy()\n\narray([710, 'Cardinals', 'Rose'], dtype=object)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 1.15 Here are some more direct ways to construct a data frame in pandas.\nIn this case, we give a list of rows, plus (optionally) the index and the names of the columns:\n\npd.DataFrame( \n    [ (\"white\", 4), (\"brown\", 8), (\"pink\", 7) ], \n    columns=[\"Color\", \"Rating\"],\n    index=[\"vanilla\", \"chocolate\", \"strawberry\"]\n    )\n\n\n\n\n\n\n\n\nColor\nRating\n\n\n\n\nvanilla\nwhite\n4\n\n\nchocolate\nbrown\n8\n\n\nstrawberry\npink\n7\n\n\n\n\n\n\n\nWe can also specify data as columns by using a dictionary:\n\npd.DataFrame( \n    { \"Color\": [\"white\", \"brown\", \"pink\"], \"Rating\": [4, 8, 7] }, \n    index=[\"vanilla\", \"chocolate\", \"strawberry\"]\n    )\n\n\n\n\n\n\n\n\nColor\nRating\n\n\n\n\nvanilla\nwhite\n4\n\n\nchocolate\nbrown\n8\n\n\nstrawberry\npink\n7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1.5.1 Common operations\n\n\nTable 1.1: Operations on pandas series\n\n\n\n\n\n\n\nDescription\nSyntax\nResult\n\n\n\n\nFirst or last entries\ns.head(), s.tail()\nSeries\n\n\nLength\nlen(s)\ninteger\n\n\nNumber of entries\ns.shape\ntuple\n\n\nAll of the values\ns.values\narray\n\n\nConvert to list\nlist(s)\nlist\n\n\nIndex\ns.index\nIndex\n\n\nUnique values\ns.unique()\narray\n\n\nAppearance counts for values\ns.value_counts()\nSeries\n\n\nExtreme values\ns.min(), s.max()\nnumber\n\n\nSum\ns.sum()\nnumber\n\n\nComparison\ns &gt; 1, s==\"foo\"\nboolean Series\n\n\nLocate missing\ns.isna()\nboolean Series\n\n\nArithmetic\ns + 1, s * t\nSeries\n\n\nDelete one or more rows\ns.drop(0)\nSeries\n\n\n\n\nPandas offers many methods for manipulating series and data frames. Table 1.1 shows common operations that can be applied to a series. Since a column of a data frame is a series, these operations can be applied in that context, too. There is an exhaustive list in the pandas documentation.\n\n\n\n\n\n\nCaution\n\n\n\nAs always in Python, you need to pay attention to the difference between applying a function, like foo(bar), accessing a property, like foo.bar, and calling an object method, foo.bar(). Extra or missing parentheses groups can cause errors.\n\n\n\n\nTable 1.2: Operations on pandas data frames\n\n\n\n\n\n\n\nDescription\nSyntax\nResult\n\n\n\n\nFirst or last rows\ndf.head(), df.tail()\nDataFrame\n\n\nNumber of rows\nlen(df)\ninteger\n\n\nNumber of rows and columns\ndf.shape\ntuple\n\n\nAll of the values\ndf.values\narray\n\n\nRow index\ndf.index\nIndex\n\n\nColumn names\nlist(df)\nlist\n\n\nColumn names\ndf.columns\nIndex\n\n\nAccess by column name(s)\ndf[\"name\"]\nSeries or DataFrame\n\n\nAccess by name\ndf.loc[rows, \"name\"]\n(varies)\n\n\nAccess by position\ndf.iloc[i, j]\n(varies)\n\n\nSum\ndf.sum()\nSeries\n\n\nComparison\ns &gt; 1, s==\"foo\"\nboolean DataFrame\n\n\nDelete a column\ndf.drop(name, axis=1)\nDataFrame\n\n\nApply columnwise\ndf.apply(myfun)\n(varies)\n\n\n\n\nTable 1.2 shows additional operations that can be applied to an entire data frame (or to any subset). It takes a lot of space to demonstrate all the ways in which these can be used, as is done in the pandas user guide, so they are just collected here for future reference. There is also an exhaustive list of them in the pandas documentation.\n\n\n1.5.2 Categorical data in pandas\nPandas has many facilities for dealing with categorical variables.\n\n\n\n\nExample 1.16 Here is a dataset about features and prices of diamonds:\n\nimport seaborn as sns\ndiamonds = sns.load_dataset(\"diamonds\")\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\nSI2\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\nSI1\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\nVS1\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\nVS2\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\nSI2\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nAs you can see above, some of the features (cut, color, clarity) have string designations. We could convert clarity, for example, to a categorical variable like so:\n\npd.Categorical(diamonds[\"clarity\"])\n\n['SI2', 'SI1', 'VS1', 'VS2', 'SI2', ..., 'SI1', 'SI1', 'SI1', 'SI2', 'SI2']\nLength: 53940\nCategories (8, object): ['IF', 'VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2', 'I1']\n\n\nHowever, these values actually have a specific ordering in this context, and that is lost in the nominal variable above. We can fix that by specifying the ordering when we create the categorical variable:\n\nclarities = [\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"]\npd.Categorical(diamonds[\"clarity\"], categories=clarities, ordered=True)\n\n['SI2', 'SI1', 'VS1', 'VS2', 'SI2', ..., 'SI1', 'SI1', 'SI1', 'SI2', 'SI2']\nLength: 53940\nCategories (8, object): ['I1' &lt; 'SI2' &lt; 'SI1' &lt; 'VS2' &lt; 'VS1' &lt; 'VVS2' &lt; 'VVS1' &lt; 'IF']\n\n\nYou often want to simply replace the ordered categories with integers:\n\ncl_num = pd.Categorical(diamonds[\"clarity\"], categories=clarities, ordered=True)\ndiamonds[\"clarity\"] = cl_num.codes\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\nIdeal\nE\n1\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\nPremium\nE\n2\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\nGood\nE\n4\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\nPremium\nI\n3\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\nGood\nJ\n1\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\nIn such cases, it’s more direct to use string replacement:\n\ncuts = [\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"]\ndiamonds[\"cut\"] = diamonds[\"cut\"].replace(\n    cuts,              # to be replaced\n    range(1, 6),       # replacements\n)\ndiamonds.head()\n\n\n\n\n\n\n\n\ncarat\ncut\ncolor\nclarity\ndepth\ntable\nprice\nx\ny\nz\n\n\n\n\n0\n0.23\n5\nE\n1\n61.5\n55.0\n326\n3.95\n3.98\n2.43\n\n\n1\n0.21\n4\nE\n2\n59.8\n61.0\n326\n3.89\n3.84\n2.31\n\n\n2\n0.23\n2\nE\n4\n56.9\n65.0\n327\n4.05\n4.07\n2.31\n\n\n3\n0.29\n4\nI\n3\n62.4\n58.0\n334\n4.20\n4.23\n2.63\n\n\n4\n0.31\n2\nJ\n1\n63.3\n58.0\n335\n4.34\n4.35\n2.75\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA common need is to convert a categorical variable to dummy variables. Pandas makes that a one-liner.\n\nExample 1.17 Here is a vector of chess pieces at the start of a game:\n\npieces = pd.Series( [\n    \"pawn\", \"pawn\", \"pawn\", \"pawn\", \"pawn\", \"pawn\", \"pawn\", \"pawn\",\n    \"knight\", \"bishop\", \"rook\", \"knight\", \"bishop\", \"rook\",\n    \"queen\", \"king\"\n] )\n\nWe can use pandas to convert this variable to dummy variables:\n\npd.get_dummies(pieces).head(9)\n\n\n\n\n\n\n\n\nbishop\nking\nknight\npawn\nqueen\nrook\n\n\n\n\n0\n0\n0\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n1\n0\n0\n\n\n2\n0\n0\n0\n1\n0\n0\n\n\n3\n0\n0\n0\n1\n0\n0\n\n\n4\n0\n0\n0\n1\n0\n0\n\n\n5\n0\n0\n0\n1\n0\n0\n\n\n6\n0\n0\n0\n1\n0\n0\n\n\n7\n0\n0\n0\n1\n0\n0\n\n\n8\n0\n0\n1\n0\n0\n0\n\n\n\n\n\n\n\nWe can also use the encoding variant in which one of the categories is left out:\n\npd.get_dummies(pieces, drop_first=True).head(6)\n\n\n\n\n\n\n\n\nking\nknight\npawn\nqueen\nrook\n\n\n\n\n0\n0\n0\n1\n0\n0\n\n\n1\n0\n0\n1\n0\n0\n\n\n2\n0\n0\n1\n0\n0\n\n\n3\n0\n0\n1\n0\n0\n\n\n4\n0\n0\n1\n0\n0\n\n\n5\n0\n0\n1\n0\n0\n\n\n\n\n\n\n\n\n\n\n1.5.3 Loading from files\nDatasets can be presented in many forms. In this course, we assume that they are given as spreadsheets or in comma-separated value (CSV) files. These files can be read by pandas locally or over the web.\n\nExample 1.18 The pandas function we use to load a dataset is read_csv. Here we use it to read a file that is available over the web:\n\nads = pd.read_csv(\"https://raw.githubusercontent.com/tobydriscoll/ds1book/master/advertising.csv\")\nads.head(6)    # show the first 6 rows\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n5\n8.7\n48.9\n75.0\n7.2\n\n\n\n\n\n\n\nNote above that we used head(6) to see just the first 6 rows.\nA data frame has a few properties that describe its contents:\n\nads.shape    # number of rows, number of columns\n\n(200, 4)\n\n\n\nads.columns   # names of the columns\n\nIndex(['TV', 'Radio', 'Newspaper', 'Sales'], dtype='object')\n\n\n\nads.dtypes    # data types of the columns\n\nTV           float64\nRadio        float64\nNewspaper    float64\nSales        float64\ndtype: object\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWhile it’s possible to import datasets directly from the web, links change and disappear frequently. If storing the dataset locally is not a problem, you may want to download your own copy before working on it.\n\n\nIt’s also possible to import data from most other general-purpose formats you might encounter, such as Excel spreadsheets (though possibly requiring one-time installation of additional libraries). There are many functions starting with pd.read_ showing the formats pandas understands."
  },
  {
    "objectID": "data.html#selecting-rows-and-columns",
    "href": "data.html#selecting-rows-and-columns",
    "title": "1  Representation of data",
    "section": "1.6 Selecting rows and columns",
    "text": "1.6 Selecting rows and columns\nIn a data frame, we access columns by giving a string name, or a list of names, in square brackets. We can access a row by the loc property with an index value, or iloc with an absolute row number starting from zero.\n\nExample 1.19 Here’s a local file that contains daily weather summaries from Newark, Delaware:\n\nweather = pd.read_csv(\"_datasets/ghcn_newark.csv\")\nweather.head()\n\n\n\n\n\n\n\n\nSTATION\nDATE\nLATITUDE\nLONGITUDE\nELEVATION\nNAME\nPRCP\nPRCP_ATTRIBUTES\nSNOW\nSNOW_ATTRIBUTES\n...\nWT08\nWT08_ATTRIBUTES\nWT11\nWT11_ATTRIBUTES\nWT14\nWT14_ATTRIBUTES\nWT16\nWT16_ATTRIBUTES\nWT18\nWT18_ATTRIBUTES\n\n\n\n\n0\nUSC00076410\n1894-04-01\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nUSC00076410\n1894-04-02\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nUSC00076410\n1894-04-03\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nUSC00076410\n1894-04-04\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n165.0\n,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nUSC00076410\n1894-04-05\n39.6682\n-75.74569\n32.3\nNEWARK AG FARM, DE US\n0.0\nP,,6,\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 56 columns\n\n\n\nThere are lots of columns here. We will work with a small subset of them:\n\ncolumns = [\"DATE\", \"PRCP\", \"SNOW\", \"TMAX\", \"TMIN\"]\nweather = weather[columns]\nweather.head()\n\n\n\n\n\n\n\n\nDATE\nPRCP\nSNOW\nTMAX\nTMIN\n\n\n\n\n0\n1894-04-01\n0.0\nNaN\n189.0\n111.0\n\n\n1\n1894-04-02\n0.0\nNaN\n133.0\n22.0\n\n\n2\n1894-04-03\n0.0\nNaN\n89.0\n-61.0\n\n\n3\n1894-04-04\n165.0\nNaN\n117.0\n17.0\n\n\n4\n1894-04-05\n0.0\nNaN\n156.0\n56.0\n\n\n\n\n\n\n\nThe first row has absolute position zero:\n\nweather.iloc[0]\n\nDATE    1894-04-01\nPRCP           0.0\nSNOW           NaN\nTMAX         189.0\nTMIN         111.0\nName: 0, dtype: object\n\n\nThe result above is a series, with the column names of the original data frame serving as the index. The following result is subtly different:\n\nweather.iloc[:1]\n\n\n\n\n\n\n\n\nDATE\nPRCP\nSNOW\nTMAX\nTMIN\n\n\n\n\n0\n1894-04-01\n0.0\nNaN\n189.0\n111.0\n\n\n\n\n\n\n\nThe slicing syntax [:1] means “the first 1 rows”. The result is a one-row data frame—not a series. To access the last 4 rows, we can use a negative index:\n\nweather.iloc[-4:]\n\n\n\n\n\n\n\n\nDATE\nPRCP\nSNOW\nTMAX\nTMIN\n\n\n\n\n29513\n2022-12-17\n0.0\nNaN\n72.0\n11.0\n\n\n29514\n2022-12-18\n0.0\nNaN\n39.0\n-22.0\n\n\n29515\n2023-01-06\n0.0\nNaN\n89.0\n33.0\n\n\n29516\n2023-01-07\n0.0\nNaN\n72.0\n17.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn Example 1.19, we did not specify an index for the data frame, so the row numbers were the index. If we do give the data frame an index, we can use loc to access rows by the index value.\n\nExample 1.20 A natural candidate for an index in the weather data is the DATE column. This can be specified when the file is read, but here we do it after the fact:\n\ndated_weather = weather.set_index(\"DATE\")\ndated_weather.head()\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nTMAX\nTMIN\n\n\nDATE\n\n\n\n\n\n\n\n\n1894-04-01\n0.0\nNaN\n189.0\n111.0\n\n\n1894-04-02\n0.0\nNaN\n133.0\n22.0\n\n\n1894-04-03\n0.0\nNaN\n89.0\n-61.0\n\n\n1894-04-04\n165.0\nNaN\n117.0\n17.0\n\n\n1894-04-05\n0.0\nNaN\n156.0\n56.0\n\n\n\n\n\n\n\nNotice above that the leftmost column of dated_weather is the index, which still has the column name DATE. We can access a row by giving a date string to loc:\n\ndated_weather.loc[\"1979-03-28\"]\n\nPRCP      0.0\nSNOW      0.0\nTMAX    111.0\nTMIN    -33.0\nName: 1979-03-28, dtype: float64\n\n\nWe can extract a row and column simultaneously by giving both to loc:\n\ndated_weather.loc[\"1979-03-28\", \"TMAX\"]\n\n111.0\n\n\nWhen the index is a date, as is the case here, we can even use a range of dates:\n\ndated_weather.loc[\"1979-03-01\":\"1979-03-06\"]\n\n\n\n\n\n\n\n\nPRCP\nSNOW\nTMAX\nTMIN\n\n\nDATE\n\n\n\n\n\n\n\n\n1979-03-01\n0.0\n0.0\n122.0\n-22.0\n\n\n1979-03-02\n3.0\n0.0\n128.0\n33.0\n\n\n1979-03-03\n0.0\n0.0\n117.0\n22.0\n\n\n1979-03-04\n0.0\n0.0\nNaN\nNaN\n\n\n1979-03-05\n61.0\n0.0\nNaN\nNaN\n\n\n1979-03-06\n173.0\n0.0\n128.0\n83.0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrequently we want to select rows from a data frame based on criteria applied to the data itself. We can use relational operators such as &gt;, &lt;, ==, etc. to select rows based on the values in a column.\n\nExample 1.21 Here is how we can select all the rows in which the value of PRCP is greater than 1200:\n\nweather.loc[weather[\"PRCP\"] &gt; 1200]\n\n\n\n\n\n\n\n\nDATE\nPRCP\nSNOW\nTMAX\nTMIN\n\n\n\n\n9396\n1952-07-09\n1377.0\n0.0\n261.0\n200.0\n\n\n12382\n1960-09-12\n1331.0\n0.0\n222.0\n167.0\n\n\n16352\n1971-08-27\n1207.0\n0.0\n256.0\n194.0\n\n\n26543\n1999-09-16\n2202.0\n0.0\n206.0\n156.0\n\n\n27938\n2004-07-12\n1580.0\n0.0\n244.0\n189.0\n\n\n27986\n2004-09-29\n1450.0\n0.0\n244.0\n178.0\n\n\n\n\n\n\n\nThere are two steps above. The expression inside the square brackets produces a series of Boolean values:\n\nis_prcp = (weather[\"PRCP\"] &gt; 1200)\nis_prcp.head()\n\n0    False\n1    False\n2    False\n3    False\n4    False\nName: PRCP, dtype: bool\n\n\nThis series can be used to select rows with loc. Actually, it works without loc as well, because pandas knows it is a row selector:\n\nprint(\"There are\",is_prcp.sum(),\"rows with PRCP &gt; 1200.\")\nweather[is_prcp]\n\nThere are 6 rows with PRCP &gt; 1200.\n\n\n\n\n\n\n\n\n\nDATE\nPRCP\nSNOW\nTMAX\nTMIN\n\n\n\n\n9396\n1952-07-09\n1377.0\n0.0\n261.0\n200.0\n\n\n12382\n1960-09-12\n1331.0\n0.0\n222.0\n167.0\n\n\n16352\n1971-08-27\n1207.0\n0.0\n256.0\n194.0\n\n\n26543\n1999-09-16\n2202.0\n0.0\n206.0\n156.0\n\n\n27938\n2004-07-12\n1580.0\n0.0\n244.0\n189.0\n\n\n27986\n2004-09-29\n1450.0\n0.0\n244.0\n178.0\n\n\n\n\n\n\n\nWe can use logical operators & (AND), | (OR), and ~ (NOT) on these Boolean series in order to combine criteria:\n\nweather[(weather[\"TMAX\"] &lt; 0) & (weather[\"SNOW\"] &gt; 0)]\n\n\n\n\n\n\n\n\nDATE\nPRCP\nSNOW\nTMAX\nTMIN\n\n\n\n\n272\n1895-01-29\n102.0\n102.0\n-22.0\n-94.0\n\n\n282\n1895-02-08\n127.0\n178.0\n-139.0\n-189.0\n\n\n692\n1896-03-24\n33.0\n51.0\n-22.0\n-100.0\n\n\n943\n1896-11-30\n107.0\n76.0\n-6.0\n-33.0\n\n\n966\n1896-12-23\n48.0\n51.0\n-6.0\n-89.0\n\n\n...\n...\n...\n...\n...\n...\n\n\n25224\n1996-01-08\n3.0\n25.0\n-28.0\n-156.0\n\n\n25249\n1996-02-02\n20.0\n25.0\n-22.0\n-56.0\n\n\n25250\n1996-02-03\n155.0\n147.0\n-33.0\n-111.0\n\n\n25594\n1997-01-12\n13.0\n13.0\n-33.0\n-100.0\n\n\n25623\n1997-03-10\n178.0\n8.0\n-17.0\n167.0\n\n\n\n\n131 rows × 5 columns\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe pandas user guide has a handy section on selections.\n\n\n\n\n\n\n\n\nTip\n\n\n\nFor practice with pandas fundamentals, try the Kaggle course."
  },
  {
    "objectID": "data.html#data-preparation",
    "href": "data.html#data-preparation",
    "title": "1  Representation of data",
    "section": "1.7 Data preparation",
    "text": "1.7 Data preparation\nRaw data often needs to be manipulated into a useable format before algorithms can be applied. Preprocessing data so that it is suitable for machine analysis is known as data wrangling or data munging.\n\n1.7.1 Preprocessing\nI’m impossible to prepare for every future situation when it comes to preprocessing data. You might find the replace() method or str. methods handy to tidy up strings, or to replace strings with numerical values. You can parse a string that represents one or more numbers using the to_numeric() or extractall There are built-in ways to parse and manipulate dates and times.\nTo demonstrate algorithms in later sections, we will be using a dataset describing loans made on the crowdfunding site LendingClub.\n\nExample 1.22 First, we load the raw data from a CSV (comma separated values) file.\n\nimport pandas as pd\nloans = pd.read_csv(\"_datasets/loans.csv\")\nloans.head()\n\n\n\n\n\n\n\n\nid\nmember_id\nloan_amnt\nfunded_amnt\nfunded_amnt_inv\nterm\nint_rate\ninstallment\ngrade\nsub_grade\n...\nnum_tl_90g_dpd_24m\nnum_tl_op_past_12m\npct_tl_nvr_dlq\npercent_bc_gt_75\npub_rec_bankruptcies\ntax_liens\ntot_hi_cred_lim\ntotal_bal_ex_mort\ntotal_bc_limit\ntotal_il_high_credit_limit\n\n\n\n\n0\n1077501\n1296599\n5000\n5000\n4975.0\n36 months\n10.65%\n162.87\nB\nB2\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1077430\n1314167\n2500\n2500\n2500.0\n60 months\n15.27%\n59.83\nC\nC4\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1077175\n1313524\n2400\n2400\n2400.0\n36 months\n15.96%\n84.33\nC\nC5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1076863\n1277178\n10000\n10000\n10000.0\n36 months\n13.49%\n339.31\nC\nC1\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1075358\n1311748\n3000\n3000\n3000.0\n60 months\n12.69%\n67.79\nB\nB5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 111 columns\n\n\n\nThe int_rate column, which gives the interest rate on the loan, has been interpreted as strings due to the percent sign. We’d like to strip out those percent signs and convert the rest to numeric types. We can use the str property of the column to access string methods:\n\nloans[\"int_rate\"].str.strip('%')\n\n0        10.65\n1        15.27\n2        15.96\n3        13.49\n4        12.69\n         ...  \n39712     8.07\n39713    10.28\n39714     8.07\n39715     7.43\n39716    13.75\nName: int_rate, Length: 39717, dtype: object\n\n\nThen, we can convert the result to numeric:\n\nloans[\"int_rate\"] = pd.to_numeric( loans[\"int_rate\"].str.strip('%') )\nloans.head()\n\n\n\n\n\n\n\n\nid\nmember_id\nloan_amnt\nfunded_amnt\nfunded_amnt_inv\nterm\nint_rate\ninstallment\ngrade\nsub_grade\n...\nnum_tl_90g_dpd_24m\nnum_tl_op_past_12m\npct_tl_nvr_dlq\npercent_bc_gt_75\npub_rec_bankruptcies\ntax_liens\ntot_hi_cred_lim\ntotal_bal_ex_mort\ntotal_bc_limit\ntotal_il_high_credit_limit\n\n\n\n\n0\n1077501\n1296599\n5000\n5000\n4975.0\n36 months\n10.65\n162.87\nB\nB2\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n1\n1077430\n1314167\n2500\n2500\n2500.0\n60 months\n15.27\n59.83\nC\nC4\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1077175\n1313524\n2400\n2400\n2400.0\n36 months\n15.96\n84.33\nC\nC5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n3\n1076863\n1277178\n10000\n10000\n10000.0\n36 months\n13.49\n339.31\nC\nC1\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n4\n1075358\n1311748\n3000\n3000\n3000.0\n60 months\n12.69\n67.79\nB\nB5\n...\nNaN\nNaN\nNaN\nNaN\n0.0\n0.0\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 111 columns\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nAs you see above, we often end up with chains of methods separated by dots. Python works from the inside out, left to right, evaluating a sub-expression and then replacing it with the output for the next segment in the chain. We could write these as a sequence of separate lines having intermediate results assigned to variable names, but it’s considered better style to chain them.\n\n\nThe title column contains strings that haphazardly formatted:\n\nloans[\"title\"]\n\n0                         Computer\n1                             bike\n2             real estate business\n3                         personel\n4                         Personal\n                   ...            \n39712             Home Improvement\n39713    Retiring credit card debt\n39714       MBA Loan Consolidation\n39715                     JAL Loan\n39716           Consolidation Loan\nName: title, Length: 39717, dtype: object\n\n\nWe might improve matters by converting everything in that column to lowercase. We can use the str.lower() method for that:\n\nloans[\"title\"] = loans[\"title\"].str.lower()\nloans[\"title\"]\n\n0                         computer\n1                             bike\n2             real estate business\n3                         personel\n4                         personal\n                   ...            \n39712             home improvement\n39713    retiring credit card debt\n39714       mba loan consolidation\n39715                     jal loan\n39716           consolidation loan\nName: title, Length: 39717, dtype: object\n\n\nLet’s add a column for the percentage of the loan request that was eventually funded. This will be a target for some of our learning methods.\n\nloans[\"percent_funded\"] = 100 * loans[\"funded_amnt\"] / loans[\"loan_amnt\"]\n\nIn future chapters, we will only use a small subset of the numerical columns:\n\ncolumns = [ \"percent_funded\", \"loan_amnt\", \"int_rate\", \"installment\", \"annual_inc\",\n    \"dti\", \"delinq_2yrs\", \"delinq_amnt\" ]\nloans = loans[columns]\n\nFinally, we’ll output this cleaned data frame to its own CSV file. By default, saving will include the index, which in this case is a meaningless row number, so we suppress that with a keyword:\n\nloans.to_csv(\"loan_clean.csv\", index=False)\n\n\n\n\n\n\n\n\n\n\n\n\n1.7.2 Missing values\nIn real data sets we often must cope with rows that have missing values. There is no universal way that such values are presented. A missing value may be indicated by a zero or nonsensical value, or with a string such as “n/a”, or just an empty spot. Within Python, it might be natural to represent missing values as None, but that is not a valid value for a NumPy array. It’s more flexible within pandas to use np.nan, for “not a number”.\nThe best practice for locating missing values in pandas is to use the isna() method. But you have to do something about them before you can apply algorithms. It’s simplest to remove all rows that contain missing values, which is the job of the pandas dropna() method. But if that leads to too much data loss, you can try imputation, which means replacing the missing values with a representative value, such as the mean or median of the non-missing values. In that case you should use the fillna() method.\n\nExample 1.23 Here is a well-known data set about penguins:\n\nimport seaborn as sns\npenguins = sns.load_dataset(\"penguins\")\npenguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nNote above that the fourth row of the frame is missing measurements. We can discover how many such rows there are using isna, which creates a Boolean series:\n\npenguins.isna()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n1\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n2\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n3\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n4\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n339\nFalse\nFalse\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\n340\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n341\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n342\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n343\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n\n\n\n\n344 rows × 7 columns\n\n\n\n\npenguins.isna().sum()\n\nspecies               0\nisland                0\nbill_length_mm        2\nbill_depth_mm         2\nflipper_length_mm     2\nbody_mass_g           2\nsex                  11\ndtype: int64\n\n\nIf all you want to do is drop the rows with missing values, you can use dropna:\n\nprint(\"Original has\", len(penguins), \"rows.\")\nprint(\"After removals, there are\", len(penguins.dropna()), \"rows.\")\n\nOriginal has 344 rows.\nAfter removals, there are 333 rows.\n\n\nIf you want to impute missing values, you can use fillna. Here, for example, we replace missing values in any column with the mean of the column:\n\nnew_penguins = penguins.fillna(penguins.mean())\nnew_penguins.head()\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.10000\n18.70000\n181.000000\n3750.000000\nMale\n\n\n1\nAdelie\nTorgersen\n39.50000\n17.40000\n186.000000\n3800.000000\nFemale\n\n\n2\nAdelie\nTorgersen\n40.30000\n18.00000\n195.000000\n3250.000000\nFemale\n\n\n3\nAdelie\nTorgersen\n43.92193\n17.15117\n200.915205\n4201.754386\nNaN\n\n\n4\nAdelie\nTorgersen\n36.70000\n19.30000\n193.000000\n3450.000000\nFemale\n\n\n\n\n\n\n\nNote that only the numerical data columns were affected. The categorical column for sex was left alone.\nThink carefully before you impute missing values, because doing so can easily misrepresent the underlying data. At the very least, you need to document how many values have been changed."
  },
  {
    "objectID": "data.html#exercises",
    "href": "data.html#exercises",
    "title": "1  Representation of data",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1.1 For each type of data, classify it as discrete quantitative, continuous quantitative, categorical, or other.\n\nHow many students are enrolled at a university\nYour favorite day of the week\nHow many inches of rain fall at an airport during one day\nWeight of a motor vehicle\nManufacturer of a motor vehicle\nText of all Yelp reviews for a restaurant\nStar ratings from all Yelp reviews for a restaurant\nSize of the living area of an apartment\nDNA nucleotide sequence of a cell\n\n\n\nExercise 1.2 Give the length of each vector or series.\n\nMorning waking times every day for a week\nNumber of siblings (max 12) for each student in a class of 30\nPosition and momentum of a roller coaster car\n\n\n\nExercise 1.3 Describe a scheme for creating dummy variables for the days of the week. Use your scheme to encode the vector:\n[Tuesday, Sunday, Friday, Tuesday, Monday]"
  },
  {
    "objectID": "stats.html#summary-statistics",
    "href": "stats.html#summary-statistics",
    "title": "2  Descriptive statistics",
    "section": "2.1 Summary statistics",
    "text": "2.1 Summary statistics\nWe will use data about car fuel efficiency for illustrations.\n\ncars = sns.load_dataset(\"mpg\")\n\nThe describe method of a data frame gives summary statistics for each column of quantitative data:\n\ncars.describe()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\n\n\n\n\ncount\n398.000000\n398.000000\n398.000000\n392.000000\n398.000000\n398.000000\n398.000000\n\n\nmean\n23.514573\n5.454774\n193.425879\n104.469388\n2970.424623\n15.568090\n76.010050\n\n\nstd\n7.815984\n1.701004\n104.269838\n38.491160\n846.841774\n2.757689\n3.697627\n\n\nmin\n9.000000\n3.000000\n68.000000\n46.000000\n1613.000000\n8.000000\n70.000000\n\n\n25%\n17.500000\n4.000000\n104.250000\n75.000000\n2223.750000\n13.825000\n73.000000\n\n\n50%\n23.000000\n4.000000\n148.500000\n93.500000\n2803.500000\n15.500000\n76.000000\n\n\n75%\n29.000000\n8.000000\n262.000000\n126.000000\n3608.000000\n17.175000\n79.000000\n\n\nmax\n46.600000\n8.000000\n455.000000\n230.000000\n5140.000000\n24.800000\n82.000000\n\n\n\n\n\n\n\nWe now discuss the definitions and interpretations of these values.\n\n2.1.1 Mean and spread\nYou may already know the Big Three summary statistics:\n\nDefinition 2.1 Given data values \\(x_1,\\ldots,x_n\\), their mean is \\[\n\\mu = \\frac{1}{n}\\sum_{i=1}^n x_i,\n\\tag{2.1}\\] their variance is \\[\n\\sigma^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\mu)^2,\n\\tag{2.2}\\] and their standard deviation (STD) is \\(\\sigma\\), the square root of the variance.\n\n\n\n\n\n\n\n\n\nMean is a measurement of central tendency. Variance and STD are measures of spread or dispersion in the data.\n\nExample 2.1 Suppose that \\(x_1=0\\), \\(x_2=t\\), and \\(x_3=-t\\), where \\(|t| \\le 6\\). What are the minimum and maximum possible values of the standard deviation?\n\nSolution. The mean is \\(\\mu=0\\), hence \\[\n\\sigma^2 = \\frac{1}{3}\\left[ 0^2 + t^2 + (-t)^2 \\right] = \\frac{2}{3} t^2.\n\\] From this we conclude \\[\n\\sigma =  \\sqrt{\\tfrac{2}{3}} |t|.\n\\] Given that \\(0 \\le |t| \\le 6\\), we see that the minimum value of \\(\\sigma\\) is 0 and the maximum is \\(2\\sqrt{6}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nVariance is in units that are the square of the data, which can be harder to interpret than STD, which has units the same as the data values.\n\n\n\n\n2.1.2 z-scores\nGiven data values \\(x_1,\\ldots,x_n\\), we can define related values known as standardized scores or z-scores:\n\\[\nz_i = \\frac{x_i-\\mu}{\\sigma}, \\qquad i=1,\\ldots,n.\n\\]\nThe z-scores have mean zero and standard deviation equal to 1; in physical terms, they are dimensionless. That is, the results don’t depend on the physical units chosen to express the data. Converting data into z-scores is referred to as standardization, and it helps make operations uniform across different datasets.\n\nTheorem 2.1 The z-scores have mean equal to zero and variance equal to 1.\n\n\nProof. Direct calculations.\n\n\nExample 2.2 Continuing with the values from Example 2.1, we assume without losing generality that \\(t\\ge 0\\). (Otherwise, we can just swap \\(x_2\\) and \\(x_3\\).) Then we have the z-scores \\[\nz_1 = \\frac{0-0}{2t\\sqrt{6}} = 0, \\quad z_2 = \\frac{t-0}{2t\\sqrt{6}} = \\frac{1}{2\\sqrt{6}} \\quad z_3 = \\frac{-t-0}{2t\\sqrt{6}} = \\frac{-1}{2\\sqrt{6}}.\n\\] These are independent of \\(t\\), which just scales the original values.\n\n\nExample 2.3 We can write a little function to compute z-scores in Python:\n\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\ncars[\"mpg_z\"] = standardize( cars[\"mpg\"] )\ncars[ [\"mpg\", \"mpg_z\"] ].describe()\n\n\n\n\n\n\n\n\nmpg\nmpg_z\n\n\n\n\ncount\n398.000000\n3.980000e+02\n\n\nmean\n23.514573\n1.071170e-16\n\n\nstd\n7.815984\n1.000000e+00\n\n\nmin\n9.000000\n-1.857037e+00\n\n\n25%\n17.500000\n-7.695221e-01\n\n\n50%\n23.000000\n-6.583596e-02\n\n\n75%\n29.000000\n7.018217e-01\n\n\nmax\n46.600000\n2.953617e+00\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nSince floating-point values are rounded off, it’s unlikely that a value derived from them that is meant to be zero will actually be exactly zero. Above, the mean value of about \\(-10^{-15}\\) should be seen as reasonable for values that have been rounded off in the 15th digit or so.\n\n\n\n\n2.1.3 Populations and samples\nIn statistics one refers to the population as the entire universe of available values. Thus, the ages of adult on Earth at some instant has a particular population mean and standard deviation. However, in order to estimate those values, we can only measure a sample of the population directly.\nWhen Equation 2.1 is used to compute the mean of a sample rather than a population, we change the notation a bit as a reminder: \\[\n\\bar{x} = \\frac{1}{n}\\sum_{i=1}^n x_i.\n\\tag{2.3}\\]\nIt can be proved that the sample mean is an accurate way to estimate the population mean, in the following precise sense. If, in a thought experiment, we could average \\(\\bar{x}\\) over all possible samples of size \\(n\\), the result would be exactly the population mean \\(\\mu\\). That is, we say that \\(\\bar{x}\\) is an unbiased estimator for \\(\\mu\\).\nThe sample mean in turn can be used within Equation 2.2 to compute sample variance: \\[\ns_n^2 = \\frac{1}{n}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\]\nHowever, sample variance is more subtle than the sample mean. If \\(s_n^2\\) is averaged over all possible sample sets, we do not get the population variance \\(\\sigma^2\\); hence, \\(s_n^2\\) is called a biased estimator of the population variance.\nAn unbiased estimator for \\(\\sigma^2\\) is\n\\[\ns_{n-1}^2 = \\frac{1}{n-1}\\sum_{i=1}^n (x_i - \\bar{x})^2.\n\\tag{2.4}\\]\n\nExample 2.4 The values [1, 4, 9, 16, 25] have mean \\(\\bar{x}=55/5 = 11\\). The sample variance is\n\\[\n\\begin{split}\n    s_n^2 &= \\frac{(1-11)^2+(4-11)^2+(9-11)^2+(16-11)^2+(25-11)^2}{5} \\\\\n    & = \\frac{374}{5} = 74.8.\n\\end{split}\n\\]\nBy contrast, the unbiased estimate of population variance from this sample is\n\\[\ns_{n-1}^2 = \\frac{374}{4} = 93.5.\n\\]\n\nAs you can see from the formulas and the example, the sample variance is always too large as an estimator, but the difference vanishes as the sample size \\(n\\) increases.\n\n\n\n\n\n\nWarning\n\n\n\nSources are not always clear about this terminology. Some use sample variance to mean \\(s_{n-1}^2\\), not \\(s_n^2\\), and many even omit the subscripts. You always have to check each source.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nNumPy computes the biased estimator of variance by default, while pandas computes the unbiased version. Whee! Fortunately, most datasets today have large enough \\(n\\) to make the difference negligible.\n\n\nFor standard deviation, neither \\(s_n\\) nor \\(s_{n-1}\\) is an unbiased estimator of \\(\\sigma\\). There is no simple correction that works for all distributions. Our practice is to use \\(s_{n-1}\\), which is what std computes in pandas. Thus, for instance, a sample z-score for \\(x_i\\) is\n\\[\nz_i = \\frac{x_i-\\bar{x}}{s_{n-1}}.\n\\tag{2.5}\\]\n\n\n\n2.1.4 Median and quantiles\nMean and variance are not the most relevant summary statistics for every dataset. There are important alternatives.\n\nDefinition 2.2 For any \\(0 &lt; p &lt; 1\\), the \\(100p\\)-percentile or quantile is the value of \\(x\\) such that \\(p\\) is the probability of observing a population value less than or equal to \\(x\\).\nThe 50th percentile is known as the median of the population.\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome sources reserve the term quantile for another meaning, but since pandas offers quantile to compute percentiles, we don’t draw a distinction.\n\n\nThe unbiased sample median of \\(x_1,\\ldots,x_n\\) can be computed by sorting the values into \\(y_1,\\ldots,y_n\\). If \\(n\\) is odd, then \\(y_{(n+1)/2}\\) is the sample median; otherwise, the average of \\(y_{n/2}\\) and \\(y_{1+(n/2)}\\) is the sample median.\n\nExample 2.5 If the sorted values are \\(1,3,3,4,5,5,5\\), then \\(n=7\\) and the sample median is \\(y_4=4\\). If the sample values are \\(1,3,3,4,5,5,5,9\\), then \\(n=8\\) and the sample median is \\((4+5)/2=4.5\\).\n\nComputing unbiased sample estimates of percentiles other than the median is complicated, and we won’t go into the details. For large datasets, the sample values are good estimators in practice.\n\nExample 2.6 Here we find the 90th percentile of the loan_amnt variable in the loans dataset:\n\nimport pandas as pd\nloans = pd.read_csv(\"_datasets/loan.csv\")\nx = loans[\"loan_amnt\"]\npct90 = x.quantile(0.9)\nprint(pct90)\n\n22000.0\n\n\nEquivalently, 90 percent of the values are no greater than that value:\n\nsum(x &lt;= pct90) / len(x)\n\n0.9008233250245486\n\n\nThe 50th percentile is the same thing as the median:\n\nprint(x.median())\nprint(x.quantile(0.50))\n\n10000.0\n10000.0\n\n\n\n\nDefinition 2.3 The 25th, 50th, and 75th percentiles are the first, second, and third quartiles of the distribution. The interquartile range (IQR) is the difference between the 75th percentile and the 25th percentile.\n\nSometimes the definition above is extended to the zeroth quartile, which is the minimum sample value, and the fourth quartile, which is the maximum sample value.\nIQR is an indication of the spread of the values. For some distributions, the median and IQR might be a good substitute for the mean and standard deviation.\n\nExample 2.7 The dataframe describe method includes mean, standard deviation, and the quartiles:\n\nx.describe()\n\ncount    39717.000000\nmean     11219.443815\nstd       7456.670694\nmin        500.000000\n25%       5500.000000\n50%      10000.000000\n75%      15000.000000\nmax      35000.000000\nName: loan_amnt, dtype: float64\n\n\nIt’s easy to write a function to compute the IQR of a series:\n\ndef IQR(x):\n    Q25, Q75 = x.quantile( [0.25, 0.75] )\n    return Q75 - Q25 \n\nIQR(x)\n\n9500.0"
  },
  {
    "objectID": "stats.html#distributions",
    "href": "stats.html#distributions",
    "title": "2  Descriptive statistics",
    "section": "2.2 Distributions",
    "text": "2.2 Distributions\nMean and STD, or median and IQR, attempt to summarize quantitative data with a couple of numbers. At the other extreme, we can express the distribution of all values precisely using a function.\n\n2.2.1 CDF\n\nDefinition 2.4 The cumulative distribution function (CDF) of a population is the function \\[\nF(t) = \\text{fraction of the population that is $\\le t$},\n\\] where \\(t\\) ranges over all possible values.\n\nNote that by its definition, \\(F\\) ranges between 0 and 1 (inclusive) and is a nondecreasing function.\n\nExample 2.8 If a population is \\(x_i=i\\) for \\(i=1,\\ldots,n\\), then \\(F(k)=k/n\\) at each \\(k=1,\\ldots,n\\). We could, however, also regard \\(F\\) as a function of a continuous variable \\(t\\), in which case \\[\nF(t) = \\frac{\\lfloor t \\rfloor}{n},\n\\] where \\(\\lfloor\\cdot\\rfloor\\) is the floor function that rounds leftward to the nearest integer. This produces a step function that looks like stairs going up from 0 to 1.\n\n\n\n\n\n\n\n\n\nExample 2.8 becomes interesting as a template for generalizing to infinite populations. If we take not \\(x_i=i\\) but \\(x_i=i/n\\) and then let \\(n\\to \\infty\\), then the graph of \\(F\\) converges to \\[\nF(t) = \\begin{cases}\n0, & t &lt; 0, \\\\\nt,& 0 \\le t \\le 1, \\\\\n1,& t &gt; 1.\n\\end{cases}\n\\tag{2.6}\\] While it doesn’t make sense to think about a fraction of the number of values in the infinite case, we can interpret \\(F(t)\\) as the *probability of observing a value less than or equal to the real number \\(t\\).\n\nDefinition 2.5 A uniform distribution gives an equal probability to every value. In particular, the uniform distribution over the interval \\([0,1]\\) has the CDF given in Equation 2.6.\n\n\n\n2.2.2 Random numbers in Python\nGenerating truly random numbers on a computer is not simple. We rely on pseudorandom numbers, which are generated by deterministic functions called random number generators (RNGs) that have extremely long periods. One consequence is repeatability: by specifying the starting state of the RNG, you can get exactly the same pseudorandom sequence every time.\n\n\n\n\n\n\nCaution\n\n\n\nThere is an older way to use random numbers in NumPy than the one presented here. You’ll still find it on the web and in some books, but the newer way is recommended.\n\n\n\nExample 2.9 We start by creating an RNG with a specific state. Every time this code is run, the same sequence of numbers will be generated from it.\n\nfrom numpy.random import default_rng\nrng = default_rng(19716)    # setting an initial state\n\nThe uniform generator method produces numbers distributed uniformly between two limits you specify.\n\nfor _ in range(5):\n    print( rng.uniform( -1, 1 ) )\n\n0.9516875346510687\n0.3153947867339544\n-0.6651579991995873\n0.42925720152795055\n0.960762541480505\n\n\n\n\n\n2.2.3 Empirical CDF\nGiven a sample of a population, we can always calculate the analog of a CDF from its values.\n\nDefinition 2.6 The empirical cumulative distribution function or ECDF of a sample is the function \\(\\hat{F}\\) whose value at \\(t\\) equals the proportion of the sample values that are less than or equal to \\(t\\).\n\n\nExample 2.10 Here is an experiment that producing the ECDF for a sample from the random number generator:\n\nfrom numpy.random import default_rng\nrng = default_rng(19716)\nx = rng.uniform( size=(100,) )\nsns.displot(x, kind=\"ecdf\");\n\n\n\n\nIf we take more samples, we expect to see a curve closer to the theoretical CDF, \\(F(t)=t\\):\n\nx = rng.uniform( size=(4000,) )\nsns.displot(x, kind=\"ecdf\");\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.4 PDF\nBy definition, we know that if \\(a&lt;b\\), \\(\\hat{F}(b) - \\hat{F}(a)\\) is the number of observations in the half-open interval \\((a,b]\\). This leads into the next definition.\n\nDefinition 2.7 Select the ordered values \\(t_1 &lt; t_2 &lt; \\cdots &lt; t_m\\), called edges, and define bins as the intervals \\[\nB_k = (t_k,t_{k+1}], \\qquad k=0,\\ldots,m,\n\\] where we adopt the convention that \\(t_0=-\\infty\\) and \\(t_{m+1}=\\infty\\). Let \\(c_k\\) be the number of data values in \\(B_k\\). Then a histogram relative to the bins is the list of \\((B_0,c_0),\\ldots,(B_m,c_m)\\).\n\nThe default for a seaborn displot is to show a histogram.\n\nExample 2.11 Continuing with the uniform distribution over \\([0,1]\\):\n\nx = rng.uniform( size=(1000,) )\nsns.displot(x);\n\n\n\n\nWe can choose the number of bins to use, or give a vector of their edges:\n\nsns.displot(x, bins=40);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgain something interesting happens in a limiting case. If we normalize the count in a bin by the length of that bin, we get \\[\n\\frac{c_k}{t_{k+1}-t_k} = \\frac{\\hat{F}(t_{k+1})-\\hat{F}(t_k)}{t_{k+1}-t_k}.\n\\tag{2.7}\\] If we let the number of observations tend to infinity, then \\(\\hat{F}\\) will converge to \\(F\\), and if we also let the number of bins go to infinity, then the fraction in Equation 2.7 converges to \\(F'(t_k)\\).\n\nDefinition 2.8 The probability density function or PDF of a distribution is the derivative of the CDF.\n\n\nExample 2.12 If we have many samples, then we can use a normalized histogram to give an approximation of the PDF:\n\nx = rng.uniform( size=(20000,) )\nsns.displot(x, bins=24, stat=\"density\");\n\n\n\n\nAlternatively, we can use process called kernel density estimation to plot a continuous estimate of the PDF:\n\nsns.displot(x, kind=\"kde\");\n\n\n\n\nIn this case we did not obtain a particularly good approximation of the true PDF. In part this is because kernel density estimation assumes that the PDF is continuous, but here it is 1 over \\([0,1]\\) and jumps down to 0 elsewhere.\n\n\n\n\n\n\n\n\n\n\n\n2.2.5 Mean and variance\nIt’s possible to compute the mean and variance (thus STD) of a distribution from its PDF: \\[\n\\begin{split}\n\\mu &= \\int x f(x) \\, dx \\\\\n\\sigma^2 &= \\int (x-\\mu)^2 f(x) \\, dx,\n\\end{split}\n\\] where the integrals are taken over the domain of \\(f\\).\n\nExample 2.13 The uniform distribution over \\([0,1]\\) has \\(f(x)=1\\) over that interval. Hence, \\[\n\\begin{split}\n\\mu &= \\int_0^1 x \\, dx = \\left[ \\frac{1}{2} x^2\\right]_0^1 = \\frac{1}{2}, \\\\\n\\sigma^2 &= \\int_0^1 \\left(x-\\tfrac{1}{2}\\right)^2 \\, dx = \\frac{1}{3} - \\frac{1}{2} + \\frac{1}{4} = \\frac{1}{12}.\n\\end{split}\n\\]\nLet’s check these results empirically:\n\nfrom numpy.random import default_rng\nimport numpy as np\n\nrng = default_rng(19716)\nx = rng.uniform( size=(2000,) )\nprint(f\"µ = {np.mean(x):.5f}, 12σ² = {12*np.var(x):.5f}\")\n\nµ = 0.50518, 12σ² = 1.00019\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.2.6 Normal distribution\nNext to perhaps the uniform distribution, the following is the most widely used distribution of a random variable.\n\nDefinition 2.9 The normal distribution or Gaussian distribution with mean \\(\\mu\\) and variance \\(\\sigma^2\\) is defined by the PDF \\[\nf(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{ -(x-\\mu)^2/(2\\sigma^2)}.\n\\tag{2.8}\\] The standard normal distribution uses \\(\\mu=0\\) and \\(\\sigma=1\\).\n\n\n\n\n\n\n\n\n\nFor data that are distributed normally, about 68% of the values lie within one standard deviation of the mean, and 95% lie within two standard deviations.\n\nExample 2.14 The normal method of a NumPy RNG simulates a standard normal distribution.\n\nrng = default_rng(19716)\nx = rng.normal( size=(10000,) )\nsns.displot(x, bins=np.linspace(-4, 4, 28), stat=\"probability\");\n\n\n\n\nWe can change the variance by multiplying by \\(\\sigma\\) and change the mean by adding \\(\\mu\\):\n\ndf = pd.DataFrame( {\"x\": x, \"3x-10\": 3*x-10} )\ndf.describe()\n\n\n\n\n\n\n\n\nx\n3x-10\n\n\n\n\ncount\n10000.000000\n10000.000000\n\n\nmean\n-0.012414\n-10.037242\n\n\nstd\n0.993568\n2.980704\n\n\nmin\n-3.436924\n-20.310773\n\n\n25%\n-0.672352\n-12.017056\n\n\n50%\n-0.008374\n-10.025122\n\n\n75%\n0.659730\n-8.020809\n\n\nmax\n4.044099\n2.132297\n\n\n\n\n\n\n\nThe KDE density estimator works pretty well for normally distributed data, except in the tails where there are few observations:\n\nsns.displot(data=df, x=\"3x-10\", kind=\"kde\");"
  },
  {
    "objectID": "stats.html#grouping-data",
    "href": "stats.html#grouping-data",
    "title": "2  Descriptive statistics",
    "section": "2.3 Grouping data",
    "text": "2.3 Grouping data\nHere is the distribution of the mpg variable in cars over the entire dataset:\n\nsns.displot(data=cars, x=\"mpg\", bins=20);\n\n\n\n\nIt’s often useful to analyze the data in groups defined by categorical values or other criteria. For instance, does the distribution of the mpg variable look the same regardless of where the cars were made? Both seaborn and pandas make investigating this kind of question much more easily and efficiently than hand-written code can do.\n\n2.3.1 Facet, box, and violin plots\nIn a facet plot, a distribution plot is repeated across columns or rows for each group.\n\nExample 2.15 Suppose we want to separate the cars data by the origin categorical column, which has 3 unique values:\n\ncars[\"origin\"].value_counts()\n\nusa       249\njapan      79\neurope     70\nName: origin, dtype: int64\n\n\nWe can tell seaborn to group by origin and apply different colors to each group using the hue keyword in a displot, or distribution plot:\n\nsns.displot(data=cars, x=\"mpg\", hue=\"origin\");\n\n\n\n\nThe above graph is a little hard to interpret because of the overlaps. We can instead plot the groups in separate columns in a facet plot:\n\nsns.displot(data=cars, \n    x=\"mpg\", col=\"origin\", height=2.2\n    );\n\n\n\n\nFrom the facet plot it is clear that the U.S.A. cars are more clustered on the left (smaller MPG) than are the Japanese and European cars.\n\n\n\n\n\n\n\n\n\nOther ways to visualize grouped data are offered by the catplot function in seaborn, including the well-known box plot and violin plot.\n\nExample 2.16 Here is a grouped box plot for mpg:\n\nsns.catplot(data=cars, \n    x=\"origin\", y=\"mpg\", kind=\"box\"\n    );\n\n\n\n\nEach colored box shows the interquartile range, with the interior horizontal line showing the median. The whiskers and dots are explained in a later section. A related visualization is a violin plot:\n\nsns.catplot(data=cars, \n    x=\"mpg\", y=\"origin\", \n    kind=\"violin\"\n    );\n\n\n\n\nIn a violin plot, the inner lines show the same information as the box plot, with the thick part showing the IQR, while the sides of the “violins” are KDE estimates of the density functions.\nIt’s also possible to make groups using a quantitative variable. The pandas cut method lets you define ranges that serve as bins:\n\ncuts = pd.cut( \n    cars[\"weight\"],         # series to cut by\n    range(1500, 5800, 1000)    # bin edges\n    )\n\ncars[\"cuts\"] = cuts\ncars.head(6)\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\nmpg_z\ncuts\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nusa\nchevrolet chevelle malibu\n-0.705551\n(3500, 4500]\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nusa\nbuick skylark 320\n-1.089379\n(3500, 4500]\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n-0.705551\n(2500, 3500]\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n-0.961437\n(2500, 3500]\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nusa\nford torino\n-0.833494\n(2500, 3500]\n\n\n5\n15.0\n8\n429.0\n198.0\n4341\n10.0\n70\nusa\nford galaxie 500\n-1.089379\n(3500, 4500]\n\n\n\n\n\n\n\n\nsns.catplot(data=cars, \n    x=\"mpg\", y=\"cuts\", \n    kind=\"violin\"\n    );\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.2 Aggregation\nIn pandas, the groupby method splits a data frame into groups based on values in a designated column. These groups can then be passed through aggregators that reduce each grouped column to a single value. A list of the most common predefined aggregation functions is given in Table 2.1.\n\n\nTable 2.1: Aggregation functions. All ignore NaN values.\n\n\nmethod\neffect\n\n\n\n\ncount\nNumber of values in each group\n\n\nmean\nMean value in each group\n\n\nsum\nSum within each group\n\n\nstd, var\nStandard deviation/variance within groups\n\n\nmin, max\nMin or max within groups\n\n\ndescribe\nDescriptive statistics\n\n\nfirst, last\nFirst or last of group values\n\n\n\n\n\nExample 2.17 Here is how we can define groups based on the categorical origin column:\n\ncars.groupby(\"origin\")\n\n&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x17fd04690&gt;\n\n\nAs you can see from the output above, a grouped data frame isn’t of much value until you apply some operation to it. For instance, we can find the mean of each quantitative column:\n\ncars.groupby(\"origin\").mean()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\nmpg_z\n\n\norigin\n\n\n\n\n\n\n\n\n\n\n\n\neurope\n27.891429\n4.157143\n109.142857\n80.558824\n2423.300000\n16.787143\n75.814286\n0.559988\n\n\njapan\n30.450633\n4.101266\n102.708861\n79.835443\n2221.227848\n16.172152\n77.443038\n0.887420\n\n\nusa\n20.083534\n6.248996\n245.901606\n119.048980\n3361.931727\n15.033735\n75.610442\n-0.438977\n\n\n\n\n\n\n\nThe result is a frame indexed by origin and whose columns contain the group means. If we want to focus on just mpg from the dataset, we can extract that column after the grouping (which needs to know origin) but before operating on the groups:\n\ncars.groupby(\"origin\")[\"mpg\"].mean()\n\norigin\neurope    27.891429\njapan     30.450633\nusa       20.083534\nName: mpg, dtype: float64\n\n\nA lot of pandas expressions end up as compound statements like the one above. It’s equivalent to a sequence of assignments to temporary names:\n\nby_origin = cars.groupby(\"origin\")\nmpg_by_origin = by_origin[\"mpg\"]\nmpg_by_origin.mean()\n\norigin\neurope    27.891429\njapan     30.450633\nusa       20.083534\nName: mpg, dtype: float64\n\n\nWhen you encounter a compound statement in pandas, just interpret it step by step, from left to right.\nHere we define bins for the weights and find the max of mpg within each bin:\n\ncuts = pd.cut(cars[\"weight\"], range(1500, 5800, 1000))\nby_weight = cars.groupby(cuts)\nby_weight[\"mpg\"].max()\n\nweight\n(1500, 2500]    46.6\n(2500, 3500]    38.0\n(3500, 4500]    26.6\n(4500, 5500]    16.0\nName: mpg, dtype: float64\n\n\nIf you want an aggregator other than those in Table 2.1, you can call agg with your own function:\n\ndef iqr(x):\n    q1, q3 = x.quantile( [.25, .75] )\n    return q3 - q1\n\nby_weight[\"mpg\"].agg(iqr)\n\nweight\n(1500, 2500]    8.450\n(2500, 3500]    6.325\n(3500, 4500]    3.125\n(4500, 5500]    1.000\nName: mpg, dtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.3 Transformation\nA transformation applies a function to each element of a column, producing a result of the same length that can be indexed the same way. This transformation is applied group by group.\n\nExample 2.18 We can convert the mpg values to z-scores using the population mean and variance:\n\ndef standardize(x):\n    return (x - x.mean()) / x.std()\n\ncars[\"mpg_z\"] = standardize(cars[\"mpg\"])\nsns.displot(data=cars, \n    x=\"mpg_z\", \n    col=\"origin\", height=2.3\n    );\n\n\n\n\nAbove, each of the distributions is shifted and scaled identically. The result is identical to the facet plot in Example 2.15 except for relabeled \\(x\\)-axes. Instead, we could standardize mpg to z-scores within groups separately, so that each group is shifted and scaled independently:\n\ncars[\"group_mpg_z\"] = by_weight[\"mpg\"].transform(standardize)\nsns.displot(data=cars, \n    x=\"group_mpg_z\", \n    col=\"origin\", height=2.3\n    );\n\n\n\n\n(Another reason the second set of plots looks different is that the histogram bins are chosen differently.)\n\n\n\n\n\n\n\n\n\n\n\n2.3.4 Filtering\nA filter is a function that operates on a column and returns either True, meaning to keep the column, or False, meaning to reject it. This filter may be applied to a grouped frame.\n\nExample 2.19 Suppose we want to group cars by horsepower, but we find out that some of the groups don’t have a lot of members:\n\ncuts = pd.cut(cars[\"horsepower\"], range(40, 220, 20))\nby_hp = cars.groupby(cuts)\nby_hp[\"mpg\"].count()\n\nhorsepower\n(40, 60]       20\n(60, 80]       99\n(80, 100]     123\n(100, 120]     48\n(120, 140]     25\n(140, 160]     40\n(160, 180]     20\n(180, 200]      7\nName: mpg, dtype: int64\n\n\nSince small groups can give misleading statistics, we can filter out the groups with fewer than, say, 30 members:\n\ndef keeper(x):\n    return len(x) &gt; 29\nhp_30 = by_hp.filter(keeper)\nhp_30.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\nmpg_z\ncuts\ngroup_mpg_z\n\n\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n-0.705551\n(2500, 3500]\n-0.956484\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n-0.961437\n(2500, 3500]\n-1.377177\n\n\n11\n14.0\n8\n340.0\n160.0\n3609\n8.0\n70\nusa\nplymouth 'cuda 340\n-1.217322\n(3500, 4500]\n-0.611515\n\n\n12\n15.0\n8\n400.0\n150.0\n3761\n9.5\n70\nusa\nchevrolet monte carlo\n-1.089379\n(3500, 4500]\n-0.249252\n\n\n14\n24.0\n4\n113.0\n95.0\n2372\n15.0\n70\njapan\ntoyota corona mark ii\n0.062107\n(1500, 2500]\n-1.130768\n\n\n\n\n\n\n\nThe result is a single, ungrouped frame in which the members of all groups that failed the filter were removed. Thus, the horsepower column has no values left less than 60 or greater than 160:\n\nhp_30[\"horsepower\"].describe()\n\ncount    310.000000\nmean      95.309677\nstd       25.298367\nmin       61.000000\n25%       75.000000\n50%       90.000000\n75%      105.000000\nmax      160.000000\nName: horsepower, dtype: float64"
  },
  {
    "objectID": "stats.html#outliers",
    "href": "stats.html#outliers",
    "title": "2  Descriptive statistics",
    "section": "2.4 Outliers",
    "text": "2.4 Outliers\nInformally, an outlier is a data value that is considered to be far from typical. In some applications, such as detecting earthquakes or cancer, outliers are the cases of real interest. But we will be thinking of them as unwelcome values that might result from equipment failure, confounding effects, mistyping a value, using an extreme value to represent missing data, and so on. In such cases we want to minimize the effect of the outliers on the statistics.\n\n\n\n\n\n\n\n\nIt is well known, for instance, that the mean is more sensitive to outliers than the median is.\n\nExample 2.20 The values \\(1,2,3,4,5\\) have a mean and median both equal to 3. If we change the largest value to be a lot larger, say \\(1,2,3,4,1000\\), then the mean changes to 202. But the median is still 3!\n\n\nIf you want to use a method that is vulnerable to outliers, it’s typical to remove such values early on. There are various ways of deciding what qualifies as an outlier, with no one-size recommendation for all applications.\n\n2.4.1 IQR\nLet \\(Q_{25}\\) and \\(Q_{75}\\) be the first and third quartiles (i.e., 25% and 75% percentiles), and let \\(I=Q_{75}-Q_{25}\\) be the interquartile range (IQR). Then \\(x\\) is an outlier value if\n\\[\nx &lt; Q_{25} - 1.5I \\text{ or } x &gt; Q_{75} + 1.5I.\n\\tag{2.9}\\]\nIn a box plot, the whiskers growing from a box show the extent of the non-outlier data, and the dots beyond the whiskers represent outliers.\n\nExample 2.21 Let’s look at another data set, based on an fMRI experiment:\n\nfmri = sns.load_dataset(\"fmri\")\nfmri.head()\n\n\n\n\n\n\n\n\nsubject\ntimepoint\nevent\nregion\nsignal\n\n\n\n\n0\ns13\n18\nstim\nparietal\n-0.017552\n\n\n1\ns5\n14\nstim\nparietal\n-0.080883\n\n\n2\ns12\n18\nstim\nparietal\n-0.081033\n\n\n3\ns11\n18\nstim\nparietal\n-0.046134\n\n\n4\ns10\n18\nstim\nparietal\n-0.037970\n\n\n\n\n\n\n\nWe want to focus on the signal column, splitting according to the event.\n\nby_event = fmri.groupby(\"event\")\nby_event[\"signal\"].describe()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\nevent\n\n\n\n\n\n\n\n\n\n\n\n\ncue\n532.0\n-0.006669\n0.047752\n-0.181241\n-0.031122\n-0.008871\n0.015825\n0.215735\n\n\nstim\n532.0\n0.013748\n0.123179\n-0.255486\n-0.062378\n-0.022202\n0.058143\n0.564985\n\n\n\n\n\n\n\nHere is a box plot of the signal for these groups.\n\nsns.catplot(data=fmri,\n    x=\"event\", y=\"signal\",\n    kind=\"box\"\n    );\n\n\n\n\nThe dots lying outside the whiskers in the plot can be considered outliers satisfying one of the inequalities in Equation 2.9.\nLet’s now remove the outliers. We start with a function that computes a Boolean-valued series for a given input. This function is applied as a transform to the data as grouped by events:\n\ndef is_outlier(x):\n    Q25, Q75 = x.quantile([.25,.75])\n    I = Q75 - Q25\n    return (x &lt; Q25 - 1.5*I) |  (x &gt; Q75 + 1.5*I)\n\noutliers = by_event[\"signal\"].transform(is_outlier)\nfmri.loc[outliers,\"event\"].value_counts()\n\nstim    40\ncue     26\nName: event, dtype: int64\n\n\nYou can see above that there are 66 outliers. To negate the outlier indicator, we can use ~outs as a row selector.\n\ncleaned = fmri[~outliers]\n\nThe median values are barely affected by the omission of the outliers:\n\nprint( \"medians with outliers:\" )\nprint( by_event[\"signal\"].median() )\nprint( \"\\nmedians without outliers:\" )\nprint( cleaned.groupby(\"event\")[\"signal\"].median() )\n\nmedians with outliers:\nevent\ncue    -0.008871\nstim   -0.022202\nName: signal, dtype: float64\n\nmedians without outliers:\nevent\ncue    -0.009006\nstim   -0.028068\nName: signal, dtype: float64\n\n\nThe means, though, show much greater change:\n\nprint( \"means with outliers:\" )\nprint( by_event[\"signal\"].mean() )\nprint( \"\\nmeans without outliers:\" )\nprint( cleaned.groupby(\"event\")[\"signal\"].mean() )\n\nmeans with outliers:\nevent\ncue    -0.006669\nstim    0.013748\nName: signal, dtype: float64\n\nmeans without outliers:\nevent\ncue    -0.008243\nstim   -0.010245\nName: signal, dtype: float64\n\n\nFor the stim case in particular, the mean value changes by almost 200%, including a sign change. (Relative to the standard deviation, it’s closer to a 20% change.)\n\n\n\n\n\n\n\n\n\n\n\n2.4.2 Mean and STD\nFor normal distributions, values more than twice the standard deviation \\(\\sigma\\) from the mean could be considered to be outliers; this would exclude 5% of the values, on average. A less aggressive criterion is to allow a distance of \\(3\\sigma\\), which excludes only about 0.3% of the values. The IQR criterion above corresponds to about \\(2.7\\sigma\\) in the normal distribution case.\n\n\n\n\n\n\nNote\n\n\n\nA criticism of classical statistics is that much of it is conditioned on the assumption of normal distributions. This assumption is often violated by real datasets; quantities that depend on normality should be used judiciously.\n\n\n\nExample 2.22 The following plot shows the outlier cutoffs for 2000 samples from a normal distribution, using the criteria for 2σ (red), 3σ (blue), and 1.5 IQR (black).\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom numpy.random import default_rng\nrandn = default_rng(1).normal \n\nx = pd.Series(randn(size=2000))\nsns.displot(data=x,bins=30);\nm,s = x.mean(),x.std()\nplt.axvline(m-2*s,color='r')\nplt.axvline(m+2*s,color='r')\nplt.axvline(m-3*s,color='b')\nplt.axvline(m+3*s,color='b')\n\nq1,q3 = x.quantile([.25,.75])\nplt.axvline(q3+1.5*(q3-q1),color='k')\nplt.axvline(q1-1.5*(q3-q1),color='k');\n\n\n\n\n\nFor asymmetric distributions, or those with a heavy tail, these criteria might show greater differences."
  },
  {
    "objectID": "stats.html#correlation",
    "href": "stats.html#correlation",
    "title": "2  Descriptive statistics",
    "section": "2.5 Correlation",
    "text": "2.5 Correlation\nThere are often variables that we believe to be linked, either because one influences the other, or because both are influenced by some other factor. In either case, we say the quantities are correlated.\nThere are several ways to measure correlation. It’s good practice to look at the data first, though, before jumping to the numbers.\n\n2.5.1 Relational plots\nThus far, we have used displot, or “distribution plot”, to make histograms, and catplot, or “categorical plot”, to make box and violin plots. The third major plot type in seaborn is relplot, or “relational plot”, to show the relationships between variables.\n\nExample 2.23 By default, relplot makes a scatter plot of two different variables:\n\nsns.relplot(data=cars, x=\"model_year\", y=\"mpg\");\n\n\n\n\nLike in the other plot types, we can use hue (color) and marker size to indicate groups within the data:\n\nsns.relplot(data=cars, \n    x=\"model_year\", y=\"mpg\",\n    hue=\"origin\", size=\"weight\",\n    );\n\n\n\n\nIf we want to emphasize a trend rather than the individual data values, we can instead plot the average value at each \\(x\\) with error bars:\n\nsns.relplot(data=cars, \n    x=\"model_year\", y=\"mpg\",\n    kind=\"line\", errorbar=\"sd\"\n    );\n\n\n\n\nThe error ribbon above is drawn at one standard deviation around the mean.\nIn order to see multiple pairwise scatter plots in one shot, we can use pairplot in seaborn:\n\ncolumns = [ \"mpg\", \"horsepower\", \n            \"displacement\", \"origin\" ]\n\nsns.pairplot(data=cars[columns],\n    hue=\"origin\", height=2\n    );\n\n\n\n\nThe panels along the diagonal show each quantitative variable’s PDF. The other panels show scatter plots putting one pair at a time of the variables on the coordinate axes.\n\n\n\n\n\n\n\n\n\n\n\n2.5.2 Covariance\n\nDefinition 2.10 Suppose we have two series of observations, \\([x_i]\\) and \\([y_i]\\), representing observations of random quantities \\(X\\) and \\(Y\\) having means \\(\\mu_X\\) and \\(\\mu_Y\\). Their covariance is defined as \\[\n\\Cov(X,Y) = \\frac{1}{n} \\sum_{i=1}^n (x_i-\\mu_X)(y_i-\\mu_Y).\n\\]\n\nNote that the values \\(x_i-\\mu_X\\) and \\(y_i-\\mu_Y\\) are deviations from the means. It follows from the definitions that \\[\n\\begin{split}\n    \\Cov(X,X) &= \\sigma_X^2, \\\\\n    \\Cov(Y,Y) &= \\sigma_Y^2,\n\\end{split}\n\\] i.e., self-covariance is simply variance.\nCovariance is not easy to interpret. Its units are the products of the units of the two variables, and it is sensitive to rescaling the variables (e.g., grams versus kilograms).\n\n\n2.5.3 Pearson coefficient\nWe can remove the dependence on units and scale by applying the covariance to standardized scores for both variables. The following is the best-known measure of correlation.\n\nDefinition 2.11 For the populations of \\(X\\) and \\(Y\\), the Pearson correlation coefficient is \\[\n\\begin{split}\n    \\rho(X,Y) &= \\frac{1}{n} \\sum_{i=1}^n \\left(\\frac{x_i-\\mu_X}{\\sigma_X}\\right)\\left(\\frac{y_i-\\mu_Y}{\\sigma_Y}\\right) \\\\\n    & = \\frac{\\Cov(X,Y)}{\\sigma_X\\sigma_Y},\n\\end{split}\n\\tag{2.10}\\] where \\(\\sigma_X^2\\) and \\(\\sigma_Y^2\\) are the population variances of \\(X\\) and \\(Y\\).\nFor samples from the two populations, we use \\[\nr_{xy} =  \\frac{\\sum_{i=1}^n (x_i-\\bar{x}) (y_i-\\bar{y})}{\\sqrt{\\sum_{i=1}^n (x_i-\\bar{x})^2}\\,\\sqrt{\\sum_{i=1}^n (y_i-\\bar{y})^2}},\n\\tag{2.11}\\] where \\(\\bar{x}\\) and \\(\\bar{y}\\) are sample means.\n\nBoth \\(\\rho_{XY}\\) and \\(r_{xy}\\) are between \\(-1\\) and \\(1\\), with the endpoints indicating perfect correlation (inverse or direct).\nAn equivalent formula for \\(r_{xy}\\) is \\[\nr_{xy} =  \\frac{1}{n-1} \\sum_{i=1}^n \\left(\\frac{x_i-\\bar{x}}{s_x}\\right)\\, \\left(\\frac{y_i-\\bar{y}}{s_y}\\right),\n\\tag{2.12}\\] where the quantities in parentheses are z-scores.\n\nExample 2.24 We might reasonably expect horsepower and miles per gallon to be negatively correlated:\n\nsns.relplot(data=cars, x=\"horsepower\", y=\"mpg\");\n\n\n\n\nCovariance allows us to quantify the relationship:\n\ncars[ [\"horsepower\", \"mpg\"] ].cov()\n\n\n\n\n\n\n\n\nhorsepower\nmpg\n\n\n\n\nhorsepower\n1481.569393\n-233.857926\n\n\nmpg\n-233.857926\n61.089611\n\n\n\n\n\n\n\nBut should these numbers considered big? The Pearson coefficient is more easily interpreted:\n\ncars[ [\"horsepower\", \"mpg\"] ].corr()\n\n\n\n\n\n\n\n\nhorsepower\nmpg\n\n\n\n\nhorsepower\n1.000000\n-0.778427\n\n\nmpg\n-0.778427\n1.000000\n\n\n\n\n\n\n\nThe value of about \\(-0.79\\) suggests that knowing one of the values would allow us to predict the other one rather well using a best-fit straight line (more on that in a future chapter).\n\n\n\n\n\n\n\n\n\nAs usual when dealing with means, however, the Pearson coefficient can be sensitive to outlier values.\n\nExample 2.25 The Pearson coefficient of any variable with itself is 1. But let’s correlate two series that differ in only one element: \\(0,1,2,\\ldots,19\\), and the same sequence but with the fifth value replaced by \\(-100\\):\n\nx = pd.Series( range(20) )\ny = x.copy()\ny[4] = -100\nx.corr(y)\n\n0.43636501543147005\n\n\nDespite the change being in a single value, over half of the predictive power was lost.\n\n\n\n2.5.4 Spearman coefficient\nThe Spearman coefficient is one way to lessen the impact of outliers when measuring correlation. The idea is that the values are used only in their orderings.\n\nDefinition 2.12 If \\(x_1,\\ldots,x_n\\) is a series of observations, let their sorted ordering be \\[\nx_{s_1},x_{s_2},\\ldots,x_{s_n}.\n\\] Then \\(s_1,s_2,\\ldots,s_n\\) is the rank series of \\(\\bfx\\).\n\n\nDefinition 2.13 The Spearman coefficient of two series of equal length is the Pearson coefficient of their rank series.\n\n\nExample 2.26 Returning to Example 2.25, we find the Spearman coefficient is barely affected by the single outlier:\n\nx = pd.Series( range(20) )\ny = x.copy()\ny[4] = -100\nx.corr(y,\"spearman\")\n\n0.9849624060150375\n\n\nIt’s trivial in this case to produce the two rank series by hand:\n\ns = pd.Series( range(1,21) )    # already sorted\nt = s.copy()\nt[:5] = [2,3,4,5,1]     # modified sort ordering\n\nt.corr(s)\n\n0.9849624060150375\n\n\nAs long as y[4] is negative, it doesn’t matter what its particular value is:\n\ny[4] = -1000000\nx.corr(y,\"spearman\")\n\n0.9849624060150375\n\n\n\n\n\n\n\n\n\n\n\nSince real data almost always features outlying or anomalous values, it’s important to think about the robustness of the statistics you choose.\n\n\n2.5.5 Categorical correlation\nAn ordinal variable, such as the days of the week, is often straightforward to quantify as integers. But a nominal variable poses a different challenge.\n\nExample 2.27 Grouped histograms suggest an association between country of origin and MPG:\n\nsns.displot(data=cars, kind=\"kde\",\n    x=\"mpg\", hue=\"origin\");\n\n\n\n\nHow can we quantify the association? The first step is to convert the origin column into dummy variables:\n\ndum = pd.get_dummies(cars, columns=[\"origin\"])\ndum.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\nname\nmpg_z\ncuts\ngroup_mpg_z\norigin_europe\norigin_japan\norigin_usa\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nchevrolet chevelle malibu\n-0.705551\n(3500, 4500]\n0.837535\n0\n0\n1\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nbuick skylark 320\n-1.089379\n(3500, 4500]\n-0.249252\n0\n0\n1\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nplymouth satellite\n-0.705551\n(2500, 3500]\n-0.956484\n0\n0\n1\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\namc rebel sst\n-0.961437\n(2500, 3500]\n-1.377177\n0\n0\n1\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nford torino\n-0.833494\n(2500, 3500]\n-1.166831\n0\n0\n1\n\n\n\n\n\n\n\nThe original origin column has been replaced by three binary indicator columns. Now we can look for correlations between them and mpg:\n\ncolumns = [\n    \"mpg\",\n    \"origin_europe\",\n    \"origin_japan\",\n    \"origin_usa\"\n    ]\ndum[columns].corr()\n\n\n\n\n\n\n\n\nmpg\norigin_europe\norigin_japan\norigin_usa\n\n\n\n\nmpg\n1.000000\n0.259022\n0.442174\n-0.568192\n\n\norigin_europe\n0.259022\n1.000000\n-0.229895\n-0.597198\n\n\norigin_japan\n0.442174\n-0.229895\n1.000000\n-0.643317\n\n\norigin_usa\n-0.568192\n-0.597198\n-0.643317\n1.000000\n\n\n\n\n\n\n\nAs you can see from the above, europe and japan are positively associated with mpg, while usa is inversely associated with mpg.\n\n\n\n\n\n\n\n\n\n\n\n\nxckd by Randall Munroe"
  },
  {
    "objectID": "stats.html#cautionary-tales",
    "href": "stats.html#cautionary-tales",
    "title": "2  Descriptive statistics",
    "section": "2.6 Cautionary tales",
    "text": "2.6 Cautionary tales\nAttaching theorem-supported numbers to real data feels precise and infallible. The theorems do what they say, of course—they’re theorems!—but our intuition can be a little too ready to attach significance to the numbers, causing misconceptions or mistakes. Proper visualizations can help us see through such issues.\n\n2.6.1 The Datasaurus\nThe Datasaurus Dozen is a collection of datasets that highlights the perils of putting blind trust into summary statistics.\n\nExample 2.28 The Datasaurus is a set of 142 points making a handsome portrait:\n\ndozen = pd.read_csv(\"_datasets/DatasaurusDozen.tsv\", delimiter=\"\\t\")\nsns.relplot(data=dozen[dozen[\"dataset\"]==\"dino\"], x=\"x\", y=\"y\");\n\n\n\n\nHowever, there are 12 other datasets that all have roughly the same mean and variance for \\(x\\) and \\(y\\), and the same correlations between them:\n\nby_set = dozen.groupby(\"dataset\")\nby_set.mean()\n\n\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\naway\n54.266100\n47.834721\n\n\nbullseye\n54.268730\n47.830823\n\n\ncircle\n54.267320\n47.837717\n\n\ndino\n54.263273\n47.832253\n\n\ndots\n54.260303\n47.839829\n\n\nh_lines\n54.261442\n47.830252\n\n\nhigh_lines\n54.268805\n47.835450\n\n\nslant_down\n54.267849\n47.835896\n\n\nslant_up\n54.265882\n47.831496\n\n\nstar\n54.267341\n47.839545\n\n\nv_lines\n54.269927\n47.836988\n\n\nwide_lines\n54.266916\n47.831602\n\n\nx_shape\n54.260150\n47.839717\n\n\n\n\n\n\n\n\nby_set.std()\n\n\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\naway\n16.769825\n26.939743\n\n\nbullseye\n16.769239\n26.935727\n\n\ncircle\n16.760013\n26.930036\n\n\ndino\n16.765142\n26.935403\n\n\ndots\n16.767735\n26.930192\n\n\nh_lines\n16.765898\n26.939876\n\n\nhigh_lines\n16.766704\n26.939998\n\n\nslant_down\n16.766759\n26.936105\n\n\nslant_up\n16.768853\n26.938608\n\n\nstar\n16.768959\n26.930275\n\n\nv_lines\n16.769959\n26.937684\n\n\nwide_lines\n16.770000\n26.937902\n\n\nx_shape\n16.769958\n26.930002\n\n\n\n\n\n\n\n\nby_set.corr()\n\n\n\n\n\n\n\n\n\nx\ny\n\n\ndataset\n\n\n\n\n\n\n\naway\nx\n1.000000\n-0.064128\n\n\ny\n-0.064128\n1.000000\n\n\nbullseye\nx\n1.000000\n-0.068586\n\n\ny\n-0.068586\n1.000000\n\n\ncircle\nx\n1.000000\n-0.068343\n\n\ny\n-0.068343\n1.000000\n\n\ndino\nx\n1.000000\n-0.064472\n\n\ny\n-0.064472\n1.000000\n\n\ndots\nx\n1.000000\n-0.060341\n\n\ny\n-0.060341\n1.000000\n\n\nh_lines\nx\n1.000000\n-0.061715\n\n\ny\n-0.061715\n1.000000\n\n\nhigh_lines\nx\n1.000000\n-0.068504\n\n\ny\n-0.068504\n1.000000\n\n\nslant_down\nx\n1.000000\n-0.068980\n\n\ny\n-0.068980\n1.000000\n\n\nslant_up\nx\n1.000000\n-0.068609\n\n\ny\n-0.068609\n1.000000\n\n\nstar\nx\n1.000000\n-0.062961\n\n\ny\n-0.062961\n1.000000\n\n\nv_lines\nx\n1.000000\n-0.069446\n\n\ny\n-0.069446\n1.000000\n\n\nwide_lines\nx\n1.000000\n-0.066575\n\n\ny\n-0.066575\n1.000000\n\n\nx_shape\nx\n1.000000\n-0.065583\n\n\ny\n-0.065583\n1.000000\n\n\n\n\n\n\n\nHowever, a plot reveals that these sets are, to put it mildly, quite distinct:\n\nothers = dozen[ dozen[\"dataset\"] != \"dino\" ]\nsns.relplot(data=others,\n    x=\"x\", y=\"y\",\n    col=\"dataset\", col_wrap=3, height=2.2\n    );\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAlways plot your data.\n\n\nAlways 👏 plot 👏 your 👏 data!\n\n\n2.6.2 Correlation vs. dependence\nIn casual speech, you might expect two variables that are uncorrelated to be unrelated. But that is not at all how the mathematical definitions play out.\n\nExample 2.29 For example, here are \\(x\\) and \\(y\\) variables that both depend on a hidden variable \\(\\theta\\):\n\ntheta = np.linspace(0,2*np.pi,50)\nx = pd.Series(np.cos(theta))\ny = pd.Series(np.sin(theta))\nsns.relplot(x=x, y=y);\n\n\n\n\nNeither informally nor mathematically would we say that \\(x\\) and \\(y\\) are independent! For example, if \\(y=0\\), then there is only one possible value for \\(x\\). Yet the correlation between \\(x\\) and \\(y\\) is zero:\n\nx.corr(y)\n\n8.276095507101827e-18\n\n\nCorrelation can only measure the extent of the relationship that is linear; \\(x\\) and \\(y\\) lying on a straight line means perfect correlation. In this case, every line you can draw that passes through \\((0,0)\\) is essentially equally bad at representing the data, and correlation cannot express the relationship.\n\n\n\n2.6.3 Simpson’s paradox\nThe penguin dataset contains a common paradox—or a counterintuitive phenomenon, at least.\n\nExample 2.30 Two of the variables show a fairly strong negative correlation:\n\npenguins = sns.load_dataset(\"penguins\")\ncolumns = [ \"body_mass_g\", \"bill_depth_mm\" ]\npenguins[columns].corr()\n\n\n\n\n\n\n\n\nbody_mass_g\nbill_depth_mm\n\n\n\n\nbody_mass_g\n1.000000\n-0.471916\n\n\nbill_depth_mm\n-0.471916\n1.000000\n\n\n\n\n\n\n\nBut something surprising happens if we compute correlations after grouping by species.\n\npenguins.groupby(\"species\")[columns].corr()\n\n\n\n\n\n\n\n\n\nbody_mass_g\nbill_depth_mm\n\n\nspecies\n\n\n\n\n\n\n\nAdelie\nbody_mass_g\n1.000000\n0.576138\n\n\nbill_depth_mm\n0.576138\n1.000000\n\n\nChinstrap\nbody_mass_g\n1.000000\n0.604498\n\n\nbill_depth_mm\n0.604498\n1.000000\n\n\nGentoo\nbody_mass_g\n1.000000\n0.719085\n\n\nbill_depth_mm\n0.719085\n1.000000\n\n\n\n\n\n\n\nWithin each individual species, the correlation between the variables is strongly positive!\nThis is an example of Simpson’s paradox. The reason for it can be seen from a scatter plot:\n\nsns.relplot(data=penguins,\n    x=columns[0], y=columns[1], \n    hue=\"species\"\n    );\n\n\n\n\nWithin each color, the positive association is clear. But what dominates the combination of all three species is the large difference between Gentoo and the others. Because the Gentoo are both larger and have shallower bills, the dominant relationship is negative.\n\n\n\n\n\n\n\n\n\n\nAs often happens in statistics, the precise framing of the question can strongly affect its answer. This can lead to honest mistakes by the naive as well as outright deception by the unscrupulous."
  },
  {
    "objectID": "stats.html#exercises",
    "href": "stats.html#exercises",
    "title": "2  Descriptive statistics",
    "section": "Exercises",
    "text": "Exercises\nFor these exercises, you may use computer help to work on a problem, but your submitted solution should be self-contained without reference to computer output (unless stated otherwise).\n\nExercise 2.1 The following parts are about the sample set of \\(n\\) values (\\(n&gt;2\\))\n\\[\n0, 0, 0, \\ldots, 0, 1000.\n\\]\n(That is, there are \\(n-1\\) copies of 0 and one copy of 1000.)\n(a) Show that the sample mean is \\(1000/n\\).\n(b) Find the sample median when \\(n\\) is odd.\n(c) Show that the corrected sample variance \\(s_{n-1}^2\\) is \\(10^6/n\\).\n(d) Find the sample z-scores of all the values.\n\n\nExercise 2.2 This exercise is about the same set of sample values as Exercise 2.1. Suppose the 2σ-outlier criterion is applied using the sample mean and sample variance.\n(a) Show that regardless of \\(n\\), the value 0 is never an outlier.\n(b) Show that the value 1000 is an outlier if \\(n \\ge 6\\).\n\n\nExercise 2.3 Suppose the samples \\(x_1,\\ldots,x_n\\) have the z-scores \\(z_1,\\ldots,z_n\\).\n(a) Show that the sample z-scores satisfy \\(\\displaystyle \\sum_{i=1}^n z_i = 0.\\)\n(b) Show that the sample z-scores satisfy \\(\\displaystyle \\sum_{i=1}^n z_i^2 = n-1.\\)\n\n\nExercise 2.4 Define a population by\n\\[\nx_i = \\begin{cases}\n1, & 1 \\le i \\le 11, \\\\\n2, & 12 \\le i \\le 14,\\\\\n4, & 15 \\le i \\le 22, \\\\\n6, & 23 \\le i \\le 32.\n\\end{cases}\n\\]\n(That is, there are 11 values of 1, 3 values of 2, 8 values of 4, and 10 values of 6.)\n(a) Find the median of the population.\n(b) Find the smallest interval containing all non-outlier values according to the 1.5 IQR criterion.\n\n\nExercise 2.5 Given a population of values \\(x_1,x_2,\\ldots,x_n\\), define the function\n\\[\nr_2(x) = \\sum_{i=1}^n (x_i-x)^2.\n\\]\nShow using calculus that \\(r_2\\) is minimized at \\(x=\\mu\\), the population mean.\n\n\nExercise 2.6 Suppose that \\(n=2k+1\\) and a population has values \\(x_1,x_2,\\ldots,x_{n}\\) in sorted order, so that the median is equal to \\(x_k\\). Define the function\n\\[\nr_1(x) = \\sum_{i=1}^n |x_i - x|.\n\\]\n(This function is called the total absolute deviation of \\(x\\) from the population.) Show that \\(r_1\\) has a global minimum at \\(x=x_k\\) by way of the following steps.\n(a) Explain why the derivative of \\(r_1\\) is undefined at every \\(x_i\\). Consequently, all of the \\(x_i\\) are critical points of \\(r_1\\).\n(b) Determine \\(r_1'\\) within each interval \\((-\\infty,x_1),\\, (x_1,x_2),\\, (x_2,x_3),\\) and so on. Explain why this shows that there cannot be any additional critical points to consider. (Note: you can replace the absolute values with a piecewise definition of \\(r_1\\), where the formula for the pieces changes as you cross over each \\(x_i\\).)\n(c) By considering the \\(r_1'\\) values between the \\(x_i\\), explain why it must be that\n\\[\nr_1(x_1) &gt; r_1(x_2) &gt; \\cdots &gt; r_1(x_k) &lt; r_1(x_{k+1}) &lt; \\cdots &lt; r_1(x_n).\n\\]\n\n\nExercise 2.7 Prove that two sample sets have a Pearson correlation coefficient equal to 1 if they have identical z-scores. (Hint: Use the results of Exercise 2.3.)\n\n\nExercise 2.8 Suppose that two sample sets satisfy \\(y_i=-x_i\\) for all \\(i\\). Prove that the Pearson correlation coefficient between the sets equals \\(-1\\)."
  },
  {
    "objectID": "classification.html#classification-basics",
    "href": "classification.html#classification-basics",
    "title": "3  Classification",
    "section": "3.1 Classification basics",
    "text": "3.1 Classification basics\nA single training example or sample is characterized by a feature vector \\(\\bfx\\) of \\(d\\) real numbers and a label \\(y\\) drawn from a finite set \\(L\\). If \\(L\\) has only two members (say, “true” and “false”), we have a binary classification problem; otherwise, we have a multiclass problem.\nWhen we have \\(n\\) training samples, it’s natural to collect them into columns of a feature matrix \\(\\bfX\\) with \\(n\\) rows and \\(d\\) columns. Using subscripts to represent the indexes of the matrix, we can write\n\\[\n\\bfX = \\begin{bmatrix}\nX_{11} & X_{12} & \\cdots & X_{1d} \\\\\nX_{21} & X_{22} & \\cdots & X_{2d} \\\\\n\\vdots & \\vdots && \\vdots \\\\\nX_{n1} & X_{n2} & \\cdots & X_{nd}\n\\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nA 2D array or matrix has elements that are addressed by two subscripts. These are always given in order row, column.\nIn math, we usually start the row and column indexes at 1, but Python starts them at 0.\n\n\nEach row of the feature matrix is a single feature vector, while each column is the value for a single feature over the entire training set.\n\nExample 3.1 Suppose we want to train an algorithm to predict whether a basketball shot will score. For one shot, we might collect three coordinates to represent the launch point, three to represent the launch velocity, and three to represent the initial angular rotation (axis and magnitude). Thus each shot will require a feature vector of length 9. A collection of 200 sample shots would be encoded as a \\(200\\times 9\\) feature matrix.\n\nWe can also collect the associated training labels into the label vector\n\\[\n\\bfy = \\begin{bmatrix}\ny_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n\\end{bmatrix}\n\\]\n\n\n\n\n\n\nImportant\n\n\n\nIn linear algebra, the default shape for a vector is usually as a single column. In Python, a vector doesn’t exactly have a row or column orientation, though when it matters, a row shape is usually preferred.\n\n\nEach component \\(y_i\\) of the label vector is drawn from the label set \\(L\\).\n\n\n3.1.1 Encoding qualitative data\nWe have defined the features as numerical values. What should we do with qualitative data? We could arbitrarily assign numbers to possible values, such as “0=red”, “1=blue”, “2=yellow,” and so on. But this is not ideal: most of the time, we would not want to say that yellow is twice as far from red as it is from blue!\nA better strategy is to use the one-hot or dummy encoding. If a particular feature can take on \\(k\\) unique values, then we introduce \\(k\\) new features indicating which value is present. (We can use \\(k-1\\) dummy features if we interpret all-zeros to mean the \\(k\\)th possibility.)\n\n\n3.1.2 Walkthrough\nThe scikit-learn package sklearn is a collection of well-known machine learning algorithms and tools. Scikit-learn offers a uniform framework across all classifier types:\n\nDefine features and labels as numpy arrays or pandas frames.\nCreate a learner object, specifying any values that specialize the behavior.\nTrain the learner on the data by calling the fit method.\nApply the learner to a feature matrix/frame using the predict method.\n\nThe package also includes a few classic example datasets. We load one derived from automatic recognition of handwritten digits:\n\n\n\n\n\n\n\n\n\nfrom sklearn import datasets \nds = datasets.load_digits()        # loads a well-known dataset\nX, digits = ds[\"data\"], ds[\"target\"]      # assign feature matrix and label vector\nprint(\"The feature matrix has shape\", X.shape)\nprint(\"The label vector has shape\", digits.shape)\nn, d = X.shape\nprint(\"there are\", d, \"features and\", n, \"samples\")\n\nThe feature matrix has shape (1797, 64)\nThe label vector has shape (1797,)\nthere are 64 features and 1797 samples\n\n\nThe entries of digits are integer values 0 through 9, indicating the true value of the corresponding handwritten digit. Let’s consider the binary problem, “is this digit a 6?” That implies the following Boolean label vector:\n\ny = (digits == 6)\nprint(\"Number of sixes in dataset:\", sum(y))\n\nNumber of sixes in dataset: 181\n\n\nIt so happens that the 64 features in the dataset are the pixel grayscale values from an \\(8\\times 8\\) bitmap of a handwritten digit. We can visualize the raw data. Here are some of the 6s, for example:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\ndef plot_digits(X):\n    fig, axes = plt.subplots(4,4)\n    for i in range(4):\n        for j in range(4):\n            row = j + 4*i\n            A = np.reshape(np.array(X[row,:]),(8,8))\n            sns.heatmap(A,ax=axes[i,j],square=True,cmap=\"gray\",cbar=False)\n            axes[i,j].axis(False)\n    return None\n\nplot_digits(X[y])\n\n\n\n\n\nThe process of training a classifier is called fitting. We first have to import a particular classifier type, then create an instance of that type. Here, we choose one that we will study in a future section:\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=20)    # specification of the model\n\nNow we can fit the learner to the training data:\n\nknn.fit(X, y)                                 # training of the model\n\nKNeighborsClassifier(n_neighbors=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=20)\n\n\nAt this point, the classifier object knn has figured out what it needs from the training data. It has methods we can now call to make predictions and evaluate the quality of the results.\nEach new prediction is for a query vector with 64 features. In practice, we can use a list in place of a vector for the query.\n\nquery = [20]*d    # list with d copies of 20  \n\nThe predict method of the classifier allows specifying multiple query vectors as rows of an array. In fact, it expects a 2D array in all cases, even if there is just one row.\n\nXq = [ query ]    # 2D array with a single row\n\nThe result of the prediction will be a vector of labels, one per row of the query.\n\n# Get vector of predictions:\nknn.predict(Xq)  \n\narray([False])\n\n\n\n\n\n\n\n\nImportant\n\n\n\nThe predict method requires a vector or list of query vectors or lists and it outputs a vector of classes. This is true even if there is just a single query.\n\n\nAt the moment, we don’t have any realistic query data at hand other than the training data. But we can investigate the question of how well the classifier does on that data, simply by using the feature matrix as the query:\n\n# Get vector of predictions for the training set:\nyhat = knn.predict(X)    \n\nNow we simply count up the number of correctly predicted labels and divide by the total number of samples to get the accuracy of the classifier.\n\nacc = sum(yhat == y) / n    # fraction of correct predictions\nprint(f\"accuracy is {acc:.1%}\")\n\naccuracy is 99.9%\n\n\nNot surprisingly, sklearn has functions for doing this measurement in fewer steps. The metrics module has functions that can compare true labels with predictions. In addition, each classifier object has a score method that allows you to skip finding the predictions vector yourself.\n\nfrom sklearn.metrics import accuracy_score\n\n# Compare original labels to predictions:\nacc = accuracy_score(y, yhat)    \nprint(f\"accuracy score is {acc:.1%}\")\n\n# Same result, if we don't want to keep the predicted values around:\nacc = knn.score(X, y)    \nprint(f\"knn score is {acc:.1%}\")\n\naccuracy score is 99.9%\nknn score is 99.9%\n\n\nDoes this mean that the classifier is a good one? The raw number looks great, but that question is more subtle than you would expect."
  },
  {
    "objectID": "classification.html#classifier-performance",
    "href": "classification.html#classifier-performance",
    "title": "3  Classification",
    "section": "3.2 Classifier performance",
    "text": "3.2 Classifier performance\nLet’s return to the (previously cleaned) loan applications dataset.\n\nimport pandas as pd\nloans = pd.read_csv(\"_datasets/loan_clean.csv\")\nloans.head()\n\n\n\n\n\n\n\n\nloan_amnt\nint_rate\ninstallment\nannual_inc\ndti\ndelinq_2yrs\ndelinq_amnt\npercent_funded\n\n\n\n\n0\n5000\n10.65\n162.87\n24000.0\n27.65\n0\n0\n100.0\n\n\n1\n2500\n15.27\n59.83\n30000.0\n1.00\n0\n0\n100.0\n\n\n2\n2400\n15.96\n84.33\n12252.0\n8.72\n0\n0\n100.0\n\n\n3\n10000\n13.49\n339.31\n49200.0\n20.00\n0\n0\n100.0\n\n\n4\n3000\n12.69\n67.79\n80000.0\n17.94\n0\n0\n100.0\n\n\n\n\n\n\n\nWe create a binary classification problem by labelling whether each loan was at least 95% funded. The other columns will form the features for the predictions.\n\nX = loans.drop(\"percent_funded\", axis=1)\ny = loans[\"percent_funded\"] &gt; 95\n\n\n\n\n\n\n\n\n\n\n3.2.1 Train–test paradigm\nIt seems desirable for a classifier to work well on the samples it was trained on. But we probably want to do more than that.\n\nDefinition 3.1 The performance of a predictor on previously unseen data is known as the generalization of the predictor.\n\nIn order to gauge generalization, we hold back some of the labeled data from training and use it only to test the performance. An sklearn helper function called train_test_split allows us to split off 20% of the data to use for testing. It’s usually recommended to shuffle the order of the samples before the split, and in order to make the results reproducible, we give a specific random seed to the RNG used for the shuffle.\n\n\n\n\n\n\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n  test_size=0.2,\n  shuffle=True,\n  random_state=3\n)\n\nprint(\"There are\", X_train.shape[0], \"training samples.\")\nprint(\"There are\", X_test.shape[0], \"testing samples.\")\n\nThere are 31773 training samples.\nThere are 7944 testing samples.\n\n\nWe can check that the test and train labels have similar characteristics:\n\nimport pandas as pd\nprint(\"labels in the training set:\")\nprint( pd.Series(y_train).describe() )\n\nprint(\"\\nlabels in the testing set:\")\nprint( pd.Series(y_test).describe() )\n\nlabels in the training set:\ncount     31773\nunique        2\ntop        True\nfreq      30351\nName: percent_funded, dtype: object\n\nlabels in the testing set:\ncount     7944\nunique       2\ntop       True\nfreq      7575\nName: percent_funded, dtype: object\n\n\nNow we train on the training data…\n\nknn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(X_train, y_train)    # fit only to train set\n\nKNeighborsClassifier(n_neighbors=9)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier(n_neighbors=9)\n\n\n…and test on the rest.\n\nacc = knn.score(X_test, y_test)    # score only on test set\nprint(f\"test accuracy is {acc:.1%}\")\n\ntest accuracy is 95.6%\n\n\nThis seems like a high accuracy, perhaps. But consider that the vast majority of loans were funded:\n\nfunded = sum(y)\nprint(f\"{funded/len(y):.1%} were funded\")\n\n95.5% were funded\n\n\nTherefore, an algorithm that simply “predicts” funding every single loan could do as well as the trained classifier!\n\nfrom sklearn.metrics import accuracy_score\ngenerous = [True]*len(y_test)\nacc = accuracy_score(y_test, generous)\nprint(f\"fund-them-all accuracy is {acc:.1%}\")\n\nfund-them-all accuracy is 95.4%\n\n\nIn this context, our trained classifier is not impressive at all. We need a metric other than accuracy.\n\n\n3.2.2 Binary classifiers\nA binary classifier is one that produces just two unique labels, which we call “yes” and “no” here. To fully understand the performance of a binary classifier, we have to account for four cases:\n\n\n\n\n\n\n\n\n\nTrue positives (TP): Predicts “yes”, actually is “yes”\nFalse positives (FP): Predicts “yes”, actually is “no”\nTrue negatives (TN): Predicts “no”, actually is “no”\nFalse negatives (FN): Predicts “no”, actually is “yes”\n\nWe often display these in a 2×2 table according to the states of the prediction and ground truth (i.e., the given label in the dataset). The table can be filled with counts or percentages of tested instances to create a confusion matrix, as illustrated in Figure 3.1.\n\n\n\nFigure 3.1: A confusion matrix. Correct predictions are on the diagonal.\n\n\n\nDefinition 3.2 The following primary metrics are defined for a binary classifier:\n\n\n\n\n\n\n\naccuracy\n\\(\\dfrac{\\TP + \\TN}{\\TP + \\FP + \\TN + \\FN}\\)\n\n\nrecall\n\\(\\dfrac{\\TP}{\\TP + \\FN}\\)\n\n\nspecificity\n\\(\\dfrac{\\TN}{\\TN + \\FP}\\)\n\n\nprecision\n\\(\\dfrac{\\TP}{\\TP + \\FP}\\)\n\n\nnegative predictive value (NPV)\n\\(\\dfrac{\\TN}{\\TN + \\FN}\\)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecall is also known as sensitivity or the true positive rate. Specificity is the true negative rate.\n\n\nAll of the primary metrics vary between 0 (worst) and 1 (best). Accuracy is self-explanatory; the other metrics answer the following questions:\n\nrecall How often are actual “yes” cases predicted correctly?\nspecificity How often are actual “no” cases predicted correctly?\nprecision How often are the “yes” predictions correct?\nNPV How often are the “no” predictions correct?\n\n\nExample 3.2 Here is a confusion matrix for a hypothetical test for COVID-19 antigens applied to 100 samples:\n\n\n\n\nPredicted \\(+\\)\nPredicted \\(-\\)\n\n\nActually \\(+\\)\n22\n4\n\n\nActually \\(-\\)\n12\n62\n\n\n\nFrom this we see that the accuracy is \\(84/100\\), or 84%. Out of 26 samples that had the antigen, the test identified 22, for a recall of \\(22/26=84.6\\)%. Out of 74 samples that did not have the antigen, 62 were predicted correctly, for a specificity of \\(62/74=83.8\\)%. Finally, the precision is \\(22/34=64.7\\)% and the NPV is \\(62/66=93.9\\)%.\n\n\n\n\n\n\n\n\n\nThe metrics to pay attention to depend on the context and application. For a pregnancy test, for example, the health consequences of a false negative might be substantial, so the manufacturer might aim mainly for a high recall rate. But a risk-averse loan officer would be most concerned about making loans to those who end up defaulting, i.e. a low false positive rate, and seek a high precision.\n\nThere are ways of combining the primary metrics in order to account for two at once.\n\nDefinition 3.3 The F₁ score is the harmonic mean of the precision and the recall, i.e., \\[\nF_1 = \\left[ \\frac{1}{2} \\left(\\frac{1}{\\text{precision}} + \\frac{1}{\\text{recall}} \\right)  \\right]^{-1} = \\frac{2\\TP}{2\\TP+\\FN+\\FP}.\n\\]\nThe balanced accuracy is the arithmetic mean of recall and specificity.\n\nLike the primary metrics, \\(F_1\\) and balanced accuracy range between 0 (worst) and 1 (best). The harmonic mean is small if either of its terms is small, so a high \\(F_1\\) means both precision and recall are good.\n\nExample 3.3 Continuing with the loan classifier trained earlier in this section, we can find the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\nyhat = knn.predict(X_test)\nC = confusion_matrix(y_test, yhat, labels=[True, False])\nC\n\narray([[7570,    5],\n       [ 343,   26]])\n\n\n\n\n\n\n\n\nCaution\n\n\n\nIt’s advisable to always call confusion_matrix with the labels argument, even though it is optional, in order to control the ordering within the matrix. In particular, False &lt; True, so the default for Boolean labels is to count the upper left corner of the matrix as “true negatives,” assuming that False represents a negative result.\n\n\nIn order order to help keep track of what the entries mean, we can also make a picture of the confusion matrix:\n\nfrom sklearn.metrics import ConfusionMatrixDisplay\nlbl = [\"fund\", \"reject\"]\nConfusionMatrixDisplay(C, display_labels=lbl).plot();\n\n\n\n\nHence, there are 7570 true positives. The accuracy is\n\nprint( f\"accuracy = {(7570 + 26) / np.sum(C):.1%}\" )\n\naccuracy = 95.6%\n\n\n(In practice, of course, we can use the accuracy function we imported earlier.) Here are the other primary scores:\n\nTP, FN, FP, TN = C.ravel()    # grab the 4 values in the confusion matrix\nprint( f\"recall = {TP/(TP+FN):.1%}\" )\nprint( f\"specificity = {TN/(TN+FP):.1%}\" )\nprint( f\"precision = {TP/(TP+FP):.1%}\" )\nprint( f\"NPV = {TN/(TN+FN):.1%}\" )\n\nrecall = 99.9%\nspecificity = 7.0%\nprecision = 95.7%\nNPV = 83.9%\n\n\nAs noted above, few who ought to get a loan will go away disappointed, a loan officer might get nervous about the low specificity score that indicates bad loans.\nIn sklearn.metrics there are functions to compute recall and precision without reference to the confusion matrix. You must put the ground-truth labels before the predicted labels, and you should also specify which label value corresponds to a “positive” result. Swapping the “positive” role effectively swaps recall with specificity and precision with NPV.\n\nfrom sklearn.metrics import precision_score, recall_score\n\nfor pos in [True, False]:\n  print(\"With\", pos, \"as positive:\")\n  s = recall_score(y_test, yhat, pos_label=pos)\n  print(f\"    recall is {s:.3f}\")\n  s = precision_score(y_test, yhat, pos_label=pos) \n  print(f\"    precision is {s:.3f}\")\n  print()\n\nWith True as positive:\n    recall is 0.999\n    precision is 0.957\n\nWith False as positive:\n    recall is 0.070\n    precision is 0.839\n\n\n\nThere are also functions for the composite scores defined in Definition 3.3:\n\nfrom sklearn.metrics import f1_score, balanced_accuracy_score\n\nprint( f\"F1 = {f1_score(y_test, yhat):.1%}\" )\nprint( f\"balanced accuracy = {balanced_accuracy_score(y_test, yhat):.1%}\" )\n\nF1 = 97.8%\nbalanced accuracy = 53.5%\n\n\nThe loan classifier has excellent recall, respectable precision, and terrible specificity, resulting in a good F₁ score and a low balanced accuracy score.\n\n\n\n\n\n\n\n\n\n\nExample 3.4 If \\(k\\) of the \\(n\\) testing samples were funded loans, then the fund-them-all loan classifier has\n\\[\n\\TP = k,\\, \\TN = 0,\\, \\FP = n-k,\\, \\FN = 0.\n\\]\nIts F₁ score is thus\n\\[\n\\frac{2\\TP}{2\\TP+\\FN+\\FP} = \\frac{2k}{2k+n-k} = \\frac{2k}{k+n}.\n\\]\nIf the fraction of funded samples in the test set is \\(k/n=a\\), then the accuracy of this classifier is \\(a\\). Its F₁ score is \\(2a/(1+a)\\), which is larger than \\(a\\) unless \\(a=1\\). That’s because the true positives greatly outweigh the other confusion matrix values.\nThe balanced accuracy is\n\\[\n\\frac{1}{2} \\left(\\frac{\\TP}{\\TP+\\FN} + \\frac{\\TN}{\\TN+\\FP} \\right)  = \\frac{1}{2},\n\\]\nindependently of \\(a\\). This quantity is sensitive to the low specificity.\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Multiclass classifiers\nWhen there are more than two unique possible labels, these metrics can be applied using the one-vs-rest paradigm. For \\(K\\) unique labels, this paradigm poses \\(K\\) binary questions: “Is it in class 1, or not?”, “Is it in class 2, or not?”, etc. The confusion matrix becomes \\(K\\times K\\).\n\n\n\n\n\n\n\n\n\nExample 3.5 We load a dataset on the characteristics of cars and use quantitative factors to predict the region of origin:\n\nimport seaborn as sns\ncars = sns.load_dataset(\"mpg\").dropna()\ncars.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nusa\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nusa\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nusa\nford torino\n\n\n\n\n\n\n\nNow we extract the quantitative features and labels:\n\nfeatures = [\"cylinders\", \"horsepower\", \"weight\", \"acceleration\", \"mpg\"]\nX = cars[features]\ny = pd.Categorical(cars[\"origin\"])\nprint(X.shape[0], \"samples and\", X.shape[1], \"features\")\n\n392 samples and 5 features\n\n\n\n\n\n\n\n\nTip\n\n\n\nIt’s not necessary to convert a vector of strings into a Categorical vector of labels, but it does make a few things handy in post-processing.\n\n\nNext, we split into training and testing subsets:\n\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y, \n  test_size=0.2, \n  shuffle=True, \n  random_state=1\n)\n\nNow we perform the fit and measure the accuracy:\n\nknn = KNeighborsClassifier(n_neighbors=8)\nknn.fit(X_train, y_train)\nyhat = knn.predict(X_test)\nprint(f\"accuracy is {accuracy_score(y_test, yhat):.1%}\")\n\naccuracy is 77.2%\n\n\nHere is the confusion matrix:\n\nlabels = y.categories\nC = confusion_matrix(y_test, yhat, labels=labels)\nConfusionMatrixDisplay(C, display_labels=labels).plot();\n\n\n\n\nFrom the confusion matrix above we can see that, for example, out of 54 predictions of “usa” on the test set, there are 8 total false positives, in the sense that the actual labels were otherwise.\nWe also get \\(K\\) versions of the metrics like accuracy, recall, F₁ score, and so on. We can get all the individual precision scores, say, automatically:\n\nprecisions = precision_score(y_test, yhat, average=None)\nfor (label,p) in zip(labels,precisions): \n    print(f\"{label}: {p:.1%}\")\n\neurope: 62.5%\njapan: 58.8%\nusa: 85.2%\n\n\nTo get a composite precision score, we have to specify an averaging method. The \"macro\" option simply takes the mean of the vector above.\n\nmac = precision_score(y_test, yhat, average=\"macro\")\nprint(mac)\n\n0.6883623819898329\n\n\n\n\n\n\n\n\n\n\n\nThere are other ways to average performance scores over the classes, depending on whether poorly represented classes should be weighted more weakly than others."
  },
  {
    "objectID": "classification.html#decision-trees",
    "href": "classification.html#decision-trees",
    "title": "3  Classification",
    "section": "3.3 Decision trees",
    "text": "3.3 Decision trees\nA decision tree is much like playing Twenty Questions. A question is asked, and the answer reduces the possible results, leading to a new question. CART (Classification And Regression Tree), which we present here, is a popular method for systematizing the idea.\nGiven feature vectors \\(\\bfx_1,\\ldots,\\bfx_n\\) with labels \\(y_1,\\ldots,y_n\\), the immediate goal is to partition the samples into two subsets, each of which has as uniform as set of labels as possible. The process is then repeated recursively: the subsets are bisected to make four subsets, and so on. These splits form a binary tree. When a prediction is required, we apply the same criteria as the splits in our tree, and when we reach a leaf of the tree (i.e., no further subdivisions), we take a vote of all the samples in the leaf subset to determine the label.\nThere are two details that need to be specified: what kind of subset splits to allow, and how to determine the uniformity of labels within each subset. We start with the latter.\n\n3.3.1 Gini impurity\n\nDefinition 3.4 Let \\(k\\) be an integer. The indicator function \\(\\mathbb{1}_k\\) is the function on integers defined as \\[\n\\mathbb{1}_k(t) = \\begin{cases}\n  1, & \\text{if } t=k, \\\\\n  0, & \\text{otherwise.}\n  \\end{cases}\n\\]\n\nLet \\(S\\) be any subset of samples, given as a list of indices into the original set. Suppose there are \\(K\\) unique labels, which we denote \\(1,2,\\ldots,K\\). Define the values\n\\[\np_k = \\frac{1}{ |S| } \\sum_{i\\in S} \\mathbb{1}_k(y_i), \\qquad k=1,\\ldots,K,\n\\tag{3.1}\\]\nwhere \\(|S|\\) is the number of elements in \\(S\\). In words, \\(p_k\\) is the proportion of samples in \\(S\\) that have label \\(k\\). Note that the sum over all the \\(p_k\\) equals 1.\n\nDefinition 3.5 The Gini impurity of sample set \\(S\\) is defined as \\[\nH(S) = \\sum_{k=1}^K p_k(1-p_k),\n\\] where \\(p_k\\) is defined in Equation 3.1.\n\n\n\n\n\n\n\n\n\nIf one of the \\(p_k\\) is 1, then the others are all zero and \\(H(S)=0\\). This is considered optimal—it indicates that all the labels in the set \\(S\\) are identical.\nAt the other extreme, if \\(p_k=1/K\\) for all \\(k\\), then \\[\nH(S) = \\sum_{k=1}^K \\frac{1}{K} \\left(1 - \\frac{1}{K} \\right) = K\\cdot \\frac{1}{K}\\cdot\\frac{K-1}{K} = \\frac{K-1}{K} &lt; 1.\n\\] In general, \\(H\\) is always nonnegative and less than 1, and it only approaches 1 in the limit of a large number of equally distributed classes.\n\nExample 3.6 Suppose a set \\(S\\) has \\(n\\) members with label A, 1 member with label B, and 1 member with label C. What is the Gini impurity of \\(S\\)?\n\nSolution. We have \\(p_A=n/(n+2)\\), \\(p_B=p_C=1/(n+2)\\). Hence\n\\[\n\\begin{split}\n    H(S) &= \\frac{n}{n+2}\\left( 1 - \\frac{n}{n+2} \\right) + 2 \\frac{1}{n+2}\\left( 1 - \\frac{1}{n+2} \\right) \\\\\n    &= \\frac{n}{n+2}\\frac{2}{n+2} + \\frac{2}{n+2}\\frac{n+1}{n+2} \\\\\n    &= \\frac{4n+2}{(n+2)^2}.\n\\end{split}\n\\]\nThis value is 1/2 for \\(n=0\\) and approaches zero as \\(n\\to\\infty\\).\n\n\n\n\n\n\n\n\n\n\n\n\n3.3.2 Partitioning\nSuppose we start with a sample set \\(S\\) that we partition into disjoint (i.e., nonoverlapping) subsets \\(S_L\\) and \\(S_R\\). We want to assign a total impurity to the partitioning so that we can compare different scenarios. One pitfall we should avoid is to put just one sample into \\(S_L\\) and all the rest into \\(S_R\\), which would make \\(H(S_L)=0\\) automatically and give an advantage. So we will choose a formula that rewards more evenly divided subsets.\n\nDefinition 3.6 The total impurity of the partition \\((S_L,S_R)\\) is \\[\nQ(S_L,S_R) = \\lvert S_L\\rvert\\, H(S_L) + \\lvert S_R \\rvert \\, H(S_R).\n\\]\n\n\n\n\n\n\n\n\n\nIf \\(S_L\\) and \\(S_R\\) are both pure subsets, then \\(Q(S_L,S_R)=0\\). Otherwise, \\(Q\\) tends to be larger in response to less purity and greater size in the subsets.\nOur goal is to choose a partition that minimizes \\(Q\\). However, we constrain ourselves to allow only certain kinds of partitions. If feature space is \\(d\\)-dimensional, we select a dimension \\(1 \\le j \\le d\\) and a real threshold value \\(\\theta\\). Then each feature vector \\(\\bfx\\) is placed in \\(S_L\\) if \\(x_j \\le \\theta\\) and in \\(S_R\\) if \\(x_j &gt; \\theta\\). Geometrically, we are dividing feature space by an axis-aligned line if \\(d=2\\) or an axis-aligned plane if \\(d=3\\). This splitting criterion can be evaluated extremely quickly, and we can find the best possible such partitioning in the sense of minimizing \\(Q(S_L,S_R)\\) in a reasonable amount of time.\n\nExample 3.7 Suppose we have \\(d=1\\) feature and are given the four samples \\((X_i,y_i) = \\{ (0,A), (1,B), (2,A), (3,B) \\}\\). What is the optimal partition?\n\nSolution. We can partition into two pieces based on a threshold value \\(\\theta\\): \\(\\{x \\le \\theta\\}\\) and \\(\\{x &gt; \\theta\\}\\). But there are really only 5 different cases.\n\n\n\n\n\n\n\n\n\n\\(\\theta\\)\nLeft\nRight\n\\(Q\\)\n\n\n\n\n\\(&lt; 0\\)\n\\(\\varnothing\\), \\(H=0\\)\n\\([A, B, A, B]\\), \\(H=\\tfrac{1}{2}\\cdot \\tfrac{1}{2} + \\tfrac{1}{2}\\cdot \\tfrac{1}{2} =\\tfrac{1}{2}\\)\n\\(0\\cdot 0 + 4\\cdot \\tfrac{1}{2} = 2\\)\n\n\n\\([0,1)\\)\n\\([A]\\), \\(H=0\\)\n\\([B, A, B]\\), \\(H=\\tfrac{2}{3}\\cdot \\tfrac{1}{3} + \\tfrac{1}{3}\\cdot \\tfrac{2}{3} =\\tfrac{4}{9}\\)\n\\(1 \\cdot 0 + 3 \\cdot\\tfrac{4}{9} = \\tfrac{4}{3}\\)\n\n\n\\([1,2)\\)\n\\([A, B]\\), \\(H=\\tfrac{1}{2}\\cdot \\tfrac{1}{2} + \\tfrac{1}{2}\\cdot \\tfrac{1}{2} =\\tfrac{1}{2}\\)\n\\([A, B]\\), \\(H=\\tfrac{1}{2}\\)\n\\(2 \\cdot \\tfrac{1}{2} + 2 \\cdot\\tfrac{1}{2} = 2\\)\n\n\n\\([2,3)\\)\n\\([A,B,A]\\), \\(H=\\tfrac{2}{3}\\cdot \\tfrac{1}{3} + \\tfrac{1}{3}\\cdot \\tfrac{2}{3} =\\tfrac{4}{9}\\)\n\\([B]\\), \\(H=0\\)\n\\(3 \\cdot\\tfrac{4}{9} + 1\\cdot 0= \\tfrac{4}{3}\\)\n\n\n\\(\\ge 3\\)\n\\([A, B, A, B]\\), \\(H=\\tfrac{1}{2}\\cdot \\tfrac{1}{2} + \\tfrac{1}{2}\\cdot \\tfrac{1}{2} =\\tfrac{1}{2}\\)\n\\(\\varnothing\\), \\(H=0\\)\n\\(4\\cdot \\tfrac{1}{2} + 0\\cdot 0 = 2\\)\n\n\n\nBoth \\(\\{ (0,A) \\}, \\{ (1,B), (2,A), (3,B) \\}\\) and \\(\\{ (0,A), (1,B), (2,A) \\}, \\{ (3,B) \\}\\) are equally optimal partitionings, with \\(Q=4/3\\).\n\n\n\n\n\n\n\n\n\n\nFinally, once we have found the optimal way to split \\(S\\) into \\((S_L,S_R)\\), we need to check whether it is an improvement—that is, whether \\[\nQ(S_L,S_R) + \\alpha &lt; Q(S,\\varnothing) = |S| H(S),\n\\] where \\(\\alpha\\) is an optional nonnegative number that requires a certain minimum amount of decrease. If the condition is satisfied, then \\(S\\) is not split and becomes a leaf of the decision tree. If so, then we add this split to the binary tree and recursively check both \\(S_L\\) and \\(S_R\\) for partitioning.\nThe above strategy is fast but can fail to get the best possible decision tree. That is, the optimal partition into 4 or 8 subsets might require you to look ahead in order to avoid a good-looking first partition that leads you astray. Because the optimization algorithm considers only one level at a time, we say it is a greedy approach.\n\n\n\n3.3.3 Decision boundary\nOne way to think about a classifier is that it divides all of feature space into zones belonging to the different classes. The boundary between two zones is the set of points where the classifier is indifferent between the two classes; together, the boundary points form the decision boundary.\nFor a decision tree, the decision boundary is always perpendicular to the feature axes, as illustrated in this animation:\n\nInitially, when the tree has a depth of 1, all of feature space is divided into two zones. At each new depth the tree is able to add more zones, and the decision boundary gradually becomes more complex.\n\n\n3.3.4 Usage and interpretation\nIn naive form, the decision tree construction continues to find partitions until every leaf in the tree represents a pure subset. In practice, we usually set a limit on the depth of the tree, which is the maximum number of partitions it takes to start from the root and reach any leaf. This obviously puts an upper limit on the computational time, but it is also desirable for other reasons we will explore in the next chapter.\n\nExample 3.8 We create a toy dataset with 20 random points in the plane, with two subsets of 10 that are shifted left/right a bit. (The details are not important.) Here is how the set looks:\n\n\nCode\nimport numpy as np\nimport pandas as pd\nfrom numpy.random import default_rng\n\nrng = default_rng(1)\ngp1 = rng.random((10,2))\ngp1[:,0] -= 0.25\ngp2 = rng.random((10,2))\ngp2[:,0] += 0.25\nX = np.vstack((gp1,gp2))\ny = np.hstack(([1]*10,[2]*10))\nprint(\"feature matrix X:\")\nprint(X)\nprint(\"\\nlabel vector y:\")\nprint(y)\n\n\nfeature matrix X:\n[[ 0.26182162  0.9504637 ]\n [-0.10584039  0.94864945]\n [ 0.06183145  0.42332645]\n [ 0.57770259  0.40919914]\n [ 0.29959369  0.02755911]\n [ 0.50351311  0.53814331]\n [ 0.07973172  0.7884287 ]\n [ 0.05319483  0.45349789]\n [-0.1159583   0.40311299]\n [-0.04654476  0.26231334]\n [ 1.00036467  0.28040876]\n [ 0.73519097  0.9807372 ]\n [ 1.21165719  0.72478994]\n [ 0.79122686  0.2768912 ]\n [ 0.41065201  0.96992541]\n [ 0.76606859  0.11586561]\n [ 0.87348976  0.77668311]\n [ 0.8630033   0.9172977 ]\n [ 0.28959288  0.52858926]\n [ 0.70933588  0.06234958]]\n\nlabel vector y:\n[1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2]\n\n\nThe features are the two point coordinates, giving a 20-by-2 feature matrix X, and the labels are a vector y of length 20. We will create a data frame for aiding in visualization:\n\nimport seaborn as sns\ndf = pd.DataFrame( {\"x₁\":X[:,0], \"x₂\":X[:,1], \"y\":y} )\nsns.scatterplot(data=df, x=\"x₁\", y=\"x₂\", hue=\"y\");\n\n\n\n\nNow we create and fit a decision tree for these samples:\n\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\ntree = DecisionTreeClassifier(max_depth=3)\ntree.fit(X,y)\n\nDecisionTreeClassifier(max_depth=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=3)\n\n\nAt this point, the tree object has created and stored all the information derived from the dataset that defines the decision tree. Since this is a small tree, we can easily look under the hood:\n\n\nCode\nfrom matplotlib.pyplot import figure\nfigure(figsize=(18,11), dpi=160)\nplot_tree(tree, feature_names=[\"x₁\", \"x₂\"]);\n\n\n\n\n\nThe root of the tree at the top shows that the best initial split was found at the vertical line \\(x_1=0.644\\). To the right of that line is a Gini value of zero: 8 samples, all with label 2. That is, any future prediction by this tree will immediately return label 2 if its value for \\(x_1\\) exceeds 0.644. Otherwise, it moves to the left child node and tests whether \\(x_2\\) is greater than \\(0.96\\). As you can see from the scatterplot above, that horizontal line has a single sample with label 2 above it. And so on.\nNotice that the bottom right node has a nonzero final Gini impurity. This node could be partitioned, but the classifier was constrained to stop at a depth of 3. If a prediction ends up here, then the classifier returns label 1, which is the most likely outcome.\n\n\n\n\n\n\n\n\n\nBecause we can follow a decision tree’s logic step by step, we say it is highly interpretable. The transparency of the prediction algorithm is an attractive aspect of decision trees, although this advantage can weaken as the power of the tree is increased to handle difficult datasets.\n\nExample 3.9 We return to the penguins and fit a decision tree to the quantitative features:\n\nimport pandas as pd\npen = sns.load_dataset(\"penguins\")\npen = pen.dropna()\nfeatures = [\n  \"bill_length_mm\",\n  \"bill_depth_mm\",\n  \"flipper_length_mm\",\n  \"body_mass_g\"\n]\nX = pen[features]\ny = pen[\"species\"]\n\ndt = DecisionTreeClassifier(max_depth=4)\ndt.fit(X, y)\n\nDecisionTreeClassifier(max_depth=4)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(max_depth=4)\n\n\nWe get some interesting information from looking at the top levels of a decision tree trained on the full dataset:\n\n\nCode\nfrom matplotlib.pyplot import figure\nfigure(figsize=(18,11), dpi=160)\nplot_tree(dt, max_depth=2, feature_names=features);\n\n\n\n\n\nThe most determinative feature for identifying the species is apparently the flipper length. If it exceeds 206.5 mm, then the penguin is rather likely to be a Gentoo.\nWe can measure the relative importance of each feature by comparing their total contributions to reducing the Gini index:\n\npd.Series(dt.feature_importances_, index=features)\n\nbill_length_mm       0.381063\nbill_depth_mm        0.051434\nflipper_length_mm    0.553866\nbody_mass_g          0.013638\ndtype: float64\n\n\nThis ranking is known as Gini importance.\nFlipper length alone accounts for about half of the resolving power of the tree, followed in importance by the bill length. The other measurements apparently have little discriminative value.\nIn order to assess the effectiveness of the tree, we use the train–test paradigm:\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\n\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y,\n  test_size=0.2,\n  shuffle=True,\n  random_state=0\n)\ndt.fit(X_train, y_train)\n\nyhat = dt.predict(X_test)\nprint(\"Confusion matrix:\")\nprint( confusion_matrix(\n  y_test, yhat, \n  labels=[\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n  ) )\nprint(\"\\nPerformance metrics:\")\nprint( classification_report(y_test, yhat) )\n\nConfusion matrix:\n[[39  0  0]\n [ 2  8  0]\n [ 1  0 17]]\n\nPerformance metrics:\n              precision    recall  f1-score   support\n\n      Adelie       0.93      1.00      0.96        39\n   Chinstrap       1.00      0.80      0.89        10\n      Gentoo       1.00      0.94      0.97        18\n\n    accuracy                           0.96        67\n   macro avg       0.98      0.91      0.94        67\nweighted avg       0.96      0.96      0.95        67\n\n\n\nThe performance is quite good, although the Chinstrap case is hindered by the relatively low number of training examples:\n\ny_train.value_counts()\n\nAdelie       107\nGentoo       101\nChinstrap     58\nName: species, dtype: int64\n\n\n\n\n\n\n\n\n\n\n\nDecision trees can depend sensitively on the sample locations; a small change might completely rewrite large parts of the tree, in which case interpretability becomes less clear."
  },
  {
    "objectID": "classification.html#nearest-neighbors",
    "href": "classification.html#nearest-neighbors",
    "title": "3  Classification",
    "section": "3.4 Nearest neighbors",
    "text": "3.4 Nearest neighbors\nThe next learner type is conceptually simple: given a point in feature space to classify, survey the nearest known examples and choose the most frequently occurring class. This is called the \\(k\\) nearest neighbors (kNN or \\(k\\)-NN) algorithm, where \\(k\\) is the number of neighboring examples to survey.\n\n3.4.1 Distances and norms\nThe existence of “closest” examples means that we need to define a notion of distance in feature spaces of any dimension. Let \\(\\real^d\\) be the space of vectors with \\(d\\) real components, and let \\(\\bfzero\\) be the vector of all zeros.\n\nDefinition 3.7 A distance metric is a function \\(\\dist\\) on pairs of vectors that satisfies the following properties for all vectors \\(\\bfu\\), \\(\\bfv\\), and \\(\\bfz\\):\n\n\\(\\dist(\\bfu,\\bfv) \\ge 0\\),\n\\(\\dist(\\bfu,\\bfv)=0\\) if and only if \\(\\bfu=\\bfv\\),\n\\(\\dist(\\bfu,\\bfv) = \\dist(\\bfv,\\bfu)\\), and\n\\(\\dist(\\bfu,\\bfv) \\le \\dist(\\bfu,\\bfz) + \\dist(\\bfz,\\bfv)\\), known as the triangle inequality.\n\n\nThese are considered the essential properties desired of a distance metric. We will define a distance metric by using a function on vectors known as a norm.\n\nDefinition 3.8 For any vector \\(\\bfu \\in \\real^d\\), we define the following norms.\n\nThe 2-norm or Euclidean norm: \\[\n\\twonorm{\\bfu} = \\bigl(u_1^2 + u_2^2 + \\cdots + u_d^2\\bigr)^{1/2}.\n\\]\nThe 1-norm or Manhattan norm: \\[\n\\onenorm{\\bfu} = |u_1| + |u_2| + \\cdots + |u_d|.\n\\]\nThe \\(\\infty\\)-norm, max norm, or Chebyshev norm: \\[\n\\infnorm{\\bfu} = \\max_{1\\le i \\le d} \\abs{u_i}.\n\\]\n\n\n\n\n\n\n\n\n\n\n\nGiven a norm, the distance between vectors \\(\\bfu\\) and \\(\\bfv\\) is defined by \\[\n\\dist(\\bfu,\\bfv) = \\norm{\\bfu - \\bfv}.\n\\]\n\nExample 3.10 Given \\(\\bfu=[-1,1,0,4]\\) and \\(\\bfv=[-2,1,2,2]\\), find the distance between them using all three common norms.\n\nSolution. We first calculate \\(\\bfu - \\bfv = [-3,0,-2,2]\\). Then \\[\n\\begin{split}\n\\twonorm{\\bfu-\\bfv} &= \\bigl( 3^2 + 0^2 + 2^2 + 2^2 \\bigr)^{1/2} = \\sqrt{17}, \\\\\n\\onenorm{\\bfu-\\bfv} &= 3  + 0 + 2 + 2 = 7, \\\\\n\\infnorm{\\bfu-\\bfv} &= \\max\\{ 3, 0, 2, 2 \\} = 3.\n\\end{split}\n\\]\n\n\nThe Euclidean norm generalizes ordinary geometric distance in \\(\\real^2\\) and \\(\\real^3\\) and is usually considered the default. One of its most important features is that \\(\\twonorm{\\bfx}^2\\) is a differentiable function of the components of \\(\\bfx\\).\n\n\n\n\n\n\nNote\n\n\n\nWhen \\(\\norm{\\,}\\) is used with no subscript, it’s usually meant to be the 2-norm, but it can also mean a generic, unspecified norm.\n\n\n\n\n3.4.2 Algorithm\nAs data, we are given labeled samples \\(\\bfx_1,\\ldots,\\bfx_n\\) in \\(\\real^d\\).\n\nDefinition 3.9 Given a new query vector \\(\\bfx\\), the kNN algorithm finds the \\(k\\) labeled samples closest to \\(\\bfx\\) and chooses the most frequently occurring label among them. Ties are broken randomly.\n\n\n\n\n\n\n\nNote\n\n\n\nThe nearest neighbor search is a well-studied problem in computer science, and there are specialized data structures used for its efficient solution.\n\n\n\nExample 3.11 Here are 6 sample points, labeled blue and red, in 2-dimensional feature space:\n\nUsing inf-norm distance and \\(k=3\\), find the kNN labels at the locations marked A and B.\n\nSolution. At point A, the nearest samples are at \\((0,2)\\) with distance \\(1\\), \\((0,0)\\) with distance \\(1\\), and \\((2,1)\\) with distance \\(1.5\\). By a 2-1 vote, the point should be labeled blue.\nAt point B, the nearest samples are at \\((0,0)\\) with distance 1, \\((-2,-1)\\) with distance 1, and \\((0,-2)\\) with distance \\(1.5\\). By a 2-1 vote, the point should be labeled red.\nNote that the blue sample point at \\((0,-2)\\) is its own closest neighbor, but the next two nearest neighbors are red. Therefore, in kNN with \\(k=3\\), the red points will outvote it and predict red! As always, we should not expect perfect performance on the training set.\n\n\n\nkNN effectively divides up the feature space into domains that are dominated by nearby instances. The evolution of the decision boundary is illustrated here for two features using 2-norm distance:\n\nAt \\(k=1\\) neighbor, each sample point defines its own local domain of influence that gives way when reaching a point equally close to a differently-labeled sample. This typically produces the most complicated decision boundaries. At the other extreme, with \\(k=n\\) neighbors, all the samples vote every time, so all of feature space is given the same label (pending tiebreakers).\n\n\n\n\n\n\n\n\n\nExample 3.12 Back to the penguins! We use dropna to drop any rows with missing values.\n\nimport seaborn as sns\nimport pandas as pd\npenguins = sns.load_dataset(\"penguins\")\npenguins = penguins.dropna()\npenguins.head(6)\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale\n\n\n6\nAdelie\nTorgersen\n38.9\n17.8\n181.0\n3625.0\nFemale\n\n\n\n\n\n\n\nThe data set has four quantitative columns that we use as features, and the species name is the label.\n\nfeatures = [\n  \"bill_length_mm\",\n  \"bill_depth_mm\",\n  \"flipper_length_mm\",\n  \"body_mass_g\"\n]\nX = penguins[features]\ny = penguins[\"species\"]\n\nEach type of classifier has to be imported before its first use in a session. (Importing more than once does no harm.)\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X, y)\n\nKNeighborsClassifier()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifierKNeighborsClassifier()\n\n\nWe can manually find the neighbors of a new vector. However, we have to make the query in the form of a data frame, since that is how the training data was provided. Here we make a query frame for values very close to the ones in the first row of the data.\n\nvals = [39, 19, 180, 3750]\nquery = pd.DataFrame([vals], columns=features)\ndist, idx = knn.kneighbors(query)\nidx[0]\n\narray([  0, 143,  53, 100, 153])\n\n\nThe result above indicates that the first sample (index 0) was the closest, followed by four others. We can look up the labels of these points:\n\ny[ idx[0] ]\n\n0         Adelie\n143       Adelie\n53        Adelie\n100       Adelie\n153    Chinstrap\nName: species, dtype: object\n\n\nBy a vote of 4–1, then, the classifier should choose Adelie as the result at this location.\n\nknn.predict(query)\n\narray(['Adelie'], dtype=object)\n\n\nNote that points can be outvoted by their neighbors. In other words, the classifier won’t necessarily be correct on every training sample. For example:\n\nprint(\"Predicted:\")\nprint( knn.predict(X.iloc[:5,:]) )\nprint()\nprint(\"Data:\")\nprint( y.iloc[:5].values )\n\nPredicted:\n['Adelie' 'Adelie' 'Chinstrap' 'Adelie' 'Chinstrap']\n\nData:\n['Adelie' 'Adelie' 'Adelie' 'Adelie' 'Adelie']\n\n\nNext, we split into training and test sets to gauge the performance of the classifier. The classification_report function creates a summary of some of the important metrics.\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y,\n  test_size=0.2,\n  shuffle=True, random_state=302\n  )\nknn.fit(X_train,y_train)\n\nyhat = knn.predict(X_test)\nprint(\"Confusion matrix:\")\nprint( confusion_matrix(\n    y_test, yhat, \n    labels=[\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n    ) )\nprint(\"\\nPerformance metrics:\")\nprint( classification_report(y_test, yhat) )\n\nConfusion matrix:\n[[34  2  2]\n [ 8  6  1]\n [ 1  0 13]]\n\nPerformance metrics:\n              precision    recall  f1-score   support\n\n      Adelie       0.79      0.89      0.84        38\n   Chinstrap       0.75      0.40      0.52        15\n      Gentoo       0.81      0.93      0.87        14\n\n    accuracy                           0.79        67\n   macro avg       0.78      0.74      0.74        67\nweighted avg       0.79      0.79      0.77        67\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe default norm in the kNN learner is the 2-norm. To use the 1-norm instead, add metric=\"manhattan\" to the classifier construction call.\n\n\n\n\n3.4.3 Standardization\nThe values in the columns of the penguin frame in Example 3.12 are scaled quite differently. In particular, the values in the body mass column are more than 20x larger than the other columns on average:\n\nX.mean()\n\nbill_length_mm         43.992793\nbill_depth_mm          17.164865\nflipper_length_mm     200.966967\nbody_mass_g          4207.057057\ndtype: float64\n\n\nConsequently, the mass feature will dominate the distance calculations. To remedy this issue, we could transform the data into z-scores:\n\nZ = X.transform( lambda x: (x - x.mean()) / x.std() )\n\nWe could then retrain the classifier using Z in place of X. Scikit-learn allows us to automate this process by creating a pipeline, which makes it easy to chain together a data transformation followed by a learner.\n\nExample 3.13 Once created, a pipeline object can mostly be treated the same as any other learner:\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler   # converts to z-scores\n\nknn = KNeighborsClassifier(n_neighbors=5)\npipe = make_pipeline(StandardScaler(), knn)\npipe.fit(X_train, y_train)\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])StandardScalerStandardScaler()KNeighborsClassifierKNeighborsClassifier()\n\n\nIn this case, standardization allows us to perform perfectly!\n\nyhat = pipe.predict(X_test)\n\nprint(\"Confusion matrix:\")\nprint( confusion_matrix(\n    y_test, yhat, \n    labels=[\"Adelie\", \"Chinstrap\", \"Gentoo\"]\n    ) )\nprint(\"\\nPerformance metrics:\")\nprint( classification_report(y_test, yhat) )\n\nConfusion matrix:\n[[38  0  0]\n [ 0 15  0]\n [ 0  0 14]]\n\nPerformance metrics:\n              precision    recall  f1-score   support\n\n      Adelie       1.00      1.00      1.00        38\n   Chinstrap       1.00      1.00      1.00        15\n      Gentoo       1.00      1.00      1.00        14\n\n    accuracy                           1.00        67\n   macro avg       1.00      1.00      1.00        67\nweighted avg       1.00      1.00      1.00        67\n\n\n\nWe can look under the hood of the pipeline. For example, we can see that the mean and variance of each of the original data columns is stored in the first part of the pipeline:\n\nprint( pipe[0].mean_ )\nprint( pipe[0].var_ )\n\n[  44.18759398   17.01503759  202.04135338 4266.72932331]\n[3.00582295e+01 3.98263101e+00 1.94114831e+02 6.53423137e+05]\n\n\nHowever, we don’t need to access that data just to use the pipeline. That’s taken care of when we use pipe.score or pipe.predict.\n\n\n\n\n\n\n\n\n\nThe StandardScaler, which converts each column (feature) into z-scores, is appropriate for data that is roughly normally distributed. For other distributions, or if there are many outliers, it may be better to use the RobustScaler, which instead centers on the median and scales using the IQR. Finally, if you just want to force all the data to fit within a fixed interval, you can use the MinMaxScaler."
  },
  {
    "objectID": "classification.html#sec-class-probabilistic",
    "href": "classification.html#sec-class-probabilistic",
    "title": "3  Classification",
    "section": "3.5 Probabilistic interpretation",
    "text": "3.5 Probabilistic interpretation\nBoth kNN and decision trees base classification on a voting procedure—for kNN, the \\(k\\) nearest neighbors cast votes, and for a decision tree, the values at a leaf cast votes. So far, we have interpreted the voting results in a winner-takes-all sense, i.e., the class with the most votes wins. But that interpretation discards a lot of potentially valuable information.\n\nDefinition 3.10 Let \\(\\bfx\\) be a query vector in a vote-based classification method. The probability vector \\(\\hat{p}(\\bfx)\\) is the vector of vote fractions received by each class.\n\n\n\n\n\n\n\n\n\n\nExample 3.14 Suppose we have trained a kNN classifier with \\(k=10\\) for data with three classes, called A, B, and C, and that the votes at the testing points are as follows:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n9\n0\n1\n\n\n1\n5\n3\n2\n\n\n2\n6\n1\n3\n\n\n3\n2\n0\n8\n\n\n4\n4\n5\n1\n\n\n\n\n\n\n\nThe values of \\(\\hat{p}\\) over the test set form a \\(5\\times 3\\) matrix: \n\np_hat = np.array( [\n    [0.9, 0, 0.1],\n    [0.5, 0.3, 0.2],\n    [0.6, 0.1, 0.3],\n    [0.2, 0, 0.8],\n    [0.4, 0.5, 0.1]\n    ] )\n\n\nIt’s natural to interpret \\(\\hat{p}\\) as predicting the probability of each label at any query point, since the values are nonnegative and sum to 100%. Given \\(\\hat{p}\\), we can still output a predicted class; it’s just that we also get additional information about how the prediction was made.\n\nExample 3.15 Consider the penguin species classification problem:\n\npenguins = sns.load_dataset(\"penguins\").dropna()\n# Select only numeric columns for features:\nX = penguins.loc[:, penguins.dtypes==\"float64\"]  \ny = penguins[\"species\"].astype(\"category\")\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2, \n    shuffle=True, random_state=5\n    )\n\nWe can train a kNN classifier and then retrieve the probabilities via predict_proba:\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\np_hat = knn.predict_proba(X_test)\np_hat[:6,:]\n\narray([[0.8, 0.2, 0. ],\n       [0.8, 0.2, 0. ],\n       [0. , 0. , 1. ],\n       [0. , 0. , 1. ],\n       [0.8, 0.2, 0. ],\n       [0.6, 0.4, 0. ]])\n\n\nFrom the output above we see that, for example, while the third and fourth test cases led to unanimous votes for Gentoo, the sixth case is deemed Adelie in a 3–2 squeaker (or is it a squawker?):\n\nyhat = knn.predict(X_test)\nyhat[:6]\n\narray(['Adelie', 'Adelie', 'Gentoo', 'Gentoo', 'Adelie', 'Adelie'],\n      dtype=object)\n\n\n\n\n3.5.1 ROC curve\nIn the binary label case, our assumption so far has been that a simple majority vote determines a positive outcome. But we could choose a different threshold—a supermajority, for example, if we want to reduce false positives. This way of thinking leads us to a new way to fine-tune classification methods.\n\nDefinition 3.11 Let \\(\\theta\\) be a number in the interval \\([0,1]\\). We say that a class \\(T\\) hits at level \\(\\theta\\) at a query point if the fraction of votes that \\(T\\) receives at that point is at least \\(\\theta\\).\n\n\nExample 3.16 Continuing with the data in Example 3.14, we find that at \\(\\theta=0\\), everything always hits:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n1\n1\n\n\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n\n\n3\n1\n1\n1\n\n\n4\n1\n1\n1\n\n\n\n\n\n\n\nAt \\(\\theta=0.05\\), say, we lose all the cases where no votes were received:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n0\n1\n\n\n1\n1\n1\n1\n\n\n2\n1\n1\n1\n\n\n3\n1\n0\n1\n\n\n4\n1\n1\n1\n\n\n\n\n\n\n\nAt \\(\\theta=0.15\\), we have also lost all those receiving 1 out of 10 votes:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n0\n0\n\n\n1\n1\n1\n1\n\n\n2\n1\n0\n1\n\n\n3\n1\n0\n1\n\n\n4\n1\n1\n0\n\n\n\n\n\n\n\nBy the time we get to \\(\\theta=0.7\\), there are only two hits left:\n\n\n\n\n\n\n\n\n\nA\nB\nC\n\n\n\n\n0\n1\n0\n0\n\n\n1\n0\n0\n0\n\n\n2\n0\n0\n0\n\n\n3\n0\n0\n1\n\n\n4\n0\n0\n0\n\n\n\n\n\n\n\n\nThe probability vector \\(\\hat{p}(\\bfx)\\) holds the largest possible \\(\\theta\\) values for which each class hits at \\(\\bfx\\). Looking at it another way, \\(\\theta=0\\) represents maximum credulity—everybody’s a winner!—while \\(\\theta=1\\) represents maximum skepticism—unanimous winners only, please.\nThe ROC curve is a way to visualize the hits as a function of \\(\\theta\\) over a fixed testing set. The idea is to tally, at each value of \\(\\theta\\), all the hits within each class that represent true positives and false positives, and present the results visually.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe name of the ROC curve is a throwback to the early days of radar, when the idea was first developed.\n\n\n\nExample 3.17 We continue with the data from Example 3.14, but now we add ground truth to the queries:\n\n\n\n\n\n\n\n\n\nA\nB\nC\ntruth\n\n\n\n\n0\n9\n0\n1\nA\n\n\n1\n5\n3\n2\nB\n\n\n2\n6\n1\n3\nA\n\n\n3\n2\n0\n8\nC\n\n\n4\n4\n5\n1\nA\n\n\n\n\n\n\n\nLet’s look at class A. At \\(\\theta=0.05\\), class A hits in every case, giving TP=3 and FP=2. At \\(\\theta=0.25\\), the fourth query drops out; we still have TP=3, but now FP=1. Here is the table of all the unique values of TP and FP that we can achieve as \\(\\theta\\) varies between 0 and 1:\n\n\n\n\n\n\n\n\n\ntheta\nFP\nTP\n\n\n\n\n0\n0.05\n2\n3\n\n\n1\n0.25\n1\n3\n\n\n2\n0.45\n1\n2\n\n\n3\n0.55\n0\n2\n\n\n4\n0.65\n0\n1\n\n\n5\n0.95\n0\n0\n\n\n\n\n\n\n\nIn order to make a graph, we convert the raw TP and FP numbers to rates. Since there are 2 positive and 3 negative over the entire test set, we can represent the rows above as the points \\[\n\\left(\\tfrac{2}{2},\\tfrac{3}{3}\\right), \\, \\left(\\tfrac{1}{2},\\tfrac{3}{3}\\right), \\, \\left(\\tfrac{1}{2},\\tfrac{2}{3}\\right), \\, \\left(\\tfrac{0}{2},\\tfrac{2}{3}\\right), \\, \\left(\\tfrac{0}{2},\\tfrac{1}{3}\\right)\\, \\left(\\tfrac{0}{2},\\tfrac{0}{3}\\right).\n\\] The ROC curve for class A is just connect-the-dots for these points:\n\ndata = pd.DataFrame({\"FP rate\": [1,1/2,1/2,0,0,0], \"TP rate\": [1,1,2/3,2/3,1/3,0]})\nsns.relplot(data=data, \n    x=\"FP rate\", y=\"TP rate\", \n    kind=\"line\", estimator=None\n    );\n\n\n\n\nAs we’re about to see, the step-by-step process above is just for illustration and is completely automated in practice.\n\n\nUnsurprisingly, sklearn can compute the points defining the ROC curve automatically, which greatly simplifies drawing them. In a multiclass problem with \\(K\\) classes, there are \\(K\\) implied binary classificiation versions of one-vs-rest, so there are \\(K\\) curves to draw, one for the identification of each class.\n\nExample 3.18 Continuing Example 3.15, we will plot ROC curves for the three species in the penguin data:\n\nfrom sklearn.metrics import roc_curve\n\np_hat = knn.predict_proba(X_test)\nresults = []\nfor i, label in enumerate(knn.classes_):\n    actual = (y_test==label)\n    fp, tp, theta = roc_curve(actual,p_hat[:,i])\n    results.extend( [(label,fp,tp) for fp,tp in zip(fp,tp)] )\nroc = pd.DataFrame( results, columns=[\"label\",\"FP rate\",\"TP rate\"] )\nroc\n\n\n\n\n\n\n\n\nlabel\nFP rate\nTP rate\n\n\n\n\n0\nAdelie\n0.000000\n0.000000\n\n\n1\nAdelie\n0.027027\n0.233333\n\n\n2\nAdelie\n0.054054\n0.566667\n\n\n3\nAdelie\n0.216216\n0.900000\n\n\n4\nAdelie\n0.405405\n0.966667\n\n\n5\nAdelie\n0.459459\n1.000000\n\n\n6\nAdelie\n1.000000\n1.000000\n\n\n7\nChinstrap\n0.000000\n0.000000\n\n\n8\nChinstrap\n0.019231\n0.333333\n\n\n9\nChinstrap\n0.153846\n0.733333\n\n\n10\nChinstrap\n0.423077\n0.933333\n\n\n11\nChinstrap\n1.000000\n1.000000\n\n\n12\nGentoo\n0.000000\n0.000000\n\n\n13\nGentoo\n0.000000\n0.909091\n\n\n14\nGentoo\n0.022222\n0.909091\n\n\n15\nGentoo\n0.022222\n1.000000\n\n\n16\nGentoo\n0.088889\n1.000000\n\n\n17\nGentoo\n0.200000\n1.000000\n\n\n18\nGentoo\n1.000000\n1.000000\n\n\n\n\n\n\n\nThe table above holds all of the key points on the ROC curves:\n\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None\n    );\n\n\n\n\nEach curve starts in the lower left corner and ends at the upper right corner. The ideal situation is in the top left corner of the plot, corresponding to perfect recall and specificity. All of the curves explicitly show the tradeoff between recall and specificity as the decision threshold is varied. The Gentoo curve comes closest to the ideal.\nIf we weight neighbors’ votes inversely to their distances from the query point, then the thresholds aren’t restricted to multiples of \\(\\tfrac{1}{5}\\):\n\nknnw = KNeighborsClassifier(n_neighbors=5, weights=\"distance\")\nknnw.fit(X_train, y_train)\np_hat = knnw.predict_proba(X_test)\n\nresults = []\nfor i, label in enumerate(knn.classes_):\n    actual = (y_test==label)\n    fp, tp, theta = roc_curve(actual,p_hat[:,i])\n    results.extend( [(label,fp,tp) for fp,tp in zip(fp,tp)] )\nroc = pd.DataFrame( results, columns=[\"label\",\"FP rate\",\"TP rate\"] )\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None\n    );\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.5.2 AUC\nROC curves lead to another classification performance metric known as area under ROC curve (AUC). Its name tells you exactly what it is, and it ranges between 0 (bad) and 1 (ideal). Unlike the other classification metrics we have encountered, AUC tries to account not just for the result of the classification at a single threshold, but over the full range from credulous to skeptical. You might think of it as grading with partial credit.\n\nExample 3.19 The AUC metric allows us to compare the standard and weighted kNN classifiers from Example 3.18. Note that the function for computing them, roc_auc_score, requires a keyword argument when there are more than two classes, to specify “one vs. rest” (our usual) or “one vs. one” matchups.\n\nfrom sklearn.metrics import roc_auc_score\ns = roc_auc_score(\n    y_test, knn.predict_proba(X_test), \n    multi_class=\"ovr\", average=None\n    )\n\nsw = roc_auc_score(\n  y_test, knnw.predict_proba(X_test), \n  multi_class=\"ovr\", average=None\n  )\n\npd.DataFrame(\n    {\"standard\": s, \"weighted\": sw},\n    index=knn.classes_\n    )\n\n\n\n\n\n\n\n\nstandard\nweighted\n\n\n\n\nAdelie\n0.903153\n0.935586\n\n\nChinstrap\n0.857051\n0.883333\n\n\nGentoo\n0.997980\n0.998990\n\n\n\n\n\n\n\nBased on the above scores, the weighted classifier seems to be better at identifying all three species."
  },
  {
    "objectID": "classification.html#exercises",
    "href": "classification.html#exercises",
    "title": "3  Classification",
    "section": "Exercises",
    "text": "Exercises\nFor these exercises, you may use computer help to work on a problem, but your answer should be self-contained without reference to computer output (unless stated otherwise).\n\nExercise 3.1 Here is a confusion matrix for a classifier of meme dankness.\n\nConsidering dank to be the positive outcome, calculate the (a) recall, (b) precision, (c) specificity, (d) accuracy, and (e) \\(F_1\\) score of the classifier.\n\n\nExercise 3.2 Here is a confusion matrix for a classifier of ice cream flavors.\n\n(a) Calculate the recall rate for chocolate.\n(b) Find the precision for vanilla.\n(c) Find the accuracy for strawberry.\n\n\nExercise 3.3 Find the Gini impurity of this set: \\[ \\{ A, B, B, C, C, C \\}.\\]\n\n\nExercise 3.4 Use the definition of Gini impurity to prove that it is never negative and always less than 1.\n\n\nExercise 3.5 Given \\(x_i=i\\) for \\(i=0,\\ldots,5\\), with labels \\[\ny_1=y_5=y_6=A, \\quad y_2=y_3=y_4=B,\n\\] find an optimal partition threshold using Gini impurity.\n\n\nExercise 3.6 Using 1-norm, 2-norm, and \\(\\infty\\)-norm, find the distance between the given vectors.\n(a) \\(\\bfu=[2,3,0]\\), \\(\\;\\bfv=[-2,2,1]\\)\n(b) \\(\\bfu=[0,1,0,1,0]\\), \\(\\;\\bfv=[1,1,1,1,1]\\)\n\n\nExercise 3.7 (a) Prove that for any \\(\\bfu \\in \\real^d\\), \\(\\infnorm{\\bfu} \\le \\twonorm{\\bfu}\\).\n(b) Prove that for any \\(\\bfu \\in \\real^d\\), \\(\\twonorm{\\bfu} \\le \\sqrt{d}\\, \\infnorm{\\bfu}\\).\n\n\nExercise 3.8 Carefully sketch the set of all points in \\(\\real^2\\) whose 1-norm distance from the origin equals 1. This is a Manhattan unit circle.\n\n\nExercise 3.9 Three points in the plane lie at the vertices of an equilateral triangle. One is labeled A and the other two are B. Carefully sketch the decision boundary for \\(k\\)-nearest neighbors with \\(k=1\\), using (a) the 2-norm and (b) the infinity-norm.\n\n\nExercise 3.10 Define 8 points on an ellipse by \\(x_k=a\\cos(\\theta_k)\\) and \\(y_k=b\\sin(\\theta_k)\\), where \\(a\\) and \\(b\\) are positive and \\[\n\\theta_1= \\frac{\\pi}{4}, \\theta_2 = \\frac{\\pi}{2}, \\theta_3 = \\frac{3\\pi}{4}, \\ldots, \\theta_8 = 2\\pi.\n\\] Let \\(u_1,\\ldots,u_8\\) and \\(v_1,\\ldots,v_8\\) be the z-scores of the \\(x_k\\) and the \\(y_k\\), respectively. Show that the points \\((u_k,v_k)\\) all lie on a circle centered at the origin for all \\(k=1,\\ldots,8\\). (By extension, standardizing points into z-scores is sometimes called sphereing them.)\n\n\nExercise 3.11 Here are blue/orange labels on an integer lattice.\n\nLet \\(\\hat{f}(x_1,x_2)\\) be the kNN probabilistic classifier with \\(k=4\\), Euclidean metric, and mean averaging that returns the probability of a blue label. In each case below, a function \\(g(t)\\) is defined from values of \\(\\hat{f}\\) along a vertical or horizontal line. Carefully sketch a plot of \\(g(t)\\) for \\(2\\le t \\le 2\\).\n(a) \\(g(t) = \\hat{f}(1.2,t)\\)\n(b) \\(g(t) = \\hat{f}(t,-0.75)\\)\n(c) \\(g(t) = \\hat{f}(t,1.6)\\)\n(d) \\(g(t) = \\hat{f}(-0.25,t)\\)"
  },
  {
    "objectID": "selection.html#sec-select-learning-curves",
    "href": "selection.html#sec-select-learning-curves",
    "title": "4  Model selection",
    "section": "4.1 Bias–variance tradeoff",
    "text": "4.1 Bias–variance tradeoff\nWhen we train a classifier, we use a particular set of training data. In a different parallel universe, we might have been handed a different training set drawn from the same overall population. While we might be optimistic and hope for receiving the best-case training set, it’s more prudent to consider what happens in the average case.\n\n4.1.1 Learner bias\nSuppose that \\(f(x)\\) is a perfect labeller, i.e., a function with 100% accuracy over an entire population. For simplicity, we can imagine that \\(f\\) is a binary classifier, i.e., \\(f(x) \\in \\{0,1\\}\\), although this assumption is not essential.\nLet \\(\\hat{f}(x)\\) denote a probabilistic classification function obtained after training. It depends on the particular training set we used. Suppose there are \\(N\\) total possible training sets, leading to labelling functions \\[\n\\hat{f}_1(x),\\hat{f}_2(x),\\dots,\\hat{f}_N(x).\n\\] Then we define the expected value of the classifier as the average over all training sets: \\[\n\\E{\\hat{f}(x)} = \\frac{1}{N} \\sum_{i=1}^N \\hat{f_i}(x).\n\\]\n\n\n\n\n\n\nNote\n\n\n\nExcept on toy problems, we don’t know how to calculate this average. This is more of a thought experiment. But we will simulate the idea later on.\n\n\nThe term expected doesn’t mean that we anticipate getting this answer for our particular \\(\\hat{f}\\). It’s just what we would get if we could average over all parallel universes receiving unique training sets.\nWe can apply the expectation operator \\(\\mathbb{E}\\) to any function of \\(x\\). In particular, the expected error in our own universe’s prediction is \\[\n\\begin{split}\n    \\E{f(x) - \\hat{f}(x)} &= \\frac{1}{N} \\sum_{i=1}^N \\left( f(x) - \\hat{f_i}(x) \\right) \\\\\n  &= \\frac{1}{N} \\left( \\sum_{i=1}^N  f(x)  \\right) - \\frac{1}{N}\\left( \\sum_{i=1}^N \\hat{f_i}(x) \\right) \\\\\n  &= f(x) - \\E{\\hat{f}(x)}.\n\\end{split}\n\\] We will set \\(y=f(x)\\) as the true label and \\(\\hat{y}=\\E{\\hat{f}(x)}\\) as the expected prediction. The quantity above, \\(y-\\hat{y}\\), is called the bias of the classifier. Bias depends on the particular algorithm and its hyperparameters, but not on the training set. Among other things, bias accounts for the fact that any particular finite algorithm can represent only some labelling functions perfectly.\n\n\n4.1.2 Variance\nIt might seem as though the only important goal is to minimize the bias. To see why this is not the case, imagine that you are playing a hole of golf where the green lies on an island at the end of the fairway. You’re capable of landing the ball on the green in one swing, but it’s near the upper end of your range, and the penalty for landing in the water instead is severe. You might be better off playing it safe by just approaching the water’s edge, which is a shot you can make much more reliably. On average over many attempts, you may well get a better score from the aggressive strategy, but the safe strategy gives you a more reliable result and better odds of doing pretty well, though not optimally.\nIn essence, a good chance of a mediocre result can outweigh a small chance of a better result. To express this tradeoff mathematically, we can compute the variance of the predicted label at any \\(x\\): \\[\n\\begin{split}\n    \\E{\\bigl(y - \\hat{f}(x)\\bigr)^2} &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{f_i}(x) \\right)^2 \\\\   \n    &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{y} + \\hat{y} - \\hat{f_i}(x) \\right)^2  \\\\\n    &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{y} \\right)^2 + \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right)^2 \\\\\n  & \\qquad + 2 \\left( y - \\hat{y} \\right) \\cdot \\frac{1}{N}\\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right).  \n\\end{split}\n\\]\n\n\n\n\n\n\n\n\nNow we find something interesting: \\[\n\\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right) =\n\\hat{y} - \\frac{1}{N} \\sum_{i=1}^N \\hat{f}_i(x) = 0,\n\\] by the definition of \\(\\hat{y}\\). So overall, \\[\n\\begin{split}\n    \\E{\\bigl(y - \\hat{f}(x)\\bigr)^2} &= \\frac{1}{N} \\sum_{i=1}^N \\left( y - \\hat{y} \\right)^2 + \\frac{1}{N} \\sum_{i=1}^N \\left( \\hat{y}  - \\hat{f}_i(x) \\right)^2 \\\\\n  &= (y-\\hat{y})^2 + \\E{\\left(\\hat{y} - \\hat{f}(x)\\right)^2}\n\\end{split}\n\\tag{4.1}\\] The first term is the squared bias. The second is the variance of the learning method. In words, the variance of the learning process has two contributions:\n\nBias\n\nHow close is the average prediction to the ground truth?\n\nVariance\n\nHow close to the average prediction is any one prediction likely to be?\n\n\n\n\nCode\nrng = default_rng(302)\nx, y, bias, var = [], [], [], []\nx.extend(rng.normal(0.04,0.08,40))\ny.extend(rng.normal(-0.03,0.06,40))\nbias.extend([\"low\"]*40)\nvar.extend([\"low\"]*40)\nx.extend(rng.normal(0.55,0.11,40))\ny.extend(rng.normal(-0.35,0.05,40))\nbias.extend([\"high\"]*40)\nvar.extend([\"low\"]*40)\nx.extend(rng.normal(-0.02,0.34,40))\ny.extend(rng.normal(0.03,0.33,40))\nbias.extend([\"low\"]*40)\nvar.extend([\"high\"]*40)\nx.extend(rng.normal(-0.25,0.33,40))\ny.extend(rng.normal(-0.35,0.33,40))\nbias.extend([\"high\"]*40)\nvar.extend([\"high\"]*40)\npoints = pd.DataFrame({\"bias\": bias, \"variance\": var, \"x₁\": x, \"x₂\": y})\nfig = sns.relplot(data=points, x=\"x₁\", y=\"x₂\", row=\"variance\", col=\"bias\", aspect=1, height=3);\nfig.set(xlim=(-1.25,1.25), ylim=(-1.25,1.25));\n\n\n\n\n\nFigure 4.1: Bias versus variance (imagine you are aiming at the center of the box)\n\n\n\n\nWhy would these two factors be in opposition? When a learning method has the capacity to capture complex behavior, it potentially has a low bias. However, that same capacity means that the learner will fit itself very well to each individual training set, which increases the potential for variance over the whole collection of training sets.\nThis tension is known as the bias–variance tradeoff. Perhaps we can view this tradeoff as a special case of Occam’s Razor: it’s best to choose the least complex method necessary to reach a particular level of explanatory power.\n\n\n4.1.3 Learning curves\nWe can illustrate the tradeoff between bias and variance by running an artificial experiment with different sizes for the training datasets.\n\nExample 4.2 We will use a subset of a realistic data set used to predict the dominant type of tree in patches of forest. We train a decision tree classifier with fixed depth throughout. (Don’t confuse the forest data for the tree classifier, haha.)\n\nforest = datasets.fetch_covtype()\nX = forest[\"data\"][:250000,:8]   # 250,000 samples, 8 dimensions\ny = forest[\"target\"][:250000]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.05, \n    shuffle=True, random_state=0\n)\n\nalln = range(300, 6001, 300)       # sizes of the training subsets\nresults = []                       # for tracking results\ntree = DecisionTreeClassifier(max_depth=3) \nfor n in alln:             # iterate over training set sizes\n    for i in range(100):        # iterate over training sets\n        X_train, y_train = shuffle(X_train, y_train, random_state=10*i)\n        XX, yy = X_train[:n,:], y_train[:n]       # training subset of size n\n        tree.fit(XX, yy)\n        results.append( (\"train\", n, 1-tree.score(XX, yy)) )\n        results.append( (\"test\", n, 1-tree.score(X_test, y_test)) )\n\ncols = [ \"kind\", \"training set size\", \"error\" ]\nresults = pd.DataFrame(results, columns=cols)\nsns.relplot(data=results, \n    x=cols[1], y=cols[2], \n    kind=\"line\", errorbar=\"sd\", hue=cols[0]\n);\n\n\n\n\nThe plot above shows learning curves. The solid line is the mean result over all trials, and the ribbon has a width of one standard deviation. For each small training set, the tree has more than enough resolving power and adapts itself too well to the training data, leading to a large gap with the testing error (i.e., a failure to generalize past the training set). You can also see large variance (ribbon widths) in the results at the smaller sizes. As the training set size increases, we observe the training error increasing, because the task of reproducing the training set is now harder. The testing errors decrease, though, because the model has been fit to a more representative subset of the data. The gap between training and testing closes as more training data is used and the model is pushed to its limits.\nNote that the curves seem to approach a horizontal asymptote at a nonzero level of error. This level indicates an unavoidable bias for this tree size, no matter how much of the data we throw at it. As a simple analogy, think about approximating curves in the plane by a parabola. You will be able to do a perfect job for linear and quadratic functions, but if you approximate one period of a cosine curve, you can’t do a great job no matter how much information you have.\n\n\n\n\n\n\n\n\n\nWhen you see a large gap between training and test errors, you should suspect that the learner will not generalize well. Ideally, you could bring more data to the table, perhaps by artificially augmenting the training examples. If not, you might as well decrease the resolving power of your learner, because the excess power is likely to make things no better, and maybe worse."
  },
  {
    "objectID": "selection.html#overfitting",
    "href": "selection.html#overfitting",
    "title": "4  Model selection",
    "section": "4.2 Overfitting",
    "text": "4.2 Overfitting\nOne important factor we have not yet considered is noise in the training data—that is, erroneous values. If a learner responds too adeptly to isolated wrong values, it will also respond incorrectly to other nearby inputs. This situation is known as overfitting.\n\n4.2.1 Overfitting in kNN\nTo illustrate overfitting, let’s use a really simple classification problem: a single feature, with the class being the sign of the feature’s value. (We arbitrarily assign zero to have class \\(+1\\).)\n\n\n\n\n\n\n\n\nConsider first a kNN classifier with \\(k=1\\). The class assigned to each value is just that of the nearest training example, making for a piecewise constant labelling. Here are the results for four different training sets, each of size 40:\n\n\n\n\n\nFigure 4.2: kNN with k=1 and perfect data\n\n\n\n\nAs you can see above, all four results are quite good. The only errors are for queries near zero.\nNow suppose we use training sets that have just 3 mislabeled examples each. Here are some resulting classifiers:\n\n\n\n\n\nFigure 4.3: kNN with k=1 and noisy data\n\n\n\n\nEvery sample is its own nearest neighbor, so this classifier responds to noisy data by reproducing it perfectly, which interferes with the larger trend we actually want to capture. We can generally expect such overfitting with \\(k=1\\), for which the deicision boundary can be complex.\nNow let’s bump up to \\(k=3\\). The results are more like we want, even with noisy data:\n\n\n\n\n\nFigure 4.4: kNN with k=3 and noisy data\n\n\n\n\nThe voting mechanism of kNN allows the classifier to ignore isolated bad examples. If we continue to \\(k=7\\), then the 3 outliers will never be able to outvote the correct values:\n\n\n\n\n\nFigure 4.5: kNN with k=7 and noisy data\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe lesson here is not simply that “bigger \\(k\\) is better.” In the case of \\(k=21\\) above, for example, the classifier will predict the same value everywhere, which we could describe as underfitting the data.\n\n\n\n\n4.2.2 Overfitting in decision trees\nAs mentioned in Example 4.1, the depth of a decision tree correlates with its ability to divide the samples more finely. For \\(n=40\\) values, a tree of depth 6 is guaranteed to reproduce every sample value perfectly. Thus, with noisy data, we see clear signs of overfitting:\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.6: Decision tree with depth=6 and noisy data\n\n\n\n\nUsing a shallower tree reduces the extent of overfitting:\n\n\n\n\n\nFigure 4.7: Decision tree with depth=3 and noisy data\n\n\n\n\nWe can eliminate the overfitting completely and get a single point as the decision boundary, although its location still might not be ideal:\n\n\n\n\n\nFigure 4.8: Decision tree with depth=2 and noisy data\n\n\n\n\n\n\n4.2.3 Overfitting and variance\nThe tendency to fit closely to training data also implies that the learner may have a good deal of variance in training (see Figure 4.3, and Figure 4.6, for example). Thus, overfitting is often associated with a large gap between training and testing variance, as observed in Section 4.1.\n\nExample 4.3 Returning to the forest data from Example 4.2, we try decision trees of maximum depth \\(r=12\\) on 100 random training subsets of size 5000:\n\n\nCode\nforest = datasets.fetch_covtype()\nX = forest[\"data\"][:50000,:8]\ny = (forest[\"target\"][:50000] == 1)\n\ndef experiment(learner, X, y, n):\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        test_size=0.2,\n        shuffle=True,\n        random_state=1\n    )\n    results = [] \n    for i in range(100):\n        X_train, y_train = shuffle(X_train, y_train, random_state=i)\n        XX, yy = X_train[:n,:], y_train[:n]\n        learner.fit(XX, yy) \n        err = 1 - balanced_accuracy_score(yy, learner.predict(XX))\n        results.append( (\"train\", err) )   # training error\n        err = 1 - balanced_accuracy_score(y_test, learner.predict(X_test))\n        results.append( (\"test\", err) )    # test error\n\n    results = pd.DataFrame( results, columns=[\"kind\", \"error\"] )\n    sns.displot(data=results, x=\"error\", hue=\"kind\", bins=20);\n\n\n\ntree = DecisionTreeClassifier(max_depth=12)\nexperiment(tree, X, y, 5000)\n\n\n\n\nFigure 4.9: Results from an overfit decision tree\n\n\n\n\nSince \\(2^{12}=4096\\), this tree is probably overfit to the training data, and we also see the wide separation between training and testing that suggests the training does not generalize well. With a depth of \\(r=4\\), the training and testing results completely overlap:\n\ntree = DecisionTreeClassifier(max_depth=4)\nexperiment(tree, X, y, 5000)\n\n\n\n\nHowever, notice above that the testing error increased substantially from the overfit case.\n\n\n\n\n\n\n\n\n\nWe could say that the last tree in Example 4.3 is underfitting the data; the behavior of the labelling function is probably too complex to be replicated well by any tree that shallow. In short, the overfitting/underfitting dilemma is another manifestation of the bias–variance tradeoff."
  },
  {
    "objectID": "selection.html#ensemble-methods",
    "href": "selection.html#ensemble-methods",
    "title": "4  Model selection",
    "section": "4.3 Ensemble methods",
    "text": "4.3 Ensemble methods\nWhen a relatively expressive learning model is used, overfitting and strong dependence on the training set are possible. One meta-strategy for reducing training variance without decreasing the model expressiveness is to use an ensemble method. The idea of an ensemble is that averaging over many different training sets will reduce the variance that comes from overfitting. It’s a way to simulate the computation of expected values.\n\nDefinition 4.2 In bootstrap aggregation, or bagging for short, samples are drawn randomly from the original training set. Usually, this is done with replacement, which means that some samples might be selected multiple times.\n\nWhy should bagging work? It comes down to the way that bias and variance behave. Suppose we produce \\(M\\) prbabilstic binary classifiers, \\(\\hat{f}_1,\\ldots,\\hat{f}_M\\), that are identical except in having received independent training sets. They create the (probabilistic) bagging classifier\n\n\n\n\n\n\n\n\n\\[\n\\hat{F}(x) = \\frac{1}{M} \\sum_{m=1}^M \\hat{f}_m(x).\n\\] The bias of the bagging classifier is the expectation of \\[\ny - \\hat{F}(x) = \\frac{1}{M} (My) -  \\frac{1}{M} \\sum_{m=1}^M \\hat{f}_m(x)  =  \\frac{1}{M} \\sum_{m=1}^M \\left(  y - \\hat{f}_m(x)  \\right).\n\\] This is simply the mean of the constitutent classifier biases. Since the original classifiers are identical, in expected value they all have the same bias, and we conclude that the bagging classifier has the same expected bias as its constituents.\nThe story for the variance is different. It can be derived that the variance of the bagging predictor is \\(1/M\\) times that of the constituent classifiers. (This effect can be used to explain the wisdom of crowds. If each person in a classroom is asked to guess the number of jellybeans in a large jar, the mean of the class’ guesses will tend to be much closer to the true value than most individuals’ guesses are.)\nWe should expect bagging to work best with highly expressive classifiers that have low bias and large variance. These tend to occur in large-depth decision trees and small-\\(k\\) kNN classifiers.\nScikit-learn has a BaggingClassifier that automates the process of generating an ensemble from just one basic type of estimator.\n\nExample 4.4 Here is a dataset collected from images of dried beans:\n\nbeans = pd.read_excel(\"_datasets/Dry_Bean_Dataset.xlsx\")\nX = beans.drop(\"Class\", axis=1)\nX.head()\n\n\n\n\n\n\n\n\nArea\nPerimeter\nMajorAxisLength\nMinorAxisLength\nAspectRation\nEccentricity\nConvexArea\nEquivDiameter\nExtent\nSolidity\nroundness\nCompactness\nShapeFactor1\nShapeFactor2\nShapeFactor3\nShapeFactor4\n\n\n\n\n0\n28395\n610.291\n208.178117\n173.888747\n1.197191\n0.549812\n28715\n190.141097\n0.763923\n0.988856\n0.958027\n0.913358\n0.007332\n0.003147\n0.834222\n0.998724\n\n\n1\n28734\n638.018\n200.524796\n182.734419\n1.097356\n0.411785\n29172\n191.272750\n0.783968\n0.984986\n0.887034\n0.953861\n0.006979\n0.003564\n0.909851\n0.998430\n\n\n2\n29380\n624.110\n212.826130\n175.931143\n1.209713\n0.562727\n29690\n193.410904\n0.778113\n0.989559\n0.947849\n0.908774\n0.007244\n0.003048\n0.825871\n0.999066\n\n\n3\n30008\n645.884\n210.557999\n182.516516\n1.153638\n0.498616\n30724\n195.467062\n0.782681\n0.976696\n0.903936\n0.928329\n0.007017\n0.003215\n0.861794\n0.994199\n\n\n4\n30140\n620.134\n201.847882\n190.279279\n1.060798\n0.333680\n30417\n195.896503\n0.773098\n0.990893\n0.984877\n0.970516\n0.006697\n0.003665\n0.941900\n0.999166\n\n\n\n\n\n\n\nAlthough the dataset has data on 7 classes of beans, we will simplify our output by making it a one-vs-rest problem for just one class:\n\ny = beans[\"Class\"] == \"SIRA\"\n\nHere is the confusion matrix we get from training a single kNN classifier on this dataset:\n\nX_train, X_test, y_train, y_test = train_test_split(\n  X, y,\n  test_size=0.2,\n  shuffle=True, random_state=302\n)\n\npipe = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=3))\npipe.fit(X_train, y_train)\n\np_hat = pipe.predict_proba(X_test)\nauc = roc_auc_score(y_test==True, p_hat[:,1])   # columns are for [False, True]\nprint(f\"AUC for single classifier is {auc:.4f}\")\n\nAUC for single classifier is 0.9633\n\n\nHere, we create an ensemble with 100 such classifiers, each trained on a different subset that is 75% of the size of the original training set:\n\nfrom sklearn.ensemble import BaggingClassifier\n\nensemble = BaggingClassifier( \n    pipe, \n    max_samples=0.75,\n    n_estimators=100,\n    random_state=18621\n    )\n\nensemble.fit(X_train, y_train)\n\nBaggingClassifier(estimator=Pipeline(steps=[('standardscaler',\n                                             StandardScaler()),\n                                            ('kneighborsclassifier',\n                                             KNeighborsClassifier(n_neighbors=3))]),\n                  max_samples=0.75, n_estimators=100, random_state=18621)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.BaggingClassifierBaggingClassifier(estimator=Pipeline(steps=[('standardscaler',\n                                             StandardScaler()),\n                                            ('kneighborsclassifier',\n                                             KNeighborsClassifier(n_neighbors=3))]),\n                  max_samples=0.75, n_estimators=100, random_state=18621)estimator: PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=3))])StandardScalerStandardScaler()KNeighborsClassifierKNeighborsClassifier(n_neighbors=3)\n\n\nWe can use the trained ensemble object much like any learner. For example, here is the prediction obtained for the last row of the training set:\n\nquery = X_test.iloc[-1:,:]\np_hat = ensemble.predict_proba(query)\nprint(f\"Predicted ensemble probability of True on query is {p_hat[0][1]:.2%}\")\n\nPredicted ensemble probability of True on query is 52.67%\n\n\nInternally, the estimators_ field of the ensemble object is a list of the individual trained classifiers. With a little work, we could find out the prediction for True from every constituent:\n\npm = [ model.predict_proba(query.to_numpy())[0,1] for model in ensemble.estimators_ ]\npm[:6]   # first 6 predictions\n\n[0.6666666666666666,\n 1.0,\n 0.3333333333333333,\n 0.6666666666666666,\n 0.3333333333333333,\n 0.6666666666666666]\n\n\nThe ensemble takes the average of this list to create its prediction:\n\nprint(f\"Mean probability of True for query is {np.mean(pm):.2%}\")\n\nMean probability of True for query is 52.67%\n\n\nThe result above matches what we got by predicting directly from the ensemble, which is the normal mode of operation.\nOver the testing set, we find that the ensemble has improved the AUC score:\n\np_hat = ensemble.predict_proba(X_test)\nauc = roc_auc_score(y_test==True, p_hat[:,1])   # columns are for [False, True]\nprint(f\"AUC for ensemble is {auc:.4f}\")\n\nAUC for ensemble is 0.9839\n\n\n\n\n\n\n\n\n\n\n\nThere is a significant catch in that the theory requires the constituent learners to be uncorrelated, which is less true as the size of the bagging sample grows relative to the original training set. This can somewhat counterintuitively lead to better results by training on smaller individual training sets.\n\n\n\n\n\n\nNote\n\n\n\nAn ensemble of decision trees is known as a random forest. We can use a RandomForestClassifier to accomplish the same thing as a bagged decision tree ensemble.\n\n\n\nExample 4.5 If we repeat the above but reduce the bagging training sets to just 20% of the full training set, we get a slightly better result:\n\nensemble = BaggingClassifier( \n    pipe, \n    max_samples=0.2,\n    n_estimators=100,\n    random_state=18621\n    )\n\nensemble.fit(X_train, y_train)\np_hat = ensemble.predict_proba(X_test)\nauc = roc_auc_score(y_test==True, p_hat[:,1])   # columns are for [False, True]\nprint(f\"AUC for the new ensemble is {auc:.4f}\")\n\nAUC for the new ensemble is 0.9873\n\n\nWe may get better results by increasing the size of the ensemble, too, though in this case there isn’t much room left for improvement.\n\n\nExample 4.6 Let’s work again with the forest cover dataset. It’s got over 500,000 samples and 54 features:\n\nforest = datasets.fetch_covtype()\nforest[\"data\"].shape\n\n(581012, 54)\n\n\nWe’ll turn this into a binary classification by looking for just one of the possible label values:\n\nX = forest[\"data\"]\ny = forest[\"target\"] == 1\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2, \n    shuffle=True, random_state=302\n)\n\nHow should we best make use of all that data? We can use a fairly deep decision tree without fear of overfitting, and the result is not bad:\n\ntree = DecisionTreeClassifier(max_depth=12) \ntree.fit(X_train, y_train)\n\nfrom sklearn.metrics import f1_score\nF1 = f1_score(y_test, tree.predict(X_test) )\nprint(f\"F₁ for a single tree is {F1:.4f}\")\n\nF₁ for a single tree is 0.8107\n\n\nA simple averaging over the same type of tree actually does worse here:\n\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(\n    max_depth=12, \n    max_samples=0.2,\n    n_estimators=100, n_jobs=-1\n    )\n\nrf.fit(X_train,y_train)\nF1 = f1_score(y_test, rf.predict(X_test) )\nprint(f\"F₁ for a random forest is {F1:.4f}\")\n\nF₁ for a random forest is 0.7787\n\n\nIt’s possible that these trees are too correlated. We can combat that by exaggerating their degree of overfitting:\n\nrf = RandomForestClassifier(\n    max_depth=24, \n    max_samples=0.2,\n    n_estimators=100, n_jobs=-1\n    )\n\nrf.fit(X_train,y_train)\nF1 = f1_score(y_test, rf.predict(X_test) )\nprint(f\"F₁ for a taller forest is {F1:.4f}\")\n\nF₁ for a taller forest is 0.9019\n\n\nIn fact, we can decorrelate even better by only using random subsets of the features on each tree. Here, for example, we construct the ensemble so that each tree randomly selects 50% of the original 54 dimensions to work with:\n\nrf = RandomForestClassifier(\n    max_depth=24, \n    max_features=0.5,\n    max_samples=0.2,\n    n_estimators=100, n_jobs=-1\n    )\n\nrf.fit(X_train,y_train)\nF1 = f1_score(y_test, rf.predict(X_test) )\nprint(f\"F₁ for a taller, skinnier forest is {F1:.4f}\")\n\nF₁ for a taller, skinnier forest is 0.9413\n\n\n\n\n\n\n\n\n\n\n\nEnsembles can be constructed for any individual model type. Their chief disadvantage is the need to repeat the fitting process multiple times, although this can be mitigated by computing the fits in parallel. For random forests in particular, we also lose the ability to interpret the decision process the way we can for an individual tree."
  },
  {
    "objectID": "selection.html#validation",
    "href": "selection.html#validation",
    "title": "4  Model selection",
    "section": "4.4 Validation",
    "text": "4.4 Validation\nWe now return to the opening questions of this chapter: how should we determine optimal hyperparameters and algorithms?\nIt’s tempting to compute some test scores over a range of hyperparameter choices and simply choose the case that scores best. However, if we base hyperparameter optimization on a fixed testing set, then we are effectively learning from that set! The hyperparameters might become too tuned—i.e., overfit—to our particular choice of the test set.\n\n\n\n\n\n\n\n\nTo avoid this pitfall, we can split the data into three subsets for training, validation, and testing. The validation set is used to tune hyperparameters. Once training is performed at values determined to be best on validation, the test set is used to assess the generalization of the optimized learner.\nUnfortunately, a fixed three-way split of the data further reduces the amount of data available for training, so we often turn to an alternative.\n\n4.4.1 Cross-validation\nIn cross-validation, each learner is trained multiple times using unique training and validation sets drawn from the same pool.\n\n\n\n\n\n\n\n\n\nDefinition 4.3 The steps for \\(k\\)-fold cross-validation are as follows:\n\nDivide the original data into training and testing sets.\nFurther divide the training data set into \\(k\\) roughly equal parts called folds.\nTrain a learner using folds \\(2,3,\\ldots,k\\) and validate on the cases in fold 1. Then train another learner on folds \\(1,3,\\ldots,k\\) and validate against the cases in fold 2. Continue until each fold has served once for validation.\nSelect the hyperparameters producing the best validation score and retrain on the entire training set.\nAssess performance using the test set.\n\n\nA different variation is stratified \\(k\\)-fold, in which the division in step 2 is constrained so that the relative membership of each class is the same in every fold as it is in the full training set. This is advisable when one or more classes is scarce and might otherwise become underrepresented in some folds.\n\nExample 4.7 Here is how 16 elements can be split into 4 folds:\n\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=4, shuffle=True, random_state=0)\nfor train,test in kf.split(range(16)): \n    print(\"train:\", train, \", test:\", test)\n\ntrain: [ 0  2  3  4  5  7 10 11 12 13 14 15] , test: [1 6 8 9]\ntrain: [ 0  1  3  5  6  7  8  9 10 11 12 15] , test: [ 2  4 13 14]\ntrain: [ 0  1  2  3  4  5  6  8  9 12 13 14] , test: [ 7 10 11 15]\ntrain: [ 1  2  4  6  7  8  9 10 11 13 14 15] , test: [ 0  3  5 12]\n\n\n\n\nExample 4.8 Let’s apply cross-validation to the beans dataset.\n\nbeans = pd.read_excel(\"_datasets/Dry_Bean_Dataset.xlsx\")\nX = beans.drop(\"Class\", axis=1)\ny = beans[\"Class\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.15, \n    shuffle=True, random_state=302\n    )\n\nA round of 6-fold cross-validation on a standardized kNN classifier looks like the following:\n\nfrom sklearn.model_selection import cross_validate\n\nknn = KNeighborsClassifier(n_neighbors=5)\nlearner = make_pipeline(StandardScaler(), knn)\n\nkf = KFold(n_splits=6, shuffle=True, random_state=18621)\nscores = cross_validate(\n    learner, \n    X_train, y_train, \n    cv=kf,\n    scoring=\"balanced_accuracy\"\n    )\n\nprint(\"Validation scores:\")\nprint( scores[\"test_score\"] )\n\nValidation scores:\n[0.93344028 0.9297571  0.92846513 0.93272807 0.94025875 0.93982356]\n\n\nThe low variance across the folds that we see above is reassurance that they are representative. Conversely, if the scores were spread more widely, we would be concerned that there was strong dependence on the training set, which might indicate overfitting.\n\n\n\n4.4.2 Hyperparameter tuning\nIf we perform cross-validations as we vary a hyperparameter, we get a validation curve.\n\nExample 4.9 Here is a validation curve for the maximum depth of a decision tree classifier on the beans data:\n\nfrom sklearn.model_selection import StratifiedKFold\n\ndepths = range(4, 16, 1)\nkf = StratifiedKFold(n_splits=8, shuffle=True, random_state=2)\nresults = []    # for keeping results\nfor d in depths:\n    tree = DecisionTreeClassifier(max_depth=d, random_state=1)\n    cv = cross_validate(tree, \n        X_train, y_train, \n        cv=kf, \n        scoring=\"balanced_accuracy\",\n        n_jobs=-1\n        )\n    for err in 1 - cv[\"test_score\"]:\n      results.append( (d, err) )\n\nresults = pd.DataFrame(results, columns=[\"depth\", \"error\"] )\nsns.relplot(data=results, \n    x=\"depth\", y=\"error\", \n    kind=\"line\", errorbar=\"sd\"\n    );\n\n\n\n\nInitially the error decreases because the shallowest decision trees are underfit. The minimum error is at max depth 9, after which overfitting seems to take over:\n\nresults.groupby(\"depth\").mean()\n\n\n\n\n\n\n\n\nerror\n\n\ndepth\n\n\n\n\n\n4\n0.206700\n\n\n5\n0.118655\n\n\n6\n0.097290\n\n\n7\n0.092586\n\n\n8\n0.088454\n\n\n9\n0.083948\n\n\n10\n0.082687\n\n\n11\n0.084315\n\n\n12\n0.085555\n\n\n13\n0.087055\n\n\n14\n0.087295\n\n\n15\n0.086693\n\n\n\n\n\n\n\nWe can now train this optimal classifier on the entire training set and measure performance on the reserved testing data:\n\ntree = DecisionTreeClassifier(max_depth=9, random_state=1)\ntree.fit(X_train, y_train)\nyhat = tree.predict(X_test)\nprint( \"score is\", balanced_accuracy_score(y_test, yhat) )\n\nscore is 0.9192506467389886\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2.1 Grid search\nWhen there is a single hyperparameter in play, the validation curve is useful way to optimize it. When multiple hyperparameters are available, it’s common to perform a grid search, in which we try cross-validated fitting using every specified combination of parameter values.\n\nExample 4.10 Let’s work with a dataset on breast cancer detection:\n\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer(as_frame=True)[\"frame\"]\nX = cancer.drop(\"target\", axis=1)\ny = cancer[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, \n    test_size=0.15, \n    shuffle=True, random_state=3383\n    )\nX_test.head()\n\n\n\n\n\n\n\n\nmean radius\nmean texture\nmean perimeter\nmean area\nmean smoothness\nmean compactness\nmean concavity\nmean concave points\nmean symmetry\nmean fractal dimension\n...\nworst radius\nworst texture\nworst perimeter\nworst area\nworst smoothness\nworst compactness\nworst concavity\nworst concave points\nworst symmetry\nworst fractal dimension\n\n\n\n\n242\n11.300\n18.19\n73.93\n389.4\n0.09592\n0.13250\n0.15480\n0.02854\n0.2054\n0.07669\n...\n12.58\n27.96\n87.16\n472.9\n0.1347\n0.48480\n0.74360\n0.12180\n0.3308\n0.12970\n\n\n375\n16.170\n16.07\n106.30\n788.5\n0.09880\n0.14380\n0.06651\n0.05397\n0.1990\n0.06572\n...\n16.97\n19.14\n113.10\n861.5\n0.1235\n0.25500\n0.21140\n0.12510\n0.3153\n0.08960\n\n\n446\n17.750\n28.03\n117.30\n981.6\n0.09997\n0.13140\n0.16980\n0.08293\n0.1713\n0.05916\n...\n21.53\n38.54\n145.40\n1437.0\n0.1401\n0.37620\n0.63990\n0.19700\n0.2972\n0.09075\n\n\n289\n11.370\n18.89\n72.17\n396.0\n0.08713\n0.05008\n0.02399\n0.02173\n0.2013\n0.05955\n...\n12.36\n26.14\n79.29\n459.3\n0.1118\n0.09708\n0.07529\n0.06203\n0.3267\n0.06994\n\n\n318\n9.042\n18.90\n60.07\n244.5\n0.09968\n0.19720\n0.19750\n0.04908\n0.2330\n0.08743\n...\n10.06\n23.40\n68.62\n297.1\n0.1221\n0.37480\n0.46090\n0.11450\n0.3135\n0.10550\n\n\n\n\n5 rows × 30 columns\n\n\n\nWe start by trying decision tree classifiers in which we vary the maximum depth as well as some other options.\n\nfrom sklearn.model_selection import GridSearchCV\n\ngrid = { \"criterion\":[\"gini\", \"entropy\"], \n         \"max_depth\":range(2, 15), \n         \"min_impurity_decrease\":np.arange(0,0.01,0.002) }\nlearner = DecisionTreeClassifier(random_state=1)\nkf = StratifiedKFold(n_splits=4, shuffle=True, random_state=302)\n\ngrid_dt = GridSearchCV(\n    learner, grid, \n    scoring=\"f1\", \n    cv=kf,\n    n_jobs=-1\n    )\ngrid_dt.fit(X_train, y_train)\n\nprint(\"Best parameters:\")\nprint(grid_dt.best_params_)\nprint()\nprint(\"Best score:\")\nprint(grid_dt.best_score_)\n\nBest parameters:\n{'criterion': 'gini', 'max_depth': 7, 'min_impurity_decrease': 0.0}\n\nBest score:\n0.9455901279430692\n\n\nNext, we do the same search over kNN classifiers. We always use standardization as a preprocessor; note how the syntax of the grid search is adapted:\n\ngrid = { \"kneighborsclassifier__metric\":[\"euclidean\", \"manhattan\"], \n         \"kneighborsclassifier__n_neighbors\":range(1, 20), \n         \"kneighborsclassifier__weights\":[\"uniform\", \"distance\"] }\n\nlearner = make_pipeline(StandardScaler(), KNeighborsClassifier())\n\ngrid_knn = GridSearchCV(\n    learner, grid, \n    scoring=\"f1\", \n    cv=kf,\n    n_jobs=-1\n    )\ngrid_knn.fit(X_train, y_train)\n\nprint(\"Best parameters:\")\nprint(grid_knn.best_params_)\nprint()\nprint(\"Best score:\")\nprint(grid_knn.best_score_)\n\nBest parameters:\n{'kneighborsclassifier__metric': 'manhattan', 'kneighborsclassifier__n_neighbors': 4, 'kneighborsclassifier__weights': 'distance'}\n\nBest score:\n0.9734627184207015\n\n\nEach fitted grid search object is itself a classifier that was trained on the full training set at the optimal hyperparameters:\n\ndt_score = f1_score( y_test, grid_dt.predict(X_test) )\nknn_score = f1_score( y_test, grid_knn.predict(X_test) )\nprint(f\"best tree f1 score: {dt_score:.5f}\")\nprint(f\"best knn f1 score: {knn_score:.5f}\")\n\nbest tree f1 score: 0.94915\nbest knn f1 score: 0.99187\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt may be instructive to rerun the competition above using different random seeds. The meaningfulness of the results is limited by their sensitivity to such choices. Don’t let floating-point values give you a false feeling of precision!\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.4.2.2 Alternatives to grid search\nGrid search is a brute-force approach. It is embarrassingly parallel, meaning that different processors can work on different locations on the grid at the same time. But it is usually too slow for large training sets, or when the search space has more than two or three dimensions. In such cases you can try searching over crude versions of the grid, perhaps with just part of the training data, and gradually narrow the search while using all the data. When desperate, one may try a randomized search and to guide the process with experience and intuition."
  },
  {
    "objectID": "regression.html#sec-regression-linear",
    "href": "regression.html#sec-regression-linear",
    "title": "5  Regression",
    "section": "5.1 Linear regression",
    "text": "5.1 Linear regression\nYou have likely previously encountered the most basic form of regression: fitting a straight line to data points \\((x_i,y_i)\\) in the \\(xy\\)-plane. In linear regression, we have a one-dimensional feature \\(x\\) and assume a relation\n\n\n\n\n\n\n\n\n\\[\ny \\approx \\hat{f}(x) = ax + b.\n\\]\nWe also define a loss function or misfit function that adds up how far predictions are from the data. The standard choice is a sum of squared differences between the predictions and the true values:\n\\[\nL(a,b) = \\sum_{i=1}^n (\\hat{f}(x_i)-y_i)^2 = \\sum_{i=1}^n (a x_i + b - y_i)^2.\n\\]\nThe loss can be minimized using a little multidimensional calculus. Momentarily suppose that \\(b\\) is held fixed and take a derivative with respect to \\(a\\):\n\\[\n\\pp{L}{a} = \\sum_{i=1}^n 2x_i(a x_i + b - y_i) = 2 a \\left(\\sum_{i=1}^n x_i^2\\right) + 2b\\left(\\sum_{i=1}^n x_i\\right) - 2\\sum_{i=1}^n x_i y_i.\n\\]\n\n\n\n\n\n\nNote\n\n\n\nThe symbol \\(\\pp{}{}\\) is called a partial derivative and is defined just as described here: differentiate in one variable while all others are temporarily held constant.\n\n\nSimilarly, if we hold \\(a\\) fixed and differentiate with respect to \\(b\\), then\n\\[\n\\pp{L}{b} = \\sum_{i=1}^n 2(a x_i + b - y_i) = 2 a \\left(\\sum_{i=1}^n x_i\\right) + 2bn - 2 \\sum_{i=1}^n y_i.\n\\]\nSetting both derivatives to zero creates a system of two linear equations to be solved for \\(a\\) and \\(b\\): \\[\n\\begin{split}\n    a \\left(\\sum_{i=1}^n x_i^2\\right) + b\\left(\\sum_{i=1}^n x_i\\right) &= \\sum_{i=1}^n x_i y_i, \\\\\n    a \\left(\\sum_{i=1}^n x_i\\right) + b n &= \\sum_{i=1}^n y_i.\n\\end{split}\n\\tag{5.1}\\]\n\nExample 5.1 Suppose we want to find the linear regressor of the points \\((-1,0)\\), \\((0,2)\\), \\((1,3)\\). We need to calculate a few sums:\n\\[\n\\begin{split}\n\\sum_{i=1}^n x_i^2 = 1+0+1=2, \\qquad & \\sum_{i=1}^n x_i = -1+0+1=0, \\\\\n\\sum_{i=1}^n x_iy_i = 0+0+3=3, \\qquad & \\sum_{i=1}^n y_i = 0+2+3=5.\n\\end{split}\n\\]\nNote that \\(n=3\\). Therefore we must solve\n\\[\n\\begin{split}\n2a + 0b &= 3, \\\\\n0a + 3b &= 5.\n\\end{split}\n\\]\nThe regression function is \\(\\hat{f}(x)=\\tfrac{3}{2} x + \\tfrac{5}{3}\\).\n\n\n\n\n\n\n\n\n\n(video example is different from the text)\n\n5.1.1 Linear algebra\nBefore moving on, we want to examine a vector-oriented description of the process. If we define \\[\n\\bfe = [1,1,\\ldots,1] \\in \\real^n,\n\\] that is, \\(\\bfe\\) as a vector of \\(n\\) ones, then \\[\nL(a,b) =  \\twonorm{a\\, \\bfx + b \\,\\bfe - \\bfy}^2,\n\\] Minimizing \\(L\\) over all values of \\(a\\) and \\(b\\) is called the least squares problem. (More specifically, this setup is called simple least squares or ordinary least squares.)\n\n\n\n\n\n\n\n\nWe can write out the equations for \\(a\\) and \\(b\\) using another important idea from linear algebra.\n\nDefinition 5.2 Given any \\(d\\)-dimensional real-values vectors \\(\\bfu\\) and \\(\\bfv\\), their inner product is \\[\n\\bfu^T \\bfv = \\sum_{i=1}^d u_i v_i = u_1v_1 + u_2v_2 + \\cdots + u_d v_d.\n\\tag{5.2}\\]\n\nThe vector inner product is defined only between two vectors of the same length (dimension). There is an important link between the inner product and the 2-norm: \\[\n\\bfu^T \\bfu = \\sum_{i=1}^d u_i^2 = \\twonorm{\\bfu}^2.\n\\tag{5.3}\\]\n\n\n\n\n\n\nNote\n\n\n\nInner product is a term from linear algebra. In physics and vector calculus with \\(d=2\\) or \\(d=3\\), the same thing is often called a dot product and written as \\(\\bfu \\cdot \\bfv\\).\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe \\({}^T\\) symbol is a transpose operation in linear algebra. We won’t need it as an independent concept, so we are just using it as notation in the inner product.\n\n\n\nExample 5.2 Let \\(\\bfu = [1,-1,1,-2]\\) and \\(\\bfv = [5,3,-1,2]\\). Then \\[\n\\bfu^T \\bfv = (1)(5) + (-1)(3) + (1)(-1) + (-2)(2) = -3.\n\\] We also have \\[\n\\twonorm{\\bfu}^2 = \\bfu^T \\bfu = (1)^2 + (-1)^2 + (1)^2 + (-2)^2 = 7.\n\\]\n\nThe equations in Equation 5.1 may now be written as \\[\n\\begin{split}\n    a \\left(\\bfx^T \\bfx\\right) + b \\left(\\bfx^T\\bfe\\right) &= \\bfx^T\\bfy, \\\\\n    a \\left(\\bfe^T \\bfx\\right) + b \\left(\\bfe^T\\bfe\\right) &= \\bfe^T\\bfy.\n\\end{split}\n\\tag{5.4}\\]\nWe can write this as a single equation between two vectors: \\[\na\n\\begin{bmatrix}\n\\bfx^T \\bfx \\\\ \\bfe^T \\bfx\n\\end{bmatrix}\n+ b\n\\begin{bmatrix}\n\\bfx^T\\bfe \\\\ \\bfe^T\\bfe\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\bfx^T\\bfy \\\\  \\bfe^T\\bfy\n\\end{bmatrix}.\n\\] In fact, the operation on the left-hand side is how we define the product of a matrix and a vector, and we can write\n\\[\n\\begin{bmatrix}\n\\bfx^T \\bfx & \\bfx^T\\bfe \\\\\n\\bfe^T \\bfx & \\bfe^T\\bfe\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\na \\\\ b\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\bfx^T\\bfy \\\\  \\bfe^T\\bfy\n\\end{bmatrix}.\n\\] This takes the form of the equation \\(\\bfA \\bfw = \\bfv\\), where \\(\\bfA\\) is a known \\(2\\times 2\\) matrix, \\(\\bfv\\) is a known 2-vector, and \\(\\bfw\\) is the 2-vector of the unknowns \\(a\\) and \\(b\\). This equation is referred to as a linear system for the unknown vector. Linear systems and their solutions are the central topic of linear algebra. In the background, it is this linear system that is being solved when you perform a linear regression fit.\n\n\n5.1.2 Performance metrics\nWe need ways to measure regression performance. Unlike with binary classification, in regression it’s not just a matter of right and wrong answers—the amount of wrongness matters, too.\nA quirk of linear regression is that it’s an older and broader idea than most of machine learning, and it’s often presented as though the training and testing sets are identical. We follow that convention for the definitions in this section. The same quantities can also be calculated for a set of labels and predictions obtained from a separate testing set, although a few of the properties stated here don’t apply in that case.\n\nDefinition 5.3 The residuals of the regression are \\[\ny_i - \\hat{y}_i, \\qquad i=1,\\ldots,n,\n\\tag{5.5}\\] where the \\(y_i\\) are the true labels and the \\(\\hat{y}_i\\) are the values predicted by the regressor. We can express them compactly as the residual vector \\(\\bfy-\\hat{\\bfy}\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe terms error and residual are frequently used interchangeably and even inconsistently. I try to follow the most common practices here, even though the names can be confusing if you think about them too hard.\n\n\n\nDefinition 5.4 The mean squared error (MSE) is \\[\n\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^n \\, \\left( y_i - \\hat{y}_i \\right)^2 = \\frac{1}{n} \\twonorm{\\bfy - \\hat{\\bfy}}^2.\n\\] The mean absolute error (MAE) is \\[\n\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^n \\abs{y_i - \\hat{y}_i }= \\frac{1}{n} \\onenorm{\\bfy - \\hat{\\bfy}}.\n\\]\n\nThe MSE is simply \\(1/n\\) times the loss function of a linear regression. MAE is less sensitive than MSE to large outliers. Both quantities are dimensional and therefore depend on how the variables are scaled, but only the units of MAE are the same as of the data.\n\nExample 5.3 In Example 5.13 the points \\((-1,0)\\), \\((0,2)\\), \\((1,3)\\) were found to have the least-squares regressor \\(\\hat{f}(x)=\\tfrac{3}{2} x + \\tfrac{5}{3}\\). Hence \\[\n\\begin{split}\ny_1 &= 0,\\; \\hat{y}_1 = \\hat{f}(-1)=\\tfrac{1}{6} \\\\\ny_2 &=2,\\; \\hat{y}_2=\\hat{f}(0) = \\tfrac{5}{3}, \\\\\ny_3 &=3; \\; \\hat{y}_3=\\hat{f}(1)=\\tfrac{19}{6}.\n\\end{split}\n\\]\nThis implies \\[\n\\begin{split}\n\\text{MSE} &= \\frac{1}{3} \\left[ \\left(0-\\tfrac{1}{6}\\right)^2 + \\left(2-\\tfrac{5}{3}\\right)^2 + \\left(3-\\tfrac{19}{6}\\right)^2 \\right]= \\frac{1}{18}, \\\\\n\\text{MAE} &= \\frac{1}{3} \\left( \\abs{0-\\tfrac{1}{6}} + \\abs{2-\\tfrac{5}{3}} + \\abs{3-\\tfrac{19}{6}} \\right) = \\frac{2}{9}. \\\\\n\\end{split}\n\\]\n\n\nDefinition 5.5 The coefficient of determination (CoD) is denoted \\(R^2\\) and defined as \\[\nR^2 = 1 - \\frac{\\displaystyle\\sum_{i=1}^n \\,\\left(y_i - \\hat{y}_i \\right)^2}{\\displaystyle\\sum_{i=1}^n \\, \\left(y_i - \\bar{y}\\right)^2},\n\\] where \\(\\bar{y}\\) is the sample mean of \\(y_1,\\ldots,y_n\\).\n\nHere are important things to know about the coefficient of determination.\n\nTheorem 5.1 Given sample values \\(y_1,\\ldots,y_n\\) with mean \\(\\bar{y}\\) and the predictions \\(\\hat{y}_1,\\ldots, \\hat{y}_n\\),\n\n\\(R^2\\) is dimensionless and therefore independent of scaling.\nIf \\(\\hat{y}_i=y_i\\) for all \\(i\\) (i.e., perfect predictions), then \\(R^2=1\\).\nIf \\(\\hat{y}_i=\\bar{y}\\) for all \\(i\\) (i.e., always predict the sample mean), then \\(R^2=0\\).\nIf the \\(\\hat{y}_i\\) are found from a linear regression, then \\(R^2\\) is the square of the Pearson correlation coefficient between \\(\\bfy\\) and \\(\\hat{\\bfy}\\).\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThe notation \\(R^2\\) is highly unfortunate. While it is the square of the Pearson coefficient in linear regression, \\(R^2\\) can actually be negative for other regression methods! Such a result indicates that the predictor is doing worse than just predicting the mean value every time. It has nothing to do with imaginary numbers.\n\n\n\nExample 5.4 Continuing with Example 5.13 and Example 5.3, we find that \\(\\bar{y}=(0+2+3)/3=\\tfrac{5}{3}\\), and\n\\[\n\\begin{split}\n    \\sum_{i=1}^n \\left(y_i - \\bar{y}\\right)^2 &= \\left(0-\\tfrac{5}{3}\\right)^2 + \\left(2-\\tfrac{5}{3}\\right)^2 + \\left(3-\\tfrac{5}{3}\\right)^2 = \\frac{14}{3}.\n\\end{split}\n\\]\nThis gives the coefficient of determination \\[\n\\begin{split}\nR^2 &= 1 - \\frac{\\left(0-\\tfrac{1}{6}\\right)^2 + \\left(2-\\tfrac{5}{3}\\right)^2 + \\left(3-\\tfrac{19}{6}\\right)^2}{14/3} \\\\\n&= 1 - \\frac{1/6}{14/3} = \\frac{27}{28}.\n\\end{split}\n\\]\nThis is quite close to 1, indicating a good fit. Compare that to the arbitrary predictor \\(\\hat{f}(x)=x\\), which has \\(\\hat{y}_1=-1\\), \\(\\hat{y}_2=0\\), \\(\\hat{y}_3=1\\): \\[\n\\begin{split}\n    R^2 & = 1 - \\frac{\\left(y_i - \\hat{y}_i \\right)^2 = \\left(0+1\\right)^2 + \\left(2-0\\right)^2 + \\left(3-1\\right)^2}{14/3}  \\\\\n    &= 1 - \\frac{9}{14/3} = -\\frac{13}{14}.\n\\end{split}\n\\] Since this result is negative, we would be better off always predicting \\(5/3\\) rather than using \\(\\hat{f}(x)=x\\).\n\n\nExample 5.5 We import data about the extent of sea ice in the Arctic circle, collected monthly since 1979:\n\nice = pd.read_csv(\"_datasets/sea-ice.csv\")\n# Simplify column names:\nice.columns = [s.strip() for s in ice.columns]   \nice.head()\n\n\n\n\n\n\n\n\nyear\nmo\ndata-type\nregion\nextent\narea\n\n\n\n\n0\n1979\n1\nGoddard\nN\n15.41\n12.41\n\n\n1\n1980\n1\nGoddard\nN\n14.86\n11.94\n\n\n2\n1981\n1\nGoddard\nN\n14.91\n11.91\n\n\n3\n1982\n1\nGoddard\nN\n15.18\n12.19\n\n\n4\n1983\n1\nGoddard\nN\n14.94\n12.01\n\n\n\n\n\n\n\nA quick plot reveals something odd-looking:\n\nsns.relplot(data=ice, x=\"mo\", y=\"extent\");\n\n\n\n\nEverything in the plot above is dominated by two large negative values. These probably represent missing data, so we make a new copy without those rows:\n\nice = ice[ice[\"extent\"] &gt; 0]\nsns.relplot(data=ice, x=\"mo\", y=\"extent\");\n\n\n\n\nEach dot in the plot above represents one measurement. As you would expect, the extent of ice rises in the winter months and falls in summer:\n\nbymonth = ice.groupby(\"mo\")\nbymonth[\"extent\"].mean()\n\nmo\n1     14.214762\n2     15.100233\n3     15.256977\n4     14.525581\n5     13.117442\n6     11.539767\n7      9.097907\n8      6.793256\n9      5.993488\n10     7.887907\n11    10.458182\n12    12.664419\nName: extent, dtype: float64\n\n\nWhile the effect of the seasonal variation somewhat cancels out over time when fitting a line, it’s preferable to remove this obvious trend before the fit takes place. To do that, we add a column that measures within each month group the relative change from the mean, \\[\n\\frac{x-\\bar{x}}{\\bar{x}}.\n\\] This is done with a transform method applied to the grouped frame:\n\nrecenter = lambda x: x/x.mean() - 1\nice[\"detrended\"] = bymonth[\"extent\"].transform(recenter)\nsns.relplot(data=ice, x=\"mo\", y=\"detrended\");\n\n\n\n\nAn lmplot in seaborn shows the least squares line:\n\nsns.lmplot(data=ice, x=\"year\", y=\"detrended\");\n\n\n\n\nHowever, we should be mindful of Simpson’s paradox. The previous plot showed considerably more variance within the warm months. How do these fits look for the data within each month? This is where a facet plot shines:\n\nsns.lmplot(data=ice,\n    x=\"year\", y=\"detrended\",\n    col=\"mo\", col_wrap=3, height=2\n    );\n\n\n\n\nThus, while the correlation is negative within each month, the effect size is clearly larger in the summer and early fall.\nWe can get numerical information about a regression line from a LinearRegression() learner in sklearn. We will focus on the data for August:\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\n\nice = ice[ ice[\"mo\"]==8 ]\nX = ice[ [\"year\"] ]  \ny = ice[\"detrended\"]\nlm.fit(X, y)\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\nWe can get the slope and \\(y\\)-intercept of the regression line from the learner’s properties. (Calculated parameters tend to have underscores at the ends of their names in sklearn.)\n\nslope, intercept = lm.coef_[0], lm.intercept_\nprint(f\"Slope is {slope:.3g} and intercept is {intercept:.3g}\")\n\nSlope is -0.011 and intercept is 22.1\n\n\nThe slope indicates average decrease over time.\nNext, we assess the performance on the training set. Both the MSE and mean absolute error are small relative to dispersion within the values themselves:\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nyhat = lm.predict(X)\nmse = mean_squared_error(y, yhat)\nmae = mean_absolute_error(y, yhat)\n\nprint(f\"MSE: {mse:.2e}, compared to variance {y.var():.2e}\")\nprint(f\"MAE: {mae:.2e}, compared to standard deviation {y.std():.2e}\")\n\nMSE: 4.01e-03, compared to variance 2.33e-02\nMAE: 4.93e-02, compared to standard deviation 1.53e-01\n\n\nThe score method of the regressor object computes the coefficient of determination:\n\nR2 = lm.score(X, y)\nprint(f\"R-squared: {R2:.3f}\")\n\nR-squared: 0.824\n\n\nAn \\(R^2\\) value this close to 1 would usually be considered a sign of a good fit, although we have not tested for generalization to new data."
  },
  {
    "objectID": "regression.html#multilinear-regression",
    "href": "regression.html#multilinear-regression",
    "title": "5  Regression",
    "section": "5.2 Multilinear regression",
    "text": "5.2 Multilinear regression\nWe can extend linear regression to \\(d\\) predictor variables \\(x_1,\\ldots,x_d\\):\n\\[\n    y \\approx \\hat{f}(\\bfx) = b + w_1 x_1 + w_2x_2 + \\cdots w_d x_d.\n\\]\n\n\n\n\n\n\n\n\nFirst, observe that we can actually drop the intercept term \\(b\\) from the discussion, because we could always define an additional constant feature \\(x_0=1\\) and get the same effect in one higher dimension. So we will use the following.\n\nDefinition 5.6 Multilinear regression is the approximation \\[\ny \\approx \\hat{f}(\\bfx) = w_1 x_1 + w_2x_2 + \\cdots w_d x_d = \\bfw^T\\bfx,\n\\] for a constant vector \\(\\bfw\\) known as the weight vector.\n\n\n\n\n\n\n\nNote\n\n\n\nMultilinear regression is also simply called linear regression most of the time. What we previously called linear regression is just a special case. The LinearRegression learner class does both types of fits. It has a keyword option fit_intercept that determines whether or not the \\(b\\) term above is used.\n\n\nAs before, we find the unknown weight vector \\(\\bfw\\) by minimizing a loss function. To create the least-squares loss function, we use \\(\\bfX_i\\) to denote the \\(i\\)th row of an \\(n\\times d\\) feature matrix \\(\\bfX\\). Then\n\\[\nL(\\bfw) = \\sum_{i=1}^n (y_i - \\hat{f}(\\bfX_i))^2 = \\sum_{i=1}^n (y_i - \\bfX_i^T\\bfw)^2.\n\\]\nWe encountered a matrix-vector product earlier. It turns out that the following definition is equivalent to that earlier one.\n\nDefinition 5.7 Given an \\(n\\times d\\) matrix \\(\\bfX\\) with rows \\(\\bfX_1,\\ldots,\\bfX_n\\) and a \\(d\\)-vector \\(\\bfw\\), the product \\(\\bfX\\bfw\\) is defined by \\[\n\\bfX \\bfw =\n\\begin{bmatrix}\n    \\bfX_1^T\\bfw \\\\ \\bfX_2^T\\bfw \\\\ \\vdots \\\\ \\bfX_n^T\\bfw\n\\end{bmatrix}.\n\\]\n\n\nExample 5.6 Suppose that \\[\n\\bfX = \\begin{bmatrix}\n3 & -1 \\\\ 0 & 2 \\\\ 1 & 4\n\\end{bmatrix},\n\\qquad\n\\bfw = [5,-2].\n\\] Then \\(\\bfX_1=[3,-1]\\), \\(\\bfX_2=[0,2]\\), \\(\\bfX_3=[1,4]\\), and \\[\n\\bfX \\bfw = \\begin{bmatrix}\n(3)(5)+(-1)(-2) \\\\ (0)(5) + (2)(-2) \\\\ (1)(5) + (4)(-2)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n17 \\\\ -4 \\\\ -3\n\\end{bmatrix}.\n\\]\n\nWe now have the compact expression\n\\[\nL(\\bfw) = \\twonorm{\\bfX \\bfw- \\bfy}^2.\n\\tag{5.6}\\]\nAs in the \\(d=1\\) case, minimizing the loss is equivalent to solving a linear system of equations known as the normal equations for the weight vector \\(\\bfw\\). We do not present the details here.\n\n\n\n\n\n\nCaution\n\n\n\nBe careful interpreting the magnitudes of regression coefficients (i.e., entries of the weight vector). These are sensitive to the units and scales of the features. For example, distances expressed in meters would have a coefficient that is 1000 times larger than the same distances expressed in kilometers. For quantitative comparisons, it helps to standardize the features first, which does not affect the quality of the fit.\n\n\n\nExample 5.7 We return to the data set regarding the fuel efficiency of cars:\n\ncars = sns.load_dataset(\"mpg\").dropna()\ncars.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nusa\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nusa\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nusa\nford torino\n\n\n\n\n\n\n\nIn order to ease experimentation, we define a function that fits a given learner to the mpg variable using a given list of features from the data frame:\n\ndef fitcars(model, features):\n    X = cars[features]\n    y = cars[\"mpg\"]\n\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y,\n        test_size=0.2, \n        shuffle=True, random_state=302\n        )\n\n\n    model.fit(X_train, y_train)\n    MSE = mean_squared_error(y_test, model.predict(X_test))\n    print(f\"MSE: {MSE:.3f}, compared to variance {y_test.var():.3f}\")\n    return None\n\n\n\n\n\n\n\nTip\n\n\n\nWhen you run the same lines of code over and over with only a slight change at the beginning, it’s advisable to put that code into a function. It makes the overall code shorter and easier to understand and adapt.\n\n\nFirst, we try using horsepower as the only feature in a linear regression to fit mpg:\n\nfeatures = [\"horsepower\"]\nlm = LinearRegression( fit_intercept=True )\nfitcars(lm, features)\n\nMSE: 26.354, compared to variance 56.474\n\n\nAs we would expect, there is an inverse relationship between horsepower and vehicle efficiency:\n\nlm.coef_\n\narray([-0.1596552])\n\n\nNext, we add displacement to the regression:\n\nfeatures = [\"horsepower\", \"displacement\"]\nfitcars(lm, features)\n\nMSE: 19.683, compared to variance 56.474\n\n\nThe error has decreased from the univariate case because we have a more capable model.\nFinally, we try using 4 features as predictors. In order to help us compare the regression coefficients, we chain the model with a StandardScaler so that all columns are z-scores:\n\nfeatures = [\"horsepower\", \"displacement\", \"cylinders\", \"weight\"]\npipe = make_pipeline(StandardScaler(), lm)\nfitcars(pipe, features)\n\nMSE: 19.266, compared to variance 56.474\n\n\nWe did not get much improvement in the fit this time. But by comparing the coefficients of the individual features, some interesting information emerges:\n\nweights = pd.Series(pipe[1].coef_, index=features).sort_values()\nweights\n\nweight         -4.847433\nhorsepower     -1.892329\ncylinders      -0.870727\ndisplacement    0.722049\ndtype: float64\n\n\nWe now have evidence that weight is the most significant negative factor for MPG, by a wide margin.\n\nIn the next example we will see that we can create new features out of the ones that are initially given. Sometimes these new features add a lot to the quality of the regression.\n\nExample 5.8 Here we load data about advertising spending on different media in many markets:\n\nads = pd.read_csv(\"_datasets/advertising.csv\")\nads.head(6)\n\n\n\n\n\n\n\n\nTV\nRadio\nNewspaper\nSales\n\n\n\n\n0\n230.1\n37.8\n69.2\n22.1\n\n\n1\n44.5\n39.3\n45.1\n10.4\n\n\n2\n17.2\n45.9\n69.3\n12.0\n\n\n3\n151.5\n41.3\n58.5\n16.5\n\n\n4\n180.8\n10.8\n58.4\n17.9\n\n\n5\n8.7\n48.9\n75.0\n7.2\n\n\n\n\n\n\n\nPairwise scatter plots yield some hints about what to expect from this dataset:\n\nsns.pairplot(data=ads, height=1.5);\n\n\n\n\nThe last column, which shows relationships with Sales, is of greatest interest. From it we see that the clearest association between Sales and spending is with TV. So we first try a univariate linear fit of sales against TV spending alone:\n\nX = ads[ [\"TV\"] ]    # has to be a frame, so [\"TV\"] not \"TV\"\ny = ads[\"Sales\"]\n\nlm = LinearRegression()\nlm.fit(X, y)\nprint(\"R^2 score:\", f\"{lm.score(X, y):.4f}\")\nprint(\"Regression weight:\", lm.coef_)\n\nR^2 score: 0.8122\nRegression weight: [0.05546477]\n\n\nThe coefficient of determination is already quite good. Since we are going to do multiple fits with different features, we write a function that does the grunt work:\n\ndef regress(lm, data, y, features):\n    X = data[features]\n    lm.fit(X, y)\n    R2 = lm.score(X,y)\n    print(\"R^2 score:\", f\"{R2:.5f}\")\n    print(\"Regression weights:\")\n    print( pd.Series(lm.coef_, index=features) )\n    return None\n\nNext, we try folding in Newspaper as well:\n\nregress(lm, ads, y, [\"TV\", \"Newspaper\"])\n\nR^2 score: 0.82364\nRegression weights:\nTV           0.055091\nNewspaper    0.026021\ndtype: float64\n\n\nThe additional feature had very little effect on the quality of fit. We go on to fit using all three features:\n\nregress(lm, ads, y, [\"TV\", \"Newspaper\", \"Radio\"])\n\nR^2 score: 0.90259\nRegression weights:\nTV           0.054446\nNewspaper    0.000336\nRadio        0.107001\ndtype: float64\n\n\nJudging by the weights of the model, it’s even clearer now that we can explain Sales very well without contributions from Newspaper. In order to reduce model variance, it would be reasonable to leave that column out. Doing so has a negligible effect:\n\nregress(lm, ads, y, [\"TV\", \"Radio\"])\n\nR^2 score: 0.90259\nRegression weights:\nTV       0.054449\nRadio    0.107175\ndtype: float64\n\n\nWhile we have a very good \\(R^2\\) score now, we can try to improve it. We can add an additional feature that is the product of TV and Radio, representing the possibility that these media reinforce one another’s effects:\n\n\n\n\n\n\nTip\n\n\n\nIn order to modify a frame, it has to be an independent copy, not just a subset of another frame.\n\n\n\nX = ads[ [\"Radio\", \"TV\"] ].copy()\nX[\"Radio*TV\"] = X[\"Radio\"]*X[\"TV\"]\nregress(lm, X, y, X.columns)\n\nR^2 score: 0.91404\nRegression weights:\nRadio       0.042270\nTV          0.043578\nRadio*TV    0.000443\ndtype: float64\n\n\nWe did see a small increase in the \\(R^2\\) score, and the combination of both types of spending does have a positive effect on Sales.\n\n\n\n\n\n\n\n\n\nIt’s not uncommon to introduce a product term as done in Example 5.8, and more exotic choices are also possible. Keep in mind, though, that additional variables usually add variance to the model, even if they don’t seriously affect the bias.\nInterpreting linear regression is a major topic in statistics. There are tests that can lend much more precision and rigor to the brief discussion in this section.\n\n5.2.1 Polynomial regression\nA special case of multilinear regression is when there is initially a single predictor variable \\(t\\), and then we define\n\\[\nx_1 = t^0, \\, x_2 = t^1, \\ldots, x_d = t^{d-1}.\n\\]\n\n\n\n\n\n\n\n\nThis makes the regressive approximation into\n\\[\ny \\approx w_1 + w_2 t + \\cdots + w_d t^{d-1},\n\\]\nwhich is a polynomial of degree \\(d-1\\). This allows representation of data that depends on \\(t\\) in ways more complicated than a straight line. However, it can lead to overfitting if taken too far.\nWe don’t have to add polynomial features manually, if we use a pipeline instead.\n\nExample 5.9 We return to the data set regarding the fuel efficiency of cars:\n\ncars = sns.load_dataset(\"mpg\").dropna()\ncars.head()\n\n\n\n\n\n\n\n\nmpg\ncylinders\ndisplacement\nhorsepower\nweight\nacceleration\nmodel_year\norigin\nname\n\n\n\n\n0\n18.0\n8\n307.0\n130.0\n3504\n12.0\n70\nusa\nchevrolet chevelle malibu\n\n\n1\n15.0\n8\n350.0\n165.0\n3693\n11.5\n70\nusa\nbuick skylark 320\n\n\n2\n18.0\n8\n318.0\n150.0\n3436\n11.0\n70\nusa\nplymouth satellite\n\n\n3\n16.0\n8\n304.0\n150.0\n3433\n12.0\n70\nusa\namc rebel sst\n\n\n4\n17.0\n8\n302.0\n140.0\n3449\n10.5\n70\nusa\nford torino\n\n\n\n\n\n\n\nAs we would expect, horsepower and miles per gallon are negatively correlated. However, the relationship is not well captured by a straight line:\n\nsns.lmplot(data=cars, x=\"horsepower\", y=\"mpg\");\n\n\n\n\nSeaborn can show us how a cubic polynomial regression would look:\n\nsns.lmplot(data=cars, x=\"horsepower\", y=\"mpg\", order=3);\n\n\n\n\nThe figure above suggests that the cubic regression produces a better fit—that is, lower bias over the full range of horsepower.\nIn order to obtain the cubic fit within sklearn, we use the PolynomialFeatures preprocessor in a pipeline. If the original predictor variable is \\(t\\), then the preprocessor will create features for \\(1\\), \\(t\\), \\(t^2\\), and \\(t^3\\). (Since the constant feature is added in by the preprocessor, we don’t need it again within the regression, so we set fit_intercept=False in LinearRegression.)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nX = cars[ [\"horsepower\"] ]\ny = cars[\"mpg\"]\nlm = LinearRegression( fit_intercept=False )\ncubic = make_pipeline(PolynomialFeatures(degree=3), lm)\ncubic.fit(X, y)\n\n# Predict MPG at horsepower=200:\nquery = pd.DataFrame([200], columns=X.columns)\nprint(\"prediction at hp=200:\", cubic.predict(query))\n\nprediction at hp=200: [12.90220247]\n\n\nLet’s compare the coefficients of determination for the linear and cubic fits:\n\nlinscore = LinearRegression().fit(X,y).score(X,y)\nprint(f\"CoD for linear fit: {linscore:4f}\")\nprint(f\"CoD for cubic fit: {cubic.score(X,y):4f}\")\n\nCoD for linear fit: 0.605948\nCoD for cubic fit: 0.688214\n\n\n\n\n\n\n\n\n\n\n\nIf a cubic polynomial can fit better than a line, it seems reasonable that increasing the degree more will lead to even better fits. In fact, the training error can only go down, because a lower-degree polynomial case is a subset of a higher-degree case. But there is a major catch, in the form of overfitting.\n\nExample 5.10 Continuing with Example 5.9, we explore the effect of polynomial degree after splitting into training and test sets:\n\n\nCode\nfrom sklearn.metrics import mean_squared_error\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2, \n    shuffle=True, random_state=302\n    )\n\nresults = []\nfor deg in range(1,9):\n    poly = make_pipeline( \n        StandardScaler(), \n        PolynomialFeatures(degree=deg), \n        lm \n        )\n    poly.fit(X_train, y_train)\n    MSE_tr = mean_squared_error(y_train, poly.predict(X_train))\n    MSE_te = mean_squared_error(y_test, poly.predict(X_test))\n    results.append((deg, \"train\", MSE_tr))\n    results.append((deg, \"test\", MSE_te))\n\nresults = pd.DataFrame(results, columns=[\"degree\",\"measuring set\",\"MSE\"])\nsns.relplot(data=results,\n    x=\"degree\", y=\"MSE\",\n    kind=\"line\", hue=\"measuring set\"\n    );\n\n\n\n\n\nThe results above demonstrate that increasing the polynomial degree eventually leads to overfitting. In fact, in this case it seems that we don’t want to go beyond the cubic fit. \n\n\n\n\n\n\n\n\n\nLet’s summarize the situation:\n\n\n\n\n\n\nCaution\n\n\n\nAdding polynomial features improves the expressive power of a regression model, which can decrease bias but also increase overfitting."
  },
  {
    "objectID": "regression.html#regularization",
    "href": "regression.html#regularization",
    "title": "5  Regression",
    "section": "5.3 Regularization",
    "text": "5.3 Regularization\nAs a general term, regularization refers to modifying something that is difficult to compute accurately with something more tractable. For learning models, regularization is a useful way to combat overfitting.\n\n\n\n\n\n\n\n\nSuppose we had an \\(\\real^{n\\times 4}\\) feature matrix in which the features are identical; that is, the predictor variables satisfy \\(x_1=x_2=x_3=x_4\\), and suppose the target \\(y\\) also equals \\(x_1\\). Clearly, we get a perfect regression if we use\n\\[\ny = 1x_1 + 0x_2 + 0x_3 + 0x_4.\n\\]\nBut an equally good regression is\n\\[\ny = \\frac{1}{4}x_1 + \\frac{1}{4}x_2 + \\frac{1}{4}x_3 + \\frac{1}{4}x_4.\n\\]\nFor that matter, so is\n\\[\ny = 1000x_1 - 500x_2 - 500x_3 + 1x_4.\n\\]\nA problem with more than one valid solution is called ill-posed. If we made tiny changes to the predictor variables in this thought experiment, the problem would technically be well-posed, but there would be a wide range of solutions that were very nearly correct, in which case the problem is said to be ill-conditioned; for practical purposes, it remains just as difficult.\nThe poor conditioning can be regularized away by modifying the least-squares loss function to penalize complexity in the model, in the form of excessively large regression coefficients. There are two dominant variants for doing this.\n\nDefinition 5.8 Ridge regression minimizes the loss function\n\\[\nL(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\twonorm{\\bfw}^2,\n\\tag{5.7}\\]\nwhere \\(\\alpha\\) is a nonnegative hyperparameter. LASSO regression minimizes the loss function \\[\nL(\\bfw) = \\twonorm{ \\bfX \\bfw- \\bfy }^2 + \\alpha \\onenorm{\\bfw},\n\\]\nagain for a nonnegative value of \\(\\alpha\\).\n\nAs \\(\\alpha\\to 0\\), both types of regularization revert to the usual least-squares loss, but as \\(\\alpha \\to \\infty\\), the optimization becomes increasingly concerned with prioritizing a small result for \\(\\bfw\\).\nWhile ridge regression is an easier function to minimize quickly, LASSO has an interesting advantage, as illustrated in this figure.\n\nLASSO tends to produce sparse results, meaning that some of the regression coefficients are zero or negligible. These zeros indicate predictor variables that have minor predictive value, which can be valuable information in itself. Moreover, these variables can often be removed from the regression to reduce variance without noticeably affecting the bias.\n\nExample 5.11 We’ll apply regularized regression to data collected about the progression of diabetes:\n\ndiabetes = datasets.load_diabetes(as_frame=True)[\"frame\"]\ndiabetes.head(10)\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nbp\ns1\ns2\ns3\ns4\ns5\ns6\ntarget\n\n\n\n\n0\n0.038076\n0.050680\n0.061696\n0.021872\n-0.044223\n-0.034821\n-0.043401\n-0.002592\n0.019907\n-0.017646\n151.0\n\n\n1\n-0.001882\n-0.044642\n-0.051474\n-0.026328\n-0.008449\n-0.019163\n0.074412\n-0.039493\n-0.068332\n-0.092204\n75.0\n\n\n2\n0.085299\n0.050680\n0.044451\n-0.005670\n-0.045599\n-0.034194\n-0.032356\n-0.002592\n0.002861\n-0.025930\n141.0\n\n\n3\n-0.089063\n-0.044642\n-0.011595\n-0.036656\n0.012191\n0.024991\n-0.036038\n0.034309\n0.022688\n-0.009362\n206.0\n\n\n4\n0.005383\n-0.044642\n-0.036385\n0.021872\n0.003935\n0.015596\n0.008142\n-0.002592\n-0.031988\n-0.046641\n135.0\n\n\n5\n-0.092695\n-0.044642\n-0.040696\n-0.019442\n-0.068991\n-0.079288\n0.041277\n-0.076395\n-0.041176\n-0.096346\n97.0\n\n\n6\n-0.045472\n0.050680\n-0.047163\n-0.015999\n-0.040096\n-0.024800\n0.000779\n-0.039493\n-0.062917\n-0.038357\n138.0\n\n\n7\n0.063504\n0.050680\n-0.001895\n0.066629\n0.090620\n0.108914\n0.022869\n0.017703\n-0.035816\n0.003064\n63.0\n\n\n8\n0.041708\n0.050680\n0.061696\n-0.040099\n-0.013953\n0.006202\n-0.028674\n-0.002592\n-0.014960\n0.011349\n110.0\n\n\n9\n-0.070900\n-0.044642\n0.039062\n-0.033213\n-0.012577\n-0.034508\n-0.024993\n-0.002592\n0.067737\n-0.013504\n310.0\n\n\n\n\n\n\n\nThe features in this dataset were standardized, making it easy to compare the magnitudes of the regression coefficients.\nFirst, we look at basic linear regression on all ten predictive features in the data:\n\nX = diabetes.drop(\"target\", axis=1)\ny = diabetes[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2, random_state=2\n    )\n\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train, y_train)\nprint(f\"linear model CoD score: {lm.score(X_test, y_test):.4f}\")\n\nlinear model CoD score: 0.4399\n\n\nFirst, we find that ridge regression can improve the score a bit:\n\nfrom sklearn.linear_model import Ridge\n\nrr = Ridge(alpha=0.5)\nrr.fit(X_train, y_train)\nprint(f\"ridge CoD score: {rr.score(X_test, y_test):.4f}\")\n\nridge CoD score: 0.4411\n\n\nRidge regularization added a penalty for the 2-norm of the regression coefficients vector. Accordingly, the regularized solution has smaller coefficients:\n\nfrom numpy.linalg import norm\nprint(f\"2-norm of unregularized coefficients: {norm(lm.coef_):.1f}\")\nprint(f\"2-norm of ridge coefficients: {norm(rr.coef_):.1f}\")\n\n2-norm of unregularized coefficients: 1525.2\n2-norm of ridge coefficients: 605.9\n\n\nAs we continue to increase the regularization parameter, the method becomes increasingly obsessed with keeping the coefficient vector small and pays ever less attention to the data:\n\nfor alpha in [0.25, 0.5, 1, 2]:\n    rr = Ridge(alpha=alpha)    # more regularization\n    rr.fit(X_train, y_train)\n    print(f\"alpha = {alpha:.2f}\")\n    print(f\"2-norm of coefficient vector: {norm(rr.coef_):.1f}\")\n    print(f\"ridge regression CoD score: {rr.score(X_test, y_test):.4f}\")\n    print()\n\nalpha = 0.25\n2-norm of coefficient vector: 711.7\nridge regression CoD score: 0.4527\n\nalpha = 0.50\n2-norm of coefficient vector: 605.9\nridge regression CoD score: 0.4411\n\nalpha = 1.00\n2-norm of coefficient vector: 480.8\nridge regression CoD score: 0.4078\n\nalpha = 2.00\n2-norm of coefficient vector: 353.5\nridge regression CoD score: 0.3478\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.12 We continue with the diabetes data from Example 5.11, but this time, we try LASSO regularization. A validation curve suggests initial gains in the \\(R^2\\) score as the regularization parameter \\(\\alpha\\) is varied, followed by a decrease:\n\nfrom sklearn.linear_model import Lasso\n\nkf = KFold(n_splits=4, shuffle=True, random_state=302)\nalpha = np.linspace(0, 0.1, 60)[1:]  # exclude alpha=0\n\n_,scores = validation_curve(\n    Lasso(),\n    X_train, y_train,\n    cv=kf,\n    n_jobs=-1,\n    param_name=\"alpha\", param_range=alpha\n    )\n\nsns.relplot(x=alpha, y=np.mean(scores, axis=1) );\n\n\n\n\nMoreover, while ridge regression still used all of the features, LASSO put zero weight on three of them:\n\nlass = Lasso(alpha=0.05)\nlass.fit(X_train, y_train)\npd.DataFrame( {\n    \"feature\": X.columns,\n    \"ridge\": rr.coef_,\n    \"LASSO\": lass.coef_\n    } )\n\n\n\n\n\n\n\n\nfeature\nridge\nLASSO\n\n\n\n\n0\nage\n43.113029\n-0.000000\n\n\n1\nsex\n-23.953301\n-155.276227\n\n\n2\nbmi\n199.535945\n529.173009\n\n\n3\nbp\n144.586873\n313.419043\n\n\n4\ns1\n25.977923\n-132.507438\n\n\n5\ns2\n2.751708\n-0.000000\n\n\n6\ns3\n-106.337626\n-165.167100\n\n\n7\ns4\n89.526889\n0.000000\n\n\n8\ns5\n185.660175\n580.262391\n\n\n9\ns6\n85.576399\n30.557703\n\n\n\n\n\n\n\nWe can isolate the coefficients that were (within small rounding errors) zeroed out:\n\n# Get the locations (indices) of the very small weights:\nzeroed = np.nonzero( np.abs(lass.coef_) &lt; 1e-5 )\n# Names of the corresponding columns:\ndropped = X.columns[zeroed].values\n\nNow we can drop these features from the dataset:\n\nX_train_reduced = X_train.drop(dropped, axis=1)\nX_test_reduced = X_test.drop(dropped, axis=1)\n\nReturning to a fit with no regularization, we find that little is lost by using the reduced feature set:\n\nprint(f\"original linear model score: {lm.score(X_test, y_test):.4f}\")\n\nlm.fit(X_train_reduced, y_train)\nRsq = lm.score(X_test_reduced, y_test)\nprint(f\"reduced linear model score: {Rsq:.4f}\")\n\noriginal linear model score: 0.4399\nreduced linear model score: 0.4388"
  },
  {
    "objectID": "regression.html#nonlinear-regression",
    "href": "regression.html#nonlinear-regression",
    "title": "5  Regression",
    "section": "5.4 Nonlinear regression",
    "text": "5.4 Nonlinear regression\nMultilinear regression limits the representation of the dataset to a function of the form \\[\n\\hat{f}(\\bfx) = \\bfw^T \\bfx.\n\\tag{5.8}\\] This is a linear function, meaning that two key properties are satisfied. For all possible vectors \\(\\bfu,\\bfv\\) and numbers \\(c\\),\n\n\n\n\n\n\n\n\n\n\\(\\hat{f}(\\bfu + \\bfv) = \\hat{f}(\\bfu) + \\hat{f}(\\bfv)\\),\n\\(\\hat{f}(c\\bfu) = c \\hat{f}(\\bfu)\\).\n\nThese properties are the essence of what makes a function easy to manipulate, solve for, and analyze.\n\nExample 5.13 In two dimensions, the function \\(\\hat{f}(\\bfx) = x_1 - x_2\\) is linear. To prove this, first suppose that \\(\\bfu\\) and \\(\\bfv\\) are any 2-vectors. Then \\[\n\\begin{split}\n\\hat{f}(\\bfu + \\bfv) &= \\hat{f}\\bigl( [u_1+v_1,u_2+v_2] \\bigr) \\\\\n    &=  (u_1 + u_2) - (v_1 + v_2)  \\\\\n    &= (u_1-v_1) + (u_2-v_2) \\\\\n    &= \\hat{f}(\\bfu) + \\hat{f}(\\bfv).\n\\end{split}\n\\] Now, suppose that \\(\\bfu\\) is a 2-vector and \\(c\\) is any real number. Then \\[\n\\begin{split}\n\\hat{f}(c\\bfu) &= \\hat{f}\\bigl([cu_1, cu_2] \\bigr) \\\\\n    &= cu_1 - cu_2 \\\\\n    &= c(u_1-u_2) \\\\\n    &= c\\cdot\\hat{f}(\\bfu).\n\\end{split}\n\\]\n\n\nExample 5.14 In two dimensions, the function \\(\\hat{f}(\\bfx) = |x_1 + x_2|\\) is not linear. Suppose \\(\\bfu\\) is any 2-vector and \\(c\\) is any real number. If we try to prove the second property of linearity, we would start with: \\[\n\\begin{split}\n\\hat{f}(c\\bfu) &= \\hat{f}\\bigl([cu_1, cu_2] \\bigr) \\\\\n    &= |cu_1 + cu_2| \\\\\n    &= |c| \\cdot |u_1 + u_2|.\n\\end{split}\n\\] We are trying to show that this equals \\(c \\cdot \\hat{f}(\\bfu)\\), which is \\(c\\cdot |u_1+u_2|\\). However, it doesn’t look as though this is equivalent to the last line above in all cases. In fact, they must be different if \\(c &lt; 0\\) and \\(|u_1+u_2|\\) is nonzero.\nTo prove nonlinearity, we only have to find one counterexample where one of the properties of a linear function doesn’t hold. The attempt above suggests, for instance, \\(\\bfu=[1,1]\\) and the number \\(c=-1\\). Then \\[\n\\hat{f}(c \\bfu) = | -1 - 1| = 2,\n\\] but at the same time, \\[\nc\\cdot \\hat{f}(\\bfu) = -1 \\cdot |1+1| = -2.\n\\] Since these values are different, \\(\\hat{f}\\) can’t be linear.\n\nFor our regression function Equation 5.8, the linearity properties follow easily from how the inner product is defined. For example, \\[\n\\hat{f}(c\\bfu) = (c\\bfu)^T\\bfw = \\sum_{i=1}^d (cu_i) w_i = c \\sum_{i=1}^d u_i w_i = c(\\bfu^T\\bfw) = c \\hat{f}(\\bfu).\n\\]\nOne major benefit of the linear approach is that the dependence of the weight vector \\(\\bfw\\) on the regressed data is also linear, which makes solving for it straightforward.\nAs the simplest type of multidimensional function, linear relationships are a good first resort. Furthermore, we can augment the features with powers in order to get polynomial relationships. However, that approach becomes infeasible for more than 2 or 3 dimensions, because the number of polynomial terms needed explodes. (While there is a way around this restriction known as the kernel trick, that’s beyond our mathematical scope here.)\nAlternatively, we can resort to fully nonlinear regression methods. Two of them come from generalizations of our staple classifiers.\n\n5.4.1 Nearest neighbors\nTo use kNN for regression, we find the \\(k\\) nearest examples as with classification, but replace voting on classes with the mean or median of the neighboring values. A simple example confirms that the resulting approximation is not linear.\n\n\n\n\n\n\n\n\n\nExample 5.15 Suppose we have just two samples with one-dimensional features: \\(x_1=0\\) and \\(x_2=2\\), and let the corresponding sample values be \\(y_1=0\\) and \\(y_2=1\\). Using kNN with \\(k=1\\), the resulting approximation \\(\\hat{f}(x)\\) is \\[\n\\hat{f}(x) =\n\\begin{cases}\n    0, & x &lt; 1, \\\\\n    \\tfrac{1}{2}, & x=1, \\\\  \n    1, & x &gt; 1.\n\\end{cases}\n\\] (Convince yourself that the result is the same whether the mean or the median is used.) Thus, for instance, \\(\\hat{f}(2 \\cdot 0.6)=\\hat{f}(1.2)=1\\), while \\(2\\cdot \\hat{f}(0.6) = 0\\).\n\nkNN regression can produce a function that conforms itself to the training data much more closely than a linear regressor does. This can both decrease bias and increase variance, especially for small values of \\(k\\). As illustrated in the following video, increasing \\(k\\) flattens out the approximation, decreasing variance while increasing bias.\n\nAs with classification, we can choose the norm to use and whether to weight the neighbors equally or by inverse distance. As a reminder, it is usually advisable to work with z-scores for the features rather than raw data.\n\nExample 5.16 We return again to the dataset of cars and their fuel efficiency. A linear regression on four quantitative features is only OK:\n\ncars = sns.load_dataset(\"mpg\").dropna()\nfeatures = [\"displacement\", \"horsepower\", \"weight\", \"acceleration\"]\nX = cars[features]\ny = cars[\"mpg\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2, random_state=0\n    )\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)\nprint(f\"linear model CoD: {lm.score(X_test, y_test):.4f}\")\n\nlinear model CoD: 0.6928\n\n\nNext we try a kNN regressor, doing a grid search to find good hyperparameters:\n\nfrom sklearn.neighbors import KNeighborsRegressor\n\nkf = KFold(n_splits=6, shuffle=True, random_state=3383)\ngrid = {\n    \"kneighborsregressor__n_neighbors\": range(2, 25),\n    \"kneighborsregressor__weights\": [\"uniform\", \"distance\"] \n    }\nknn = make_pipeline( StandardScaler(), KNeighborsRegressor() )\noptim = GridSearchCV(\n    knn, grid, \n    cv=kf, \n    n_jobs=-1\n    )\noptim.fit(X_train, y_train)\n\nprint(f\"best kNN CoD: {optim.score(X_test, y_test):.4f}\")\n\nbest kNN CoD: 0.7439\n\n\nAs you can see above, we got some improvement over the linear regressor.\n\n\n\n\n\n\n\n\n\n\n\n5.4.2 Decision tree\nRecall that a decision tree recursively divides the examples into subsets. As with kNN regression, we can replace taking a classification vote over a leaf subset with taking a mean or median of the values in the leaf. As in classification, a decision tree regressor seeks the best possible split of samples along coordinate planes. A proposal to split into subsets \\(S\\) and \\(T\\) is assigned the weighted score \\[\nQ = |S| H(S) + |T| H(T),\n\\] where \\(H\\) is an empirical error measure. The split location is chosen to minimize \\(Q\\). After the process is applied recursively, each leaf of the decision tree contains a subset of the training samples. There are two common variations:\n\n\n\n\n\n\n\n\n\nThe mean of the leaf labels is the regression value, and \\(H\\) is mean square error from the prediction.\nThe median of the leaf labels is the regression value, and \\(H\\) is the mean absolute error from the prediction.\n\n\nExample 5.17 Suppose we are given the observations \\(x_i=i\\), \\(i=1,\\ldots,4\\), where \\(y_1=2\\), \\(y_2=-1\\), \\(y_3=1\\), \\(y_4=0\\). Let’s find the decision tree regressor using medians.\nThe original value set has median \\(\\frac{1}{2}\\), so its MAE is \\[\n\\frac{1}{4} \\left( \\abs{2-\\tfrac{1}{2}} +  \\abs{{-1}-\\tfrac{1}{2}} + \\abs{1-\\tfrac{1}{2}} + \\abs{0-\\tfrac{1}{2}}  \\right) = \\frac{1}{4}\\cdot \\frac{8}{2} = 1.\n\\] Thus, the \\(Q\\)-score of the full set is 4.\nThere are three allowable ways to split the data, as it is ordered from left to right:\n\n\\(S=\\{2\\},\\,T=\\{-1,1,0\\}\\). Note that \\(\\abs{S}=1\\) and \\(\\abs{T}=3\\), so \\[\n\\begin{split}\nQ &= 1\\left[ \\frac{1}{1} |2-2|  \\right] +  3 \\left[ \\frac{1}{3}\\bigl( | -1-0 | + |1-0| + |0-0|  \\bigr)  \\right]\\\\\n&=  0 + 2 = 2.\n\\end{split}\n\\]\n\\(S=\\{2,-1\\},\\, T=\\{1,0\\}\\) \\[\n\\begin{split}\n  Q &= 2\\left[ \\frac{1}{2}\\bigl( \\left| 2-\\tfrac{1}{2} \\right| + \\left| -1-\\tfrac{1}{2} \\right| \\bigr)  \\right] +  2 \\left[ \\frac{1}{2}\\bigl( \\left|1-\\tfrac{1}{2} \\right| + \\left|0-\\tfrac{1}{2} \\right|  \\bigr)  \\right]\\\\\n  &=  3 + 1 = 4.\n\\end{split}\n\\]\n\\(S=\\{2,-1,1\\},\\, T=\\{0\\}\\) \\[\n\\begin{split}\n  Q &= 3\\left[ \\frac{1}{3}\\bigl( \\left| 2-1 \\right| + \\left| -1-1 \\right|+ |1-1| \\bigr)  \\right] +  1 \\left[ \\frac{1}{1} \\left|0-0 \\right|  \\right]\\\\\n  &=  3 + 0 = 3.\n\\end{split}\n\\]\n\nThus, the first split above produces the smallest Q, and it improves on the orignal set.\n\n\n\n\n\n\n\n\n\n\nExample 5.18 Here is some simple 2D data:\n\n\nCode\nrng = default_rng(1)\nx1 = rng.random((10,2))\nx1[:,0] -= 0.25\nx2 = rng.random((10,2))\nx2[:,0] += 0.25\nX = np.vstack((x1,x2))\ny = np.exp( X[:,0]-2*X[:,1]**2+X[:,0]*X[:,1] )\n\ndf = pd.DataFrame({\"x₁\": X[:,0], \"x₂\": X[:,1], \"y\": y})\nsns.scatterplot(data=df, x=\"x₁\", y=\"x₂\", hue=\"y\");\n\n\n\n\n\nThe default in sklearn is to use means on leaves and MSE (called squared_error in sklearn) as the quality measure. Here is a shallow tree fitted to the data:\n\nfrom sklearn.tree import DecisionTreeRegressor, plot_tree\ndtree = DecisionTreeRegressor(max_depth=2)\ndtree.fit(X, y)\n\nDecisionTreeRegressor(max_depth=2)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeRegressorDecisionTreeRegressor(max_depth=2)\n\n\n\n\nCode\nfrom matplotlib.pyplot import figure\nfigure(figsize=(17,10), dpi=160)\nplot_tree(dtree,feature_names=[\"x₁\",\"x₂\"]);\n\n\n\n\n\nAll of the original samples end up in one of the four leaves. We can find out the tree node number that each sample ends up at using apply:\n\nleaf = dtree.apply(X)\nprint(leaf)\n\n[3 3 2 5 2 2 3 2 2 2 5 6 5 5 3 5 6 6 2 5]\n\n\nFrom the above we deduce that the leaves are the nodes numbered 2, 3, 5, and 6. With some pandas grouping, we can find out the mean value for the samples within each of these:\n\nleaves = pd.DataFrame( {\"y\": y, \"leaf\": leaf} )\nleaves.groupby(\"leaf\")[\"y\"].mean()\n\nleaf\n2    0.911328\n3    0.270725\n5    2.378427\n6    1.003786\nName: y, dtype: float64\n\n\nAll values of the regressor will be one of the four values above. This is exactly what is done internally by the predict method of the regressor:\n\nprint( dtree.predict(X) )\n\n[0.27072468 0.27072468 0.91132782 2.37842709 0.91132782 0.91132782\n 0.27072468 0.91132782 0.91132782 0.91132782 2.37842709 1.00378567\n 2.37842709 2.37842709 0.27072468 2.37842709 1.00378567 1.00378567\n 0.91132782 2.37842709]\n\n\n\n\n\n\n\n\n\n\n\n\nExample 5.19 Continuing with the data from Example 5.16, we find that we can do even better with a random forest of decision tree regressors:\n\nX = cars[features]\ny = cars[\"mpg\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2, random_state=302\n    )\n\nfrom sklearn.ensemble import RandomForestRegressor\n\ngrid = {\n    \"max_depth\": range(3, 8),\n    \"max_samples\": np.arange(0.2, 0.6, 0.1),\n    }\nknn = RandomForestRegressor(n_estimators=60)\noptim = GridSearchCV(\n    knn,\n    grid, \n    cv=kf, \n    n_jobs=-1\n    )\noptim.fit(X_train, y_train)\n\nprint(f\"best forest CoD: {optim.score(X_test, y_test):.4f}\")\n\nbest forest CoD: 0.7297"
  },
  {
    "objectID": "regression.html#logistic-regression",
    "href": "regression.html#logistic-regression",
    "title": "5  Regression",
    "section": "5.5 Logistic regression",
    "text": "5.5 Logistic regression\nSometimes a regressed value is subject to certain known bounds or other conditions. A major example is probability, which has to be between 0 and 1 (inclusive).\nA linear regressor, \\(\\hat{f}(\\bfx) = \\bfw^T \\bfx\\) for a constant vector \\(\\bfw\\), typically ranges over all of \\((-\\infty,\\infty)\\). In order to get a result that must lie within \\([0,1]\\), we can transform its output.\n\nDefinition 5.9 The logistic function is \\[\n\\sigma(x) = \\frac{1}{1+e^{-x}},\n\\] defined for all real values of \\(x\\).\n\n\n\n\n\n\n\n\n\nThe logistic function takes the form of a smoothed step increasing from 0 to 1:\n\n\n\n\n\nGiven samples of a probability variable \\(p(\\bfx)\\), the regression task is to find a weight vector \\(\\bfw\\) so that \\[\np \\approx \\sigma(\\bfx^T\\bfw).\n\\] The result is known as logistic regression. A common way to use logistic regression is for binary classification. Suppose we have training samples \\((\\bfx_i, y_i)\\), \\(i=1,\\ldots,n\\), where for each \\(i\\) either \\(y_i=0\\) or \\(y_i=1\\). The resulting approximation to \\(p\\) at some query \\(\\bfx\\) can then be interpreted as the probability of observing a 1 at \\(\\bfx\\).\nIn order to fully specify the regressor, we need to specify a loss function to be optimized.\n\n5.5.1 Loss function\nDefining \\(\\hat{p}_i = \\sigma(\\bfx_i^T\\bfw)\\) at all the training points, a straightforward loss function would be \\[\n\\sum_{i=1}^n \\left( \\hat{p}_i - y_i \\right)^2.\n\\]\n\n\n\n\n\n\n\n\nHowever, it’s more common to use the cross-entropy loss function \\[\nL(\\bfw) = -\\sum_{i=1}^n \\left[ y_i \\log(\\hat{p}_i) + (1-y_i) \\log(1-\\hat{p}_i) \\right].\n\\tag{5.9}\\] (The logarithms in Equation 5.9 can be in any base, since that choice only affects \\(L\\) by a constant factor.) In cross-entropy loss, sample \\(i\\) contributes \\[\n-\\log(1-\\hat{p}_i)\n\\] if \\(y_i=0\\), which becomes infinite as \\(\\hat{p}_i\\to 1^-\\), and \\[\n-\\log(\\hat{p}_i)\n\\] if \\(y_i=1\\), which becomes infinite as \\(\\hat{p}_i\\to 0^+\\). Thus, there is a steep penalty for being almost completely wrong about an observation.\n\nExample 5.20 Suppose we have true labels \\(\\bfy=[0,0,0,1]\\) and predicted probabilities \\(\\hat{\\mathbf{p}} = [0.1,0.4,0.2,0.3].\\) This gives the following terms in the cross-entropy sum: \\[\n\\begin{split}\n\\cancel{y_1 \\log(\\hat{p}_1)} + (1-y_1) \\log(1-\\hat{p}_1) &= \\log(0.9), \\\\\n\\cancel{y_2 \\log(\\hat{p}_2)} + (1-y_2) \\log(1-\\hat{p}_2) &= \\log(0.6), \\\\\n\\cancel{y_3 \\log(\\hat{p}_3)} + (1-y_3) \\log(1-\\hat{p}_3) &= \\log(0.8), \\\\\ny_4 \\log(\\hat{p}_4) + \\cancel{(1-y_4) \\log(1-\\hat{p}_4)} &= \\log(0.3).\\\\\n\\end{split}\n\\]\nHence the total cross-entropy is \\[\n-\\log\\bigl[ (0.9)(0.6)(0.8)(0.3)\\bigr] = -\\log(0.1296).\n\\] Using the natural log, this is about \\(2.043\\).\n\nLogistic regression does have a major disadvantage compared to linear regression: the minimization of loss does not lead to a linear problem for the weight vector \\(\\bfw\\). The difference in practice is usually not a concern, though.\nCross-entropy is the default loss function for a LogisticRegression in scikit-learn. In Example 5.21, we use logistic regression on a binary classification problem, where we interpret the regression output as the probability of being in class 1, as opposed to class 0.\n\nExample 5.21 We will try logistic regression for a simple spam filter. The data set is based on work and personal emails for one individual. The features are calculated word and character frequencies, as well as the appearance of capital letters.\n\nspam = pd.read_csv(\"_datasets/spambase.csv\")\nspam.head()\n\n\n\n\n\n\n\n\nword_freq_make\nword_freq_address\nword_freq_all\nword_freq_3d\nword_freq_our\nword_freq_over\nword_freq_remove\nword_freq_internet\nword_freq_order\nword_freq_mail\n...\nchar_freq_%3B\nchar_freq_%28\nchar_freq_%5B\nchar_freq_%21\nchar_freq_%24\nchar_freq_%23\ncapital_run_length_average\ncapital_run_length_longest\ncapital_run_length_total\nclass\n\n\n\n\n0\n0.00\n0.64\n0.64\n0.0\n0.32\n0.00\n0.00\n0.00\n0.00\n0.00\n...\n0.00\n0.000\n0.0\n0.778\n0.000\n0.000\n3.756\n61\n278\n1\n\n\n1\n0.21\n0.28\n0.50\n0.0\n0.14\n0.28\n0.21\n0.07\n0.00\n0.94\n...\n0.00\n0.132\n0.0\n0.372\n0.180\n0.048\n5.114\n101\n1028\n1\n\n\n2\n0.06\n0.00\n0.71\n0.0\n1.23\n0.19\n0.19\n0.12\n0.64\n0.25\n...\n0.01\n0.143\n0.0\n0.276\n0.184\n0.010\n9.821\n485\n2259\n1\n\n\n3\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n...\n0.00\n0.137\n0.0\n0.137\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n4\n0.00\n0.00\n0.00\n0.0\n0.63\n0.00\n0.31\n0.63\n0.31\n0.63\n...\n0.00\n0.135\n0.0\n0.135\n0.000\n0.000\n3.537\n40\n191\n1\n\n\n\n\n5 rows × 58 columns\n\n\n\nThe labels in this case are 0 for “ham” (not spam) and 1 for “spam”. We create a feature matrix and label vector, and split into train/test sets:\n\nX = spam.drop(\"class\", axis=\"columns\")\ny = spam[\"class\"]\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.2,\n    shuffle=True, random_state=19716\n    )\n\nWe fit a logistic regression just like any other learner. (The meaning of the argument penalty in the call below is explained in Section 5.5.2.)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(penalty=\"none\")\nlogreg.fit(X_train, y_train)\n\nLogisticRegression(penalty='none')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(penalty='none')\n\n\nDespite the name, though, scikit-learn treats a LogisticRegression object like a classifier, and its preductions are true/false:\n\nlogreg.predict( X_test.iloc[:5,:] )\n\narray([1, 0, 0, 0, 0])\n\n\nThe default scoring function is classification accuracy:\n\nacc = logreg.score(X_test, y_test)\nprint(f\"spam classification accuracy is {acc:.2%}\")\n\nspam classification accuracy is 93.59%\n\n\nThe actual logistic outputs are the probabilistic predictions:\n\nlogreg.predict_proba( X_test.iloc[:5,:] )\n\narray([[4.65419538e-01, 5.34580462e-01],\n       [9.87165442e-01, 1.28345580e-02],\n       [5.58611611e-01, 4.41388389e-01],\n       [7.54138243e-01, 2.45861757e-01],\n       [1.00000000e+00, 3.21186479e-12]])\n\n\nFrom these, we could use ROC and AUC metrics as we did in Section 3.5.\nLet’s repeat the computation with a standardization pipeline, so that we can easily compare the magnitudes of the weights in the coefficient vector:\n\npipe = make_pipeline(StandardScaler(), logreg)\npipe.fit(X_train, y_train)\n\nweights = pd.Series( logreg.coef_[0], index=X.columns) \nweights = weights.sort_values()\n\nprint(\"most hammy features:\")\nprint(weights[:4])\nprint()\nprint(\"most spammy features:\")\nprint(weights[-4:])\n\nmost hammy features:\nword_freq_george    -30.595550\nword_freq_cs        -12.567214\nword_freq_hp         -3.609056\nword_freq_meeting    -1.606267\ndtype: float64\n\nmost spammy features:\nword_freq_remove              0.864465\nchar_freq_%24                 1.104584\ncapital_run_length_longest    1.759774\nword_freq_3d                  4.509272\ndtype: float64\n\n\n\n\n\n\n\n\nCaution\n\n\n\nFor a LinearRegression and its regularized siblings, the coef_ property of a trained learner is a vector of coefficients, while for LogisticRegression, it is a matrix with one row. The reason is that logistic regression is often applied to a multiclass problem, and there is one row for each version of the one-vs-rest fit.\n\n\nWe see above that the word “george” is a strong counter-indicator for spam; remember that this data set comes from an individual’s inbox. Its presence makes the inner product \\(\\bfx^T\\bfw\\) more negative, which drives the logistic function closer to 0. Conversely, the presence of consecutive capital letters increases the inner product and pushes the probability of spam closer to 1.\n\n\n\n\n\n\n\n\n\n\n\n5.5.2 Regularization\nAs with other forms of regression, the loss function may be regularized using the ridge or LASSO penalty. The standard formulation is\n\\[\n\\widetilde{L}(\\bfw) = C \\, L(\\bfw) + \\norm{\\bfw},\n\\tag{5.10}\\]\nwhere the vector norm is either the 1-norm (LASSO style) or 2-norm (ridge style). In either case, \\(C\\) is a positive hyperparameter. For \\(C\\) close to zero, the regression is mainly concerned with the penalty term, and as \\(C\\) increases, the regression gradually pays more and more attention to the data, at the expense of the penalty term.\n\n\n\n\n\n\nImportant\n\n\n\nThe parameter \\(C\\) functions like the inverse of the regularization parameter \\(\\alpha\\) we used in the linear regressor. It’s because of a different convention chosen historically.\n\n\n\nExample 5.22 We will continue Example 5.21 with an exploration of regularization. When using norm-based regularization, it’s good practice to standardize the variables, so we will use a standardization pipeline:\nIn Example 5.21, we disabled regularization. The default call uses 2-norm regularization with \\(C=1\\), which, in this case, improves the accuracy a bit:\n\n\n\n\n\n\nTip\n\n\n\nThe use of the solver keyword argument in LogisticRegression is optional, but the default choice does not work with the LASSO-style penalty term.\n\n\n\nlogreg = LogisticRegression(solver=\"liblinear\")\nlogreg.fit(X_train, y_train)\nacc = logreg.score(X_test, y_test)\nprint(f\"accuracy with default regularization is {acc:.2%}\")\n\naccuracy with default regularization is 93.81%\n\n\nThe regularization parameter in a LogisticRegression can be set using the C= keyword:\n\nlogreg = LogisticRegression(solver=\"liblinear\", C=0.5)\nlogreg.fit(X_train, y_train)\nacc = logreg.score(X_test, y_test)\nprint(f\"accuracy with C=0.5 is {acc:.2%}\")\n\naccuracy with C=0.5 is 93.92%\n\n\nA validation-based grid search is one way to look for the optimal regularization. Usually, we want the grid values of \\(C\\) to be spaced exponentially, not equally. For example,\n\n10 ** np.linspace(-2, 2, 13)\n\narray([1.00000000e-02, 2.15443469e-02, 4.64158883e-02, 1.00000000e-01,\n       2.15443469e-01, 4.64158883e-01, 1.00000000e+00, 2.15443469e+00,\n       4.64158883e+00, 1.00000000e+01, 2.15443469e+01, 4.64158883e+01,\n       1.00000000e+02])\n\n\nWe use that strategy along with a search over both the 2-norm and the 1-norm for the regularization penalty term:\n\ngrid = { \"logisticregression__C\": 10 ** np.linspace(-1, 4, 40), \n         # ridge and LASSO cases:\n         \"logisticregression__penalty\": [\"l2\", \"l1\"]   \n        }\n\nlearner = make_pipeline(\n    StandardScaler(),\n    LogisticRegression( solver=\"liblinear\" )\n    )\n\nkf = StratifiedKFold(n_splits=6, shuffle=True, random_state=302)\n\nsearch = GridSearchCV(\n    learner, grid, \n    cv=kf,\n    n_jobs=-1\n    )\n\nsearch.fit(X_train, y_train)\n\nprint(\"Best parameters:\")\nprint(search.best_params_)\nprint()\nprint(f\"Best score is {search.best_score_:.2%}\")\n\nBest parameters:\n{'logisticregression__C': 1.0608183551394483, 'logisticregression__penalty': 'l1'}\n\nBest score is 92.55%\n\n\nIn this case, we failed to improve on the default regularization. (Recall that cross-validation means that each learner is trained on less data, so the grid metrics might not be completely accurate.)\n\n\n\n\n\n\n\n\n\n\n\n5.5.3 Multiclass classification\nWhen there are more than two unique labels possible in a classification, logistic regression can be extended through the one-vs-rest (OVR) paradigm we have used previously. Given \\(K\\) classes, there are \\(K\\) binary regressors fit for the outcomes “class 1/not class 1,” “class 2/not class 2,” and so on. These give \\(K\\) different weight vectors, \\(\\bfw_1,\\ldots,\\bfw_K\\).\nFor a query vector \\(\\bfx\\), we can predict the probabilities for it being in each class:\n\\[\n\\hat{q}_{k}(\\bfx) = \\sigma(\\bfx^T \\bfw_k), \\qquad k=1,\\ldots,K.\n\\]\nHowever, since the regressions were performed separately, there is no reason to think the \\(q_k\\) are probabilities that sum to 1 over all the classes. So we must normalize them:\n\\[\n\\hat{p}_{k}(\\bfx) = \\frac{\\hat{q}_{k}(\\bfx)}{\\sum_{k=1}^K \\hat{q}_{k}(\\bfx)}.\n\\]\nNow we can interpret \\(\\hat{p}_{k}(\\bfx)\\) as the probability of \\(\\bfx\\) belonging to class \\(k\\).\n\nExample 5.23 Suppose we have \\(d=2\\) features and three classes, and that the three logistic regressions gave the weight vectors \\[\n\\bfw_1 = [-2,0], \\qquad \\bfw_2=[1,1], \\qquad \\bfw_3=[0,1].\n\\]\nFor the query \\(\\bfx=[1,0]\\), we get the predictions \\[\n\\begin{split}\n\\hat{q}_1 &= \\sigma(\\bfx^T\\bfw_1) =  \\sigma\\bigl((1)(-2)+(0)(0)\\bigr) \\approx 0.11920, \\\\\n\\hat{q}_2 &= \\sigma(\\bfx^T\\bfw_2) = \\sigma\\bigl((1)(1)+(0)(1)\\bigr) \\approx 0.73106, \\\\\n\\hat{q}_3 &= \\sigma(\\bfx^T\\bfw_3) = \\sigma\\bigl((1)(0)+(0)(1)\\bigr) = 0.5.\n\\end{split}\n\\]\nSo, if we ask, “How much does this \\(\\bfx\\) look like class 1?” the answer is, “11.9%”, for “How much like class 2?” it’s “73.1%”, and so on. Since there are three exclusive options for the class of \\(\\bfx\\), the probabilities assigned to the classes are \\[\n\\begin{split}\n\\hat{p}_1 &= \\frac{\\hat{q}_1}{\\hat{q}_1 + \\hat{q_2} + \\hat{q_3}}  \\approx 0.08828, \\\\\n\\hat{p}_2 &= \\frac{\\hat{q}_2}{\\hat{q}_1 + \\hat{q_2} + \\hat{q_3}}  \\approx 0.54142, \\\\\n\\hat{p}_3 &= \\frac{\\hat{q}_3}{\\hat{q}_1 + \\hat{q_2} + \\hat{q_3}} \\approx 0.37030.\n\\end{split}\n\\] These probabilities, in exact arithmetic, add up to 100%.\n\nThe situation is now the same as for probabilistic classification in Section 3.5. Over a testing set, we get a matrix of probabilities. Each of the rows gives the class probabilities at a single query point. We can simply select the most likely class at each test query, or use ROC and AUC to better understand the probabilistic results.\n\nExample 5.24 As a multiclass example, we return to the dataset for classifying forest cover:\n\nforest = datasets.fetch_covtype()\nX = forest[\"data\"][:250000,:12]   # 250,000 samples, 12 features\ny = forest[\"target\"][:250000]\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y,\n    test_size=0.15, \n    shuffle=True, random_state=302\n)\n\nlogr = LogisticRegression(solver=\"liblinear\")\npipe = make_pipeline(StandardScaler(), logr)\npipe.fit(X_train, y_train)\nprint(f\"accuracy score is {pipe.score(X_test, y_test):.2%}\")\n\naccuracy score is 72.74%\n\n\nWe can now look at probabilistic predictions for each class:\n\np_hat = pipe.predict_proba(X_test)\np_hat[:3]\n\narray([[3.27753115e-01, 6.71344154e-01, 8.83821740e-06, 1.47289531e-05,\n        6.20692039e-04, 2.57492056e-04, 9.79562864e-07],\n       [4.20545770e-01, 5.66374061e-01, 2.90422085e-06, 4.50982065e-07,\n        1.40842869e-03, 8.15605577e-06, 1.16602291e-02],\n       [2.40111415e-01, 7.59067843e-01, 1.21609701e-05, 1.42404726e-05,\n        3.93333619e-04, 3.98538413e-04, 2.46805163e-06]])\n\n\nThe ROC curves show that the performance is much worse for 3 of the classes than for the others:\n\nresults = []\nfor i, label in enumerate(pipe.classes_):\n    actual = (y_test==label)\n    fp, tp, theta = roc_curve(actual, p_hat[:,i])\n    results.extend( [ (label,fp,tp) for fp,tp in zip(fp,tp) ] )\n\nroc = pd.DataFrame( results, columns=[\"label\", \"FP rate\", \"TP rate\"] )\nsns.relplot(data=roc, \n    x=\"FP rate\", y=\"TP rate\", \n    hue=\"label\", kind=\"line\", estimator=None,\n    palette=\"Dark2\"\n    );"
  },
  {
    "objectID": "regression.html#exercises",
    "href": "regression.html#exercises",
    "title": "5  Regression",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 5.1 (5.1) Suppose that the distinct plane points \\((x_i,y_i)\\) for \\(i=1,\\ldots,n\\) are to be fit using a linear function without intercept, \\(\\hat{f}(x)=w x\\). Use calculus to find a formula for the value of \\(w\\) that minimizes the sum of squared residuals, \\[ r = \\sum_{i=1}^n \\bigl(y_i - \\hat{f}(x_i)\\bigr)^2. \\]\n\n\n\nExercise 5.2 (5.1) Using the formulas derived in Section 5.1, show that the point \\((\\bar{x},\\bar{y})\\) always lies on the linear regression line. (Hint: You only have to show that \\(\\hat{f}(\\bar{x}) = \\bar{y}\\), which can be done without first solving for \\(a\\) and \\(b\\).)\n\n\nExercise 5.3 (5.1) Suppose that \\[\n\\begin{split}\n    \\bfx &= [-2, 0, 1, 3] \\\\\n    \\bfy &= [4, 1, 2, 0].\n\\end{split}\n\\] Find the (a) MSE, (b) MAE, and (c) coefficient of determination on this set for the regression function \\(\\hat{f}(x)=1-x\\).\n\n\nExercise 5.4 (5.2) Suppose for \\(d=3\\) features you have the \\(n=4\\) sample vectors \\[\n\\bfx_1 = [1,0,1], \\quad \\bfx_2 = [-1,2,2],\\quad \\bfx_3=[3,-1,0], \\quad \\bfx_4 = [0,2,-2],\n\\] and a multilinear regression computes the weight vector \\(\\bfw = [2,1,-1]\\). Find (a) the matrix-vector product \\(\\bfX\\bfw\\), and (b) the predictions of the regressor on the sample vectors.\n\n\nExercise 5.5 (5.2) Suppose that values \\(y_i\\) for \\(i=1,\\ldots,n\\) are to be fit to 2D sample vectors using a multilinear regression function \\(\\hat{f}(\\bfx)=w_1 x_1 + w_2 x_2\\). Define the sum of squared residuals \\[\nr = \\sum_{i=1}^n \\bigl(y_i - \\hat{f}(\\bfx_i)\\bigr)^2.\n\\] Show that by holding \\(w_1\\) constant and taking a derivative with respect to \\(w_2\\), and then holding \\(w_2\\) constant and taking a derivative with respect to \\(w_1\\), at the minimum residual we must have \\[\n\\begin{split}\n\\left(\\sum X_{i,1}^2 \\right) w_1 + \\left(\\sum X_{i,1} X_{i,2}  \\right) w_2 &= \\sum X_{i,1}\\, y_i, \\\\\n\\left(\\sum X_{i,1} X_{i,2} \\right) w_1 + \\left(\\sum X_{i,2}^2 \\right) w_2 &= \\sum X_{i,2} \\, y_i,\n\\end{split}\n\\] where \\(X_{i,1}\\) and \\(X_{i,2}\\) are the entries in the \\(i\\)th row of the feature matrix \\(\\bfX\\). (In each case above the sum is from \\(i=1\\) to \\(i=n\\).)\n\n\nExercise 5.6 (5.3) If we fit the model \\(\\hat{f}(x)=w x\\) to the single data point \\((2,6)\\), then the ridge loss is \\[\nr(w) = (2w-6)^2 + \\alpha w^2,\n\\] where \\(\\alpha\\) is a nonnegative constant. When \\(\\alpha = 0\\), it’s clear that \\(w=3\\) is the minimizer of \\(r(w)\\). Show that if \\(\\alpha&gt;0\\), then \\(r'(w)\\) is zero at a value of \\(w\\) in the interval \\((0,3)\\). (This shows that the weight decreases in the presence of the regularization penalty.)\n\n\nExercise 5.7 (5.3) If we fit the model \\(\\hat{f}(x)=w x\\) to the single data point \\((2,6)\\), then the LASSO loss is \\[\nr(w) = (2w-6)^2 + \\alpha |w|,\n\\] where \\(\\alpha\\) is a nonnegative constant. When \\(\\alpha = 0\\), it’s clear that \\(w=3\\) is the minimizer of \\(r(w)\\). Below you will show that the minimizer is less than this if \\(\\alpha &gt; 0\\).\n(a) Show that if \\(w &lt; 0\\), then \\(r'(w)\\) cannot be zero. (Remember that for such \\(w\\), \\(|w|=-w\\).)\n(b) Show that if \\(w&gt;0\\) and \\(0&lt;\\alpha &lt; 24\\), then \\(r'(w)\\) has a single root in the interval \\((0,3)\\).\n\n\n\nExercise 5.8 (5.4) For each function on two-dimensional vectors, either prove that it is linear or produce a counterexample that shows it cannot be linear.\n(a) \\(\\hat{f}(\\bfx) = x_1 x_2\\)\n(b) \\(\\hat{f}(\\bfx) = x_2\\)\n(c) \\(\\hat{f}(\\bfx) = x_1 + x_2 + 1\\)\n\n\nExercise 5.9 (5.4) Given the data set \\((x_i,y_i)=\\{(0,-1),(1,1),(2,3),(3,0),(4,3)\\}\\), find the MAE-based \\(Q\\) score for the following hypothetical decision tree splits.\n(a) \\(x \\le 0.5, \\qquad\\) (b) \\(x \\le 1.5, \\qquad\\) (c) \\(x \\le 2.5,\\qquad\\) (d) \\(x \\le 3.5\\).\n\n\nExercise 5.10 (5.4) Here are values (labels) on an integer lattice.\n\nLet \\(\\hat{f}(x_1,x_2)\\) be the kNN regressor using \\(k=4\\), Euclidean metric, and mean averaging. In each case below, a function \\(g(t)\\) is defined from values of \\(\\hat{f}\\) along a vertical or horizontal line. Carefully sketch a plot of \\(g(t)\\) for \\(-2\\le t \\le 2\\).\n(a) \\(g(t) = \\hat{f}(1.2,t)\\)\n(b) \\(g(t) = \\hat{f}(t,-0.75)\\)\n(c) \\(g(t) = \\hat{f}(t,1.6)\\)\n(d) \\(g(t) = \\hat{f}(-0.25,t)\\)\n\n\nExercise 5.11 (5.5) Here are some label values and probabilistic predictions by a logistic regressor: \\[\n\\begin{split}\n    \\bfy  &= [0,0,1,1], \\\\\n    \\hat{\\mathbf{p}} &= [\\tfrac{3}{4},0,1,\\tfrac{1}{2}].\n\\end{split}\n\\] Using base-2 logarithms, calculate the cross-entropy loss for these predictions.\n\n\nExercise 5.12 (5.5) Let \\(\\bfx=[-1,0,1]\\) and \\(\\bfy=[0,1,0]\\). This small dataset is fit to a probabilistic predictor \\(\\hat{p}(x) = \\sigma(w x)\\) for weight \\(w\\).\n(a) Let \\(L(w)\\) be the cross-entropy loss function using natural logarithms. Show that \\[\nL'(w) = \\frac{e^w-1}{e^w+1}.\n\\]\n(b) Explain why part (a) implies that \\(w=0\\) is the global minimizer of the loss \\(L\\).\n(c) Using the result of part (b), simplify the optimum predictor function \\(\\hat{p}(x)\\).\n\n\nExercise 5.13 (5.5) Let \\(\\bfx=[-1,1]\\) and \\(\\bfy=[0,1]\\). This small dataset is fit to a probabilistic predictor \\(\\hat{p}(x) = \\sigma(w x)\\) for weight \\(w\\). Without regularization, the best fit takes \\(w\\to\\infty\\), which makes the predictor become infinitely steep at \\(x=0\\). To combat this behavior, let \\(L\\) be the cross-entropy loss function with LASSO penalty, i.e.,\n\\[\nL(w) = \\ln[1-\\hat{p}(-1)] - \\ln[\\hat{p}(1)] + \\alpha |w|,\n\\]\nfor a positive regularization constant \\(\\alpha\\).\n(a) Show that \\(L'\\) is never zero for \\(w &lt; 0\\).\n(b) Show that if \\(0 &lt;\\alpha &lt;1\\), then \\(L'\\) has a zero at\n\\[\nw = \\ln\\left( \\frac{2}{\\alpha}-1 \\right).\n\\]\n(c) Show that \\(w\\) from part (b) is a decreasing function of \\(\\alpha\\). (Therefore, increasing \\(\\alpha\\) makes the predictor less steep as a function of \\(x\\).)"
  },
  {
    "objectID": "clustering.html#similarity",
    "href": "clustering.html#similarity",
    "title": "6  Clustering",
    "section": "6.1 Similarity",
    "text": "6.1 Similarity\nIdeally, samples within a cluster are more similar to each other than they are to samples in other clusters. The first decision we have to make is how to define similarity between an arbitrary pair.\nWhen a distance metric is available, we intuitively expect similarity to be inversely related to distance. There are various ways to quantify the relationship, but we will not use them. Instead, we will take “maximize similarity” to be equivalent to “minimize distance.”\n\n\n6.1.1 Distance matrix\n\nDefinition 6.2 Given the feature vectors \\(\\bfX_1,\\ldots,\\bfX_n\\), the pairwise distances between them are collected in the \\(n\\times n\\) distance matrix\n\\[\nD_{ij} = \\text{dist}(\\bfX_i,\\bfX_j).\n\\]\n\n\n\n\n\n\n\n\n\nNote that \\(D_{ii}=0\\) and \\(D_{ji}=D_{ij}\\).\n\nExample 6.1 Using 1-norm for distance, find the distance matrix for the feature matrix \\[\n\\bfX = \\begin{bmatrix}\n1 & 2 & 1 & -2 \\\\ 0 & 3 & 3 & 1 \\\\ 1 & -1 & 0 & 4\n\\end{bmatrix}.\n\\]\n\nSolution. There are \\(n=3\\) feature vectors. \\[\n\\begin{split}\nD_{12} &=  \\abs{1} + \\abs{-1} + \\abs{-2} + \\abs{-3} = 7 \\\\\nD_{13} &=  \\abs{0} + \\abs{3} + \\abs{1} + \\abs{-6}  = 10 \\\\\nD_{23} &=  \\abs{-1} + \\abs{4} + \\abs{3} + \\abs{-3}  = 11.\n\\end{split}\n\\] Hence, \\[\n\\mathbf{D} = \\begin{bmatrix}\n0 & 7 & 10 \\\\ 7 & 0 & 11 \\\\ 10 & 11 & 0\n\\end{bmatrix}.\n\\]\n\n\n\n\n\n\n\n\nTip\n\n\n\nMany clustering algorithms allow supplying \\(\\mathbf{D}\\) in lieu of the feature vectors. When you consider that in many problems, \\(n\\) is much larger than \\(d\\), it can be faster and easier to work with.\n\n\n\n\n\n\n\n\nNote\n\n\n\nA practical advantage of working with similarity rather than distance is that small values of similarity can be rounded down to zero. Such rounding has negligible effect on the results, but it can create big gains in execution time and memory usage.\n\n\nThe scikit-learn function pairwise_distances computes a distance matrix efficiently.\n\nExample 6.2 The distance matrix of our bullseye dataset has some interesting structure:\n\nfrom sklearn.metrics import pairwise_distances\nX, y = bullseye_data()\nD2 = pairwise_distances(X, metric=\"euclidean\")   # use 2-norm metric\nax = sns.heatmap(D2)\nax.set_aspect(1);\n\n\n\n\nBecause we set up three geometrically distinct groups of points, the distances of pairs within and between groups are fairly homogeneous. The lower-right corner, for example, shows that points in the outermost ring tend to be separated by the greatest distance.\nIn the 1-norm, the stripes dataset is also interesting:\n\nX, y = stripes_data()\nD1 = pairwise_distances(X, metric=\"manhattan\")   # use 1-norm metric\nax = sns.heatmap(D1)\nax.set_aspect(1);\n\n\n\n\nPoints in different stripes are always separated by at least the inter-stripe distance, while points within the same stripe have a range of possible distances.\n\n\n\n6.1.2 Angular distance\nAn alternative to vector norms for distances is angular distance. In three dimensions, imagine drawing a ray from the origin to each of the two given vectors. Those rays lie in a common plane, and we can let \\(\\theta\\) be the (smallest) angle between them. The general formula in any number of dimensions is \\[\n\\cos(\\theta) = \\frac{\\mathbf{u}^T\\mathbf{v}}{\\twonorm{\\mathbf{u}} \\, \\twonorm{\\mathbf{v}}}.\n\\tag{6.1}\\]\n\n\n\n\n\n\n\n\nWe then let \\(\\dist(\\bfx,\\bfy)=\\theta \\in [0,\\pi]\\), using the standard branch of arccos.\nHowever, arccos is a relatively expensive computational operation, and sometimes we use the pseudodistance \\[\nd(\\bfx,\\bfy) = 1 - \\cos(\\theta)\n\\] in place of \\(\\theta\\). This function (which ranges between 0 and 2) is not a true distance function, though, as \\(d\\) fails to satisfy the triangle inequality in all cases (see Exercise 6.2).\nAngular distance and cosine pseudodistance are useful when we want to ignore the magnitudes of vectors and consider only their directions.\n\nExample 6.3 We might represent a text document by a vector of the number of occurrences of certain keywords. In sentiment analysis, these would be terms such as happy, excited, sad, angry, confused, and so on. For sake of argument, suppose three documents have the feature matrix \\[\n\\bfX =\n\\begin{bmatrix}\n6 & 1 & 10 & 2 & 5 \\\\  14 & 0 & 23 & 3 & 7 \\\\ 2 & 3 & 1 & 5 & 0\n\\end{bmatrix}.\n\\] Using the 2-norm, we get the distances:\n\nX = np.array( [[6,1,10,2,5], [14,0,23,3,7], [2,3,1,5,0]] )\npairwise_distances( X, metric=\"euclidean\")\n\narray([[ 0.        , 15.45962483, 11.61895004],\n       [15.45962483,  0.        , 26.26785107],\n       [11.61895004, 26.26785107,  0.        ]])\n\n\nAccoding to this metric, the most similar documents are the first and the last. However, cosine distance tells a different story:\n\npairwise_distances( X, metric=\"cosine\")\n\narray([[0.        , 0.01532383, 0.56500757],\n       [0.01532383, 0.        , 0.62231412],\n       [0.56500757, 0.62231412, 0.        ]])\n\n\nNow the first two documents look most similar. Looking at the vectors, we see that these two have the most similar relative frequencies, which is related to what we want to measure. The apparent dissimilarity in the 2-norm arises solely because the total word counts are very different.\n\n\n\n\n\n6.1.3 Distance in high dimensions\nHigh-dimensional space does not conform to some intuitions formed by our experiences in 2D and 3D.\n\n\n\n\n\n\n\n\nFor example, consider the unit hyperball \\(\\twonorm{\\bfx}\\le 1\\) in \\(d\\) dimensions. We’ll take it as given that scaling a \\(d\\)-dimensional object by a number \\(r\\) will scale the volume by \\(r^d\\). Then for any \\(r&lt;1\\), the fraction of the unit hyperball’s volume lying outside the smaller hyperball of fixed radius \\(r\\) is \\(1-r^d\\), which approaches \\(1\\) as \\(d\\to \\infty\\). That is, if we choose points randomly within a hyperball, almost all of them will be near the outer boundary.\nThe volume of the unit hyperball also vanishes as \\(d\\to \\infty\\). This is because the inequality\n\\[\nx_1^2 + x_2^2 + \\cdots + x_d^2 \\le 1,\n\\]\nwhere each \\(x_i\\) is chosen randomly in \\([-1,1]\\), becomes ever harder to satisfy as the number of terms in the sum grows, and the relative occurrence of such points is increasingly rare.\nThere are other, similar mathematical results demonstrating the weirdness of distances in high-dimensional space. These results and their implications go under the colorful name of the curse of dimensionality. Using distances in high-dimensional space is difficult not just for clustering but in nearest-neighbor and other algorithms as well.\nIt turns out that in most practical problems, there is a way out of the mess: the data points are usually not uniformly or randomly distributed. That means that distance-based approaches can be made to work. Often, though, a first step is to find a way to reduce the dimensionality without changing the essential structure of the data, a step that is beyond what we go into here."
  },
  {
    "objectID": "clustering.html#performance-measures",
    "href": "clustering.html#performance-measures",
    "title": "6  Clustering",
    "section": "6.2 Performance measures",
    "text": "6.2 Performance measures\nBefore we start generating clusterings, we will discuss how to evaluate them. A clustering is essentially a partitioning of the samples into disjoint subsets. We will use some nonstandard terminology that makes the definitions a bit easier to state and read.\n\nDefinition 6.3 We say that two sample points in a clustering are buddies if they are in the same cluster, and strangers otherwise.\n\n\n6.2.1 Rand index and ARI\nA clustering can be interpreted as a classification, and vice versa. If a reference classification is available, then we can compare any clustering result to it. This allows us to use classification datasets as proving grounds for clustering.\n\nDefinition 6.4 We define the Rand index between two clusterings of \\(n\\) samples as\n\\[\n\\text{RI} = \\frac{b+s}{\\binom{n}{2}},\n\\tag{6.2}\\]\nwhere \\(b\\) is the number of pairs that are buddies in both clusterings and \\(s\\) is the number of pairs that are strangers in both clusterings.\n\n\n\n\n\n\n\n\n\nThe reason for the denominator in Equation 6.2 is that there are \\(\\binom{n}{2}\\) distinct pairs of \\(n\\) sample points, so we know that \\(0 \\le \\text{RI} \\le 1\\).\nThe Rand index has some attractive features:\n\nThe value is between 0 (complete disagreement) and 1 (complete agreement).\nIt is symmetric in the two clusterings; i. e., it doesn’t matter which is considered the reference.\nThere is no need to find a correspondence between the clusters in the two clusterings. In fact, the clusterings need not even have the same number of clusters.\n\n\nExample 6.4 Suppose that samples \\(\\bfX_1,\\bfX_2,\\bfX_4\\) are truly classified as positive, and \\(\\bfX_3,\\bfX_5\\) are truly classified as negative. This implies the reference clustering \\(C_1=\\{\\bfX_1,\\bfX_2,\\bfX_4\\}\\) and \\(C_2=\\{\\bfX_3,\\bfX_5\\}\\).\nCompute the Rand index relative to the reference of another clustering given by\n\\[\n\\widetilde{C}_1=\\{\\bfX_1,\\bfX_2\\}, \\quad \\widetilde{C}_2=\\{\\bfX_3\\}, \\quad \\widetilde{C}_3=\\{\\bfX_4,\\bfX_5\\}\n\\]\n\nSolution. Here is a table showing, for every pair of samples, the buddy–stranger status in the reference and in the new clustering:\n\n\n\n\n\\(\\bfX_1\\)\n\\(\\bfX_2\\)\n\\(\\bfX_3\\)\n\\(\\bfX_4\\)\n\\(\\bfX_5\\)\n\n\n\n\n\\(\\bfX_1\\)\n\nB/B\nS/S\nB/S\nS/S\n\n\n\\(\\bfX_2\\)\n\n\nS/S\nB/S\nS/S\n\n\n\\(\\bfX_3\\)\n\n\n\nS/S\nB/S\n\n\n\\(\\bfX_4\\)\n\n\n\n\nS/B\n\n\n\\(\\bfX_5\\)\n\n\n\n\n\n\n\n\nHence, we have \\(b=1\\) and \\(s=5\\), and the Rand index is 6/10 = 0.6.\n\n\nA weakness of the Rand index is that it can be fairly close to 1 even when samples are assigned to clusters randomly. The adjusted Rand index rescales the result by comparing it to how a random clustering would fare. It can be written as\n\\[\n\\text{ARI} = \\frac{\\text{RI} - \\E{\\text{RI}}}{\\text{max}(\\text{RI}) - \\E{\\text{RI}}},\n\\]\nwhere the expectation and max operations are performed over all possible clusterings. (These values can be worked out exactly using combinatorics, but we do not give them here.)\nAn ARI of 0 indicates no better agreement than a random clustering, while an ARI of 1 is complete agreement. Unlike the RI, the ARI value can be negative, indicating a performance worse than on average.\n\nExample 6.5 Our blobs dataset has a reference clustering shown in Figure 6.1 that we take to be the ground truth. Let’s create another clustering based entirely on the quadrants of the plane:\n\nX, y = blobs_data()\n\ndef quad(x):\n    if x[0] &gt; 0:\n        if x[1] &gt; 0: return 1\n        else: return 4\n    else:\n        if x[1] &gt; 0: return 2\n        else: return 3\n\n# find the quadrant for every sample\nq = X.apply(quad, axis=1)\nq.head()\n\n0    1\n1    1\n2    4\n3    2\n4    4\ndtype: int64\n\n\nBecause this toy dataset has only 2 features, it’s easy to visualize the clusters by plotting all the samples:\n\nsns.relplot(data=X, x=\"x1\", y=\"x2\", hue=q, palette=\"Dark2\");\n\n\n\n\nAs you see above, there are four clusters, divided along the coordinate axes. The reference clustering has only three classes, but we can still compare them with the adjusted Rand index:\n\nfrom sklearn.metrics import adjusted_rand_score\nadjusted_rand_score(y, q)\n\n0.904092765401111\n\n\nNot surprisingly, the clusterings are found to be quite similar.\n\n\n\n\n\n\n\n\n\n\n\n6.2.2 Silhouettes\nIf no reference clustering/classification is available, then the only way we can assess the quality of a clustering is to check how well it creates clusters whose members are more similar to their buddies than to strangers.\n\nDefinition 6.5 Suppose \\(\\bfX_i\\) is a sample in a clustering. Let \\(\\bar{b}_i\\) be the mean distance between \\(\\bfX_i\\) and its buddies, and let \\(\\bar{r}_i\\) be the mean distance between \\(\\bfX_i\\) and the members of the nearest cluster of strangers. Then the silhouette value of \\(\\bfX_i\\) is\n\\[\ns_i = \\frac{\\bar{r}_i-\\bar{b}_i}{\\max\\{\\bar{r}_i,\\bar{b}_i\\}}.\n\\]\n\n\n\n\n\n\n\n\n\nThe value \\(s_i\\) is always in the interval \\([-1,1]\\). A negative value indicates that the sample seems to be more similar to a cluster other than its own.\n\nExample 6.6 Suppose we have one-dimensional features, and they are divided into three clusters as follows:\n\\[\nA=\\{-4,-1,1\\}, \\quad B=\\{2,6\\}, \\quad C=\\{8,10\\}\n\\]\nTo compute the silhouette score for the sample \\(X_1=-4\\), we first find the mean distance between it and its buddies:\n\\[\n\\bar{b}_1 = \\frac{1}{2} \\bigl( \\abs{-4+1} + \\abs{-4-1} \\bigr) = 4.\n\\]\nNow we need to find the mean distance between \\(X_1\\) and the two stranger clusters:\n\\[\n\\bar{r}_{1,B} = \\frac{1}{2} \\bigl( \\abs{-4-2} + \\abs{-4-6} \\bigr) = 8, \\quad \\bar{r}_{1,C} = \\frac{1}{2} \\bigl( \\abs{-4-8} + \\abs{-4-10} \\bigr) = 13.\n\\]\nThe closest group of strangers is therefore cluster B, and \\(\\bar{r}_1 = 8\\). Hence, the silhouette value for \\(X_1\\) is\n\\[\ns_1 = \\frac{8-4}{8} = \\frac{1}{2}.\n\\]\nLet’s also compute the silhouette value for \\(X_5=6\\). We have\n\\[\n\\bar{b}_5 = \\frac{1}{1} \\abs{6-2} = 4,\n\\]\nand\n\\[\n\\bar{r}_{5,A} = \\frac{1}{3} \\bigl( \\abs{6+4} + \\abs{6+1} + \\abs{6-1} \\bigr) = \\frac{22}{3}, \\quad \\bar{r}_{5,C} = \\frac{1}{2} \\bigl( \\abs{6-8} + \\abs{6-10} \\bigr) = 3.\n\\]\nHence \\(\\bar{r}_5 = 3\\) and \\(s_5 = (3 - 4) / 3 = -1/3\\). A negative silhouette value indicates that the sample may be in the wrong cluster; here it seems that \\(X_5\\) would be a better fit in cluster C.\n\n\n\n\n\n\n\nImportant\n\n\n\nMost sources define an overall silhouette score as the mean of the \\(s_i\\). However, because the distributions of these values are often asymmetric and have significant outliers, I happen to think that medians are preferred when a summary score is needed. It’s hard to fight tradition, though, and silhouette_score in scikit-learn reports the mean values.\n\n\n\nExample 6.7 Let’s compute the silhouette scores for the reference cluster assignments in our blobs dataset.\n\nfrom sklearn.metrics import silhouette_samples\nX, y = blobs_data()\nsil = pd.Series( silhouette_samples(X, y) )\nsil.head()\n\n0    0.722419\n1    0.765660\n2    0.678717\n3    0.865919\n4    0.705436\ndtype: float64\n\n\nBecause these samples are 2-dimensional, it’s easy to visualize their scores by varying the size of the points used to plot them:\n\nsns.relplot(data=X, x=\"x1\", y=\"x2\",\n    hue=y, size=sil, palette=\"Dark2\"\n    );\n\n\n\n\nThe samples that don’t belong comfortably with their cluster have negative scores and therefore the smallest dots. We can find the median score in each cluster through a grouping:\n\nsil.groupby(y).median()\n\n0    0.827518\n1    0.692668\n2    0.639722\ndtype: float64\n\n\nWe can also compute the silhouette scores for the quadrant-based clustering defined in Example 6.5:\n\nsil = pd.Series( silhouette_samples(X, q) )\nsns.relplot(data=X, x=\"x1\", y=\"x2\",\n    hue=q, size=sil, palette=\"Dark2\"\n    );\n\n\n\n\n\nsil.groupby(q).median()\n\n1    0.701266\n2    0.829030\n3    0.391247\n4    0.156942\ndtype: float64\n\n\nThe scores for clusters labelled as 1 and 2 are pretty good and almost the same as for the original reference clustering. But the other two clusters score much more poorly separately than they did when they are considered a single cluster.\n\n\n\n\n\n\n\n\n\n\nExample 6.8 sklearn has a well-known dataset that contains labeled handwritten digits. Let’s extract the examples for just the numerals 4, 5, and 6:\n\ndigits = datasets.load_digits(as_frame=True)[\"frame\"]\nkeep = digits[\"target\"].isin([4, 5, 6])  # keep these samples\ndigits = digits[keep]\nX = digits.drop(\"target\", axis=1)\ny = digits.target\ny.value_counts()\n\n5    182\n4    181\n6    181\nName: target, dtype: int64\n\n\nWe can check the silhouette scores for the reference labelling in order to set expectations for how well a clustering method might do:\n\ndigits[\"sil\"] = silhouette_samples(X,y)\ndigits.groupby(\"target\")[\"sil\"].median()\n\ntarget\n4    0.207595\n5    0.245201\n6    0.342689\nName: sil, dtype: float64\n\n\nA look at the distributions of the silhouette values reveals more details:\n\nsns.catplot(data=digits, x=\"target\", y=\"sil\",\n    kind=\"violin\"\n    );\n\n\n\n\nThe values are mostly positive, which indicates nearly all of the samples for a digit are at least somewhat closer to each other than to the other samples. The 6s are the most distinct. However, the existence of scores close to and below zero suggest that a clustering algorithm ia unlikely to reproduce the true classification perfectly. To put it another way, the features chosen to represent the digits don’t seem to create three clear, well-separated balls of points representing the three different types of digits.\n\n\n\n\n\n\n\n\n\nSilhouette values are fairly easy to use, but they are an imperfect tool. They favor clusters that have a compact, roughly spherical shape, in which all the members of the cluster are mutually close to one another.\n\nExample 6.9 The bullseye dataset shown in Figure 6.3 has only one central cluster that silhouette values approve of:\n\nX, y = bullseye_data()\ns = silhouette_samples(X, y)\nclusters = pd.DataFrame({\"cluster\": y, \"s value\": s})\nsns.catplot(data=clusters, x=\"cluster\", y=\"s value\", \n    kind=\"violin\"\n    )\n\n\n\n\nThe central cluster scores well, but the rings score poorly because they are not compact and ball-shaped.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWhile we have no trouble validating good clusterings by eye in two dimensions in cases such as Figure 6.2 and Figure 6.3, it can be challenging to assess such nonspherical clusters in higher dimensions if there is no reference or validation available."
  },
  {
    "objectID": "clustering.html#k-means",
    "href": "clustering.html#k-means",
    "title": "6  Clustering",
    "section": "6.3 k-means",
    "text": "6.3 k-means\nThe \\(k\\)-means algorithm is one of the best-known clustering methods. It is often the method of first resort, even though it has some significant limitations.\nGiven a sample matrix \\(\\bfX\\) with \\(n\\) rows \\(\\bfX_i\\), the algorithm divides the sample points into disjoint sets \\(C_1,\\ldots,C_k\\), where \\(k\\) is a hyperparameter. We need a pair of key definitions.\n\nDefinition 6.6 The centroid of cluster \\(C_j\\), denoted by \\(\\bfmu_j\\), is the mean of the vectors in \\(C_j\\): \\[\n\\bfmu_j = \\frac{1}{\\abs{C_j}} \\sum_{\\bfx\\in C_j} \\bfx.\n\\] The inertia of \\(C_j\\) is \\[\nI_j = \\sum_{\\bfx\\in C_j} \\norm{ \\bfx - \\bfmu_j }_2^2.\n\\tag{6.3}\\]\n\n\n\n\n\n\n\n\n\nThe goal of the \\(k\\)-means algorithm is to choose the clusters in order to minimize the total inertia,\n\\[\nI = \\sum_{j=1}^k I_j.\n\\]\n\n\nExample 6.10 Given the one-dimensional samples \\(-3,-2,-1,2,5,7\\), find the inertia of the clustering \\[\nC_1=\\{-3,-2,-1\\}, \\qquad C_2=\\{2,5,7\\},\n\\] and of the clustering \\[\nC_1=\\{-3,-2,-1,2\\}, \\qquad C_2=\\{5,7\\}.  \n\\]\n\nSolution. The first clustering has centroids\n\\[\n\\mu_1= \\frac{1}{3}\\bigl( -3 -2 -1 \\bigr) = -2, \\quad \\mu_2= \\frac{1}{3}\\bigl(2+5 +7 \\bigr) =14/3.\n\\]\nThis gives inertia\n\\[\n\\begin{split}\nI_1 &= (-3+2)^2 + (-2+2)^2 + (-1+2)^2 = 2, \\\\\nI_2 &= \\bigl(2-\\tfrac{14}{3}\\bigr)^2 + \\bigl(5-\\tfrac{14}{3}\\bigr)^2 + \\bigl(7-\\tfrac{14}{3}\\bigr)^2 = 13.78,\n\\end{split}\n\\]\nfor a total inertia of 15.78.\nThe second clustering has centroids\n\\[\n\\mu_1= \\frac{1}{4}\\bigl( -3 -2 -1 + 2\\bigr) = -1, \\quad \\mu_2= \\frac{1}{2}\\bigl(5 +7 \\bigr) =6,\n\\]\nwith cluster inertias\n\\[\n\\left[   \\right]  + \\left[   \\right] = 14 + 2 = 16.\n\\]\n\\[\n\\begin{split}\nI_1 &= (-3+1)^2 + (-2+1)^2 + (-1+1)^2  + (2+1)^2 = 14, \\\\\nI_2 &=  (5-6)^2 + (7-6)^2  = 2,\n\\end{split}\n\\]\nfor a total inertia of 16. The first clustering would be considered better because it has lower total inertia.\n\n\nFinding the minimum inertia among all possible \\(k\\)-clusterings is an infeasible problem to solve exactly at any practical size. Instead, the computational approach is to iteratively improve on a starting clustering.\n\n6.3.1 Lloyd’s algorithm\nThe standard iteration is known as Lloyd’s algorithm. Starting with values for the \\(k\\) centroids, each iteration consists of two steps:\n\n\n\n\n\n\n\n\n\nEach sample point is assigned to the cluster whose centroid is the nearest. (Ties are broken randomly.)\nRecalculate the centroids based on the cluster assignments: \\[\n\\bfmu_j^+ = \\frac{1}{|C_j|} \\sum_{\\bfx\\in C_j} \\bfx.\n\\]\n\nThese steps are repeated alternately until the assignment step does not change any of the clusters. In practice, this almost always happens quickly. Here is a demonstration:\n\nIf Lloyd’s algorithm converges, it finds a local minimum of total inertia, in the sense that small changes to the cluster assignments cannot decrease it. But there is no guarantee of converging to the global minimum, and often, this does not happen.\n\n\n6.3.2 Practical issues\n\nInitialization. The performance of \\(k\\)-means depends a great deal on the initial set of centroids. Traditionally, the centroids were chosen as random members of the sample set, but more reliable heuristics, such as \\(k\\)-means++, have since become dominant.\nMultiple runs. All the initialization methods include an element of randomness, and since the Lloyd algorithm usually converges quickly, it is often run with multiple instances of the initialization. The run with the lowest final inertia is kept.\nDistance metric. The Lloyd algorithm often fails to converge for norms other than the 2-norm and must be modified if another norm is preferred.\n\nThe algorithm treats \\(k\\) as a hyperparameter. Increasing \\(k\\) tends to lower the total inertia—at the extreme, taking \\(k=n\\) puts each sample in its own cluster, with a total inertia of zero. Hence, one should use silhouette scores or some other measure in order to optimize \\(k\\). Occam’s Razor dictates preferring smaller values to large ones. There are many suggestions on how to find the choice that gives the most “bang for the buck,” but none is foolproof.\nBecause of its dependence on the norm, the inertia criterion of \\(k\\)-means disfavors long, skinny clusters and clusters of unequal dispersion. Essentially, it wants to find spherical blobs of roughly equal size.\n\nExample 6.11 We apply \\(k\\)-means to the blobs dataset that has 3 reference clusters, starting with \\(k=2\\) clusters. For illustration, we instruct the algorithm to try 3 initializations and report on its progress:\n\nfrom sklearn.cluster import KMeans\n\nX, y = blobs_data()\nkm2 = KMeans(n_clusters=2, n_init=3, verbose=1, random_state=302)\nkm2.fit(X)\n\nInitialization complete\nIteration 0, inertia 1059.6987886807747.\nIteration 1, inertia 731.1354331556078.\nIteration 2, inertia 710.0834960683221.\nConverged at iteration 2: strict convergence.\nInitialization complete\nIteration 0, inertia 991.8887143415382.\nIteration 1, inertia 742.3898509432538.\nIteration 2, inertia 710.0834960683221.\nConverged at iteration 2: strict convergence.\nInitialization complete\nIteration 0, inertia 1379.944166430521.\nIteration 1, inertia 917.3997019815874.\nIteration 2, inertia 903.6145145551955.\nIteration 3, inertia 845.0278508241745.\nIteration 4, inertia 736.5347784099483.\nIteration 5, inertia 710.0834960683221.\nConverged at iteration 5: strict convergence.\n\n\nKMeans(n_clusters=2, n_init=3, random_state=302, verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KMeansKMeans(n_clusters=2, n_init=3, random_state=302, verbose=1)\n\n\nThe fitted clustering object can tell us the final inertia and cluster centroids:\n\nprint(f\"final inertia: {km2.inertia_:.5g}\")\nprint(\"cluster centroids:\")\nprint(km2.cluster_centers_)\n\nfinal inertia: 710.08\ncluster centroids:\n[[-2.01977462  2.86247767]\n [ 2.01902057 -0.69722874]]\n\n\nThere is a predict method that can make cluster assignments for arbitrary points in feature space. But to get the cluster assignments for the samples, it’s easier to use the labels_ property of the fitted clustering object.\n\nc = pd.Series( km2.labels_ )\nc.value_counts()\n\n1    89\n0    61\ndtype: int64\n\n\nHere, we encapsulate using those values to compute and then show the silhouette values. Again, since we only have 2 features, it’s easy to visualize the clusters directly.\n\ndef report(clustering):\n    result = X.copy()\n    y_hat = clustering.labels_    # cluster assignments\n    result[\"cluster\"] = y_hat\n    result[\"sil\"] = silhouette_samples(X, y_hat)\n    print(f\"inertia: {clustering.inertia_:.5g}\")\n    print(\"silhouette medians by cluster:\")\n    print( result.groupby(\"cluster\")[\"sil\"].median() )\n    \n    sns.catplot(data=result, x=\"cluster\", y=\"sil\",\n        kind=\"violin\", height=3\n        );\n    \n    sns.relplot(data=result, x=\"x1\", y=\"x2\",\n        hue=\"cluster\", size=\"sil\",\n        height=3, palette=\"Dark2\"\n        );\n    return result\n\n\nreport(km2);\n\ninertia: 710.08\nsilhouette medians by cluster:\ncluster\n0    0.839082\n1    0.440536\nName: sil, dtype: float64\n\n\n\n\n\n\n\n\nIt’s clear from both plots above that one cluster is more tightly packed than the other. Now let’s repeat the computation for \\(k=3\\) clusters:\n\nkm3 = KMeans(n_clusters=3, n_init=1, random_state=302)\nkm3.fit(X)\nreport(km3);\n\ninertia: 203.3\nsilhouette medians by cluster:\ncluster\n0    0.828146\n1    0.688014\n2    0.645840\nName: sil, dtype: float64\n\n\n\n\n\n\n\n\nThis result shows a modest reduction in silhouette scores for the original good cluster, but improvement for the problematic one.\nMoving on to \\(k=4\\) clusters shows clear degradation of the silhouette values:\n\nkm4 = KMeans(n_clusters=4, n_init=1, random_state=302)\nkm4.fit(X)\nreport(km4);\n\ninertia: 166.02\nsilhouette medians by cluster:\ncluster\n0    0.829120\n1    0.268430\n2    0.423066\n3    0.647724\nName: sil, dtype: float64\n\n\n\n\n\n\n\n\nBased on silhouette values, we would be justified to stop at \\(k=3\\) clusters.\n\n\n\n\n\n\n\n\n\n\nExample 6.12 The stripes dataset does not conform to compact clusters of equal size, and \\(k\\)-means performs poorly on it:\n\n\nCode\nX, y = stripes_data()\nresults = pd.DataFrame()\nfor k in [2, 3, 4]:\n    km = KMeans(n_clusters=k, n_init=3, random_state=302)\n    km.fit(X)\n    results = pd.concat( (\n        results, \n            pd.DataFrame( {\n            \"x1\": X[\"x1\"], \"x2\": X[\"x2\"], \n            \"cluster\": km.labels_,\n            \"k\": k\n            })\n        ) )\n    \nsns.relplot(data=results,\n    x=\"x1\", y=\"x2\",\n    hue=\"cluster\", col=\"k\", \n    height=3, palette=\"Dark2\"\n    );\n\n\n\n\n\nIt’s usually a good idea to standardize the data before clustering. But that’s not much help here:\n\n\nCode\nresults = pd.DataFrame()\nfor k in [2, 3, 4]:\n    km = make_pipeline(\n        StandardScaler(), \n        KMeans(n_clusters=k, n_init=3, random_state=302)\n        )\n    km.fit(X)\n    results = pd.concat( (\n        results, \n            pd.DataFrame( {\n            \"x1\": X[\"x1\"], \"x2\": X[\"x2\"], \n            \"cluster\": km[1].labels_,\n            \"k\": k\n            })\n        ) )\n     \nsns.relplot(data=results,\n    x=\"x1\", y=\"x2\",\n    hue=\"cluster\", col=\"k\", \n    height=3, palette=\"Dark2\"\n    );\n\n\n\n\n\nClustering is hard!\n\n\nExample 6.13 We return to the handwriting recognition dataset. Again we keep only the samples labeled 4, 5, or 6:\n\ndigits = datasets.load_digits(as_frame=True)[\"frame\"]\nkeep = digits[\"target\"].isin([4, 5, 6])\ndigits = digits[keep]\n\nX = digits.drop(\"target\", axis=\"columns\")\ny = digits[\"target\"]\n\nWe first fit 3 clusters to the feature matrix:\n\nkm = make_pipeline( StandardScaler(), KMeans(n_clusters=3, n_init=3, random_state=302) )\nkm.fit(X)\ny_hat =  km[1].labels_\ndigits[\"cluster number\"] = y_hat\ndigits[[\"target\", \"cluster number\"]].head(9)\n\n\n\n\n\n\n\n\ntarget\ncluster number\n\n\n\n\n4\n4\n2\n\n\n5\n5\n0\n\n\n6\n6\n1\n\n\n14\n4\n2\n\n\n15\n5\n0\n\n\n16\n6\n1\n\n\n24\n4\n2\n\n\n25\n5\n0\n\n\n26\n6\n1\n\n\n\n\n\n\n\nThe adjusted Rand index suggests that we have reproduced the classification very well:\n\nARI = adjusted_rand_score(y, y_hat)\nprint(f\"ARI: {ARI:.4f}\")\n\nARI: 0.9457\n\n\nHowever, that conclusion benefits from our prior knowledge of the classification. What if we have no such reference to check against? Let’s look over a range of \\(k\\) choices, recording the median silhouette values for each:\n\nresults = []\nkvals = range(2,7)\nfor k in kvals:\n    km = KMeans(n_clusters=k, n_init=3, random_state=19716)\n    km.fit(X)\n    y_hat = km.labels_\n\n    sil = np.median( silhouette_samples(X, y_hat) )\n    results.append(sil)\n\npd.Series(results, index=kvals)\n\n2    0.237310\n3    0.258975\n4    0.257826\n5    0.249603\n6    0.204824\ndtype: float64\n\n\nThe silhouette score is maximized at \\(k=3\\), which could be considered a reason to choose 3 clusters. While the score for 4 clusters is close to it, we should prefer the less complex model."
  },
  {
    "objectID": "clustering.html#hierarchical-clustering",
    "href": "clustering.html#hierarchical-clustering",
    "title": "6  Clustering",
    "section": "6.4 Hierarchical clustering",
    "text": "6.4 Hierarchical clustering\nThe idea behind hierarchical clustering is to organize all the sample points into a tree structure called a dendrogram. At the root of the tree is the entire sample set, while each leaf of the tree is a single sample. Groups of similar samples are connected as nearby relatives in the tree, with less-similar groups located as more distant relatives.\n\n\n\n\n\n\n\n\nDendrograms can be found by starting with the root and recursively splitting, or by starting at the leaves and recursively merging. We will describe the latter approach, known as agglomerative clustering.\nThe algorithm begins with \\(n\\) singleton clusters, i.e., \\(C_i=\\{\\bfX_i\\}\\) for \\(i=1,\\dots,n\\). Then, the similarity or distance between each pair of clusters is determined. The pair with the minimum distance is merged, and the process repeats. Effectively, we end up with an entire family of clusterings; we can stop at any number of clusters between \\(n\\) and 1.\n\n\n\n\n\n\n\n\nWe now need to specify how to compute the distance between clusters, a quantity called their linkage.\n\nDefinition 6.7 Various ways to define the distance between clusters \\(C_i\\) and \\(C_j\\) are:\n\n\nsingle linkage\n\n(also called minimum linkage) \\[\n\\displaystyle \\min_{\\bfx\\in C_i,\\,\\bfy\\in C_j} \\{ \\norm{\\bfx-\\bfy } \\}\n\\tag{6.4}\\]\n\n\n\ncomplete linkage\n\n(also called maximum linkage) \\[\n\\displaystyle \\max_{\\bfx\\in C_i,\\,\\bfy\\in C_j} \\{ \\norm{\\bfx-\\bfy} \\}\n\\tag{6.5}\\]\n\n\n\naverage linkage\n\n\\[\n\\displaystyle \\frac{1}{|C_i|\\,|C_j|} \\sum_{\\bfx\\in C_i,\\,\\bfy\\in C_j} \\norm{ \\bfx-\\bfy }\n\\tag{6.6}\\]\n\n\n\nWard linkage\n\nDefined as the increase in inertia resulting from merging \\(C_i\\) and \\(C_j\\), which is equal to \\[\n\\frac{ |C_i|\\,|C_j| }{|C_i| + |C_j|} \\norm{\\bfmu_i - \\bfmu_j}_2^2,\n\\tag{6.7}\\] where \\(\\bfmu_i\\) and \\(\\bfmu_j\\) are the centroids of \\(C_i\\) and \\(C_j\\).\n\n\n\n\nSingle linkage only pays attention to the gaps between clusters, not the size or spread of them. Complete linkage, on the other hand, wants to keep every cluster packed tightly together. Average linkage is a compromise between these extremes. Agglomerative clustering with Ward linkage amounts to trying to minimize the increase of inertia with each merger. In that sense, it has a similar objective to \\(k\\)-means, but it is usually not as successful at minimizing inertia.\nBoth \\(k\\)-means and Ward linkage require the ability to compute changing centroid locations, which in turn requires knowing all the sample locations. By contrast, agglomerative clustering with single, complete, or average linkage needs only the pairwise distances between samples and can be found from just a distance matrix in place of the samples themselves.\n\nExample 6.14 Given clusters \\(C_1=\\{-3,-2,-1\\}\\) and \\(C_2=\\{3,4,5\\}\\), find the different linkages between them.\n\nSolution. Ward. The centroids of the clusters are \\(-2\\) and \\(4\\). So the linkage is\n\\[\n\\frac{3\\cdot 3}{3+3} \\, 6^2 = 54.\n\\]\nSingle. The pairwise distances between members of \\(C_1\\) and \\(C_2\\) form a \\(3\\times 3\\) matrix:\n\n\n\n\n-3\n-2\n-1\n\n\n\n\n3\n6\n5\n4\n\n\n4\n7\n6\n5\n\n\n5\n8\n7\n6\n\n\n\n(Note that this is a submatrix of the full \\(6\\times 6\\) distance matrix.) The single linkage is therefore 4.\nComplete. The maximum of the matrix above is 8.\nAverage. The average value of the matrix entries is \\(54/9\\), which is 6.\n\n\n\n\n\n\n\n\n\n\n(video example is different from the text)\n\nExample 6.15 Let’s use 5 sample points in the plane, and agglomerate them by single linkage. The pairwise_distances function converts sample points into a distance matrix:\n\nX = np.array( [[-2,-1] ,[-2,-2], [1,0.5], [0,2], [-1,1]] )\nD = pairwise_distances(X, metric=\"euclidean\")\nD\n\narray([[0.        , 1.        , 3.35410197, 3.60555128, 2.23606798],\n       [1.        , 0.        , 3.90512484, 4.47213595, 3.16227766],\n       [3.35410197, 3.90512484, 0.        , 1.80277564, 2.06155281],\n       [3.60555128, 4.47213595, 1.80277564, 0.        , 1.41421356],\n       [2.23606798, 3.16227766, 2.06155281, 1.41421356, 0.        ]])\n\n\nThe minimum value in the upper triangle of the distance matrix is in row 0, column 1. So our first merge results in the cluster \\(C_1=\\{\\bfX_0,\\bfX_1\\}\\). The next-smallest entry in the upper triangle is at position \\((3,4)\\), so we want to merge those samples together next, resulting in \\[\nC_1=\\{\\bfX_0,\\bfX_1\\},\\, C_2 = \\{\\bfX_3,\\bfX_4\\},\\, C_3=\\{\\bfX_2\\}.\n\\] The next-smallest element in the matrix is at \\((2,3)\\), resulting in \\[\nC_1=\\{\\bfX_0,\\bfX_1\\},\\, C_2 = \\{\\bfX_2,\\bfX_3,\\bfX_4\\}.\n\\] The final merge is to combine these.\nThe entire dendrogram can be visualized with seaborn’s clustermap:\n\nsns.clustermap(X, \n    col_cluster=False,\n    dendrogram_ratio=(.75,.15),\n    figsize=(6,4)\n    );\n\n\n\n\nThe root of the tree above is at the left, with the leaves at the right. The leaves have been ordered so that the lines in the dendrogram don’t ever cross. The two colored columns show the values of the features of the samples.\nThe horizontal position in the dendrogram indicates the linkage value, which is largest at the left. Working from right to left, we first see the merger of samples 0 and 1 at the minimum linkage in the entire distance matrix. The next merger is between \\(\\bfX_3\\) and \\(\\bfX_4\\), and so on.\nWe can choose to stop at any horizontal position (linkage value). Based on the distance matrix, if we stop at a linkage value of 2.0, then we perform only the first 2 merges and get 3 clusters. The largest gap between merges is in the merge from 2 clusters to 1, which could be used to justify \\(k=2\\) as the best number of clusters.\n\n\n\n\n\n\n\n\n\nThe AgglomerativeClustering class in sklearn.cluster performs the clustering process to create a learner object.\n\nExample 6.16 We define a function that allows us to run all three linkages for a given feature matrix:\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ndef run_experiment(X):\n    results = pd.DataFrame()\n    data = X.copy()\n    for linkage in [\"single\", \"complete\", \"ward\"]:\n        agg = AgglomerativeClustering(n_clusters=3, linkage=linkage)\n        agg.fit(X)\n        data[\"cluster\"] = agg.labels_\n        data[\"linkage\"] = linkage\n        results = pd.concat( (results, data) )\n    return results\n\nWe first try the blobs dataset:\n\nX, y = blobs_data()\nresults = run_experiment(X)\nsns.relplot(data=results, x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", \n        height=2.5, palette=\"Dark2\"\n        );\n\n\n\n\nAs you can see, the simple linkage was badly confused by the two blobs that nearly run together. The others are good at reproducing the ball-like clusters.\nNext, we try the stripes data:\n\nX, y = stripes_data()\nresults = run_experiment(X)\nsns.relplot(data=results, x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", \n        height=2.5, palette=\"Dark2\"\n        );\n\n\n\n\nNow the tendency of complete and Ward linkages to find compact, roughly spherical clusters is a bug, not a feature. They group together points across stripes rather than clusters extending lengthwise. The geometric flexibility of single linkage pays off here.\nFinally, we try the most demanding test, bullseye:\n\nX, y = bullseye_data()\nresults = run_experiment(X)\nfig = sns.relplot(data=results, x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", \n        height=2.5, palette=\"Dark2\"\n        )\nfig.set(aspect=1);\n\n\n\n\nThe single linkage is the only one to cluster the rings properly. However, it’s a delicate situation, and it can be sensitive to individual samples. Here, for instance, we add just one sample to the dataset and get a major change:\n\nX = pd.concat( ( X, pd.DataFrame({\"x1\": [0], \"x2\": [2.25]}) ) )\nresults = run_experiment(X)\nfig = sns.relplot(data=results, x=\"x1\", y=\"x2\", \n        hue=\"cluster\", col=\"linkage\", \n        height=2.5, palette=\"Dark2\"\n        )\nfig.set(aspect=1);\n\n\n\n\nThat instability can make single linkage difficult to work with.\n\n\n\n\n\n\n\n\n\n\nExample 6.17 Let’s try agglomerative clustering to “discover” the species of the penguins, pretending we don’t know them in advance. First, let’s recall how many of each species we actually do have:\n\npenguins = sns.load_dataset(\"penguins\").dropna()\nfeatures = [\"bill_length_mm\", \"bill_depth_mm\", \"flipper_length_mm\", \"body_mass_g\"]\nX = penguins[features]\ny = penguins[\"species\"]\ny.value_counts()\n\nAdelie       146\nGentoo       119\nChinstrap     68\nName: species, dtype: int64\n\n\nOur first attempt is single linkage. Because 2-norm distances are involved, we use standardization in a pipeline with the clustering method. After fitting, the labels_ property of the cluster object is a vector of cluster assignments.\n\nfrom sklearn.cluster import AgglomerativeClustering\n\nsingle = AgglomerativeClustering(n_clusters=3, linkage=\"single\")\npipe = make_pipeline(StandardScaler(), single)\npipe.fit(X)\ny_hat = single.labels_        # cluster assignments\npenguins[\"single\"] = y_hat\npenguins.loc[::26, [\"species\", \"single\"]]\n\n\n\n\n\n\n\n\nspecies\nsingle\n\n\n\n\n0\nAdelie\n0\n\n\n31\nAdelie\n0\n\n\n58\nAdelie\n0\n\n\n84\nAdelie\n0\n\n\n110\nAdelie\n0\n\n\n136\nAdelie\n0\n\n\n162\nChinstrap\n0\n\n\n188\nChinstrap\n0\n\n\n214\nChinstrap\n0\n\n\n240\nGentoo\n2\n\n\n267\nGentoo\n2\n\n\n294\nGentoo\n2\n\n\n320\nGentoo\n2\n\n\n\n\n\n\n\nPerhaps Gentoo is associated with cluster number 2, but the situation with the other species is less clear. Here are the value counts:\n\nprint(\"single linkage results:\")\npenguins[\"single\"].value_counts()\n\nsingle linkage results:\n\n\n0    213\n2    119\n1      1\nName: single, dtype: int64\n\n\nAs we saw with the bullseye dataset in Example 6.16, single linkage is susceptible to declaring one isolated point to be a cluster, while grouping together other points we would like to separate. Here is the ARI for this clustering, compared to the true classification:\n\nfrom sklearn.metrics import adjusted_rand_score\nARI = adjusted_rand_score(y, y_hat)\nprint(f\"single linkage ARI: {ARI:.4f}\")\n\nsingle linkage ARI: 0.6506\n\n\nNow let’s try Ward linkage (which is the default):\n\nward = AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\npipe = make_pipeline(StandardScaler(), ward)\npipe.fit(X)\ny_hat = ward.labels_\npenguins[\"ward\"] = y_hat\n\nprint(\"Ward linkage results:\")\nprint(penguins[\"ward\"].value_counts())\n\nWard linkage results:\n1    157\n0    119\n2     57\nName: ward, dtype: int64\n\n\nThis result looks more promising. The ARI confirms that hunch:\n\nARI = adjusted_rand_score(y, penguins[\"ward\"])\nprint(f\"Ward linkage ARI: {ARI:.4f}\")\n\nWard linkage ARI: 0.9132\n\n\nIf we guess at the likely correspondence between the cluster numbers and the different species, then we can find the confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Convert cluster numbers into labels:\nreplacements = {1: \"Adelie\", 0: \"Gentoo\", 2: \"Chinstrap\"}\ny_hat = penguins[\"ward\"].replace(replacements) \nC = confusion_matrix(y, y_hat)\n\nConfusionMatrixDisplay(C, display_labels=y.unique()).plot();\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhile the performance of clustering as a classifier in Example 6.17 is inferior to supervised methods we used for that purpose, the clustering process reveals that the Gentoo penguins are the most dissimilar from the other species. By inspecting the mislabeled cases in the confusion matrix, further insight might be gained about what particular traits can make the other species difficult to distinguish. In clustering, the goal is often more about insight than prediction."
  },
  {
    "objectID": "clustering.html#exercises",
    "href": "clustering.html#exercises",
    "title": "6  Clustering",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 6.1 (6.1) Prove that the angular distance between any nonzero vector and itself is zero.\n\n\nExercise 6.2 (6.1) Find an example for which the cosine distance does not satisfy the triangle inequality. That is, find three vectors \\(\\bfx\\), \\(\\bfy\\), and \\(\\bfz\\) such that \\[\n\\dist(\\bfx,\\bfy) &gt; \\dist(\\bfx,\\bfz) + \\dist(\\bfz,\\bfy).\n\\] (Hint: it’s enough to play around with some simple vectors in two dimensions. Use vectors for which the angle between is a multiple of \\(\\pi/6\\) or \\(\\pi/4\\).)\n\n\nExercise 6.3 (6.1) Here is a feature matrix. \\[\n\\bfX = \\begin{bmatrix}\n-1 & -1 & 0 \\\\ 1 & 1 & 1 \\\\ 2 & 0 & -2 \\\\ 1 & 3 & 1\n\\end{bmatrix}.\n\\]\n(a) Find the associated distance matrix using the 1-norm.\n(b) Find the associated distance matrix using the infinity-norm.\n\n\nExercise 6.4 (6.2) Here are two clusterings on 6 samples: \\[\n\\begin{split}\n\\text{A}:\\: & C_1 = \\{\\bfX_1,\\bfX_4\\}, \\; C_2=\\{\\bfX_2,\\bfX_3,\\bfX_5,\\bfX_6 \\}  \\\\\n\\text{B}:\\: & C_1 = \\{\\bfX_1,\\bfX_2\\}, \\; C_2=\\{\\bfX_3,\\bfX_4\\}, \\; C_3 = \\{\\bfX_5,\\bfX_6 \\}\n\\end{split}\n\\]\nHere is a distance matrix for these samples: \\[\n\\begin{bmatrix}\n0 & 2 & 2 & 1 & 4 & 1 \\\\ 2 & 0 & 3 & 5& 1  & 2 \\\\ 2 & 3 & 0 & 6 & 2 & 1 \\\\ 1 & 5 & 6 & 0 & 8 & 4 \\\\ 4 & 1 & 2 & 8 & 0 & 3 \\\\ 1 & 2 & 1 & 4 & 3 & 0\n\\end{bmatrix}\n\\]\n(a) Find the Rand index between these clusterings.\n(b) Find the silhouette values for all samples in clustering A.\n\n\nExercise 6.5 (6.3) Let \\(z\\) be a positive number, and consider the 12 planar samples \\[\n\\begin{split}\n& [z,-3],\\, [z,-2],\\, [z,-1],\\,[z,1],\\,[z,2],\\,[z,3], \\\\\n& [-z,-3],\\, [-z,-2],\\, [-z,-1],\\,[-z,1],\\,[-z,2],\\,[-z,3].\n\\end{split}\n\\] See Figure 6.4.\n\n\n\nFigure 6.4: Samples for Exercise 6.5.\n\n\n(a) Using the 2-norm, calculate the inertia, which may depend on \\(z\\), of the clustering \\[\nC_1 = \\{ [z,j]: j=-3,-2,-1,1,2,3\\}, \\qquad C_2 = \\{ [-z,j]: j=-3,-2,-1,1,2,3\\}.\n\\] (This divides the sample points by the \\(y\\)-axis.)\n(b) Using the 2-norm, calculate the inertia, which may depend on \\(z\\), of the clustering \\[\nC_1 = \\{ [-z,j] \\text{ and } [z,j]: j=-3,-2,-1\\}, \\qquad C_2 = \\{ [-z,j] \\text{ and } [z,j]: j=1,2,3\\}.\n\\] (This divides the sample points by the \\(x\\)-axis.)\n(c) For which values of \\(z\\), if any, does clustering from part (a) have less inertia than the clustering from part (b)?\n\n\nExercise 6.6 (6.4) Here is a distance matrix for samples \\(\\bfX_1,\\ldots,\\bfX_5\\):\n\\[\n\\left[\n\\begin{array}{ccccc}\n0 & 2 & 3.5 & 5 & 6 \\\\\n2 & 0 & 2.5 & 3 & 4 \\\\\n3.5 & 2.5 & 0 & 1 & 1.5 \\\\\n5 & 3 & 1 & 0 & 5.5 \\\\\n6 & 4 & 1.5 & 5.5 & 0 \\\\\n\\end{array}\n\\right]\n\\]\n(a) Compute the average linkage between the clusters \\(C_1=\\{\\bfX_1,\\bfX_3\\}\\) and \\(C_2=\\{\\bfX_2,\\bfX_4,\\bfX_5\\}\\).\n(b) Compute the single linkage between the clusters \\(C_1=\\{\\bfX_1,\\bfX_3,\\bfX_4\\}\\) and \\(C_2=\\{\\bfX_2,\\bfX_5\\}\\).\n\n\nExercise 6.7 (6.4) Find the agglomerative clustering, using complete linkage, for the samples given in Exercise 6.6. (This means finding four individual merge steps based on the distance matrix.)"
  },
  {
    "objectID": "networks.html#graphs",
    "href": "networks.html#graphs",
    "title": "7  Networks",
    "section": "7.1 Graphs",
    "text": "7.1 Graphs\nIn mathematics, a network is represented as a graph.\n\nDefinition 7.1 A graph is a collection of nodes (also called vertices) and edges that connect pairs of nodes.\nIn an undirected graph, the edge \\((a,b)\\) is identical to \\((b,a)\\), while in a directed graph or digraph, \\((a,b)\\) and \\((b,a)\\) are different potential edges.\nIn either type of graph, each edge might be labeled with a numerical value, which results in a weighted graph.\n\n\n\n\n\n\n\n\n\nUndirected, unweighted graphs will give us plenty to handle, and we will not go beyond them. We also will not allow graphs that have an edge from a node to itself.\nWe will use the NetworkX package to work with graphs.\n\nimport networkx as nx\n\n\nExample 7.1 One way to create a graph is from a list of edges:\n\nstar = nx.Graph( [ (1,2),(1,3),(1,4),(1,5),(1,6) ] )\nnx.draw(star, with_labels=True, node_color=\"lightblue\")\n\n\n\n\nAnother way to create a graph is to give the start and end nodes of the edges as columns in a data frame:\n\nnetwork = pd.DataFrame( \n    {'from': [1,2,3,4,5,6], 'to': [2,3,4,5,6,1]} \n    )\nprint(network)\nH = nx.from_pandas_edgelist(network, 'from', 'to')\nnx.draw(H, with_labels=True, node_color=\"lightblue\")\n\n   from  to\n0     1   2\n1     2   3\n2     3   4\n3     4   5\n4     5   6\n5     6   1\n\n\n\n\n\nWe can deconstruct a graph object into its constituent nodes and edges, as they are properties of the graph object. The results have special types that can be converted into sets, lists, or other objects.\n\nprint(\"Nodes as a list:\")\nprint( list(star.nodes) )\nprint(\"\\nNodes as an Index:\")\nprint( pd.Index(star.nodes) )\nprint(\"\\nEdges as a list:\")\nprint( list(star.edges) )\n\nNodes as a list:\n[1, 2, 3, 4, 5, 6]\n\nNodes as an Index:\nInt64Index([1, 2, 3, 4, 5, 6], dtype='int64')\n\nEdges as a list:\n[(1, 2), (1, 3), (1, 4), (1, 5), (1, 6)]\n\n\nWe can change the labels of the nodes to be whatever we want:\n\nrelabel = pd.Series( [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"], index=range(1,7) )\nstar_letter = nx.relabel_nodes(star, relabel )\nprint(\"Edges as a list:\")\nprint( list(star_letter.edges) )\n\nEdges as a list:\n[('a', 'b'), ('a', 'c'), ('a', 'd'), ('a', 'e'), ('a', 'f')]\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.2 There are many ways to read graphs from, and write them to, files. For example, here is a friend network among Twitch users:\n\ntwitch = nx.read_edgelist(\n    \"_datasets/musae_edges.csv\",\n     delimiter=',', \n     nodetype=int\n     )\n\nThe file just imported has a pair of node labels separated by commas on each line, representing one edge. The node labels can be any strings, but we overrode that above to interpret them as integers.\nWe will explore many functions that tell us facts about a graph. Here are two of the most fundamental:\n\nprint(\"Twitch network has\", \n    twitch.number_of_nodes(), \n    \"nodes and\",\n    twitch.number_of_edges(),\n    \"edges\"\n    )\n\nTwitch network has 7126 nodes and 35324 edges\n\n\nDue to its large size, this graph is difficult to draw in its entirety.\n\n\n\n\n\n\n\nNote\n\n\n\nAs pointed out in its documentation, the drawing tools provided by NetworkX are bare-bones, and you should look into alternatives if you will be using them heavily.\n\n\n\n7.1.1 A graph menagerie\nThere are functions that generate different well-studied types of graphs. The first graph constructed above is a star graph, and the graph H above is a cycle graph.\n\nnx.draw(nx.cycle_graph(9))\n\n\n\n\n\n\n\n\n\n\n\n\nA cross between the star and the cycle is a wheel graph.\n\nnx.draw(nx.wheel_graph(9))\n\n\n\n\nA complete graph is one that has every possible edge.\n\nK5 = nx.complete_graph(5)\nprint(\"5 nodes,\", nx.number_of_edges(K5), \"edges\")\nnx.draw(K5)\n\n5 nodes, 10 edges\n\n\n\n\n\nIn a graph on \\(n\\) nodes, there are\n\\[\n\\binom{n}{2} = \\frac{n!}{(n-2)!2!} = \\frac{n(n-1)}{2}\n\\]\nunique pairs of distinct nodes. Hence, there are \\(\\binom{n}{2}\\) edges in the undirected complete graph on \\(n\\) nodes.\nA lattice graph has a regular structure, like graph paper.\n\nlat = nx.grid_graph( (5,4) )\nprint(lat.number_of_nodes(), \"nodes,\", lat.number_of_edges(), \"edges\")\nnx.draw(lat, node_size=100)\n\n20 nodes, 31 edges\n\n\n\n\n\nIn an \\(m\\times n\\) lattice graph, there are \\(m-1\\) edges in one direction repeated \\(n\\) times, plus \\(n-1\\) edges in the other direction, repeated \\(m\\) times. Thus there are\n\\[\n(m-1)n + (n-1)m = 2mn-(m+n)\n\\]\nedges altogether.\nThere are different ways to draw a particular graph in the plane, as determined by the positions of the nodes. The default is to imagine that the edges are springs pulling on the nodes. But there are alternatives that may be useful at times.\n\nnx.draw_circular(lat, node_size=100)\n\n\n\n\nAs you can see, it’s not easy to tell how similar two graphs are by comparing renderings of them.\n\n\n7.1.2 Neighbors\nThe essence of a graph is how each node connects to others.\n\nDefinition 7.2 Nodes are said to be adjacent if they share an edge. The neighbors of a node are the nodes that are adjacent to it.\n\n\n\n\n\n\n\n\n\n\nExample 7.3 We get the neighbors of a node by using it as an index on the graph object. As with nodes and edges, we usually need to convert the result into a more convenient form:\n\nprint( \"Neighbors of node 1 in the star graph:\", list(star[1]) )\nprint( \"Neighbors of node 6 in the star graph:\", list(star[6]) )\n\nNeighbors of node 1 in the star graph: [2, 3, 4, 5, 6]\nNeighbors of node 6 in the star graph: [1]\n\n\n\n\nExample 7.4 We can zoom in on a subset of the Twitch graph by selecting a node and its ego graph, which includes its neighbors along with all edges between the captured nodes:\n\nego = nx.ego_graph(twitch, 400)\nnx.draw(ego, with_labels=True, node_size=800, node_color=\"yellow\")\n\n\n\n\nNotice that the nodes of the ego network have the same labels as they did in the graph that it was taken from. We can widen the ego graph to include the ego graphs of all the neighbors:\n\nbig_ego = nx.ego_graph(twitch, 400, radius=2)\n\nprint(big_ego.number_of_nodes(), \"nodes and\", \n    big_ego.number_of_edges(), \"edges\")\n\nnx.draw(big_ego, \n    width=0.2, node_size=10, node_color=\"purple\")\n\n528 nodes and 1567 edges\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can represent all the adjacency relationships at once using a matrix.\n\nDefinition 7.3 If we number the nodes of a graph from \\(0\\) to \\(n-1\\), then its adjacency matrix is the \\(n\\times n\\) matrix whose entries are \\[\nA_{ij} = \\begin{cases}\n    1, & \\text{if $(i,j)$ is an edge}, \\\\\n    0, & \\text{otherwise.}\n    \\end{cases}\n\\]\n\nIn an undirected graph, we have \\[\nA_{ij}=A_{ji}\n\\] for all valid indices, and we say that \\(A\\) is symmetric.\n\nExample 7.5 The star graph defined in Section 7.1.1 has the following adjacency matrix:\n\nA = nx.adjacency_matrix(star)\nA\n\n&lt;6x6 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 10 stored elements in Compressed Sparse Row format&gt;\n\n\nThe matrix A is not stored in the format we have been used to. In a large network we would expect most of its entries to be zero, so it makes more sense to store it as a sparse matrix, where we keep track of only the nonzero entries.\n\nprint(A)\n\n  (0, 1)    1\n  (0, 2)    1\n  (0, 3)    1\n  (0, 4)    1\n  (0, 5)    1\n  (1, 0)    1\n  (2, 0)    1\n  (3, 0)    1\n  (4, 0)    1\n  (5, 0)    1\n\n\nWe can easily convert A to a standard array, if it is not too large to fit in memory.\n\nA.toarray()\n\narray([[0, 1, 1, 1, 1, 1],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0],\n       [1, 0, 0, 0, 0, 0]])\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.1.3 Degree\n\nDefinition 7.4 The degree of a node is the number of neighbors it has. The average degree of a graph is the mean of the degrees of all of its nodes.\n\n\n\n\n\n\n\n\n\nIt’s common to use the notation \\(\\bar{k}\\) for the average degree of a graph.\nThe degree property of a NetworkX graph gives an object of all nodes with their degrees.\n\nExample 7.6 Continuing with the graph from Example 7.4:\n\nego.degree\n\nDegreeView({897: 2, 400: 9, 5394: 2, 3379: 3, 4406: 1, 5079: 1, 6136: 2, 5049: 1, 6107: 1, 639: 2})\n\n\nThe result here can be a bit awkward to work with; it’s actually a generator of a list, rather than the list itself. (This “lazy” attitude is useful when dealing with very large networks.) We can transform it into different forms, such as a dictionary:\n\ndict(ego.degree)\n\n{897: 2,\n 400: 9,\n 5394: 2,\n 3379: 3,\n 4406: 1,\n 5079: 1,\n 6136: 2,\n 5049: 1,\n 6107: 1,\n 639: 2}\n\n\nGiven our comfort level with pandas, it might be most convenient to use a series or frame to keep track of such quantities:\n\ndegrees = pd.Series( dict(ego.degree) )\ndegrees\n\n897     2\n400     9\n5394    2\n3379    3\n4406    1\n5079    1\n6136    2\n5049    1\n6107    1\n639     2\ndtype: int64\n\n\nWe can then do familiar calculations:\n\nprint(\"mean degree of ego graph:\", degrees.mean())\nprint(\"median degree of ego graph:\", degrees.median())\n\nmean degree of ego graph: 2.4\nmedian degree of ego graph: 2.0\n\n\nAs we are about to see, though, there’s an easier way to compute the average degree.\n\n\n\n\n\n\n\n\n\nHere is our first nontrivial fact about graphs.\n\nTheorem 7.1 Suppose a graph has \\(n\\) nodes and \\(m\\) edges. Then its average degree satisfies \\[\n\\bar{k} = \\frac{2m}{n},\n\\]\n\n\nProof. If we sum the degrees of all the nodes, we must get \\(2m\\), because each edge will have been contributed twice to the summation. Dividing by \\(n\\) gives the mean.\n\n\nExample 7.7 Just to check the theorem on the ego graph from Example 7.4:\n\n2*ego.number_of_edges() / ego.number_of_nodes()\n\n2.4"
  },
  {
    "objectID": "networks.html#random-graphs",
    "href": "networks.html#random-graphs",
    "title": "7  Networks",
    "section": "7.2 Random graphs",
    "text": "7.2 Random graphs\nOne way of understanding a real-world network is by comparing it to ones that are constructed randomly, but according to relatively simple rules. The idea is that if the real network behaves similarly to members of some random family, then perhaps it is constructed according to similar principles.\n\n7.2.1 Erdős-Rényi graphs\nOur first construction gives every potential edge an equal chance to exist.\nStart with \\(n\\) nodes and no edges. Suppose you have a weighted coin that comes up heads (100p)% of the time. For each pair of nodes, you toss the coin, and if it comes up heads, you add the edge between those nodes. This is known as an ER graph.\n\n\n\n\n\n\n\n\n\nDefinition 7.5 An Erdős-Rényi graph (ER graph) is a graph in which each potential edge occurs with a fixed probability \\(p\\).\n\nSince there are \\(\\binom{n}{2}\\) unique unordered pairs among \\(n\\) nodes, the mean number of edges in an ER graph is \\[\np\\binom{n}{2} = \\frac{pn(n-1)}{2}.\n\\] This fact is usually stated in terms of the average node degree.\n\nTheorem 7.2 The average degree \\(\\bar{k}\\) satisfies\n\\[\n\\E{\\bar{k}} = \\frac{1}{n} pn(n-1) = p(n-1),\n\\] where the expectation operation is taken over all realizations of ER graphs on \\(n\\) nodes.\n\nThere are two senses of averaging going on in Theorem 7.2:. We take the average (expectation operation) over all random instances of the average degree over all nodes within the instance.\n\nExample 7.8  \n\nn, p = 50, 0.08\nER = nx.erdos_renyi_graph(n, p, seed=2)\nprint(ER.number_of_nodes(), \"nodes,\", ER.number_of_edges(), \"edges\")\nnx.draw_circular(ER, node_size=50, edge_color=\"gray\")\n\n50 nodes, 91 edges\n\n\n\n\n\nHere is the distribution of \\(\\bar{k}\\) over 20000 ER instances:\n\ndef average_degree(G):\n    return 2*G.number_of_edges() / G.number_of_nodes()\n\nkbar = []\nn, p = 41, 0.1\nfor iter in range(20000):\n    ER = nx.erdos_renyi_graph(n, p, seed=iter + 302)\n    kbar.append( average_degree(ER) )\n\nsns.displot( x=kbar, bins=23 );\n\n\n\n\nThe mean of this distribution converges to \\(p(n-1)=4\\) as the number of random instances goes to infinity.\n\n\n\n\n\n\n\n\n\nTheorem 7.1 states that the average degree in a graph with \\(n\\) nodes and \\(m\\) edges is \\(2m/n\\). According to Theorem 7.2, an ER graph will have the same expected average degree if\n\\[\n\\frac{2m}{n} = p(n-1),\n\\tag{7.1}\\]\nor \\(p=2m/(n^2-n)\\).\nIn Example 7.2, for example, we found that our Twitch network has \\(n=7126\\) nodes and \\(m=35324\\) edges, which suggests an ER equivalent of\n\\[\np = \\frac{2(35324)}{(7126)(7125)} \\approx 0.001391.\n\\]\nThat is, if connections in the Twitch network were made completely at random, they would have to occur at a probability of about \\(0.14\\)%.\n\n\n7.2.2 Watts–Strogatz graphs\nA Watts–Strogatz graph (WS graph) is a crude model of human social relationships. A WS graph has three parameters: \\(n\\), an even integer \\(k\\), and a probability \\(q\\).\n\n\n\n\n\n\n\n\nImagine \\(n\\) nodes arranged in a circle. In the first phase, we connect each node with an edge to all of its \\(k/2\\) left neighbors and \\(k/2\\) right neighbors. This setup, called a ring lattice, represents the idea that people have a local network of friends.\nIn the next phase, we rewire some of the local relationships into far-flung ones. We visit each node \\(i\\) in turn. For each edge from \\(i\\) to a neighbor, with probability \\(q\\) we replace it with an edge between \\(i\\) and a node chosen at random from all the nodes \\(i\\) currently not adjacent to \\(i\\).\n\nExample 7.9 If we set \\(q=0\\), we disable the rewiring phase and get a regular ring lattice:\n\nWS = nx.watts_strogatz_graph(13, 4, 0, seed=302)\nnx.draw_circular(WS, node_size=100)\n\n\n\n\nAs \\(q\\) increases, we introduce more randomness:\n\nWS = nx.watts_strogatz_graph(13, 4, 0.25, seed=302)\nnx.draw_circular(WS, node_size=100)\n\n\n\n\nTaken to the limit \\(q=1\\), we end up with something that loses the mostly-local structure:\n\nWS = nx.watts_strogatz_graph(13, 4, 1.0, seed=302)\nnx.draw_circular(WS, node_size=100)\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the initial setup phase, every node in the WS graph has degree equal to \\(k\\). The rewiring phase only changes edges—it does not insert nor delete them—so the average degree remains \\(k\\).\n\nTheorem 7.3 A WS graph of type \\((n,k,q)\\) has average node degree equal to \\(k\\).\n\nBecause of this result and Theorem 7.1, we have \\[\nk = \\frac{2m}{n}\n\\] to get a WS graph of the same average degree as any other graph with \\(n\\) nodes and \\(m\\) edges. Keep in mind that we are constrained to use an even integer for \\(k\\), so this relation usually can hold only approximately."
  },
  {
    "objectID": "networks.html#clustering",
    "href": "networks.html#clustering",
    "title": "7  Networks",
    "section": "7.3 Clustering",
    "text": "7.3 Clustering\n\n\n\n\n\n\nNote\n\n\n\nThe term clustering has a meaning in network analysis that has little to do with our earlier use of clustering for samples in a feature matrix.\n\n\nIn your own social networks, your friends are probably more likely to be friends with each other than pure randomness would imply. There are various ways to quantify this effect precisely, but here is one of the simplest.\n\nDefinition 7.6 The local clustering coefficient for node \\(i\\) is \\[\nC(i) = \\frac{ 2 T(i) }{d_i(d_i-1)},\n\\] where \\(d_i\\) is the degree of the node and \\(T(i)\\) is the number of triangles in the ego graph of node \\(i\\). If \\(d_i=0\\) or \\(d_i=1\\), we set \\(C(i)=0\\).\n\nThe value of \\(T(i)\\) is the same as the number of edges between node \\(i\\)’s neighbors. Because there are \\(\\binom{d_i}{2}\\) unique pairs of neighbors, the value of \\(C(i)\\) is between 0 and 1.\n\nExample 7.10 Let’s find the clustering coefficient for each node in this wheel graph:\n\nW = nx.wheel_graph(7)\nnx.draw(W, with_labels=True, node_color=\"lightblue\")\n\n\n\n\nNode 0 is adjacent to 6 other nodes, and there are 6 triangles that include it. Thus, its clustering coefficient is \\[\nC(0) = \\frac{6}{6 \\cdot 5 / 2} = \\frac{2}{5}.\n\\] Every other node has 3 neighbors and 2 triangles, so they each have \\[\nC(i) = \\frac{2}{3 \\cdot 2 / 2} = \\frac{2}{3}, \\quad i\\neq 0.\n\\]\nThe clustering function in NetworkX computes \\(C(i)\\) for any single node: or for all the nodes in a graph.\n\nprint(f\"node 0 clustering is {nx.clustering(W, 0):.7f}\")\n\nnode 0 clustering is 0.4000000\n\n\nIf not given a node label, the clustering function returns a dictionary of the clustering coefficients for all of the nodes. As usual, we can convert that output to a series or data frame.\n\nprint(\"\\nclustering at each node:\")\nprint( pd.Series(nx.clustering(W)) )\n\n\nclustering at each node:\n0    0.400000\n1    0.666667\n2    0.666667\n3    0.666667\n4    0.666667\n5    0.666667\n6    0.666667\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\nExample 7.11 Let’s examine clustering within the Twitch network:\n\n\nCode\ntwitch = nx.read_edgelist(\"_datasets/musae_edges.csv\", delimiter=',', nodetype=int)\n\ncluster = pd.Series(nx.clustering(twitch))\ncluster.head()\n\n\n6194    0.066667\n255     0.133333\n980     0.065359\n2992    0.000000\n2507    0.073593\ndtype: float64\n\n\nThe average clustering coefficient is\n\nprint( f\"average Twitch clustering is {cluster.mean():.4f}\" )\n\naverage Twitch clustering is 0.1309\n\n\nSuppose we want to explore the relationship between clustering and degree in the network. It’s most convenient to store both values as columns of a data frame:\n\nprops = pd.DataFrame()\nprops[\"clustering\"] = nx.clustering(twitch)\nprops[\"degree\"] = dict(twitch.degree)   # degree view converted to dict\nprops.head()\n\n\n\n\n\n\n\n\nclustering\ndegree\n\n\n\n\n6194\n0.066667\n6\n\n\n255\n0.133333\n6\n\n\n980\n0.065359\n18\n\n\n2992\n0.000000\n4\n\n\n2507\n0.073593\n22\n\n\n\n\n\n\n\nHere’s the distribution of clustering coefficients:\n\nsns.displot(data=props, x=\"clustering\");\n\n\n\n\nThere is a huge number of clustering values at zero. A plot of both quantities clarifies that these are due to nodes of degree 1:\n\nsns.relplot(data=props, x=\"degree\", y=\"clustering\", kind=\"line\");\n\n\n\n\nOther than that massive group, there is a negative association between degree and clustering coefficient in this network.\n\n\n\n\n\n\n\n\n\n\n7.3.1 ER graphs\n\nTheorem 7.4 The expected value of the average clustering coefficient in ER graphs of type \\((n,p)\\) is \\(p\\).\n\nA formal proof of this theorem is largely superfluous. Considering that each edge in the graph has a probability \\(p\\) of inclusion, then \\(p\\) is also the expected fraction of edges that appear within the neighborhood subgraph of any node.\n\nExample 7.12 Let’s compute average clustering within multiple ER random graphs.\n\nn,p = 121,1/20\nresults = []\nfor iter in range(400):\n    ER = nx.erdos_renyi_graph(n, p, seed=iter+5000)\n    results.append( nx.average_clustering(ER) )\n\nsns.displot(x=results);\n\n\n\n\nThe distribution above can’t be normal, because there are hard bounds at 0 and 1, but it looks similar to a normal distribution. The peak is at the value of \\(p\\) used in the simulation, in accordance with Theorem 7.4.\n\n\n\n\n\n\n\n\n\nIn Example 7.11, we saw that the Twitch network has \\(n=7126\\) and mean clustering equal to about 0.131. If an ER graph is expected to have that same mean clustering, then we must take \\(p=0.131\\), according to Theorem 7.4. But by Theorem 7.2, this would imply an average node degree of \\(p(n-1)\\approx 933\\), which is almost 10 times larger than what is actually observed! Thus, clustering strongly suggests that the Twitch network is not much like an ER graph.\n\n\n7.3.2 WS graphs\nThe initial setup of a WS graph of type \\((n,k,q)\\) is highly clustered by design (see Exercise 7.9). If \\(q\\) is close to zero, the final graph will retain much of this initial clustering. However, the average clustering coefficient tends to decrease as \\(q\\) increases.\n\nExample 7.13 Here is an experiment to observe how the average clustering in a WS graph depends on the rewiring probability \\(q\\):\n\n\nCode\nn, k = 60, 6\nresults = []\nseed = 302\nfor q in np.arange(0.05, 1.05, 0.05):\n    for iter in range(60):\n        WS = nx.watts_strogatz_graph(n, k, q, seed=seed+10*iter)\n        results.append( (q, nx.average_clustering(WS)) )\n\nresults = pd.DataFrame( results, columns=[\"q\", \"mean clustering\"] )\n\nprint(\"Mean clustering in WS graphs on 60 nodes:\")\nsns.relplot(data=results,\n    x=\"q\", y=\"mean clustering\",\n    kind=\"line\"\n    );\n\n\nMean clustering in WS graphs on 60 nodes:\n\n\n\n\n\nLet’s scale the experiment above up to the size of the Twitch network. Conveniently, the average degree in that network is roughly 10, which is the value we will use for \\(k\\) in the WS construction. To save computation time, we use just one WS realization at each value of \\(q\\).\n\n\nCode\nn, k = twitch.number_of_nodes(), 10\nseed = 19716\nresults = []\nfor q in np.arange(0.15, 0.51, 0.05):\n    WS = nx.watts_strogatz_graph(n, k, q, seed=seed)\n    results.append( (q, nx.average_clustering(WS)) )\n    seed += 1\n\npd.DataFrame( results, columns=[\"q\", \"mean clustering\"] )\n\n\n\n\n\n\n\n\n\nq\nmean clustering\n\n\n\n\n0\n0.15\n0.415477\n\n\n1\n0.20\n0.347262\n\n\n2\n0.25\n0.286947\n\n\n3\n0.30\n0.232891\n\n\n4\n0.35\n0.184559\n\n\n5\n0.40\n0.146123\n\n\n6\n0.45\n0.112606\n\n\n7\n0.50\n0.085669\n\n\n\n\n\n\n\nThe mean clustering for the Twitch network was found to be about 0.131 in Example 7.11. From the above, it appears that the WS graphs will have a similar mean clustering at around \\(q=0.42\\). Let’s freeze \\(q\\) there and try more WS realizations to check:\n\nseed = 3383\nn, k, q = twitch.number_of_nodes(), 10, 0.42\ncbar = []\nfor iter in range(10):\n    WS = nx.watts_strogatz_graph(n, k, q, seed=seed)\n    cbar.append( nx.average_clustering(WS) )\n    seed += 10\nprint( f\"avg WS clustering at q = 0.42 is {np.mean(cbar):.4f}\" )\n\navg WS clustering at q = 0.42 is 0.1327\n\n\n\n\n\n\n\n\n\n\n\nSo far, the WS construction gives a plausible way to reconstruct the clustering observed in the Twitch network. However, there are many other graph properties left to examine!"
  },
  {
    "objectID": "networks.html#distance",
    "href": "networks.html#distance",
    "title": "7  Networks",
    "section": "7.4 Distance",
    "text": "7.4 Distance\nThe small-world phenomenon is, broadly speaking, the observation that any two people in a large group can be connected by a surprisingly short path of acquaintances. This concept appears, for instance, in the Bacon number game, where actors are nodes, appearing in the same movie creates an edge between them, and one tries to find the distance between Kevin Bacon and some other designated actor.\n\nDefinition 7.7 A path from node \\(x\\) to node \\(y\\) is a sequence of edges in the graph, either the singleton sequence \\((x,y)\\), or \\[\n(x,z_1),\\, (z_1,z_2)\\, (z_2,z_3)\\, \\ldots, (z_{t-1},z_t)\\, (z_t,y),\n\\] for some \\(t\\ge 1\\). The path above has length \\(t+1\\), which is the number of edges in the path. The distance between two nodes in a graph is the length of the shortest path between them. The maximum distance over all pairs of nodes in a graph is called its diameter.\n\n\n\n\n\nExample 7.14 In a complete graph, all possible pairs of nodes are directly connected by edges. Therefor, distance between any pair of distinct nodes is 1, and the graph has diameter 1.\n\nBecause the diameter depends on the extreme outlier of the distance distribution, it’s often more useful to consider the average distance by taking the mean.\n\nExample 7.15 Here is a small wheel graph:\n\nW = nx.wheel_graph(7)\nnx.draw(W, with_labels=True, node_color=\"lightblue\")\n\n\n\n\nNo node is more than two hops away from another, so the diameter of this graph is 2. This graph has so few nodes that we can easily compute the entire matrix of pairwise distances:\n\ndistance = dict( nx.all_pairs_shortest_path_length(W) )\nD = np.array( [ [distance[i][j] for j in W.nodes] for i in W.nodes] )\nprint(D)\n\n[[0 1 1 1 1 1 1]\n [1 0 1 2 2 2 1]\n [1 1 0 1 2 2 2]\n [1 2 1 0 1 2 2]\n [1 2 2 1 0 1 2]\n [1 2 2 2 1 0 1]\n [1 1 2 2 2 1 0]]\n\n\nIf all we want is the average distance, though, there is a convenience function for computing it directly:\n\nmean_dist = nx.average_shortest_path_length(W)\nprint(f\"The average distance is {mean_dist:.5f}\" )\n\nThe average distance is 1.42857\n\n\nThis function becomes quite slow as the number of nodes grows. In practice, the average distance might be estimated by taking a sample from the set of all possible pairs.\n\n\n7.4.1 Connectedness\nIt could happen that there is a pair of nodes that is not connected by any path of edges. In that case, the distance between them is either undefined or infinite, and we say the graph is disconnected. In such a situation, it’s common to decompose the graph into connected components, a separation of the nodes into connected subgraphs.\n\nExample 7.16 The following graph is clearly connected:\n\nG = nx.lollipop_graph(5,3)\nnx.draw(G, with_labels=True, node_color=\"yellow\" )\n\n\n\n\nNetworkX has a function to determine this for us:\n\nnx.is_connected(G)\n\nTrue\n\n\nIf we remove the edge between nodes 4 and 5, then the graph is no longer connected:\n\nG.remove_edge(4,5)\nprint(\"Is it connected?\", nx.is_connected(G))\nnx.draw(G, with_labels=True, node_color=\"yellow\" )\n\nIs it connected? False\n\n\n\n\n\nWe get an error if we try to compute all the distances in a disconnected graph:\n\nnx.average_shortest_path_length(G)\n\nNetworkXError: Graph is not connected.\n\n\nThe connected_components function creates a generator of subgraphs that are the connected components. We can collect the components into a list, for example:\n\nlist( nx.connected_components(G) )\n\n[{0, 1, 2, 3, 4}, {5, 6, 7}]\n\n\nEach element of the list above is a set of nodes.\n\n\n\n\n\n\n\n\n\n\n\n7.4.2 ER graphs\nAn ER graph may be disconnected. In order to study the statistics of distances for ER graphs, it’s customary to use the largest connected component (often called the big component) of each instance.\n\nExample 7.17 Let’s examine average distances within ER graphs of a fixed type. First, we need to be able to extract the largest component of a disconnected graph. Here is the idiom (referring back to Example 7.16):\n\nG = nx.lollipop_graph(5,3)\nG.remove_edge(4,5)\nG_sub = G.subgraph( max(nx.connected_components(G), key=len) )\nset( G_sub.nodes )\n\n{0, 1, 2, 3, 4}\n\n\nUsing this idiom, we set up and run the experiment:\n\nn, p = 121, 1/20\ndbar = []\nfor iter in range(100):\n    ER = nx.erdos_renyi_graph(n, p, seed=iter+5000)\n    ER_sub = ER.subgraph( max(nx.connected_components(ER), key=len) )\n    dbar.append( nx.average_shortest_path_length(ER_sub) )\n\nprint(\"average distance in the big component of ER graphs:\")\nsns.displot(x=dbar, bins=13);\n\naverage distance in the big component of ER graphs:\n\n\n\n\n\nThe chances are good, therefore, that any message could be passed along in three hops or fewer within the big component of this type of ER graph.\n\nTheory states that as \\(n\\to\\infty\\), the mean distance in ER graphs is expected to be approximately \\[\n\\frac{\\ln(n)}{\\ln(\\bar{k})}.\n\\tag{7.2}\\] For Example 7.17, we have \\(n=121\\) and \\(\\bar{k}=6\\), which gives about 2.68 in this formula.\n\n\n7.4.3 WS graphs\nThe Watts–Strogatz model was originally proposed to demonstrate small-world networks. The initial ring-lattice structure of the construction exhibits both large clustering and large mean distance:\n\n# q=0 ==&gt; initial ring lattice\nG = nx.watts_strogatz_graph(400, 6, 0)  \nC0 = nx.average_clustering(G)\nL0 = nx.average_shortest_path_length(G)\nprint(f\"Ring lattice has average clustering {C0:.4f}\")\nprint(f\"and average shortest path length {L0:.2f}\")\n\nRing lattice has average clustering 0.6000\nand average shortest path length 33.75\n\n\n\n\n\n\n\n\n\n\nAt the other extreme of \\(q=1\\), we get small clustering and small average distance. The most interesting aspect of WS graphs is the transition between these extremes as \\(q\\) varies.\n\n\nCode\nresults = []\nfor lq in np.arange(-3.5, 0.01, 0.25):\n    for iter in range(8):\n        G = nx.watts_strogatz_graph(400, 6, 10**lq, seed=302 + 10*iter)\n        C = nx.average_clustering(G) / C0 \n        L = nx.average_shortest_path_length(G) / L0\n        results.append( (lq, C, L) )\n    \nresults = pd.DataFrame( results, \n    columns=[ \"log10(q)\", \"mean clustering\", \"mean distance\" ] \n    )  \n\nsns.relplot(data=pd.melt(results, id_vars=\"log10(q)\"),\n            x=\"log10(q)\", y=\"value\",\n            hue=\"variable\", kind=\"line\"\n            );\n\n\n\n\n\nThe \\(y\\)-axis shows the average clustering and distance relative to their values at \\(q=0\\), computed above as C0 and L0. Watts and Strogatz raised awareness of the fact that for quite small values of \\(q\\), i.e., relatively few nonlocal connections, there are networks with both large clustering and small average distance—in short, the small-world effect.\n\n\n7.4.4 Twitch network\nNow we consider distances within the Twitch network.\n\ntwitch = nx.read_edgelist(\"_datasets/musae_edges.csv\", delimiter=',', nodetype=int)\nn, e = twitch.number_of_nodes(), twitch.number_of_edges()\nkbar = 2*e/n\nprint(n, \"nodes and\", e, \"edges\")\nprint(f\"average degree is {kbar:.3f}\")\n\n7126 nodes and 35324 edges\naverage degree is 9.914\n\n\nComputing the distances between all pairs of nodes in this graph would take a long time, so we sample pairs randomly to get an estimate.\n\n\nCode\nrng = default_rng(19716)\n\n# Compute the distance between a random pair of distinct nodes:\ndef pairdist(G):\n    n = nx.number_of_nodes(G)\n    i = j = rng.integers(0,n)\n    while i==j: j=rng.integers(0,n)   # get distinct nodes\n    return nx.shortest_path_length(G, source=i, target=j)\n\ndistances = [ pairdist(twitch) for i in range(50000) ]\nprint(\"Pairwise distances in Twitch graph:\")\nsns.displot(x=distances, discrete=True)\nprint( \"estimated mean =\", np.mean(distances) )\n\n\nPairwise distances in Twitch graph:\nestimated mean = 3.67584\n\n\n\n\n\n\n\n\n\n\n\n\n\nNext we explore WS graphs with the same \\(n\\) as the Twitch network and \\(k=10\\) to get a similar average degree.\n\n\nCode\nresults = []\nseed = 44044\nn, k = twitch.number_of_nodes(), 10\nfor q in np.arange(0.1, 0.76, 0.05):\n    for iter in range(10):\n        WS = nx.watts_strogatz_graph(n, k, q, seed=seed)\n        dbar = sum(pairdist(WS) for _ in range(60))/60\n        results.append( (q,dbar) )\n        seed += 7\n\nresults = pd.DataFrame( results, columns=[\"q\", \"mean distance\"] )\nprint(\"Pairwise distances in WS graphs:\")\nsns.relplot(data=results, x=\"q\", y=\"mean distance\", kind=\"line\");\n\n\nPairwise distances in WS graphs:\n\n\n\n\n\nThe decrease with \\(q\\) here is less pronounced that we saw for the smaller WS graphs in Section 7.4.3.\nIn Example 7.13, we found that \\(q=0.42\\) reproduces the same average clustering as in the Twitch network. In the graph above, that corresponds to a mean distance of about 4.5, which is a bit above the estimated Twitch mean distance of 3.7.\nThus, it’s fair to say that our Twitch network has a mean distance comparable to that of a small-world network with the same size, average degree, and local clustering."
  },
  {
    "objectID": "networks.html#degree-distributions",
    "href": "networks.html#degree-distributions",
    "title": "7  Networks",
    "section": "7.5 Degree distributions",
    "text": "7.5 Degree distributions\nAs we well know, the mean of a distribution does not tell the entire story about it.\n\nExample 7.18 The distribution of the degrees of all the nodes in our Twitch network has some surprising features:\n\ntwitch = nx.read_edgelist(\"_datasets/musae_edges.csv\", delimiter=',', nodetype=int)\ntwitch_degrees = pd.Series( dict(twitch.degree) )\ntwitch_degrees.describe()\n\ncount    7126.000000\nmean        9.914117\nstd        22.190263\nmin         1.000000\n25%         2.000000\n50%         5.000000\n75%        11.000000\nmax       720.000000\ndtype: float64\n\n\nObserve above that that there is a significant disparity between the mean and median values of the degree distribution, and that the standard deviation is much larger than the mean. A histogram plot confirms that the degree distribution is widely dispersed:\n\nprint(\"Twitch network degree distribution:\")\nsns.displot(twitch_degrees);\n\nTwitch network degree distribution:\n\n\n\n\n\nIn fact, a few nodes in the network have hundreds of friends:\n\nfriend_counts = twitch_degrees.value_counts()           # histogram bar heights\nfriend_counts.sort_index(ascending=False).head(5)\n\n720    1\n691    1\n465    1\n378    1\n352    1\ndtype: int64\n\n\nThese “gregarious nodes” or hubs are characteristic of many social and other real-world networks.\n\n\n\n\n\n\n\n\n\nNeither ER nor WS graphs show degree distributions even remotely like that of the Twitch network.\n\nExample 7.19 We can compare the distribution from Example 7.18 to that in a sample of ER graphs with the same size and expected average degree:\n\nn, m = twitch.number_of_nodes(), twitch.number_of_edges()\nkbar = 2*m/n\np = kbar/(n-1)\ndegrees = []\nfor iter in range(3):\n    ER = nx.erdos_renyi_graph(n, p, seed=111+iter)\n    degrees.extend( [ER.degree(i) for i in ER.nodes] )\n\nprint(\"ER graphs degree distribution:\")\nsns.displot(degrees, discrete=True);\n\nER graphs degree distribution:\n\n\n\n\n\nTheory proves that the plot above converges to a binomial distribution. A WS graph has a similar distribution:\n\nk, q = 10, 0.42\ndegrees = []\nfor iter in range(3):\n    WS = nx.watts_strogatz_graph(n, k, q, seed=222+iter)\n    degrees.extend( [WS.degree(i) for i in WS.nodes] )\n\nprint(\"WS graphs degree distribution:\")\nsns.displot(degrees, discrete=True);\n\nWS graphs degree distribution:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.5.1 Power-law distribution\nThe behavior of the Twitch degree distribution in Example 7.18 gets very interesting when the axes are transformed to use log scales:\n\nhist = sns.displot(data=twitch_degrees, log_scale=True)\nhist.axes[0,0].set_yscale(\"log\")\n\n\n\n\n\n\n\n\n\n\n\n\nFor degrees between 10 and several hundred, the counts lie nearly on a straight line. That is, if \\(x\\) is degree and \\(y\\) is the node count at that degree, then \\[\n\\log(y) \\approx  - a\\cdot \\log(x) + b,\n\\] i.e., \\[\ny \\approx B x^{-a},\n\\] for some \\(a &gt; 0\\). This relationship is known as a power law, a relationship often seen in physics. The decay of \\(x^{-a}\\) to zero as \\(x\\to\\infty\\) is much slower than the normal distribution’s \\(e^{-x^2/2}\\), for instance, and we often say that the power-law network has a heavy-tailed distribution.\nMany social networks seem to follow a power-law distribution of node degrees, to some extent. The precise extent is a subject of heated debate.\n\n\n\n7.5.2 Barabási–Albert graphs\nNeither the ER nor the WS model can reproduce a power-law degree distribution. A random Barabási–Albert graph (BA graph) is a better approach.\n\n\n\n\n\n\n\n\nThe construction starts with a small seed graph. One by one, new nodes are added with \\(m\\) edges connecting it to existing nodes. The new edges are chosen randomly, but higher probability is given for connections to nodes that already have higher degree (i.e., are more popular). This is a rich-get-richer concept known as preferential attachment.\nBecause of preferential attachment, there is a strong tendency to develop some hubs of very high degree, as is required for a power-law degree distribution.\n\nExample 7.20 Here is a BA graph with 200 nodes and \\(m=3\\) connections per new node:\n\nBA = nx.barabasi_albert_graph(200, 3, seed=302)\nBA_degrees = pd.Series( dict(BA.degree) )\nnx.draw(\n    BA, \n    node_size=6*BA_degrees, \n    node_color=\"red\", edge_color=\"gray\"\n    )\n\n\n\n\nThe size of each node in the drawing above is proportional to its degree. In the default rendering, the large hubs crowd into the center of the picture, while the lowest-degree nodes are pushed to the periphery.\n\nProbability theory predicts that a BA graph has a power-law degree distribution with exponent \\(-3\\) as \\(n\\to \\infty\\).\nExcept fot the initial seed stage, each node of a BA graph introduces \\(m\\) new edges, for a total edge count of about \\(mn\\). By Theorem 7.1, the average degree of the graph is roughly \\(\\bar{k} = 2mn/n = 2m\\).\n\nExample 7.21 For the Twitch network, \\(\\bar{k}\\) is almost 10, so we use \\(m=5\\) to get a comparable BA graph. Here we construct such a graph with the same number of nodes as the Twitch network.\n\nn = twitch.number_of_nodes()\nm = round(kbar/2)\nBA = nx.barabasi_albert_graph(n, m, seed=5)\nBA_degrees = pd.Series( dict(BA.degree) )\nhist = sns.displot(data=BA_degrees, log_scale=True)\nhist.axes[0,0].set_yscale(\"log\")\n\n\n\n\nWhile BA graphs capture the heavy-tailed degree distribution of the Twitch network rather well, they are way off when it comes to clustering. For the Twitch network, we have:\n\nprint(f\"Mean clustering in Twitch: {nx.average_clustering(twitch):.5f}\")\n\nMean clustering in Twitch: 0.13093\n\n\nAnd for BA, we get:\n\ncbar = []\nseed = 3383\nfor iter in range(40):\n    BA = nx.barabasi_albert_graph(n, m, seed=seed)\n    cbar.append( nx.average_clustering(BA) )\n    seed += 1\n    \nprint(f\"Mean clustering in BA graphs: {np.mean(cbar):.5f}\")\n\nMean clustering in BA graphs: 0.00923\n\n\n\n\n\n\n\n\n\n\n\nThe Twitch network has characteristics of both small-world (high clustering with small distance) and power-law (superstar hub nodes) networks. Possibly, some combination of the WS and BA constructions would get closer to the Twitch network than either one does alone."
  },
  {
    "objectID": "networks.html#centrality",
    "href": "networks.html#centrality",
    "title": "7  Networks",
    "section": "7.6 Centrality",
    "text": "7.6 Centrality\nIn some applications, we want to know which nodes of a network are the most important in some sense. For instance, we might want to find the most informative website, the most influential members of a social network, or nodes that are critical for efficient transfer within the network. These traits go under the general name of centrality.\n\n7.6.1 Degree centrality\nA clear candidate for measuring the centrality of a node is its degree.\n\nDefinition 7.8 The degree centrality of a node is its degree, divided by the maximum possible degree for a node in the graph.\n\nFor an undirected graph without self-links, the maximum degree of a node in a graph with \\(n\\) nodes is just \\(n-1\\). NetworkX has a function degree_centrality to compute this quantity.\nWhile degree centrality yields useful information in some networks, it is not always a reliable measuring stick. For example, a search engine for the Web that uses degree centrality would be easily fooled by creating a large collection of sites that all link to each other, even if few outside sources also link to the collection. Other aspects of network layout also argue against relying on degree centrality.\n\nExample 7.22 We start with a small graph:\n\nG = nx.lollipop_graph(4,2)\nnx.draw(G, with_labels=True, node_color=\"pink\")\n\n\n\n\nSince this graph has 6 nodes, the degree centrality is equal to the degree divided by 5:\n\nvalues = pd.DataFrame( index=G.nodes )\nvalues[\"degree\"] = dict(G.degree)\nvalues[\"degree centrality\"] = nx.degree_centrality(G)\nvalues\n\n\n\n\n\n\n\n\ndegree\ndegree centrality\n\n\n\n\n0\n3\n0.6\n\n\n1\n3\n0.6\n\n\n2\n3\n0.6\n\n\n3\n4\n0.8\n\n\n4\n2\n0.4\n\n\n5\n1\n0.2\n\n\n\n\n\n\n\nNow consider the following Watts–Strogatz graph:\n\nG = nx.watts_strogatz_graph(60, 2, .1, seed=6)\ndeg_cen = pd.Series( nx.degree_centrality(G) )\n\npos = nx.spring_layout(G, seed=1)\nstyle = dict(pos=pos, edge_color=\"gray\", node_color=\"pink\", with_labels=True)\nnx.draw(G, **style, node_size=3000*deg_cen.values)\n\n\n\n\nThere is little variation in the degrees of the nodes. In fact, there are only 3 unique values of the degree centrality. From the rendering of the graph, though, it’s clear that the nodes with degree equal to 2 are not all roughly equivalent. If we remove node 6, for instance, then node 5 is the only other node that will be affected. But if we remove node 22, we cut off a large branch from the rest of the network.\n\n\n\n\n\n\n\n\n\n\n\n7.6.2 Betweenness centrality\nA different way to measure centrality is to consider the paths between nodes. If \\(i\\) and \\(j\\) are nodes in a graph, then there is at least one path between them whose length is the shortest possible, that is, whose length is the distance between \\(i\\) and \\(j\\). If there is only one such path, then removing it would increase the distance between these nodes. But if there are multiple paths of shortest length between the nodes, then the graph has more redundancy or connectedness built-in.\n\nDefinition 7.9 For a graph on \\(n\\) nodes, let \\(\\sigma(i,j)\\) denote the number of shortest paths between nodes \\(i\\) and \\(j\\). For any other node \\(k\\), let \\(\\sigma(i,j|k)\\) be the number of such shortest paths that pass through node \\(k\\). Then the betweenness centrality of node \\(k\\) is \\[\nc_B(k) = \\frac{1}{\\binom{n-1}{2}}\\, \\displaystyle\\sum_{\\substack{\\text{all pairs }i,j\\\\[1mm]i\\neq k,\\,j\\neq k}} \\frac{\\sigma(i,j|k)}{\\sigma(i,j)}.\n\\tag{7.3}\\]\n\n\n\n\n\n\n\n\n\nIn the sum above, node \\(k\\) remains fixed while \\(i\\) and \\(j\\) range over all the other possible pairings. If no shortest path between a particular \\(i\\) and \\(j\\) passes through node \\(k\\), then that term is zero. Otherwise, it equals the fraction of shortest paths that do pass through \\(k\\). Finally, the binomial coefficient \\(\\binom{n-1}{2}\\) is the number of terms in the sum.\nIt’s easy to prove that \\(0\\le c_B(k) \\le 1\\) for any node and that \\(c_B(k)=0\\) if node \\(k\\) has degree zero or one.\n\nExample 7.23 We will find the betweenness centrality of the following barbell graph:\n\n\n\nBarbell graph\n\n\nLet’s begin with node \\(k=3\\), in the middle. It might help to see a matrix for the terms of the sum in Equation 7.3: \\[\n\\begin{array}{r|cccccc}\n& 0 & 1 & 2 & 4 & 5 & 6 \\\\\n0 &  & & & 1/1 & 1/1 & 1/1 \\\\\n1 & & & & 1/1  & 1/1  & 1/1 \\\\\n2 & & & & 1/1 & 1/1 & 1/1 \\\\\n4 & 1/1 & 1/1 & 1/1 & & & \\\\\n5 & 1/1 & 1/1 & 1/1 & & & \\\\\n6 & 1/1 & 1/1 & 1/1 & & & \\\\\n\\end{array}\n\\] How was this filled in? Note that we can divide the nodes into the right-side triangle \\(\\{0,1,2\\}\\) and the left-side triangle \\(\\{4,5,6\\}\\). If we take both \\(i\\) and \\(j\\) from the same triangle, then no shortest paths between \\(i\\) and \\(j\\) ever go through node \\(3\\). This explains the empty entries. On the other hand, if \\(i\\) and \\(j\\) are in different triangles, then there is a unique shortest path between them, and it does pass through node 3. Therefore, \\(\\sigma(i,j)=\\sigma(i,j|3)=1\\) for those pairs. As usual, we sum over only the upper triangle of the matrix to avoid counting both \\((i,j)\\) and \\((j,i)\\), and since \\(n=7\\), we divide by \\((6)(5)/2=15\\) to obtain \\[\nc_B(3) = \\frac{9}{15} = \\frac{3}{5}.\n\\]\nNext, consider node \\(k=2\\). The reasoning is similar to the above if we divide the other nodes into the sets \\(\\{0,1\\}\\) and \\(\\{3,4,5,6\\}\\). Hence their betweenness matrix looks like \\[\n\\begin{array}{r|cccccc}\n& 0 & 1 & 3 & 4 & 5 & 6 \\\\\n0 & & & 1/1 & 1/1 & 1/1 & 1/1 \\\\\n1 & & & 1/1 & 1/1  & 1/1  & 1/1 \\\\\n3 & 1/1 & 1/1 & & & & \\\\\n4 & 1/1 & 1/1 & & & & \\\\\n5 & 1/1 & 1/1 & & & & \\\\\n6 & 1/1 & 1/1 & & & & \\\\\n\\end{array}\n\\] This gives \\(c_B(2)=8/15\\). By symmetry, we get the same value for \\(c_B(4)\\).\nFinally, all the other nodes appear in no shortest paths. For instance, any path passing through node 0 can be replaced with a shorter one that follows the edge between nodes 1 and 2. Hence \\(c_B\\) is zero on nodes 0, 1, 5, and 6.\n\n\n\n\n\n\n\n\n\n(video example is slightly different)\nThe betweenness_centrality function returns a dictionary with nodes as keys and \\(c_B\\) as values. (However, look at both of the next two examples.)\n\nExample 7.24 Let’s take a look at centrality measures for a power-law graph. Degree centrality points out a dominant hub:\n\nP = nx.barabasi_albert_graph(60, 1, seed=2)\ncdeg = pd.Series( nx.degree_centrality(P) )\n\nstyle = dict(edge_color=\"gray\", node_color=\"pink\", with_labels=True)\nstyle[\"pos\"] = nx.spring_layout(P, seed=3)\nnx.draw(P, node_size=1000*cdeg, **style)\n\n\n\n\nHowever, betweenness centrality also highlights some secondary hubs:\n\ncbet = pd.Series( nx.betweenness_centrality(P) )\nnx.draw(P, node_size=600*cbet, **style)\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nThe exact computation of betweenness centrality takes a very long time in a graph with a large number of nodes.\n\n\nIn practice, the \\(\\sigma\\) values in the definition of \\(c_B\\) are often estimated by sampling. This capability is also offered by the betweenness_centrality function.\n\nExample 7.25 With \\(n&gt;7000\\), the Twitch network is much too large for calculating betweenness centrality exactly. So we provide a parameter k to use estimation by sampling:\n\ntcb = pd.Series( \n    nx.betweenness_centrality(twitch, k=400, seed=302), \n    index=twitch.nodes \n    )\n\nhist = sns.displot(x=tcb, bins=24);\nhist.axes[0,0].set_yscale(\"log\");\n\n\n\n\nAs you can see above, only a few nodes stand out as having relatively large betweenness centrality:\n\ntcb.sort_values().tail(10)\n\n2732    0.024132\n1103    0.030459\n2447    0.034628\n166     0.038451\n6136    0.042333\n1924    0.043946\n5842    0.047202\n3401    0.063231\n1773    0.116461\n4949    0.124590\ndtype: float64\n\n\n\n\n\n\n\n\n\n\n\n\n\n7.6.3 Eigenvector centrality\nA different way of distinguishing nodes of high degree is to suppose that not all links are equally valuable. By analogy with ranking sports teams, where wins over good teams should count for more than wins over bad teams, we should assign more importance to nodes that link to other important nodes.\n\n\n\n\n\n\n\n\nWe can try to turn this idea into an algorithm as follows. Suppose we initially assign uniform centrality scores \\(x_1,\\ldots,x_n\\) to all of the nodes. Now we can update the scores by looking at the current scores for all the neighbors. Specifically, the new scores are\n\\[\nx_i^+ = \\sum_{j\\text{ adjacent to }i} x_j = \\sum_{j=1}^n A_{ij} x_j,\\quad i=1,\\ldots,n,\n\\]\nwhere \\(A_{ij}\\) are entries of the adjacency matrix. Once we have updated the scores, we can repeat the process to update them again, and so on. If the scores were to converge, in the sense that \\(x_i^+\\) approaches \\(x_i\\), then we would have a solution of the equation\n\\[\nx_i \\stackrel{?}{=} \\sum_{j=1}^n A_{ij} x_j, \\quad i=1,\\ldots,n.\n\\]\nIn fact, since the sums are all inner products across rows of \\(\\bfA\\), this is simply\n\\[\n\\bfx \\stackrel{?}{=} \\bfA \\bfx.\n\\]\nExcept for \\(\\bfx\\) equal to the zero vector, this equation does not have a solution in general. However, if we relax it just a bit, we get somewhere important. Instead of equality, let’s look for proportionality, i.e.,\n\\[\n\\lambda \\bfx = \\bfA \\bfx\n\\]\nfor a number \\(\\lambda\\). This is an eigenvalue equation, one of the fundamental problems in linear algebra.\n\nExample 7.26  \nEigenvectors!\n\n\n\n\n\n\n\nEvery \\(n\\times n\\) matrix has at least one nonzero solution to the eigenvalue equation, although complex numbers might be involved. For an adjacency matrix, the Perron–Frobenius theorem guarantees a real solution for some \\(\\lambda &gt; 0\\) and for which the \\(x_i\\) all have the same sign. That last property allows us to interpret the \\(x_i\\) as relative importance or centrality of the nodes.\n\nDefinition 7.10 The eigenvector centrality of node \\(i\\) is the value of \\(x_i\\), where \\(\\bfx\\) is a positive eigenvector of the adjacency matrix.\n\n\nExample 7.27 Consider the complete graph \\(K_3\\), which is just a triangle. Its adjacency matrix is \\[\n\\bfA = \\begin{bmatrix}\n0 & 1 & 1 \\\\ 1 & 0 & 1 \\\\ 1 & 1 & 0\n\\end{bmatrix}.\n\\] We would expect to discover that all three vertices are ranked equally. In fact, if we define \\(\\bfx=\\tfrac{1}{3}[1,1,1]\\), then \\[\n\\bfA \\bfx = \\bigl[\\tfrac{2}{3},\\tfrac{2}{3},\\tfrac{2}{3} \\bigr] = 2 \\bfx,\n\\] is an eigenvalue to go with eigenvector \\(\\bfx\\). Note that any (nonzero) multiple of \\(\\bfx\\) would work just as well: \\[\n\\bfA (c \\bfx) =  \\bigl[\\tfrac{2}{3}c,\\tfrac{2}{3}c,\\tfrac{2}{3}c \\bigr] = 2 (c\\bfx),\n\\] so that \\(c\\bfx\\) is also an eigenvector. All that the eigenvector gives us, in other words, is the relative centrality of the nodes.\n\n\n\n\n\n\n\n\n\n(video example is slightly different)\n\n\n\n\n\n\nNote\n\n\n\nWith a small tweak to avoid overestimating the neighbors of a major hub, eigenvector centrality becomes the PageRank algorithm, which launched Google.\n\n\nNetworkX has two functions for computing eigenvector centrality. One that relies on numpy to solve the eigenvalue problem tends to be the faster one and is called eigenvector_centrality_numpy. As with betweenness centrality, the return value is a dictionary with nodes as the keys.\n\nExample 7.28 Continuing with the small-world graph G from Example 7.22, we look at all three centrality measures side by side:\n\n\nCode\ncentrality = pd.DataFrame( {\"degree\": nx.degree_centrality(G)}, index=G.nodes )\ncentrality[\"between\"] = nx.betweenness_centrality(G)\ncentrality[\"eigen\"] = nx.eigenvector_centrality_numpy(G)\nsns.displot(data=pd.melt(centrality, var_name=\"centrality\"),\n    x=\"value\", col=\"centrality\",\n    height=2.4, bins=24\n    )\n\n\n\n\n\nOf the three, eigenvector centrality does the most to create a relatively small distinguished group. Correlation coefficients suggest that while the three centrality measures are related, they are far from redundant:\n\ncentrality.corr()\n\n\n\n\n\n\n\n\ndegree\nbetween\neigen\n\n\n\n\ndegree\n1.000000\n0.630732\n0.601884\n\n\nbetween\n0.630732\n1.000000\n0.736732\n\n\neigen\n0.601884\n0.736732\n1.000000\n\n\n\n\n\n\n\nHere is a ranking based on betweenness:\n\ncentrality.sort_values(by=\"between\", ascending=False).head(8)\n\n\n\n\n\n\n\n\ndegree\nbetween\neigen\n\n\n\n\n42\n0.050847\n0.595850\n0.405831\n\n\n23\n0.050847\n0.576856\n0.458056\n\n\n41\n0.033898\n0.385739\n0.232910\n\n\n40\n0.033898\n0.368206\n0.133669\n\n\n51\n0.050847\n0.363822\n0.392303\n\n\n39\n0.033898\n0.349503\n0.076714\n\n\n38\n0.033898\n0.329632\n0.044027\n\n\n22\n0.033898\n0.329632\n0.262883\n\n\n\n\n\n\n\nAs you can see, the top two are quite clear. A drawing of the graph supports the case that they are indeed central:\n\nstyle[\"pos\"] = nx.spring_layout(G, seed=1)\nnx.draw(G, node_size=500*centrality[\"between\"], **style)\n\n\n\n\nA drawback, though, is that there are many secondary nodes whose values taper off only slowly as we enter the remote branches.\nHere is a ranking according to eigenvector centrality:\n\ncentrality.sort_values(by=\"eigen\", ascending=False).head(8)\n\n\n\n\n\n\n\n\ndegree\nbetween\neigen\n\n\n\n\n23\n0.050847\n0.576856\n0.458056\n\n\n42\n0.050847\n0.595850\n0.405831\n\n\n51\n0.050847\n0.363822\n0.392303\n\n\n22\n0.033898\n0.329632\n0.262883\n\n\n4\n0.033898\n0.311806\n0.249077\n\n\n41\n0.033898\n0.385739\n0.232910\n\n\n52\n0.033898\n0.134717\n0.225527\n\n\n50\n0.033898\n0.212741\n0.225125\n\n\n\n\n\n\n\nThis ranking has a clear top choice, followed by two that are nearly identical. Here we visualize the scores on the rendering:\n\nnx.draw(G, node_size=800*centrality[\"eigen\"], **style)\n\n\n\n\nEigenvector centrality identifies a more compact and distinct center."
  },
  {
    "objectID": "networks.html#friendship-paradox",
    "href": "networks.html#friendship-paradox",
    "title": "7  Networks",
    "section": "7.7 Friendship paradox",
    "text": "7.7 Friendship paradox\nA surprising fact about social networks is that on average, your friends have more friends than you do, a fact that is called the friendship paradox. Let \\(\\mathbf{d}\\) be an \\(n\\)-vector whose components are the degrees of the nodes in the network. On average, the number of “friends” (i.e., adjacent nodes) is the average degree, which is equal to\n\\[\n\\frac{\\onenorm{\\mathbf{d}}}{n}.\n\\]\nNow imagine that we create a list as follows: for each node \\(i\\), add to the list the number of friends of each of \\(i\\)’s friends. The mean value of this list is the average number of “friends of friends.”\nFor example, consider the following graph:\n\nL = nx.lollipop_graph(4, 1)\nnx.draw(L, with_labels=True, node_color=\"lightblue\")\n\n\n\n\nThe average degree is \\((3+3+3+4+1)/5=14/5\\). Here are the entries in our friends-of-friends list contributed by each node:\n\nNode 0: 3 (from node 1), 3 (from node 2), 4 (from node 3)\nNode 1: 3 (from node 0), 3 (from node 2), 4 (from node 3)\nNode 2: 3 (from node 0), 3 (from node 1), 4 (from node 3)\nNode 3: 3 (from node 0), 3 (from node 1), 3 (from node 2), 1 (from node 4)\nNode 4: 4 (from node 3)\n\nThe average value of this list, i.e., the average number of friends’ friends, is \\(44/14=3.143\\), which is indeed larger than the average degree.\nThere is an easy way to calculate this value in general. Node \\(i\\) contributes \\(d_i\\) terms to the list, so the total number of terms is \\(\\onenorm{\\mathbf{d}}\\). We observe that node \\(i\\) appears \\(d_i\\) times in the list, each time contributing the value \\(d_i\\), so the sum of the entire list must be\n\\[\n\\sum_{i=1}^n d_i^2 = \\twonorm{\\mathbf{d}}^2 = \\mathbf{d}^T \\mathbf{d}.\n\\]\nHence the mathematical statement of the friendship paradox is\n\\[\n\\frac{\\onenorm{\\mathbf{d}}}{n} \\le \\frac{\\mathbf{d}^T \\mathbf{d}}{\\onenorm{\\mathbf{d}}}.\n\\tag{7.4}\\]\nYou are asked to prove this inequality in the exercises. Here is a verification for the BA graph above:\n\nn = G.number_of_nodes()\nd = pd.Series(dict(G.degree))\ndbar = d.mean()\ndbar_friends = np.dot(d,d) / d.sum()\n\nprint(dbar, \"is less than\", dbar_friends)\n\n2.0 is less than 2.066666666666667\n\n\nThe friendship paradox generalizes to eigenvector centrality: the average centrality of all nodes is less than the average of the centrality of all nodes’ friends. The mathematical statement is \\[\n\\frac{\\onenorm{\\mathbf{x}}}{n} \\le \\frac{\\mathbf{x}^T \\mathbf{d}}{\\onenorm{\\mathbf{d}}},\n\\tag{7.5}\\] where \\(\\bfx\\) is the eigenvector defining centrality of the nodes.\n\nx = centrality[\"eigen\"]\nxbar = x.mean()\nxbar_friends = np.dot(x,d) / sum(d)\nprint(xbar, \"is less than\", xbar_friends)\n\n0.07365890048370116 is less than 0.08530969685141405\n\n\nIn fact, the friendship paradox inequality for any vector \\(\\bfx\\) is equivalent to \\(\\bfx\\) having nonnegative correlation with the degree vector."
  },
  {
    "objectID": "networks.html#communities",
    "href": "networks.html#communities",
    "title": "7  Networks",
    "section": "7.8 Communities",
    "text": "7.8 Communities\nIn applications, one may want to identify communities within a network. There are many ways to define this concept precisely. We will choose a random-walk model.\nImagine that a bunny sits on node \\(i\\). In one second, the bunny hops to one of \\(i\\)’s neighbors, chosen randomly. In the next second, the bunny hops to another node chosen randomly from the neighbors of the one it is sitting on, etc. This is a random walk on the nodes of the graph.\nNow imagine that we place another bunny on node \\(i\\) and track its path as it hops around the graph. Then we place another bunny, etc., so that we have an ensemble of walks. We can now reason about the probability of the location of the walk after any number of hops. Initially, the probability of node \\(i\\) is 100%. If \\(i\\) has \\(m\\) neighbors, then each of them will have probability \\(1/m\\) after one hop, and all the other nodes (including \\(i\\) itself) have zero probability.\nLet’s keep track of the probabilities for this simple wheel graph:\n\nG = nx.wheel_graph(5)\nnx.draw(G, node_size=300, with_labels=True, node_color=\"yellow\")\n\n\n\n\nWe start at node 4. This corresponds to the probability vector\n\\[\n\\bfp = [0,0,0,0,1].\n\\]\nOn the first hop, we are equally likely to visit each of the nodes 0, 1, or 3. This implies the probability distribution\n\\[\n\\mathbf{q} = \\left[\\tfrac{1}{3},\\tfrac{1}{3},0,\\tfrac{1}{3},0\\right].\n\\]\nLet’s now find the probability of standing on node 0 after the next hop. The two possible histories are 4-1-0 and 4-3-0, with total probability\n\\[\n\\underbrace{\\frac{1}{3}}_{\\text{to 1}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 0}} + \\underbrace{\\frac{1}{3}}_{\\text{to 3}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 0}} = \\frac{2}{9}.\n\\]\nWhat about node 2 after two hops? The viable paths are 4-0-2, 4-1-2, and 4-3-2. Keeping in mind that node 0 has 4 neighbors, we get\n\\[\n\\underbrace{\\frac{1}{3}}_{\\text{to 0}} \\cdot \\underbrace{\\frac{1}{4}}_{\\text{to 2}} + \\underbrace{\\frac{1}{3}}_{\\text{to 1}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 2}} + \\underbrace{\\frac{1}{3}}_{\\text{to 3}} \\cdot \\underbrace{\\frac{1}{3}}_{\\text{to 2}}= \\frac{11}{36}.\n\\]\nThis quantity is actually an inner product between the vector \\(\\mathbf{q}\\) (probabilities of the prior location) and\n\\[\n\\bfw_2 = \\left[ \\tfrac{1}{4},\\, \\tfrac{1}{3},\\, 0,\\, \\tfrac{1}{3},\\, 0 \\right],\n\\]\nwhich encodes the chance of hopping directly to node 2 from anywhere. In fact, the entire next vector of probabilities is just\n\\[\n\\bigl[ \\bfw_1^T \\mathbf{q},\\, \\bfw_2^T \\mathbf{q},\\, \\bfw_3^T \\mathbf{q},\\, \\bfw_4^T \\mathbf{q},\\, \\bfw_5^T \\mathbf{q} \\bigr] = \\bfW \\mathbf{q},\n\\]\nwhere \\(\\bfW\\) is the \\(n\\times n\\) matrix whose rows are \\(\\bfw_1,\\bfw_2,\\ldots.\\) In terms of matrix-vector multiplications, we have the easy statement that the probability vectors after each hop are\n\\[\n\\bfp, \\bfW\\bfp, \\bfW(\\bfW\\bfp), \\ldots.\n\\]\nExplicitly, the matrix \\(\\bfW\\) is\n\\[\n\\bfW = \\begin{bmatrix}\n0 & \\tfrac{1}{3}  & \\tfrac{1}{3}  & \\tfrac{1}{3}  & \\tfrac{1}{3} \\\\\n\\tfrac{1}{4} & 0 & \\tfrac{1}{3}  & 0  & \\tfrac{1}{3} \\\\\n\\tfrac{1}{4} & \\tfrac{1}{3} & 0 & \\tfrac{1}{3} & 0 \\\\\n\\tfrac{1}{4} & 0 & \\tfrac{1}{3} & 0 & \\tfrac{1}{3} \\\\\n\\tfrac{1}{4} & \\tfrac{1}{3} & 0 & \\tfrac{1}{3} & 0\n\\end{bmatrix}.\n\\]\nThis has a lot of resemblance to the adjacency matrix\n\\[\n\\bfA = \\begin{bmatrix}\n0 & 1  & 1  & 1  & 1 \\\\\n1 & 0 & 1 & 0  & 1 \\\\\n1 & 1 & 0 & 1 & 0 \\\\\n1 & 0 & 1 & 0 & 1 \\\\\n1 & 1 & 0 & 1 & 0\n\\end{bmatrix}.\n\\]\nThe only difference is that each column has to be normalized by the number of options outgoing at that node, i.e., the degree of the node. Thus,\n\\[\nW_{ij} = \\frac{1}{\\operatorname{deg}(j)}\\,A_{ij}.\n\\]\n\n7.8.1 Simulating the random walk\nLet’s do a simulation for a more interesting graph:\n\nWS = nx.watts_strogatz_graph(40, 4, 0.04, seed=11)\npos = nx.spring_layout(WS, k=0.25, seed=1, iterations=200)\nstyle = dict(pos=pos, with_labels=True, node_color=\"pink\", edge_color=\"gray\")\n\nnx.draw(WS, node_size=240, **style)\n\n\n\n\nFirst, we construct the random-walk matrix \\(\\bfW\\).\n\nn = WS.number_of_nodes()\nA = nx.adjacency_matrix(WS).astype(float)\ndegree = [ WS.degree[i] for i in WS.nodes ] \n\nW = A.copy()\nfor j in range(n):\n    W[:,j] /= degree[j]\n\nsns.heatmap(W.toarray()).set_aspect(1);\n\n\n\n\nWe set up a probability vector to start at node 0, and then use W.dot to compute the first hop. The result is to end up at 5 other nodes with equal probability:\n\ninit = 33\np = np.zeros(n)\np[init] = 1\np = W.dot(p)\nsz = 3000*p\nprint( f\"Total probability after 1 hop: {p.sum():.6f}\" )\nnx.draw(WS, node_size=sz, **style)\n\nTotal probability after 1 hop: 1.000000\n\n\n\n\n\nAfter the next hop, there will again be a substantial probability of being at node 33. But we could also be at some second-generation nodes as well.\n\np = W.dot(p)\nprint( f\"Total probability after 2 hops: {p.sum():.6f}\" )\nnx.draw(WS, node_size=3000*p, **style)\n\nTotal probability after 2 hops: 1.000000\n\n\n\n\n\nWe’ll take 3 more hops. That lets us penetrate a little into the distant nodes.\n\nfor k in range(3):\n    p = W.dot(p)\nprint( f\"Total probability after 5 hops: {p.sum():.6f}\" )\nnx.draw(WS, node_size=3000*p, **style)\n\nTotal probability after 5 hops: 1.000000\n\n\n\n\n\nIn the long run, the probabilities even out, as long as the graph is connected.\n\nfor k in range(200):\n    p = W.dot(p)\nnx.draw(WS, node_size=3000*p, **style)\n\n\n\n\n\n\n7.8.2 Label propagation\nThe random walk brings us to a type of algorithm known as label propagation. We start off by “labelling” one or several nodes whose community we want to identify. This is equivalent to initializing the probability vector \\(\\bfp\\). Then, we take a running total over the entire history of the random walk:\n\\[\n\\hat{\\bfx} = \\lambda \\bfp_1  + \\lambda^2 \\bfp_2 +  \\lambda^3 \\bfp_3 + \\cdots,\n\\]\nwhere \\(0 &lt; \\lambda &lt; 1\\) is a damping parameter, and\n\\[\n\\bfp_1 = \\bfW \\bfp, \\, \\bfp_2 = \\bfW \\bfp_1, \\, \\bfp_3 = \\bfW \\bfp_2,\\, \\ldots.\n\\]\n\nIn practice, we terminate the sum once \\(\\lambda^k\\) is sufficiently small. The resulting \\(\\hat{\\bfx}\\) can be normalized to a probability distribution,\n\\[\n\\bfx = \\frac{\\hat{\\bfx}}{\\norm{\\hat{\\bfx}}_1}.\n\\]\nThe value \\(x_i\\) can be interpreted as the probability of membership in the community.\nLet’s try looking for a community of node 0 in the WS graph above.\n\np = np.zeros(n)\np[init] = 1\nlam = 0.8\n\nWe will compute \\(\\bfx\\) by accumulating terms in a loop. Note that there is no need to keep track of the entire history of random-walk probabilities; we just use one generation at a time.\n\nx = np.zeros(n)\nmult = 1\nfor k in range(200):\n    p = W.dot(p)\n    mult *= lam\n    x += mult*p\n\nx /= np.sum(x)  # normalize to probability distribution\n\nThe probabilities tend to be distributed logarithmically:\nIn the following rendering, any node \\(i\\) with a value of \\(x_i &lt; 10^{-2}\\) gets a node size of 0. (You can ignore the warning below. It happens because we have negative node sizes.)\n\nx[x&lt;0.01] = 0\nstyle[\"node_color\"] = \"lightblue\"\nnx.draw(WS, node_size=4000*x, **style)\n\n\n\n\nThe parameter \\(\\lambda\\) controls how quickly the random-walk process is faded out. A smaller value puts more weight on the early iterations, generally localizing the community more strictly.\n\np = np.zeros(n)\np[init] = 1\nlam = 0.4\nx = np.zeros(n)\nmult = 1\nfor k in range(200):\n    p = W.dot(p)\n    mult *= lam\n    x += mult*p\n\nx /= np.sum(x)  \n\n\nx[x&lt;0.01] = 0\nnx.draw(WS, node_size=4000*x, **style)\n\n\n\n\nIn practice, we could define a threshold cutoff on the probabilities, or set a community size and take the highest-ranking nodes. Then a new node could be selected and a community identified for it in the subgraph without the first community, etc.\nA more sophisticated version of the label propagation algorithm (and many other community detection methods) is offered in a special module.\n\nfrom networkx.algorithms.community import label_propagation_communities\ncomm = label_propagation_communities(WS)\n[ print(c) for c in comm ];\n\n{0, 1, 2, 3, 39}\n{4, 5, 6, 7, 8, 29, 30, 31}\n{9, 10, 11}\n{12, 13, 14, 15}\n{16, 17, 18, 19, 20}\n{32, 33, 34, 35, 21}\n{22, 23, 24, 25, 26, 27, 28}\n{36, 37, 38}\n\n\n\ncolor = np.array( [\"lightblue\",\"pink\",\"yellow\",\"lightgreen\",\"purple\",\"orange\",\"red\",\"lightgray\"] )\ncolor_index = [0]*n\nfor i,S in enumerate(comm):\n    for k in S:\n        color_index[k] = i\nnx.draw( WS, node_size=100, pos=pos, node_color=color[color_index] )"
  },
  {
    "objectID": "networks.html#exercises",
    "href": "networks.html#exercises",
    "title": "7  Networks",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 7.1 For each graph, give the number of nodes, the number of edges, and the average degree.\n(a) The complete graph \\(K_6\\).\n(b) \n(c) \n\n\nExercise 7.2 Give the adjacency matrix for the graphs in Exercise 7.1 (parts (a) and (b) only).\n\n\nExercise 7.3 For the graph below, draw the ego graph of (a) node 4 and (b) node 8.\n\n\n\n\n\n\n\nExercise 7.4 To construct an Erdős-Rényi graph on 25 nodes with expected average degree 8, what should the edge inclusion probability \\(p\\) be?\n\n\nExercise 7.5 Find the diameters of the graphs in Exercise 7.1.\n\n\nExercise 7.6 Suppose that \\(\\bfA\\) is the adjacency matrix of an undirected graph on \\(n\\) nodes. Let \\(\\boldsymbol{1}\\) be the \\(n\\)-vector whose components all equal 1, and let \\[\n\\mathbf{d} = \\bfA \\boldsymbol{1}.\n\\] Explain why \\(\\mathbf{d}\\) is the vector whose components are the degrees of the nodes.\n\n\nExercise 7.7 Find (a) the clustering coefficient and (b) the betweenness centrality for each node in the following graph:\n\n\n\n\n\n\n\nExercise 7.8 A star graph with \\(n\\) nodes and \\(n-1\\) edges has a central node that has an edge to each other node. In terms of \\(n\\), find (a) the clustering coefficient and (b) the betweenness centrality of the central node of the star graph.\n\n\nExercise 7.9 The Watts–Strogatz construction starts with a ring lattice in which the nodes are arranged in a circle and each is connected to its \\(k\\) nearest neighbors (i.e., \\(k/2\\) on each side). Show that the clustering coefficient of an arbitrary node in the ring lattice is \\[\n\\frac{3(k-2)}{4(k-1)}.\n\\]\n(Hint: Count up all the edges between the neighbors on one side of the node of interest, then all the edges between neighbors on the other side, and finally, the edges going from a neighbor on one side to a neighbor on the other side. It might be easier to work with \\(m=k/2\\) and then eliminate \\(m\\) at the end.)\n\n\nExercise 7.10 Recall that the complete graph \\(K_n\\) contains every possible edge on \\(n\\) nodes. Prove that the vector \\(\\bfx=[1,1,\\ldots,1]\\) is an eigenvector of the adjacency matrix of \\(K_n\\). (Therefore, the eigenvector centrality is uniform over the nodes.)\n\n\nExercise 7.11 Prove that for the star graph on \\(n\\) nodes as described in Exercise 8, the vector \\[\n\\bfx = \\bigl[ \\sqrt{n-1},1,1,\\ldots,1 \\bigr]\n\\] is an eigenvector of the adjacency matrix, where the central node corresponds to the first element of the vector.\n\n\nExercise 7.12 Prove the friendship paradox, i.e., inequality Equation 7.4. (Hint: Start with Equation 6.1 using \\(\\bfu=\\mathbf{d}\\) and \\(\\bfv\\) equal to a vector of all ones. Convert from equality to inequality to get rid of the angle \\(\\theta\\). Simplify the inner product, square both sides, and show that it can be rearranged into Equation 7.4.)"
  }
]