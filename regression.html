<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Data Science 1 - 5&nbsp; Regression</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./clustering.html" rel="next">
<link href="./selection.html" rel="prev">
<link href="./_media/logo_small.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/nutshell-1.0.0/nutshell.min.js"></script>
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>
<script> 
MathJax = {
  chtml: {
    scale: 0.92,
  }
}
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto"><div class="quarto-sidebar-header"><div class="sidebar-header-item">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="_media/logo_small.png" height="120" class="figure-img"></p>
</figure>
</div>
</div></div>
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science 1</a> 
        <div class="sidebar-tools-main">
    <a href="./Data-Science-1.pdf" title="Download PDF" class="sidebar-tool px-1"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">Resources</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./starting.html" class="sidebar-item-text sidebar-link">Getting started</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Representation of data</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./selection.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./networks.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>Copyright 2023 by Toby Driscoll</p>
</div></div></nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-regression-linear" id="toc-sec-regression-linear" class="nav-link active" data-scroll-target="#sec-regression-linear"><span class="toc-section-number">5.1</span>  Linear regression</a>
  <ul class="collapse">
  <li><a href="#linear-algebra" id="toc-linear-algebra" class="nav-link" data-scroll-target="#linear-algebra"><span class="toc-section-number">5.1.1</span>  Linear algebra</a></li>
  <li><a href="#performance-metrics" id="toc-performance-metrics" class="nav-link" data-scroll-target="#performance-metrics"><span class="toc-section-number">5.1.2</span>  Performance metrics</a></li>
  </ul></li>
  <li><a href="#multilinear-regression" id="toc-multilinear-regression" class="nav-link" data-scroll-target="#multilinear-regression"><span class="toc-section-number">5.2</span>  Multilinear regression</a>
  <ul class="collapse">
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression"><span class="toc-section-number">5.2.1</span>  Polynomial regression</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="toc-section-number">5.3</span>  Regularization</a>
  <ul class="collapse">
  <li><a href="#case-study-diabetes" id="toc-case-study-diabetes" class="nav-link" data-scroll-target="#case-study-diabetes"><span class="toc-section-number">5.3.1</span>  Case study: Diabetes</a></li>
  </ul></li>
  <li><a href="#nonlinear-regression" id="toc-nonlinear-regression" class="nav-link" data-scroll-target="#nonlinear-regression"><span class="toc-section-number">5.4</span>  Nonlinear regression</a>
  <ul class="collapse">
  <li><a href="#nearest-neighbors" id="toc-nearest-neighbors" class="nav-link" data-scroll-target="#nearest-neighbors"><span class="toc-section-number">5.4.1</span>  Nearest neighbors</a></li>
  <li><a href="#decision-tree" id="toc-decision-tree" class="nav-link" data-scroll-target="#decision-tree"><span class="toc-section-number">5.4.2</span>  Decision tree</a></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="toc-section-number">5.5</span>  Logistic regression</a>
  <ul class="collapse">
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="toc-section-number">5.5.1</span>  Loss function</a></li>
  <li><a href="#regularization-1" id="toc-regularization-1" class="nav-link" data-scroll-target="#regularization-1"><span class="toc-section-number">5.5.2</span>  Regularization</a></li>
  <li><a href="#case-study-personal-spam-filter" id="toc-case-study-personal-spam-filter" class="nav-link" data-scroll-target="#case-study-personal-spam-filter"><span class="toc-section-number">5.5.3</span>  Case study: Personal spam filter</a></li>
  <li><a href="#multiclass-case" id="toc-multiclass-case" class="nav-link" data-scroll-target="#multiclass-case"><span class="toc-section-number">5.5.4</span>  Multiclass case</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<div class="hidden">
<p><span class="math display">\[
    \newcommand{\float}{\mathbb{F}}
    \newcommand{\real}{\mathbb{R}}
    \newcommand{\complex}{\mathbb{C}}
    \newcommand{\nat}{\mathbb{N}}
    \newcommand{\integer}{\mathbb{Z}}
    \newcommand{\bfa}{\mathbf{a}}
    \newcommand{\bfe}{\mathbf{e}}
    \newcommand{\bfh}{\mathbf{h}}
    \newcommand{\bfp}{\mathbf{p}}
    \newcommand{\bfq}{\mathbf{q}}
    \newcommand{\bfu}{\mathbf{u}}
    \newcommand{\bfv}{\mathbf{v}}
    \newcommand{\bfw}{\mathbf{w}}
    \newcommand{\bfx}{\mathbf{x}}
    \newcommand{\bfy}{\mathbf{y}}
    \newcommand{\bfz}{\mathbf{z}}
    \newcommand{\bfA}{\mathbf{A}}
    \newcommand{\bfW}{\mathbf{W}}
    \newcommand{\bfX}{\mathbf{X}}
    \newcommand{\bfzero}{\boldsymbol{0}}
    \newcommand{\bfmu}{\boldsymbol{\mu}}
    \newcommand{\TP}{\text{TP}}
    \newcommand{\TN}{\text{TN}}
    \newcommand{\FP}{\text{FP}}
    \newcommand{\FN}{\text{FN}}
    \newcommand{\rmn}[2]{\mathbb{R}^{#1 \times #2}}
    \newcommand{\dd}[2]{\frac{d #1}{d #2}}
    \newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
    \newcommand{\norm}[1]{\left\lVert \mathstrut #1 \right\rVert}
    \newcommand{\abs}[1]{\left\lvert \mathstrut #1 \right\rvert}
    \newcommand{\twonorm}[1]{\norm{#1}_2}
    \newcommand{\onenorm}[1]{\norm{#1}_1}
    \newcommand{\infnorm}[1]{\norm{#1}_\infty}
    \newcommand{\innerprod}[2]{\langle #1,#2 \rangle}
    \newcommand{\pr}[1]{^{(#1)}}
    \newcommand{\diag}{\operatorname{diag}}
    \newcommand{\sign}{\operatorname{sign}}
    \newcommand{\dist}{\operatorname{dist}}
    \newcommand{\simil}{\operatorname{sim}}
    \newcommand{\ee}{\times 10^}
    \newcommand{\floor}[1]{\lfloor#1\rfloor}
    \newcommand{\argmin}{\operatorname{argmin}}
    \newcommand{\E}[1]{\operatorname{\mathbb{E}}\left[\mathstrut #1\right]}
    \newcommand{\Cov}{\operatorname{Cov}}
    \newcommand{\logit}{\operatorname{logit}}
\]</span></p>
</div>
<div class="cell" data-execution_count="2">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> default_rng</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> shuffle</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, f1_score, balanced_accuracy_score</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, StratifiedKFold</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_validate, validation_curve</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="def-regression-regression" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1 </strong></span><strong>Regression</strong> is the task of approximating the value of a dependent quantitative variable as a function of independent variables, sometimes called <em>predictors</em>.</p>
</div>
<p>Regression and classification are distinct but not altogether different. Abstractly, both are concerned with reproducing a function <span class="math inline">\(f\)</span> whose domain is feature space. In classification, the range of <span class="math inline">\(f\)</span> is a finite set of class labels, while in regression, the range is the real number line (or an interval in it). We can always take the output of a regression and round or bin it to get a finite set of classes; therefore, any regression method can also be used for classification. Likewise, most classification methods have a generalization to regression.</p>
<p>In addition to prediction tasks, some regression methods can be used to identify the relative significance of each feature and whether is has a direct or inverse relationship to the function value. Unimportant features can then be removed to help minimize overfitting.</p>
<section id="sec-regression-linear" class="level2" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-regression-linear"><span class="header-section-number">5.1</span> Linear regression</h2>
<p>You have likely previously encountered the most basic form of regression: fitting a straight line to data points <span class="math inline">\((x_i,y_i)\)</span> in the <span class="math inline">\(xy\)</span>-plane. In <strong>linear regression</strong>, we have a one-dimensional feature <span class="math inline">\(x\)</span> and assume a relation</p>
<p><span class="math display">\[
y \approx \hat{f}(x) = ax + b.
\]</span></p>
<p>We also define a <strong>loss function</strong> or <em>misfit function</em> that adds up how far predictions are from the data. The standard choice is a sum of squared differences between the predictions and the true values:</p>
<p><span class="math display">\[
L(a,b) = \sum_{i=1}^n (\hat{f}(x_i)-y_i)^2 = \sum_{i=1}^n (a x_i + b - y_i)^2.
\]</span></p>
<p>The loss can be minimized using a little multidimensional calculus. Momentarily suppose that <span class="math inline">\(b\)</span> is held fixed and take a derivative with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[
\pp{L}{a} = \sum_{i=1}^n 2x_i(a x_i + b - y_i) = 2 a \left(\sum_{i=1}^n x_i^2\right) + 2b\left(\sum_{i=1}^n x_i\right) - 2\sum_{i=1}^n x_i y_i.
\]</span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The symbol <span class="math inline">\(\pp{}{}\)</span> is called a <strong>partial derivative</strong> and is defined just as described here: differentiate in one variable while all others are temporarily held constant.</p>
</div>
</div>
<p>Similarly, if we hold <span class="math inline">\(a\)</span> fixed and differentiate with respect to <span class="math inline">\(b\)</span>, then</p>
<p><span class="math display">\[
\pp{L}{b} = \sum_{i=1}^n 2(a x_i + b - y_i) = 2 a \left(\sum_{i=1}^n x_i\right) + 2bn - 2 \sum_{i=1}^n y_i.
\]</span></p>
<p>Setting both derivatives to zero creates a system of two linear equations to be solved for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>: <span id="eq-regression-linear1d"><span class="math display">\[
\begin{split}
    a \left(\sum_{i=1}^n x_i^2\right) + b\left(\sum_{i=1}^n x_i\right) &amp;= \sum_{i=1}^n x_i y_i, \\
    a \left(\sum_{i=1}^n x_i\right) + b n &amp;= \sum_{i=1}^n y_i.
\end{split}
\tag{5.1}\]</span></span></p>
<div id="exm-regression-linear" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.1 </strong></span>Suppose we want to find the linear regressor of the points <span class="math inline">\((-1,0)\)</span>, <span class="math inline">\((0,2)\)</span>, <span class="math inline">\((1,3)\)</span>. We need to calculate a few sums:</p>
<p><span class="math display">\[
\begin{split}
\sum_{i=1}^n x_i^2 = 1+0+1=2, \qquad &amp; \sum_{i=1}^n x_i = -1+0+1=0, \\
\sum_{i=1}^n x_iy_i = 0+0+3=3, \qquad &amp; \sum_{i=1}^n y_i = 0+2+3=5.
\end{split}
\]</span></p>
<p>Note that <span class="math inline">\(n=3\)</span>. Therefore we must solve</p>
<p><span class="math display">\[
\begin{split}
2a + 0b &amp;= 3, \\
0a + 3b &amp;= 5.
\end{split}
\]</span></p>
<p>The regression function is <span class="math inline">\(\hat{f}(x)=\tfrac{3}{2} x + \tfrac{5}{3}\)</span>.</p>
</div>
<section id="linear-algebra" class="level3" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="linear-algebra"><span class="header-section-number">5.1.1</span> Linear algebra</h3>
<p>Before moving on, we want to adopt a vector-oriented description of the process. If we define <span class="math display">\[
\bfe = [1,1,\ldots,1] \in \real^n,
\]</span> that is, <span class="math inline">\(\bfe\)</span> as a vector of <span class="math inline">\(n\)</span> ones, then <span class="math display">\[
L(a,b) =  \twonorm{a\, \bfx + b \,\bfe - \bfy}^2,
\]</span> Minimizing <span class="math inline">\(L\)</span> over all values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is called the <strong>least squares</strong> problem. (More specifically, this setup is called <em>simple least squares</em> or <em>ordinary least squares</em>.)</p>
<p>We can write out the equations for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> using another important idea from linear algebra.</p>
<div id="def-regression-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2 </strong></span>Given any <span class="math inline">\(d\)</span>-dimensional real-values vectors <span class="math inline">\(\bfu\)</span> and <span class="math inline">\(\bfv\)</span>, their <strong>inner product</strong> is <span id="eq-regression-inner-product"><span class="math display">\[
\bfu^T \bfv = \sum_{i=1}^d u_i v_i = u_1v_1 + u_2v_2 + \cdots + u_d v_d.
\tag{5.2}\]</span></span></p>
</div>
<p>The vector inner product is defined only between two vectors of the same length (dimension). There is an important link between the inner product and the 2-norm: <span id="eq-regression-hilbert"><span class="math display">\[
\bfu^T \bfu = \sum_{i=1}^d u_i^2 = \twonorm{\bfu}^2.
\tag{5.3}\]</span></span></p>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Inner product</em> is a term from linear algebra. In physics and vector calculus with <span class="math inline">\(d=2\)</span> or <span class="math inline">\(d=3\)</span>, the same thing is often called a <em>dot product</em> and written as <span class="math inline">\(\bfu \cdot \bfv\)</span>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <span class="math inline">\({}^T\)</span> symbol is a <em>transpose</em> operation in linear algebra. We won’t need it as an independent concept, so we are just using it as notation in the inner product.</p>
</div>
</div>
<div id="exm-regression-dot" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.2 </strong></span>Let <span class="math inline">\(\bfu = [1,-1,1,-2]\)</span> and <span class="math inline">\(\bfv = [5,3,-1,2]\)</span>. Then <span class="math display">\[
\bfu^T \bfv = (1)(5) + (-1)(3) + (1)(-1) + (-2)(2) = -3.
\]</span> We also have <span class="math display">\[
\twonorm{\bfu}^2 = \bfu^T \bfu = (1)^2 + (-1)^2 + (1)^2 + (-2)^2 = 7.
\]</span></p>
</div>
<p>The equations in <a href="#eq-regression-linear1d">Equation&nbsp;<span>5.1</span></a> may now be written as <span id="eq-regression-linear1d-dot"><span class="math display">\[
\begin{split}
    a \left(\bfx^T \bfx\right) + b \left(\bfx^T\bfe\right) &amp;= \bfx^T\bfy, \\
    a \left(\bfe^T \bfx\right) + b \left(\bfe^T\bfe\right) &amp;= \bfe^T\bfy.
\end{split}
\tag{5.4}\]</span></span></p>
<p>We can write this as a single equation between two vectors: <span class="math display">\[
a
\begin{bmatrix}
\bfx^T \bfx \\ \bfe^T \bfx
\end{bmatrix}
+ b
\begin{bmatrix}
\bfx^T\bfe \\ \bfe^T\bfe
\end{bmatrix}
=
\begin{bmatrix}
\bfx^T\bfy \\  \bfe^T\bfy
\end{bmatrix}.
\]</span> In fact, the operation on the left-hand side is how we define the product of a matrix and a vector, and we can write<br>
<span class="math display">\[
\begin{bmatrix}
\bfx^T \bfx &amp; \bfx^T\bfe \\
\bfe^T \bfx &amp; \bfe^T\bfe
\end{bmatrix}
\cdot
\begin{bmatrix}
a \\ b
\end{bmatrix}
=
\begin{bmatrix}
\bfx^T\bfy \\  \bfe^T\bfy
\end{bmatrix}.
\]</span> This is referred to as a <em>linear system</em> of equations for the unknown vector <span class="math inline">\([a,b]\)</span>. Linear systems and their solutions are the central topic of linear algebra. In the background, it is this linear system that is being solved when you perform a linear regression fit.</p>
</section>
<section id="performance-metrics" class="level3" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="performance-metrics"><span class="header-section-number">5.1.2</span> Performance metrics</h3>
<p>We need to establish ways to measure regression performance. Unlike with binary classification, in regression it’s not just a matter of right and wrong answers—the amount of wrongness matters, too.</p>
<p>In this section, we will use <span class="math inline">\(x_i\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span> to mean the training set features, <span class="math inline">\(y_i\)</span> to mean the corresponding training values, and <span class="math inline">\(\hat{y}_i\)</span> to mean the values predicted on the training set by the regressor.</p>
<div id="def-regression-residual-error" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3 </strong></span>The <strong>residuals</strong> of the regression are <span id="eq-regression-residual"><span class="math display">\[
y_i - \hat{y}_i, \qquad i=1,\ldots,n.
\tag{5.5}\]</span></span> We can express them as the vector <span class="math inline">\(\bfy-\hat{\bfy}\)</span>.</p>
</div>
<p>A quirk of linear regression is that it’s an older idea than most of machine learning, and it’s often presented as though the training and testing sets are identical. We therefore give the following definitions in terms of <span class="math inline">\(\bfy\)</span> and <span class="math inline">\(\hat{\bfy}\)</span> arising from the training data. The same quantities can also be calculated for a set of labels and predictions obtained from a separate testing set, though a few of the properties stated here don’t apply in that case.</p>
<div class="callout callout-style-default callout-caution callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>The terms <em>error</em> and <em>residual</em> are frequently used interchangeably and even inconsistently. I try to follow the most common practices here, even though the names can be confusing if you think about them too hard.</p>
</div>
</div>
<div id="def-regression-mse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4 </strong></span>The <strong>mean squared error</strong> (MSE) is <span class="math display">\[
\text{MSE} = \frac{1}{m} \sum_{i=1}^m \, \left( y_i - \hat{y}_i \right)^2 = \frac{1}{m} \twonorm{\bfy - \hat{\bfy}}^2.
\]</span> The <strong>mean absolute error</strong> (MAE) is <span class="math display">\[
\text{MAE} = \frac{1}{m} \sum_{i=1}^m \abs{y_i - \hat{y}_i }= \frac{1}{m} \onenorm{\bfy - \hat{\bfy}}.
\]</span></p>
</div>
<p>MAE is less sensitive than MSE to large outliers. Both quantities are dimensional and therefore depend on how the variables are scaled, but at least the units of MAE are the same as of the data.</p>
<div id="def-regression-cod" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.5 </strong></span>The <strong>coefficient of determination</strong> (CoD) is denoted <span class="math inline">\(R^2\)</span> and defined as <span class="math display">\[
R^2 = 1 - \frac{\displaystyle\sum_{i=1}^n \,\left(y_i - \hat{y}_i \right)^2}{\displaystyle\sum_{i=1}^n \, \left(y_i - \bar{y}\right)^2},
\]</span> where <span class="math inline">\(\bar{y}\)</span> is the mean of <span class="math inline">\(y_1,\ldots,y_n\)</span>.</p>
</div>
<p>Here are important things to know about the coefficient of determination.</p>
<div id="thm-regression-cod" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1 </strong></span>&nbsp;</p>
<ol type="1">
<li>The CoD is dimensionless and therefore independent of scaling.</li>
<li>If the <span class="math inline">\(\hat{y}_i\)</span> are found from a linear regression, then <span class="math inline">\(R^2\)</span> is the square of the Pearson correlation coefficient between <span class="math inline">\(\bfy\)</span> and <span class="math inline">\(\hat{\bfy}\)</span>.</li>
<li>If <span class="math inline">\(\hat{y}_i=y_i\)</span> for all <span class="math inline">\(i\)</span> (i.e., perfect predictions), then <span class="math inline">\(R^2=1\)</span>.</li>
<li>If <span class="math inline">\(\hat{y}_i=\bar{y}\)</span> for all <span class="math inline">\(i\)</span> (i.e., always predict the sample mean), then <span class="math inline">\(R^2=0\)</span>.</li>
</ol>
</div>
<div class="callout callout-style-default callout-warning callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>The notation <span class="math inline">\(R^2\)</span> is <em>highly</em> unfortunate, because for other regression methods, <span class="math inline">\(R^2\)</span> can actually be negative! Such a result indicates that the predictor is doing worse than just predicting the mean value every time. It has nothing to do with imaginary numbers.</p>
</div>
</div>
<div id="exm-regression-cod" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.3 </strong></span>Let’s find the coefficient of determination for the fit in <a href="#exm-regression-linear">Example&nbsp;<span>5.1</span></a>, where we found the regressor <span class="math inline">\(\hat{f}(x)=\tfrac{3}{2} x + \tfrac{5}{3}\)</span>. Now <span class="math inline">\(\bar{y} = \frac{1}{3}(0+2+3)=\frac{5}{3}\)</span>, and</p>
<p><span class="math display">\[
\begin{split}
    \sum_{i=1}^m \sum_{i=1}^n \,\left(y_i - \hat{y}_i \right)^2 &amp;= \left(0-\tfrac{1}{6}\right)^2 + \left(2-\tfrac{5}{3}\right)^2 + \left(3-\tfrac{19}{6}\right)^2 = \frac{1}{6}, \\
    \sum_{i=1}^m \left(y_i - \bar{y}\right)^2 &amp;= \left(0-\tfrac{5}{3}\right)^2 + \left(2-\tfrac{5}{3}\right)^2 + \left(3-\tfrac{5}{3}\right)^2 = \frac{14}{3}.
\end{split}
\]</span></p>
<p>This yields <span class="math inline">\(R^2 = 1 - (1/6)(3/14) = 27/28\)</span>.</p>
<p>If we instead use the arbitrary regressor <span class="math inline">\(\hat{f}(x)=x\)</span>, then</p>
<p><span class="math display">\[
\begin{split}
    \sum_{i=1}^m \sum_{i=1}^n \,\left(y_i - \hat{y}_i \right)^2 &amp;= \left(0+1\right)^2 + \left(2-0\right)^2 + \left(3-1\right)^2 = 9, \\
    \sum_{i=1}^m \left(y_i - \bar{y}\right)^2 &amp;= \frac{14}{3}.
\end{split}
\]</span></p>
<p>This yields <span class="math inline">\(R^2 = 1 - (9)(3/14) = -13/14\)</span>. Since the result is negative, we would be better off always predicting <span class="math inline">\(5/3\)</span>.</p>
</div>
<div id="exm-regression-ice-linear" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.4 </strong></span>We import data about the extent of sea ice in the Arctic circle, collected monthly since 1979:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>ice <span class="op">=</span> pd.read_csv(<span class="st">"_datasets/sea-ice.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplify column names:</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>ice.columns <span class="op">=</span> [s.strip() <span class="cf">for</span> s <span class="kw">in</span> ice.columns]   </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>ice.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>year</th>
<th>mo</th>
<th>data-type</th>
<th>region</th>
<th>extent</th>
<th>area</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>1979</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>15.41</td>
<td>12.41</td>
</tr>
<tr class="even">
<th>1</th>
<td>1980</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>14.86</td>
<td>11.94</td>
</tr>
<tr class="odd">
<th>2</th>
<td>1981</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>14.91</td>
<td>11.91</td>
</tr>
<tr class="even">
<th>3</th>
<td>1982</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>15.18</td>
<td>12.19</td>
</tr>
<tr class="odd">
<th>4</th>
<td>1983</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>14.94</td>
<td>12.01</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>A quick plot reveals something odd-looking:</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"mo"</span>, y<span class="op">=</span><span class="st">"extent"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-4-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>Everything in the plot above is dominated by two large negative values. These probably represent missing data, so we make a new copy without those rows:</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>ice <span class="op">=</span> ice[ice[<span class="st">"extent"</span>] <span class="op">&gt;</span> <span class="dv">0</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"mo"</span>, y<span class="op">=</span><span class="st">"extent"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-5-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>Each dot in the plot above represents one measurement. As you would expect, the extent of ice rises in the winter months and falls in summer:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>bymonth <span class="op">=</span> ice.groupby(<span class="st">"mo"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>bymonth[<span class="st">"extent"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>mo
1     14.214762
2     15.100233
3     15.256977
4     14.525581
5     13.117442
6     11.539767
7      9.097907
8      6.793256
9      5.993488
10     7.887907
11    10.458182
12    12.664419
Name: extent, dtype: float64</code></pre>
</div>
</div>
<p>While the effect of the seasonal variation somewhat cancels out over time when fitting a line, it’s preferable to remove this obvious trend before the fit takes place. To do that, we add a column that measures within each month group the relative change from the mean, <span class="math display">\[
\frac{x-\bar{x}}{\bar{x}}.
\]</span> This is done with a <code>transform</code> method applied to the grouped frame:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>recenter <span class="op">=</span> <span class="kw">lambda</span> x: x<span class="op">/</span>x.mean() <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>ice[<span class="st">"detrended"</span>] <span class="op">=</span> bymonth[<span class="st">"extent"</span>].transform(recenter)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"mo"</span>, y<span class="op">=</span><span class="st">"detrended"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-7-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>An <code>lmplot</code> in seaborn shows the least squares line:</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"year"</span>, y<span class="op">=</span><span class="st">"detrended"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-8-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>However, we should be mindful of <a href="@sec-stats-simpson">Simpson’s paradox</a>. The previous plot showed considerably more variance within the warm months. How do these fits look for the data <em>within</em> each month? This is where a facet plot shines:</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>ice,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"year"</span>, y<span class="op">=</span><span class="st">"detrended"</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    col<span class="op">=</span><span class="st">"mo"</span>, col_wrap<span class="op">=</span><span class="dv">3</span>, height<span class="op">=</span><span class="dv">2</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-9-output-1.png" width="565" height="755"></p>
</div>
</div>
<p>Thus, while the correlation is negative within each month, the effect size is clearly larger in the summer and early fall.</p>
<p>We can get numerical information about a regression line from a <code>LinearRegression()</code> learner in sklearn. We will focus on the data for August:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>ice <span class="op">=</span> ice[ ice[<span class="st">"mo"</span>]<span class="op">==</span><span class="dv">8</span> ]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> ice[ [<span class="st">"year"</span>] ]  </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> ice[<span class="st">"detrended"</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>lm.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">LinearRegression</label><div class="sk-toggleable__content"><pre>LinearRegression()</pre></div></div></div></div></div>
</div>
</div>
<p>We can get the slope and <span class="math inline">\(y\)</span>-intercept of the regression line from the learner’s properties. (Calculated parameters tend to have underscores at the ends of their names in sklearn.)</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>slope, intercept <span class="op">=</span> lm.coef_[<span class="dv">0</span>], lm.intercept_</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Slope is </span><span class="sc">{</span>slope<span class="sc">:.3g}</span><span class="ss"> and intercept is </span><span class="sc">{</span>intercept<span class="sc">:.3g}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Slope is -0.011 and intercept is 22.1</code></pre>
</div>
</div>
<p>The slope indicates average decrease over time.</p>
<p>Next, we assess the performance on the training set. Both the MSE and mean absolute error are small relative to dispersion within the values themselves:</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, mean_absolute_error</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> lm.predict(X)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y, yhat)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y, yhat)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MSE: </span><span class="sc">{</span>mse<span class="sc">:.2e}</span><span class="ss">, compared to variance </span><span class="sc">{</span>y<span class="sc">.</span>var()<span class="sc">:.2e}</span><span class="ss">"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAE: </span><span class="sc">{</span>mae<span class="sc">:.2e}</span><span class="ss">, compared to standard deviation </span><span class="sc">{</span>y<span class="sc">.</span>std()<span class="sc">:.2e}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 4.01e-03, compared to variance 2.33e-02
MAE: 4.93e-02, compared to standard deviation 1.53e-01</code></pre>
</div>
</div>
<p>The <code>score</code> method of the regressor object computes the coefficient of determination:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>R2 <span class="op">=</span> lm.score(X, y)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"R-squared: </span><span class="sc">{</span>R2<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R-squared: 0.824</code></pre>
</div>
</div>
<p>An <span class="math inline">\(R^2\)</span> value this close to 1 would usually be considered a sign of a good fit, although we have not tested for generalization to new data.</p>
</div>
</section>
</section>
<section id="multilinear-regression" class="level2 page-columns page-full" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="multilinear-regression"><span class="header-section-number">5.2</span> Multilinear regression</h2>
<p>We can extend linear regression to <span class="math inline">\(d\)</span> predictor variables <span class="math inline">\(x_1,\ldots,x_d\)</span>:</p>
<p><span class="math display">\[
    y \approx \hat{f}(\bfx) = b + w_1 x_1 + w_2x_2 + \cdots w_d x_d.
\]</span></p>
<p>First, observe that we can actually drop the intercept term <span class="math inline">\(b\)</span> from the discussion, because we could always define an additional constant feature <span class="math inline">\(x_0=1\)</span> and get the same effect in one higher dimension. So we will use the following.</p>
<div id="def-regression-multilinear" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.6 </strong></span><strong>Multilinear regression</strong> is the approximation <span class="math display">\[
y \approx \hat{f}(\bfx) = w_1 x_1 + w_2x_2 + \cdots w_d x_d = \bfw^T\bfx,
\]</span> for a constant vector <span class="math inline">\(\bfw\)</span> known as the <strong>weight vector</strong>.</p>
</div>
<div class="callout callout-style-default callout-note callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Multilinear regression is also simply called <em>linear regression</em> most of the time. What we previously called linear regression is just a special case. The <code>LinearRegression</code> learner class does both types of fits.</p>
</div>
</div>
<p>As before, we find the unknown weight vector <span class="math inline">\(\bfw\)</span> by minimizing a loss function. To create the least-squares loss function, we use <span class="math inline">\(\bfx_i\)</span> to denote the <span class="math inline">\(i\)</span>th row of an <span class="math inline">\(n\times d\)</span> feature matrix <span class="math inline">\(\bfX\)</span>. Then</p>
<p><span class="math display">\[
L(\bfw) = \sum_{i=1}^n (y_i - \hat{f}(\bfx_i))^2 = \sum_{i=1}^n (y_i - \bfx_i^T\bfw)^2.
\]</span></p>
<p>We encountered a matrix-vector product earlier. It turns out that the following definition is equivalent.</p>
<div id="def-regression-matvec" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.7 </strong></span>Given an <span class="math inline">\(n\times d\)</span> matrix <span class="math inline">\(\bfX\)</span> with rows <span class="math inline">\(\bfx_1,\ldots,\bfx_n\)</span> and a <span class="math inline">\(d\)</span>-vector <span class="math inline">\(\bfw\)</span>, the product <span class="math inline">\(\bfX\bfw\)</span> is defined by <span class="math display">\[
\bfX \bfw =
\begin{bmatrix}
    \bfx_1^T\bfw \\ \bfx_2^T\bfw \\ \vdots \\ \bfx_n^T\bfw
\end{bmatrix}.
\]</span></p>
</div>
<p>We now have the compact expression</p>
<p><span id="eq-regression-multilinear-loss"><span class="math display">\[
L(\bfw) = \twonorm{\bfX \bfw- \bfy}^2.
\tag{5.6}\]</span></span></p>
<p>As in the <span class="math inline">\(d=1\)</span> case, minimizing the loss is equivalent to solving a linear system of equations known as the <em>normal equations</em> for <span class="math inline">\(\bfw\)</span>. We do not present them here.</p>
<div class="callout callout-style-default callout-caution callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Danger
</div>
</div>
<div class="callout-body-container callout-body">
<p>Be careful interpreting the magnitudes of regression coefficients. These are sensitive to the units and scales of the features. For example, distances expressed in meters would have a coefficient that is 1000 times larger than the same distances expressed in kilometers. For quantitative comparisons, it helps to standardize the features first, which does not affect the quality of the fit.</p>
</div>
</div>
<div id="exm-regression-cars-multi" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.5 </strong></span>We return to the data set regarding the fuel efficiency of cars:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>cars <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>cars.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>mpg</th>
<th>cylinders</th>
<th>displacement</th>
<th>horsepower</th>
<th>weight</th>
<th>acceleration</th>
<th>model_year</th>
<th>origin</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>18.0</td>
<td>8</td>
<td>307.0</td>
<td>130.0</td>
<td>3504</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>chevrolet chevelle malibu</td>
</tr>
<tr class="even">
<th>1</th>
<td>15.0</td>
<td>8</td>
<td>350.0</td>
<td>165.0</td>
<td>3693</td>
<td>11.5</td>
<td>70</td>
<td>usa</td>
<td>buick skylark 320</td>
</tr>
<tr class="odd">
<th>2</th>
<td>18.0</td>
<td>8</td>
<td>318.0</td>
<td>150.0</td>
<td>3436</td>
<td>11.0</td>
<td>70</td>
<td>usa</td>
<td>plymouth satellite</td>
</tr>
<tr class="even">
<th>3</th>
<td>16.0</td>
<td>8</td>
<td>304.0</td>
<td>150.0</td>
<td>3433</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>amc rebel sst</td>
</tr>
<tr class="odd">
<th>4</th>
<td>17.0</td>
<td>8</td>
<td>302.0</td>
<td>140.0</td>
<td>3449</td>
<td>10.5</td>
<td>70</td>
<td>usa</td>
<td>ford torino</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>In order to ease experimentation, we define a function that fits a given learner to the <em>mpg</em> variable using a given list of features from the data frame:</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fitcars(model, features):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> cars[features]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        X, y,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        test_size<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">302</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    MSE <span class="op">=</span> mean_squared_error(y_test, model.predict(X_test))</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"MSE: </span><span class="sc">{</span>MSE<span class="sc">:.3f}</span><span class="ss">, compared to variance </span><span class="sc">{</span>y_test<span class="sc">.</span>var()<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-tip callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>When you run the same lines of code over and over with only a slight change at the beginning, it’s advisable to put that code into a function. It makes the overall code shorter and easier to understand and adapt.</p>
</div>
</div>
<p>First, we try using <em>horsepower</em> as the only feature in a linear regression to fit <em>mpg</em>:</p>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"horsepower"</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression( fit_intercept<span class="op">=</span><span class="va">True</span> )</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>fitcars(lm, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 26.354, compared to variance 56.474</code></pre>
</div>
</div>
<p>As we would expect, there is an inverse relationship between horsepower and vehicle efficiency:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>lm.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>array([-0.1596552])</code></pre>
</div>
</div>
<p>Next, we add <em>displacement</em> to the regression:</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"horsepower"</span>, <span class="st">"displacement"</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>fitcars(lm, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 19.683, compared to variance 56.474</code></pre>
</div>
</div>
<p>The error has decreased from the univariate case because we have a more capable model.</p>
<p>Finally, we try using 4 features as predictors. In order to help us compare the regression coefficients, we chain the model with a <code>StandardScaler</code> so that all columns are z-scores:</p>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"horsepower"</span>, <span class="st">"displacement"</span>, <span class="st">"cylinders"</span>, <span class="st">"weight"</span>]</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), lm)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>fitcars(pipe, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 19.266, compared to variance 56.474</code></pre>
</div>
</div>
<p>We did not get much improvement in the fit this time. But by comparing the coefficients of the individual features, some interesting information emerges:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>pd.Series(pipe[<span class="dv">1</span>].coef_, index<span class="op">=</span>features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>horsepower     -1.892329
displacement    0.722049
cylinders      -0.870727
weight         -4.847433
dtype: float64</code></pre>
</div>
</div>
<p>We now have evidence that <em>weight</em> is the most significant negative factor for MPG, by a wide margin.</p>
</div>
<p>In the next example, we will see that we can create new features from the ones that are initially given. Sometimes these new features add a lot to the quality of the regression.</p>
<div id="exm-regression-sales-multi" class="theorem example page-columns page-full">
<p><span class="theorem-title"><strong>Example 5.6 </strong></span>Here we load data about advertising spending on different media in many markets:</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>ads <span class="op">=</span> pd.read_csv(<span class="st">"_datasets/advertising.csv"</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>ads.head(<span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>TV</th>
<th>Radio</th>
<th>Newspaper</th>
<th>Sales</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>230.1</td>
<td>37.8</td>
<td>69.2</td>
<td>22.1</td>
</tr>
<tr class="even">
<th>1</th>
<td>44.5</td>
<td>39.3</td>
<td>45.1</td>
<td>10.4</td>
</tr>
<tr class="odd">
<th>2</th>
<td>17.2</td>
<td>45.9</td>
<td>69.3</td>
<td>12.0</td>
</tr>
<tr class="even">
<th>3</th>
<td>151.5</td>
<td>41.3</td>
<td>58.5</td>
<td>16.5</td>
</tr>
<tr class="odd">
<th>4</th>
<td>180.8</td>
<td>10.8</td>
<td>58.4</td>
<td>17.9</td>
</tr>
<tr class="even">
<th>5</th>
<td>8.7</td>
<td>48.9</td>
<td>75.0</td>
<td>7.2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Pairwise scatter plots yield some hints about what to expect from this dataset:</p>
<div class="cell page-columns page-full" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>sns.pairplot(data<span class="op">=</span>ads, height<span class="op">=</span><span class="fl">1.8</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display column-body-outset">
<p><img src="regression_files/figure-html/cell-22-output-1.png" width="682" height="681"></p>
</div>
</div>
<p>The clearest association between <em>Sales</em> and spending is with <em>TV</em>. So we first try a univariate linear fit of sales against TV spending alone:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> ads[ [<span class="st">"TV"</span>] ]    <span class="co"># has to be a frame, so ["TV"] not "TV"</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> ads[<span class="st">"Sales"</span>]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>lm.fit(X, y)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"R^2 score:"</span>, <span class="ss">f"</span><span class="sc">{</span>lm<span class="sc">.</span>score(X, y)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Regression weight:"</span>, lm.coef_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2 score: 0.8122
Regression weight: [0.05546477]</code></pre>
</div>
</div>
<p>The coefficient of determination is already quite good. Since we are going to do multiple fits with different features, we write a function that does the grunt work:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> regress(lm, data, y, features):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> data[features]</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    lm.fit(X, y)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    R2 <span class="op">=</span> lm.score(X,y)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"R^2 score:"</span>, <span class="ss">f"</span><span class="sc">{</span>R2<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Regression weights:"</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>( pd.Series(lm.coef_, index<span class="op">=</span>features) )</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we try folding in <em>Newspaper</em> as well:</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>regress(lm, ads, y, [<span class="st">"TV"</span>, <span class="st">"Newspaper"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2 score: 0.82364
Regression weights:
TV           0.055091
Newspaper    0.026021
dtype: float64</code></pre>
</div>
</div>
<p>The additional feature had very little effect on the quality of fit. We go on to fit using all three features:</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>regress(lm, ads, y, [<span class="st">"TV"</span>, <span class="st">"Newspaper"</span>, <span class="st">"Radio"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2 score: 0.90259
Regression weights:
TV           0.054446
Newspaper    0.000336
Radio        0.107001
dtype: float64</code></pre>
</div>
</div>
<p>Judging by the weights of the model, it’s even clearer now that we can explain <em>Sales</em> very well without contributions from <em>Newspaper</em>. In order to reduce model variance, it would be reasonable to leave that column out. Doing so has a negligible effect:</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>regress(lm, ads, y, [<span class="st">"TV"</span>, <span class="st">"Radio"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2 score: 0.90259
Regression weights:
TV       0.054449
Radio    0.107175
dtype: float64</code></pre>
</div>
</div>
<p>While we have a very good <span class="math inline">\(R^2\)</span> score now, we can try to improve it. We can add an additional feature that is the product of <em>TV</em> and <em>Radio</em>, representing the possibility that these media reinforce one another’s effects:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> ads[ [<span class="st">"Radio"</span>, <span class="st">"TV"</span>] ].copy()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"Radio*TV"</span>] <span class="op">=</span> X[<span class="st">"Radio"</span>]<span class="op">*</span>X[<span class="st">"TV"</span>]</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>regress(lm, X, y, X.columns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>R^2 score: 0.91404
Regression weights:
Radio       0.042270
TV          0.043578
Radio*TV    0.000443
dtype: float64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In order to modify a frame, it has to be an independent copy, not just a subset of another frame.</p>
</div>
</div>
<p>We did see some increase in the <span class="math inline">\(R^2\)</span> score, and therefore the combination of both types of spending does have a positive effect on <em>Sales</em>.</p>
</div>
<p>It’s not uncommon to introduce a product term as done in <a href="#exm-regression-sales-multi">Example&nbsp;<span>5.6</span></a>, and more exotic choices are also possible. Keep in mind, though, that additional variables usually add variance to the model, even if they don’t seriously affect the bias.</p>
<p>Interpreting linear regression is a major topic in statistics. There are tests that can lend much more precision and rigor to the brief discussion above.</p>
<section id="polynomial-regression" class="level3" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="polynomial-regression"><span class="header-section-number">5.2.1</span> Polynomial regression</h3>
<p>A special case of multilinear regression is when there is initially a single predictor variable <span class="math inline">\(t\)</span>, and then we define</p>
<p><span class="math display">\[
x_1 = t^0, \, x_2 = t^1, \ldots, x_d = t^{d-1}.
\]</span></p>
<p>This makes the regressive approximation into</p>
<p><span class="math display">\[
y \approx w_1 + w_2 t + \cdots + w_d t^{d-1},
\]</span></p>
<p>which is a polynomial of degree <span class="math inline">\(d-1\)</span>. This allows representation of data that depends on <span class="math inline">\(t\)</span> in ways more complicated than a straight line. However, it can lead to overfitting if taken too far.</p>
<p>We don’t have to add polynomial features manually, if we use a pipeline instead.</p>
<div id="exm-regression-polynomial" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.7 </strong></span>We return to the data set regarding the fuel efficiency of cars:</p>
<div class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>cars <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>cars.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>mpg</th>
<th>cylinders</th>
<th>displacement</th>
<th>horsepower</th>
<th>weight</th>
<th>acceleration</th>
<th>model_year</th>
<th>origin</th>
<th>name</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>18.0</td>
<td>8</td>
<td>307.0</td>
<td>130.0</td>
<td>3504</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>chevrolet chevelle malibu</td>
</tr>
<tr class="even">
<th>1</th>
<td>15.0</td>
<td>8</td>
<td>350.0</td>
<td>165.0</td>
<td>3693</td>
<td>11.5</td>
<td>70</td>
<td>usa</td>
<td>buick skylark 320</td>
</tr>
<tr class="odd">
<th>2</th>
<td>18.0</td>
<td>8</td>
<td>318.0</td>
<td>150.0</td>
<td>3436</td>
<td>11.0</td>
<td>70</td>
<td>usa</td>
<td>plymouth satellite</td>
</tr>
<tr class="even">
<th>3</th>
<td>16.0</td>
<td>8</td>
<td>304.0</td>
<td>150.0</td>
<td>3433</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>amc rebel sst</td>
</tr>
<tr class="odd">
<th>4</th>
<td>17.0</td>
<td>8</td>
<td>302.0</td>
<td>140.0</td>
<td>3449</td>
<td>10.5</td>
<td>70</td>
<td>usa</td>
<td>ford torino</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>As we would expect, horsepower and miles per gallon are negatively correlated. However, the relationship is not well captured by a straight line:</p>
<div class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>cars, x<span class="op">=</span><span class="st">"horsepower"</span>, y<span class="op">=</span><span class="st">"mpg"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-30-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>A cubic polynomial produces a much more plausible fit, especially on the right half of the plot:</p>
<div class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>cars, x<span class="op">=</span><span class="st">"horsepower"</span>, y<span class="op">=</span><span class="st">"mpg"</span>, order<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-31-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>In order to produce the cubic fit within sklearn, we use the <code>PolynomialFeatures</code> preprocessor in a pipeline. If the original predictor variable is <span class="math inline">\(t\)</span>, then the preprocessor will create features for <span class="math inline">\(1\)</span>, <span class="math inline">\(t\)</span>, <span class="math inline">\(t^2\)</span>, and <span class="math inline">\(t^3\)</span>. (Since the constant feature is added in, we don’t need to fit the intercept with the linear regressor.)</p>
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars[ [<span class="st">"horsepower"</span>] ]</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression( fit_intercept<span class="op">=</span><span class="va">False</span> )</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>cubic <span class="op">=</span> make_pipeline(PolynomialFeatures(degree<span class="op">=</span><span class="dv">3</span>), lm)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>cubic.fit(X, y)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> pd.DataFrame([<span class="dv">200</span>], columns<span class="op">=</span>X.columns)</span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"prediction at hp=200:"</span>, cubic.predict(query))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>prediction at hp=200: [12.90220247]</code></pre>
</div>
</div>
<p>The prediction above is consistent with the earlier figure.</p>
<p>We can get the coefficients of the cubic polynomial from the trained regressor:</p>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>cubic[<span class="dv">1</span>].coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>array([ 6.06847849e+01, -5.68850128e-01,  2.07901126e-03, -2.14662591e-06])</code></pre>
</div>
</div>
<p>The coefficients go in order of increasing degree.</p>
</div>
<p>If a cubic polynomial can fit better than a line, it’s plausible that increasing the degree more will lead to even better fits. In fact, the training error can only go down, because a lower-degree polynomial case is a subset of a higher-degree case.</p>
<div id="exm-regression-polynomial-high" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.8 </strong></span>Continuing with <a href="#exm-regression-polynomial">Example&nbsp;<span>5.7</span></a>, we explore the effect of polynomial degree, we split into train and test sets:</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> deg <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">2</span>,<span class="dv">11</span>):</span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>    poly <span class="op">=</span> make_pipeline( PolynomialFeatures(degree<span class="op">=</span>deg), lm )</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>    poly.fit(X_tr, y_tr)</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    MSE <span class="op">=</span> mean_squared_error(y_te, poly.predict(X_te))</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"MSE for degree </span><span class="sc">{</span>deg<span class="sc">:2}</span><span class="ss">: </span><span class="sc">{</span>MSE<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE for degree  2: 16.013
MSE for degree  3: 15.911
MSE for degree  4: 15.819
MSE for degree  5: 15.653
MSE for degree  6: 15.649
MSE for degree  7: 15.593
MSE for degree  8: 18.177
MSE for degree  9: 28.510
MSE for degree 10: 55.261</code></pre>
</div>
</div>
<p>The results above are a clear example of overfitting and the bias–variance tradeoff. A plot of the degree-10 fit shows that the polynomial becomes more oscillatory:</p>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>cars, x<span class="op">=</span><span class="st">"horsepower"</span>, y<span class="op">=</span><span class="st">"mpg"</span>, order<span class="op">=</span><span class="dv">10</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-35-output-1.png" width="469" height="468"></p>
</div>
</div>
<p>In the above plot, note the widening of the confidence intervals near the ends of the domain, indicating increased variance in the predictions.</p>
<p>Finally, we can combine the use of multiple features and higher degree:</p>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(</span>
<span id="cb52-2"><a href="#cb52-2" aria-hidden="true" tabindex="-1"></a>    StandardScaler(),</span>
<span id="cb52-3"><a href="#cb52-3" aria-hidden="true" tabindex="-1"></a>    PolynomialFeatures(degree<span class="op">=</span><span class="dv">2</span>),</span>
<span id="cb52-4"><a href="#cb52-4" aria-hidden="true" tabindex="-1"></a>    lm</span>
<span id="cb52-5"><a href="#cb52-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb52-6"><a href="#cb52-6" aria-hidden="true" tabindex="-1"></a>fitcars(pipe, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 15.749, compared to variance 56.474</code></pre>
</div>
</div>
<p>This is our best regression fit so far, by a mile.</p>
</div>
</section>
</section>
<section id="regularization" class="level2" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="regularization"><span class="header-section-number">5.3</span> Regularization</h2>
<p>As a general term, <em>regularization</em> refers to modifying something that is difficult to compute accurately with something more tractable. For learning models, regularization is a common way to combat overfitting.</p>
<p>Suppose we had an <span class="math inline">\(\real^{n\times 4}\)</span> feature matrix in which the features are identical; that is, the predictor variables satisfy <span class="math inline">\(x_1=x_2=x_3=x_4\)</span>, and suppose the target <span class="math inline">\(y\)</span> also equals <span class="math inline">\(x_1\)</span>. Clearly, we get a perfect regression if we use</p>
<p><span class="math display">\[
y = 1x_1 + 0x_2 + 0x_3 + 0x_4.
\]</span></p>
<p>But an equally good regression is</p>
<p><span class="math display">\[
y = \frac{1}{4}x_1 + \frac{1}{4}x_2 + \frac{1}{4}x_3 + \frac{1}{4}x_4.
\]</span></p>
<p>For that matter, so is</p>
<p><span class="math display">\[
y = 1000x_1 - 500x_2 - 500x_3 + 1x_4.
\]</span></p>
<p>A problem with more than one valid solution is called <strong>ill-posed</strong>. If we made tiny changes to the predictor variables in this thought experiment, the problem would technically be well-posed, but there would be a wide range of solutions that were very nearly correct, in which case the problem is said to be <strong>ill-conditioned</strong>; for practical purposes, it remains just as difficult.</p>
<p>The poor conditioning can be regularized away by modifying the least-squares loss function to penalize complexity in the model, in the form of excessively large regression coefficients. The common choices are <strong>ridge regression</strong>,</p>
<p><span class="math display">\[
L(\bfw) = \twonorm{ \bfX \bfw- \bfy }^2 + \alpha \twonorm{\bfw}^2,
\]</span></p>
<p>and <strong>LASSO</strong>,</p>
<p><span class="math display">\[
L(\bfw) = \twonorm{ \bfX \bfw- \bfy }^2 + \alpha \onenorm{\bfw}.
\]</span></p>
<p>As <span class="math inline">\(\alpha\to 0\)</span>, both forms revert to the usual least-squares loss, but as <span class="math inline">\(\alpha \to \infty\)</span>, the optimization becomes increasingly concerned with prioritizing a small result for <span class="math inline">\(\bfw\)</span>.</p>
<p>While ridge regression is an easier function to minimize quickly, LASSO has an interesting advantage, as illustrated in this figure.</p>
<p><img src="_media/regularization.png" class="img-fluid"></p>
<p>LASSO tends to produce <strong>sparse</strong> results, meaning that some of the regression coefficients are zero or negligible. These zeros indicate predictor variables that have minor predictive value, which can be valuable information in itself. Moreover, when regression is run without these variables, there may be little effect on the bias, but a reduction in variance.</p>
<section id="case-study-diabetes" class="level3" data-number="5.3.1">
<h3 data-number="5.3.1" class="anchored" data-anchor-id="case-study-diabetes"><span class="header-section-number">5.3.1</span> Case study: Diabetes</h3>
<p>We’ll apply regularized regression to data collected about the progression of diabetes:</p>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb54"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb54-1"><a href="#cb54-1" aria-hidden="true" tabindex="-1"></a>diabetes <span class="op">=</span> datasets.load_diabetes(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb54-2"><a href="#cb54-2" aria-hidden="true" tabindex="-1"></a>diabetes.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>age</th>
<th>sex</th>
<th>bmi</th>
<th>bp</th>
<th>s1</th>
<th>s2</th>
<th>s3</th>
<th>s4</th>
<th>s5</th>
<th>s6</th>
<th>target</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>0.038076</td>
<td>0.050680</td>
<td>0.061696</td>
<td>0.021872</td>
<td>-0.044223</td>
<td>-0.034821</td>
<td>-0.043401</td>
<td>-0.002592</td>
<td>0.019907</td>
<td>-0.017646</td>
<td>151.0</td>
</tr>
<tr class="even">
<th>1</th>
<td>-0.001882</td>
<td>-0.044642</td>
<td>-0.051474</td>
<td>-0.026328</td>
<td>-0.008449</td>
<td>-0.019163</td>
<td>0.074412</td>
<td>-0.039493</td>
<td>-0.068332</td>
<td>-0.092204</td>
<td>75.0</td>
</tr>
<tr class="odd">
<th>2</th>
<td>0.085299</td>
<td>0.050680</td>
<td>0.044451</td>
<td>-0.005670</td>
<td>-0.045599</td>
<td>-0.034194</td>
<td>-0.032356</td>
<td>-0.002592</td>
<td>0.002861</td>
<td>-0.025930</td>
<td>141.0</td>
</tr>
<tr class="even">
<th>3</th>
<td>-0.089063</td>
<td>-0.044642</td>
<td>-0.011595</td>
<td>-0.036656</td>
<td>0.012191</td>
<td>0.024991</td>
<td>-0.036038</td>
<td>0.034309</td>
<td>0.022688</td>
<td>-0.009362</td>
<td>206.0</td>
</tr>
<tr class="odd">
<th>4</th>
<td>0.005383</td>
<td>-0.044642</td>
<td>-0.036385</td>
<td>0.021872</td>
<td>0.003935</td>
<td>0.015596</td>
<td>0.008142</td>
<td>-0.002592</td>
<td>-0.031988</td>
<td>-0.046641</td>
<td>135.0</td>
</tr>
<tr class="even">
<th>5</th>
<td>-0.092695</td>
<td>-0.044642</td>
<td>-0.040696</td>
<td>-0.019442</td>
<td>-0.068991</td>
<td>-0.079288</td>
<td>0.041277</td>
<td>-0.076395</td>
<td>-0.041176</td>
<td>-0.096346</td>
<td>97.0</td>
</tr>
<tr class="odd">
<th>6</th>
<td>-0.045472</td>
<td>0.050680</td>
<td>-0.047163</td>
<td>-0.015999</td>
<td>-0.040096</td>
<td>-0.024800</td>
<td>0.000779</td>
<td>-0.039493</td>
<td>-0.062917</td>
<td>-0.038357</td>
<td>138.0</td>
</tr>
<tr class="even">
<th>7</th>
<td>0.063504</td>
<td>0.050680</td>
<td>-0.001895</td>
<td>0.066629</td>
<td>0.090620</td>
<td>0.108914</td>
<td>0.022869</td>
<td>0.017703</td>
<td>-0.035816</td>
<td>0.003064</td>
<td>63.0</td>
</tr>
<tr class="odd">
<th>8</th>
<td>0.041708</td>
<td>0.050680</td>
<td>0.061696</td>
<td>-0.040099</td>
<td>-0.013953</td>
<td>0.006202</td>
<td>-0.028674</td>
<td>-0.002592</td>
<td>-0.014960</td>
<td>0.011349</td>
<td>110.0</td>
</tr>
<tr class="even">
<th>9</th>
<td>-0.070900</td>
<td>-0.044642</td>
<td>0.039062</td>
<td>-0.033213</td>
<td>-0.012577</td>
<td>-0.034508</td>
<td>-0.024993</td>
<td>-0.002592</td>
<td>0.067737</td>
<td>-0.013504</td>
<td>310.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The features in this dataset were standardized, making it easy to compare the magnitudes of the regression coefficients.</p>
<p>First, we look at basic linear regression on all ten predictive features in the data:</p>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> diabetes.drop(<span class="st">"target"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> diabetes[<span class="st">"target"</span>]</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-4"><a href="#cb55-4" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb55-5"><a href="#cb55-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb55-6"><a href="#cb55-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">2</span></span>
<span id="cb55-7"><a href="#cb55-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb55-8"><a href="#cb55-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb55-9"><a href="#cb55-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb55-10"><a href="#cb55-10" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb55-11"><a href="#cb55-11" aria-hidden="true" tabindex="-1"></a>lm.fit(X_tr, y_tr)</span>
<span id="cb55-12"><a href="#cb55-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"linear model CoD score: </span><span class="sc">{</span>lm<span class="sc">.</span>score(X_te, y_te)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>linear model CoD score: 0.4399</code></pre>
</div>
</div>
<p>First, we find that ridge regression can improve the score a bit:</p>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>rr <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>rr.fit(X_tr, y_tr)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ridge CoD score: </span><span class="sc">{</span>rr<span class="sc">.</span>score(X_te, y_te)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ridge CoD score: 0.4411</code></pre>
</div>
</div>
<p>Ridge regularization added a penalty for the 2-norm of the regression coefficients vector. Accordingly, the regularized solution has smaller coefficients:</p>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"2-norm of unregularized coefficients: </span><span class="sc">{</span>norm(lm.coef_)<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"2-norm of ridge coefficients: </span><span class="sc">{</span>norm(rr.coef_)<span class="sc">:.1f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2-norm of unregularized coefficients: 1525.2
2-norm of ridge coefficients: 605.9</code></pre>
</div>
</div>
<p>As we continue to increase the regularization parameter, the method becomes increasingly obsessed with keeping the coefficient vector small and pays ever less attention to the data:</p>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> [<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>]:</span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>    rr <span class="op">=</span> Ridge(alpha<span class="op">=</span>alpha)    <span class="co"># more regularization</span></span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a>    rr.fit(X_tr, y_tr)</span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"alpha = </span><span class="sc">{</span>alpha<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb61-5"><a href="#cb61-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"2-norm of coefficient vector: </span><span class="sc">{</span>norm(rr.coef_)<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb61-6"><a href="#cb61-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"ridge regression CoD score: </span><span class="sc">{</span>rr<span class="sc">.</span>score(X_te, y_te)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb61-7"><a href="#cb61-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>alpha = 0.25
2-norm of coefficient vector: 711.7
ridge regression CoD score: 0.4527

alpha = 0.50
2-norm of coefficient vector: 605.9
ridge regression CoD score: 0.4411

alpha = 1.00
2-norm of coefficient vector: 480.8
ridge regression CoD score: 0.4078

alpha = 2.00
2-norm of coefficient vector: 353.5
ridge regression CoD score: 0.3478
</code></pre>
</div>
</div>
<p>LASSO penalizes the 1-norm of the coefficient vector. Here’s a LASSO regression fit:</p>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a>lass <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.1</span>)</span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>lass.fit(X_tr, y_tr)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>R2 <span class="op">=</span> lass.score(X_te, y_te)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"LASSO model CoD score: </span><span class="sc">{</span>R2<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>LASSO model CoD score: 0.4335</code></pre>
</div>
</div>
<p>A validation curve suggests modest gains in the <span class="math inline">\(R^2\)</span> score as the regularization parameter is varied:</p>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="dv">0</span>,<span class="fl">0.1</span>,<span class="dv">80</span>)[<span class="dv">1</span>:]  <span class="co"># exclude alpha=0</span></span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>_,scores <span class="op">=</span> validation_curve(</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a>    lass,</span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>    X_tr, y_tr,</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>kf,</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    param_name<span class="op">=</span><span class="st">"alpha"</span>, param_range<span class="op">=</span>alpha</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>sns.relplot(x<span class="op">=</span>alpha, y<span class="op">=</span>np.mean(scores, axis<span class="op">=</span><span class="dv">1</span>) )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-43-output-1.png" width="451" height="450"></p>
</div>
</div>
<p>Moreover, while ridge regression still used all of the features, LASSO put zero weight on three of them:</p>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1" aria-hidden="true" tabindex="-1"></a>lass <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb66-2"><a href="#cb66-2" aria-hidden="true" tabindex="-1"></a>lass.fit(X_tr, y_tr)</span>
<span id="cb66-3"><a href="#cb66-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame( {</span>
<span id="cb66-4"><a href="#cb66-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"feature"</span>:X.columns,</span>
<span id="cb66-5"><a href="#cb66-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ridge"</span>:rr.coef_,</span>
<span id="cb66-6"><a href="#cb66-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LASSO"</span>:lass.coef_</span>
<span id="cb66-7"><a href="#cb66-7" aria-hidden="true" tabindex="-1"></a>    } )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="43">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>feature</th>
<th>ridge</th>
<th>LASSO</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>age</td>
<td>43.113029</td>
<td>-0.000000</td>
</tr>
<tr class="even">
<th>1</th>
<td>sex</td>
<td>-23.953301</td>
<td>-155.276227</td>
</tr>
<tr class="odd">
<th>2</th>
<td>bmi</td>
<td>199.535945</td>
<td>529.173009</td>
</tr>
<tr class="even">
<th>3</th>
<td>bp</td>
<td>144.586873</td>
<td>313.419043</td>
</tr>
<tr class="odd">
<th>4</th>
<td>s1</td>
<td>25.977923</td>
<td>-132.507438</td>
</tr>
<tr class="even">
<th>5</th>
<td>s2</td>
<td>2.751708</td>
<td>-0.000000</td>
</tr>
<tr class="odd">
<th>6</th>
<td>s3</td>
<td>-106.337626</td>
<td>-165.167100</td>
</tr>
<tr class="even">
<th>7</th>
<td>s4</td>
<td>89.526889</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<th>8</th>
<td>s5</td>
<td>185.660175</td>
<td>580.262391</td>
</tr>
<tr class="even">
<th>9</th>
<td>s6</td>
<td>85.576399</td>
<td>30.557703</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We can rank the relative importance of the features by ordering them in terms of decreasing coefficient magnitude:</p>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the permutation that sorts values in increasing order.</span></span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> np.argsort( np.<span class="bu">abs</span>(lass.coef_) )  </span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>order <span class="op">=</span> order[::<span class="op">-</span><span class="dv">1</span>]    <span class="co"># reverse the order</span></span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>pd.Series( order, index<span class="op">=</span>X.columns )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>age    8
sex    2
bmi    3
bp     6
s1     1
s2     4
s3     9
s4     7
s5     5
s6     0
dtype: int64</code></pre>
</div>
</div>
<p>The last three features were dropped by LASSO:</p>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>zeroed <span class="op">=</span> X.columns[order[<span class="op">-</span><span class="dv">3</span>:]]</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(zeroed)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Index(['s4', 's2', 'age'], dtype='object')</code></pre>
</div>
</div>
<p>Now we can drop these features from the dataset:</p>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a>X_tr_reduced <span class="op">=</span> X_tr.drop(zeroed, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>X_te_reduced <span class="op">=</span> X_te.drop(zeroed, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Returning to a fit with no regularization, we find that little is lost by using the reduced feature set:</p>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"original linear model score: </span><span class="sc">{</span>lm<span class="sc">.</span>score(X_te,y_te)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a>lm.fit(X_tr_reduced, y_tr)</span>
<span id="cb72-3"><a href="#cb72-3" aria-hidden="true" tabindex="-1"></a>R2 <span class="op">=</span> lm.score(X_te_reduced, y_te)</span>
<span id="cb72-4"><a href="#cb72-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"reduced linear model score: </span><span class="sc">{</span>R2<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>original linear model score: 0.4399
reduced linear model score: 0.4388</code></pre>
</div>
</div>
</section>
</section>
<section id="nonlinear-regression" class="level2" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="nonlinear-regression"><span class="header-section-number">5.4</span> Nonlinear regression</h2>
<p>Multilinear regression limits the representation of the dataset to a function of the form <span class="math display">\[
\hat{f}(\bfx) = \bfw^T \bfx.
\]</span> This is a <strong>linear</strong> function, meaning that two key properties are satisfied. For all possible vectors <span class="math inline">\(\bfu,\bfv\)</span> and numbers <span class="math inline">\(c\)</span>,</p>
<ol type="1">
<li><span class="math inline">\(\hat{f}(\bfu + \bfv) = \hat{f}(\bfu) + \hat{f}(\bfv)\)</span>,</li>
<li><span class="math inline">\(\hat{f}(c\bfu) = c \hat{f}(\bfu)\)</span>.</li>
</ol>
<p>These properties are the essence of what makes a function easy to manipulate, solve for, and analyze. For our particular <span class="math inline">\(\hat{f}\)</span>, they follow easily from how the inner product is defined. For example, <span class="math display">\[
\hat{f}(c\bfu) = (c\bfu)^T\bfw = \sum_{i=1}^d (cu_i) w_i = c \sum_{i=1}^d u_i w_i = c(\bfu^T\bfw) = c \hat{f}(\bfu).
\]</span></p>
<p>One benefit of the linear approach is that the dependence of the weight vector <span class="math inline">\(\bfw\)</span> on the regressed data is also linear, which makes solving for it relatively straightforward.</p>
<p>As the simplest type of multidimensional function, linear relationships are a good first resort. Furthermore, we can augment the features with powers in order to get polynomial relationships. However, that approach becomes infeasible for more than 2 or 3 dimensions, because the number of polynomial terms needed explodes. While there is a way around this restriction known as the <em>kernel trick</em>, that’s beyond our mathematical scope here.</p>
<p>Alternatively, we can resort to fully nonlinear regression methods. Two of them come from generalizations of our staple classifiers.</p>
<section id="nearest-neighbors" class="level3" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="nearest-neighbors"><span class="header-section-number">5.4.1</span> Nearest neighbors</h3>
<p>To use kNN for regression, we find the <span class="math inline">\(k\)</span> nearest examples as with classification, but replace voting on classes with the mean or median of the neighboring values. A simple example confirms that the resulting approximation is not linear.</p>
<div id="exm-regression-knn" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.9 </strong></span>Suppose we have just two samples with one-dimensional features: <span class="math inline">\(x_1=0\)</span> and <span class="math inline">\(x_2=2\)</span>, and let the corresponding sample values be <span class="math inline">\(y_1=0\)</span> and <span class="math inline">\(y_2=1\)</span>. Using kNN with <span class="math inline">\(k=1\)</span>, the resulting approximation <span class="math inline">\(\hat{f}(x)\)</span> is <span class="math display">\[
\hat{f}(x) =
\begin{cases}
    0, &amp; x &lt; 1, \\
    \tfrac{1}{2}, &amp; x=1, \\  
    1, &amp; x &gt; 1.
\end{cases}
\]</span> (Convince yourself that the result is the same whether the mean or the median is used.) Thus, for instance, <span class="math inline">\(\hat{f}(1.2)=1\)</span>, while <span class="math inline">\(2\hat{f}(0.6) = 0\)</span>, which is not equal to <span class="math inline">\(\hat{f}(2 \cdot 0.6)\)</span>.</p>
</div>
<p>kNN regression can produce a function that conforms itself to the training data much more closely than a linear regressor does. This can both decrease bias and increase variance, especially for small values of <span class="math inline">\(k\)</span>. As illustrated in the following video, increasing <span class="math inline">\(k\)</span> flattens out the approximation, decreasing variance while increasing bias.</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="600" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="_media/knn_regression.mp4"></video></div>
<p>As with classification, we can choose the norm to use and whether to weight the neighbors equally or by inverse distance. As a reminder, it is usually advisable to work with z-scores for the features rather than raw data.</p>
<div id="exm-regression-knn-demo" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.10 </strong></span>We return again to the dataset of cars and their fuel efficiency. A linear regression on four quantitative features is only OK:</p>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>cars <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"displacement"</span>, <span class="st">"horsepower"</span>, <span class="st">"weight"</span>, <span class="st">"acceleration"</span>]</span>
<span id="cb74-3"><a href="#cb74-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars[features]</span>
<span id="cb74-4"><a href="#cb74-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb74-5"><a href="#cb74-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-6"><a href="#cb74-6" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb74-7"><a href="#cb74-7" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb74-8"><a href="#cb74-8" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb74-9"><a href="#cb74-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb74-10"><a href="#cb74-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb74-11"><a href="#cb74-11" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb74-12"><a href="#cb74-12" aria-hidden="true" tabindex="-1"></a>lm.fit(X_tr, y_tr)</span>
<span id="cb74-13"><a href="#cb74-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"linear model CoD: </span><span class="sc">{</span>lm<span class="sc">.</span>score(X_te, y_te)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>linear model CoD: 0.6928</code></pre>
</div>
</div>
<p>Next we try a kNN regressor, doing a grid search to find good hyperparameters:</p>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor</span>
<span id="cb76-2"><a href="#cb76-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-3"><a href="#cb76-3" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">6</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb76-4"><a href="#cb76-4" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> {</span>
<span id="cb76-5"><a href="#cb76-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"kneighborsregressor__n_neighbors"</span>: <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">25</span>),</span>
<span id="cb76-6"><a href="#cb76-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"kneighborsregressor__weights"</span>: [<span class="st">"uniform"</span>, <span class="st">"distance"</span>] </span>
<span id="cb76-7"><a href="#cb76-7" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb76-8"><a href="#cb76-8" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> make_pipeline( StandardScaler(), KNeighborsRegressor() )</span>
<span id="cb76-9"><a href="#cb76-9" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> GridSearchCV(</span>
<span id="cb76-10"><a href="#cb76-10" aria-hidden="true" tabindex="-1"></a>    knn,</span>
<span id="cb76-11"><a href="#cb76-11" aria-hidden="true" tabindex="-1"></a>    grid, </span>
<span id="cb76-12"><a href="#cb76-12" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>kf, </span>
<span id="cb76-13"><a href="#cb76-13" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb76-14"><a href="#cb76-14" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb76-15"><a href="#cb76-15" aria-hidden="true" tabindex="-1"></a>optim.fit(X_tr, y_tr)</span>
<span id="cb76-16"><a href="#cb76-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb76-17"><a href="#cb76-17" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"best kNN CoD: </span><span class="sc">{</span>optim<span class="sc">.</span>score(X_te, y_te)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>best kNN CoD: 0.7439</code></pre>
</div>
</div>
<p>As you can see above, we got some improvement over the linear regressor.</p>
</div>
</section>
<section id="decision-tree" class="level3" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="decision-tree"><span class="header-section-number">5.4.2</span> Decision tree</h3>
<p>Recall that a decision tree recursively divides the examples into subsets. As with kNN, we can replace taking a classification vote over a leaf subset with taking a mean or median of the values. But the method of determining splits needs to be changed as well.</p>
<p>Instead of using a measure of subset impurities to determine the best split, the split is chosen to cause the greatest reduction in dispersion within the two subsets. The most common choices for the dispersion measure <span class="math inline">\(H\)</span> are:</p>
<ol type="1">
<li>If using the mean of subset values, then let <span class="math inline">\(H\)</span> be standard deviation.</li>
<li>If using the median of subset values, then let <span class="math inline">\(H\)</span> be mean absolute deviation (MAD), defined as <span class="math display">\[
\text{MAD} = \frac{1}{m} \sum_{i=1}^m | t_i - t_\text{med} |
\]</span> for any list of values <span class="math inline">\(t_1,\ldots,t_m\)</span> and <span class="math inline">\(t_\text{med}\)</span> equal to the median value.</li>
</ol>
<p>As with classification, a proposal to split into subsets <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> is assigned the weighted score <span class="math display">\[
Q = |S| H(S) + |T| H(T).
\]</span> The split location is chosen to minimize <span class="math inline">\(Q\)</span>.</p>
<div id="exm-regression-split" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.11 </strong></span>Suppose we are given the observations <span class="math inline">\(x_i=i\)</span>, <span class="math inline">\(i=1,\ldots,4\)</span>, where <span class="math inline">\(y_1=2\)</span>, <span class="math inline">\(y_2=-1\)</span>, <span class="math inline">\(y_3=1\)</span>, <span class="math inline">\(y_4=0\)</span>. Let’s find the decision tree regressor using medians/MAD.</p>
<p>The original value set has median <span class="math inline">\(\frac{1}{2}\)</span> and gets a weighted dispersion of <span class="math inline">\(\frac{5}{2}(3+3+1+1)=20\)</span>. There are three ways to split the data, depending on where the partition falls in relation to the <span class="math inline">\(x_i\)</span>:</p>
<ul>
<li><span class="math inline">\(S=\{2\},\,T=\{-1,1,0\}\)</span> <span class="math display">\[
\begin{split}
Q &amp;= 1\left[ \frac{1}{1} |2-2|  \right] +  3 \left[ \frac{1}{3}\bigl( | -1-0 | + |1-0| + |0-0|  \bigr)  \right]\\
&amp;=  0 + 2 = 2.
\end{split}
\]</span></li>
<li><span class="math inline">\(S=\{2,-1\},\, T=\{1,0\}\)</span> <span class="math display">\[
\begin{split}
  Q &amp;= 2\left[ \frac{1}{2}\bigl( \left| 2-\tfrac{1}{2} \right| + \left| -1-\tfrac{1}{2} \right| \bigr)  \right] +  2 \left[ \frac{1}{2}\bigl( \left|1-\tfrac{1}{2} \right| + \left|0-\tfrac{1}{2} \right|  \bigr)  \right]\\
  &amp;=  3 + 1 = 4.
\end{split}
\]</span></li>
<li><span class="math inline">\(S=\{2,-1,1\},\, T=\{0\}\)</span> <span class="math display">\[
\begin{split}
  Q &amp;= 3\left[ \frac{1}{3}\bigl( \left| 2-1 \right| + \left| -1-1 \right|+ |1-1| \bigr)  \right] +  1 \left[ \frac{1}{1} \left|0-0 \right|  \right]\\
  &amp;=  3 + 0 = 3.
\end{split}
\]</span></li>
</ul>
<p>Thus, the first split above produces the smallest total dispersion.</p>
</div>
<p>To predict a value for a query <span class="math inline">\(x\)</span>, we follow the tree until ending at a leaf, where we use the mean (if dispersion is STD) or median (if dispersion is MAD) of the examples in the leaf.</p>
<div id="exm-regression-tree" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.12 </strong></span>Here is some simple 2D data:</p>
<div class="cell" data-execution_count="51">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> default_rng(<span class="dv">1</span>)</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> rng.random((<span class="dv">10</span>,<span class="dv">2</span>))</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a>x1[:,<span class="dv">0</span>] <span class="op">-=</span> <span class="fl">0.25</span></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> rng.random((<span class="dv">10</span>,<span class="dv">2</span>))</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>x2[:,<span class="dv">0</span>] <span class="op">+=</span> <span class="fl">0.25</span></span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack((x1,x2))</span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.exp(X[:,<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span><span class="op">*</span>X[:,<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span><span class="op">+</span>X[:,<span class="dv">0</span>]<span class="op">*</span>X[:,<span class="dv">1</span>])</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"x₁"</span>:X[:,<span class="dv">0</span>],<span class="st">"x₂"</span>:X[:,<span class="dv">1</span>],<span class="st">"y"</span>:y})</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>df,x<span class="op">=</span><span class="st">"x₁"</span>,y<span class="op">=</span><span class="st">"x₂"</span>,hue<span class="op">=</span><span class="st">"y"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-51-output-1.png" width="589" height="429"></p>
</div>
</div>
<p>The default in <code>sklearn</code> is to use STD as the dispersion measure (called <code>squared_error</code> in sklearn). Here is a shallow tree fitted to the data:</p>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb79-1"><a href="#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor, plot_tree</span>
<span id="cb79-2"><a href="#cb79-2" aria-hidden="true" tabindex="-1"></a>dtree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb79-3"><a href="#cb79-3" aria-hidden="true" tabindex="-1"></a>dtree.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked=""><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">DecisionTreeRegressor</label><div class="sk-toggleable__content"><pre>DecisionTreeRegressor(max_depth=2)</pre></div></div></div></div></div>
</div>
</div>
<div class="cell" data-execution_count="53">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">17</span>,<span class="dv">10</span>), dpi<span class="op">=</span><span class="dv">160</span>)</span>
<span id="cb80-3"><a href="#cb80-3" aria-hidden="true" tabindex="-1"></a>plot_tree(dtree,feature_names<span class="op">=</span>[<span class="st">"x₁"</span>,<span class="st">"x₂"</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-53-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>All of the original samples end up in one of the four leaves. We can find out the tree node number that each sample ends up at using <code>apply</code>:</p>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>leaf <span class="op">=</span> dtree.<span class="bu">apply</span>(X)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(leaf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[3 3 2 5 2 2 3 2 2 2 5 6 5 5 3 5 6 6 2 5]</code></pre>
</div>
</div>
<p>From the above we deduce that the leaves are the nodes numbered 2, 3, 5, and 6. With some pandas grouping, we can find out the mean value for the samples within each of these:</p>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>leaves <span class="op">=</span> pd.DataFrame( {<span class="st">"y"</span>: y, <span class="st">"leaf"</span>: leaf} )</span>
<span id="cb83-2"><a href="#cb83-2" aria-hidden="true" tabindex="-1"></a>leaves.groupby(<span class="st">"leaf"</span>)[<span class="st">"y"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="54">
<pre><code>leaf
2    0.911328
3    0.270725
5    2.378427
6    1.003786
Name: y, dtype: float64</code></pre>
</div>
</div>
<p>All values of the regressor will be one of the four values above. This is exactly what is done internally by the <code>predict</code> method of the regressor:</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( dtree.predict(X) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.27072468 0.27072468 0.91132782 2.37842709 0.91132782 0.91132782
 0.27072468 0.91132782 0.91132782 0.91132782 2.37842709 1.00378567
 2.37842709 2.37842709 0.27072468 2.37842709 1.00378567 1.00378567
 0.91132782 2.37842709]</code></pre>
</div>
</div>
</div>
<div id="exm-regression-tree-demo" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.13 </strong></span>Continuing with the data from <a href="#exm-regression-knn-demo">Example&nbsp;<span>5.10</span></a>, we find that we can do even better with a random forest of decision tree regressors:</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars[features]</span>
<span id="cb87-2"><a href="#cb87-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb87-3"><a href="#cb87-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-4"><a href="#cb87-4" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb87-5"><a href="#cb87-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb87-6"><a href="#cb87-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb87-7"><a href="#cb87-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb87-8"><a href="#cb87-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-9"><a href="#cb87-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb87-10"><a href="#cb87-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-11"><a href="#cb87-11" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> {</span>
<span id="cb87-12"><a href="#cb87-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_depth"</span>: <span class="bu">range</span>(<span class="dv">3</span>, <span class="dv">8</span>),</span>
<span id="cb87-13"><a href="#cb87-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_samples"</span>: np.arange(<span class="fl">0.2</span>, <span class="fl">0.6</span>, <span class="fl">0.1</span>),</span>
<span id="cb87-14"><a href="#cb87-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb87-15"><a href="#cb87-15" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">60</span>)</span>
<span id="cb87-16"><a href="#cb87-16" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> GridSearchCV(</span>
<span id="cb87-17"><a href="#cb87-17" aria-hidden="true" tabindex="-1"></a>    knn,</span>
<span id="cb87-18"><a href="#cb87-18" aria-hidden="true" tabindex="-1"></a>    grid, </span>
<span id="cb87-19"><a href="#cb87-19" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>kf, </span>
<span id="cb87-20"><a href="#cb87-20" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb87-21"><a href="#cb87-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb87-22"><a href="#cb87-22" aria-hidden="true" tabindex="-1"></a>optim.fit(X_tr, y_tr)</span>
<span id="cb87-23"><a href="#cb87-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb87-24"><a href="#cb87-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"best forest CoD: </span><span class="sc">{</span>optim<span class="sc">.</span>score(X_te, y_te)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>best forest CoD: 0.7950</code></pre>
</div>
</div>
</div>
</section>
</section>
<section id="logistic-regression" class="level2" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">5.5</span> Logistic regression</h2>
<p>Sometimes a regressed value is subject to certain known bounds or other conditions. A major example is probability, which has to be between 0 and 1 (inclusive).</p>
<p>A linear regressor, <span class="math inline">\(\hat{f}(\bfx) = \bfw^T \bfx\)</span> for a constant vector <span class="math inline">\(\bfw\)</span>, typically ranges over all of <span class="math inline">\((-\infty,\infty)\)</span>. In order to get a result that must lie within <span class="math inline">\([0,1]\)</span>, we can transform its output using the <strong>logistic function</strong>, defined as</p>
<p><span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}}.
\]</span></p>
<p>The logistic function has the real line as its domain and takes the form of a smoothed step increasing from 0 to 1:</p>
<div class="cell" data-execution_count="58">
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-58-output-1.png" width="493" height="324"></p>
</div>
</div>
<p>Given samples of a probability variable <span class="math inline">\(p(\bfx)\)</span>, the regression task is to find a weight vector <span class="math inline">\(\bfw\)</span> so that <span class="math display">\[
p \approx \sigma(\bfx^T\bfw).
\]</span> The result is known as <strong>logistic regression</strong>. A common way to use logistic regression is for binary classification. Suppose we have training samples <span class="math inline">\((\bfx_i, y_i)\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, where for each <span class="math inline">\(i\)</span> either <span class="math inline">\(y_i=0\)</span> or <span class="math inline">\(y_i=1\)</span>. The resulting approximation to <span class="math inline">\(p\)</span> at some query <span class="math inline">\(\bfx\)</span> can then be interpreted as the probability of observing a 1 at <span class="math inline">\(\bfx\)</span>.</p>
<p>In order to fully specify the regressor, we need to specify a loss function to be optimized.</p>
<section id="loss-function" class="level3" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">5.5.1</span> Loss function</h3>
<p>Defining <span class="math inline">\(\hat{p}_i = \sigma(\bfx_i^T\bfw)\)</span> at all the training points, a straightforward loss function would be <span class="math display">\[
\sum_{i=1}^n \left( \hat{p}_i - y_i \right)^2.
\]</span> For binary classification, however, it’s more common to use the <strong>cross-entropy</strong> loss function <span id="eq-regression-cross-entropy"><span class="math display">\[
L(\bfw) = -\sum_{i=1}^n \left[ y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right].
\tag{5.7}\]</span></span> (The logarithms in <a href="#eq-regression-cross-entropy">Equation&nbsp;<span>5.7</span></a> can be in any base, since that choice only affects <span class="math inline">\(L\)</span> by a constant factor.) In cross-entropy loss, sample <span class="math inline">\(i\)</span> contributes <span class="math display">\[
-\log(1-\hat{p}_i)
\]</span> if <span class="math inline">\(y_i=0\)</span>, which becomes infinite as <span class="math inline">\(\hat{p}_i\to 1^-\)</span>, and <span class="math display">\[
-\log(\hat{p}_i)
\]</span> if <span class="math inline">\(y_i=1\)</span>, which becomes infinite as <span class="math inline">\(\hat{p}_i\to 0^+\)</span>. In words, there is a steep penalty for being almost completely wrong about an observation.</p>
<p>Logistic regression does have a major disadvantage compared to linear regression: the minimization of loss does <em>not</em> lead to a linear problem for the weight vector <span class="math inline">\(\bfw\)</span>. The difference in practice is usually not a concern, though.</p>
</section>
<section id="regularization-1" class="level3" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="regularization-1"><span class="header-section-number">5.5.2</span> Regularization</h3>
<p>As with other forms of regression, the loss function may be regularized using the ridge or LASSO penalty. The standard formulation is</p>
<p><span class="math display">\[
\widetilde{L}(\bfw) = C \, L(\bfw) + \norm{\bfw},
\]</span></p>
<p>where <span class="math inline">\(C\)</span> is a positive hyperparameter and the vector norm is either the 2-norm (ridge) or 1-norm (LASSO).</p>
<div class="callout callout-style-default callout-important callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The parameter <span class="math inline">\(C\)</span> functions like the inverse of the regularization parameter <span class="math inline">\(\alpha\)</span> we used in the linear regressor. It’s just a different convention chosen historically. As <span class="math inline">\(C\)</span> decreases, the regularization strength increases.</p>
</div>
</div>
</section>
<section id="case-study-personal-spam-filter" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="case-study-personal-spam-filter"><span class="header-section-number">5.5.3</span> Case study: Personal spam filter</h3>
<p>We will try logistic regression for a simple spam filter. The data set is based on work and personal emails for one individual. The features are calculated word and character frequencies, as well as the appearance of capital letters.</p>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>spam <span class="op">=</span> pd.read_csv(<span class="st">"_datasets/spambase.csv"</span>)</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>spam.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>word_freq_make</th>
<th>word_freq_address</th>
<th>word_freq_all</th>
<th>word_freq_3d</th>
<th>word_freq_our</th>
<th>word_freq_over</th>
<th>word_freq_remove</th>
<th>word_freq_internet</th>
<th>word_freq_order</th>
<th>word_freq_mail</th>
<th>...</th>
<th>char_freq_%3B</th>
<th>char_freq_%28</th>
<th>char_freq_%5B</th>
<th>char_freq_%21</th>
<th>char_freq_%24</th>
<th>char_freq_%23</th>
<th>capital_run_length_average</th>
<th>capital_run_length_longest</th>
<th>capital_run_length_total</th>
<th>class</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>0.00</td>
<td>0.64</td>
<td>0.64</td>
<td>0.0</td>
<td>0.32</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>...</td>
<td>0.00</td>
<td>0.000</td>
<td>0.0</td>
<td>0.778</td>
<td>0.000</td>
<td>0.000</td>
<td>3.756</td>
<td>61</td>
<td>278</td>
<td>1</td>
</tr>
<tr class="even">
<th>1</th>
<td>0.21</td>
<td>0.28</td>
<td>0.50</td>
<td>0.0</td>
<td>0.14</td>
<td>0.28</td>
<td>0.21</td>
<td>0.07</td>
<td>0.00</td>
<td>0.94</td>
<td>...</td>
<td>0.00</td>
<td>0.132</td>
<td>0.0</td>
<td>0.372</td>
<td>0.180</td>
<td>0.048</td>
<td>5.114</td>
<td>101</td>
<td>1028</td>
<td>1</td>
</tr>
<tr class="odd">
<th>2</th>
<td>0.06</td>
<td>0.00</td>
<td>0.71</td>
<td>0.0</td>
<td>1.23</td>
<td>0.19</td>
<td>0.19</td>
<td>0.12</td>
<td>0.64</td>
<td>0.25</td>
<td>...</td>
<td>0.01</td>
<td>0.143</td>
<td>0.0</td>
<td>0.276</td>
<td>0.184</td>
<td>0.010</td>
<td>9.821</td>
<td>485</td>
<td>2259</td>
<td>1</td>
</tr>
<tr class="even">
<th>3</th>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.0</td>
<td>0.63</td>
<td>0.00</td>
<td>0.31</td>
<td>0.63</td>
<td>0.31</td>
<td>0.63</td>
<td>...</td>
<td>0.00</td>
<td>0.137</td>
<td>0.0</td>
<td>0.137</td>
<td>0.000</td>
<td>0.000</td>
<td>3.537</td>
<td>40</td>
<td>191</td>
<td>1</td>
</tr>
<tr class="odd">
<th>4</th>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.0</td>
<td>0.63</td>
<td>0.00</td>
<td>0.31</td>
<td>0.63</td>
<td>0.31</td>
<td>0.63</td>
<td>...</td>
<td>0.00</td>
<td>0.135</td>
<td>0.0</td>
<td>0.135</td>
<td>0.000</td>
<td>0.000</td>
<td>3.537</td>
<td>40</td>
<td>191</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>5 rows × 58 columns</p>
</div>
</div>
</div>
<p>We create a feature matrix and label vector, and split into train/test sets:</p>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> spam.drop(<span class="st">"class"</span>, axis<span class="op">=</span><span class="st">"columns"</span>)</span>
<span id="cb90-2"><a href="#cb90-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> spam[<span class="st">"class"</span>]</span>
<span id="cb90-3"><a href="#cb90-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb90-4"><a href="#cb90-4" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb90-5"><a href="#cb90-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb90-6"><a href="#cb90-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb90-7"><a href="#cb90-7" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb90-8"><a href="#cb90-8" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When using norm-based regularization, it’s good practice to standardize the variables, so we will use scaling pipelines. First we use a large value of <span class="math inline">\(C\)</span> to emphasize the regressive loss over the regularization penalty:</p>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>logr <span class="op">=</span> LogisticRegression(C<span class="op">=</span><span class="dv">100</span>, solver<span class="op">=</span><span class="st">"liblinear"</span>)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), logr)</span>
<span id="cb91-5"><a href="#cb91-5" aria-hidden="true" tabindex="-1"></a>pipe.fit(X_tr, y_tr)</span>
<span id="cb91-6"><a href="#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"accuracy:"</span>, pipe.score(X_te, y_te))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy: 0.9337676438653637</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-tip callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The use of the <code>solver</code> keyword is optional, but the solver used above seems to be far faster and more reliable for small datasets than the default.</p>
</div>
</div>
<p>Let’s look at the most extreme regression coefficients, associating them with the feature names and then sorting the results:</p>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>coef <span class="op">=</span> pd.Series(logr.coef_[<span class="dv">0</span>], index<span class="op">=</span>X.columns).sort_values()</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"most hammy features:"</span>)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef[:<span class="dv">4</span>])</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb93-5"><a href="#cb93-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"most spammy features:"</span>)</span>
<span id="cb93-6"><a href="#cb93-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(coef[<span class="op">-</span><span class="dv">4</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>most hammy features:
word_freq_george    -24.055810
word_freq_cs         -8.573920
word_freq_hp         -3.512677
word_freq_meeting    -1.940989
dtype: float64

most spammy features:
char_freq_%23                 1.155189
char_freq_%24                 1.262551
capital_run_length_longest    1.974990
word_freq_3d                  2.112905
dtype: float64</code></pre>
</div>
</div>
<p>The word “george” is a strong counter-indicator for spam; remember that this data set comes from an individual’s inbox. Its presence makes the inner product <span class="math inline">\(\bfx^T\bfw\)</span> more negative, which drives the logistic function closer to 0. Conversely, the presence of consecutive capital letters increases the inner product and pushes the probability closer to 1.</p>
<p>The ultimate predictions by the regressor are all either 0 or 1. But we can also see the forecasted probabilities before thresholding:</p>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"predicted classes:"</span>)</span>
<span id="cb95-2"><a href="#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( pipe.predict(X_tr.iloc[:<span class="dv">5</span>,:]) )</span>
<span id="cb95-3"><a href="#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"</span><span class="ch">\n</span><span class="st">probabilities:"</span>)</span>
<span id="cb95-4"><a href="#cb95-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( pipe.predict_proba(X_tr.iloc[:<span class="dv">5</span>,:]) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>predicted classes:
[0 0 0 0 0]

probabilities:
[[0.53768185 0.46231815]
 [0.99694715 0.00305285]
 [0.63975941 0.36024059]
 [0.99634195 0.00365805]
 [0.93740669 0.06259331]]</code></pre>
</div>
</div>
<p>The probabilities might be useful for making decisions based on the results. For example, the first instance above was much less certain about the classification than the second. A more skeptical threshold greater than <span class="math inline">\(0.54\)</span> would change the class to 1. As in <a href="classification.html#sec-class-quant"><span>Section&nbsp;3.5</span></a>, the probability matrix can be used to create an ROC curve showing the tradeoffs over all thresholds.</p>
<p>For a validation-based selection of the best regularization parameter value, we can use <code>LogisticRegressionCV</code>, which is a convenience method for a grid search. You can specify which values of <span class="math inline">\(C\)</span> to search over, or just say how many, as we do here:</p>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegressionCV</span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>logr <span class="op">=</span> LogisticRegressionCV(</span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>    Cs<span class="op">=</span><span class="dv">40</span>,    <span class="co"># 40 automatically chosen values of C</span></span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span><span class="dv">5</span>, </span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>    solver<span class="op">=</span><span class="st">"liblinear"</span>, </span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), logr)</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a>pipe.fit(X_tr, y_tr)</span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"best C value: </span><span class="sc">{</span>logr<span class="sc">.</span>C_[<span class="dv">0</span>]<span class="sc">:.3g}</span><span class="ss">"</span>)</span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"accuracy score: </span><span class="sc">{</span>pipe<span class="sc">.</span>score(X_te,y_te)<span class="sc">:.5f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>best C value: 21.5
accuracy score: 0.93485</code></pre>
</div>
</div>
</section>
<section id="multiclass-case" class="level3" data-number="5.5.4">
<h3 data-number="5.5.4" class="anchored" data-anchor-id="multiclass-case"><span class="header-section-number">5.5.4</span> Multiclass case</h3>
<p>When there are more than two unique labels possible, logistic regression can be extended through the one-vs-rest (OVR) paradigm we have used previously.</p>
<p>Given <span class="math inline">\(K\)</span> classes, there are <span class="math inline">\(K\)</span> binary regressors fit for the outcomes “class 1/not class 1,” “class 2/not class 2,” and so on, giving <span class="math inline">\(K\)</span> different coefficient vectors, <span class="math inline">\(\bfw_k\)</span>. Now for a query point <span class="math inline">\(\bfx\)</span>, we can predict probabilities for it being in each class:</p>
<p><span class="math display">\[
\hat{q}_{k}(\bfx) = \sigma(\bfx^T \bfw_k), \qquad k=1,\ldots,K.
\]</span></p>
<p>Since the <span class="math inline">\(K\)</span> OVR regressors are done independently, there is no reason to think these probabilities will sum to 1 over all the classes. So we must normalize them:</p>
<p><span class="math display">\[
\hat{p}_{k}(bfx) = \frac{\hat{q}_{k}(\bfx)}{\sum_{k=1}^K \hat{q}_{k}(\bfx)}.
\]</span></p>
<p>Computed over a testing set, we get a matrix of probabilities. Each of the rows gives the class probabilities at a single query point, and each of the <span class="math inline">\(K\)</span> columns gives the probability of one class at all the points.</p>
<div id="exm-regression-logistic-gas" class="theorem example">
<p><span class="theorem-title"><strong>Example 5.14 </strong></span>As a multiclass example, we use a data set about gas sensors recording values over long periods of time:</p>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>gas <span class="op">=</span> pd.read_csv(<span class="st">"_datasets/gas_drift.csv"</span>)</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> gas[<span class="st">"Class"</span>]</span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> gas.drop(<span class="st">"Class"</span>, axis<span class="op">=</span><span class="st">"columns"</span>)</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>X_tr, X_te, y_tr, y_te <span class="op">=</span> train_test_split(</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">1</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>logr <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>)</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), logr)</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>pipe.fit(X_tr, y_tr)</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"accuracy score:"</span>, pipe.score(X_te, y_te))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy score: 0.98705966930266</code></pre>
</div>
</div>
<p>We can now look at probabilistic predictions for each class:</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> pipe.predict_proba(X_te)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> [<span class="st">"Class "</span><span class="op">+</span><span class="bu">str</span>(i) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">7</span>)]</span>
<span id="cb101-3"><a href="#cb101-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame(p_hat, columns<span class="op">=</span>cols).head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<div>


<table class="dataframe table table-sm table-striped" data-border="1">
<thead>
<tr class="header">
<th></th>
<th>Class 1</th>
<th>Class 2</th>
<th>Class 3</th>
<th>Class 4</th>
<th>Class 5</th>
<th>Class 6</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<th>0</th>
<td>0.000154</td>
<td>5.324116e-06</td>
<td>0.020485</td>
<td>4.207072e-03</td>
<td>0.003196</td>
<td>9.719529e-01</td>
</tr>
<tr class="even">
<th>1</th>
<td>0.000004</td>
<td>9.999198e-01</td>
<td>0.000074</td>
<td>6.812414e-07</td>
<td>0.000002</td>
<td>5.869686e-17</td>
</tr>
<tr class="odd">
<th>2</th>
<td>0.008624</td>
<td>3.847762e-03</td>
<td>0.000017</td>
<td>1.282243e-03</td>
<td>0.001295</td>
<td>9.849346e-01</td>
</tr>
<tr class="even">
<th>3</th>
<td>0.237558</td>
<td>5.503335e-08</td>
<td>0.000020</td>
<td>7.222371e-01</td>
<td>0.000054</td>
<td>4.013053e-02</td>
</tr>
<tr class="odd">
<th>4</th>
<td>0.016611</td>
<td>3.643991e-02</td>
<td>0.010126</td>
<td>2.032083e-01</td>
<td>0.032053</td>
<td>7.015607e-01</td>
</tr>
<tr class="even">
<th>5</th>
<td>0.002096</td>
<td>9.976914e-01</td>
<td>0.000163</td>
<td>4.242972e-05</td>
<td>0.000007</td>
<td>7.264576e-11</td>
</tr>
<tr class="odd">
<th>6</th>
<td>0.000212</td>
<td>2.805245e-05</td>
<td>0.003276</td>
<td>5.669049e-04</td>
<td>0.017867</td>
<td>9.780503e-01</td>
</tr>
<tr class="even">
<th>7</th>
<td>0.080161</td>
<td>7.056994e-08</td>
<td>0.000025</td>
<td>8.963731e-01</td>
<td>0.000091</td>
<td>2.334934e-02</td>
</tr>
<tr class="odd">
<th>8</th>
<td>0.002059</td>
<td>9.640239e-06</td>
<td>0.000399</td>
<td>7.548506e-02</td>
<td>0.002079</td>
<td>9.199678e-01</td>
</tr>
<tr class="even">
<th>9</th>
<td>0.979774</td>
<td>1.129101e-03</td>
<td>0.000001</td>
<td>1.136161e-02</td>
<td>0.000003</td>
<td>7.730162e-03</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>This allows us to see that the ROC curves are nearly perfect:</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb102-2"><a href="#cb102-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(pipe.classes_):</span>
<span id="cb102-3"><a href="#cb102-3" aria-hidden="true" tabindex="-1"></a>    actual <span class="op">=</span> (y_te<span class="op">==</span>label)</span>
<span id="cb102-4"><a href="#cb102-4" aria-hidden="true" tabindex="-1"></a>    fp, tp, theta <span class="op">=</span> roc_curve(actual, p_hat[:,i])</span>
<span id="cb102-5"><a href="#cb102-5" aria-hidden="true" tabindex="-1"></a>    results.extend( [ (label,fp,tp) <span class="cf">for</span> fp,tp <span class="kw">in</span> <span class="bu">zip</span>(fp,tp) ] )</span>
<span id="cb102-6"><a href="#cb102-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb102-7"><a href="#cb102-7" aria-hidden="true" tabindex="-1"></a>roc <span class="op">=</span> pd.DataFrame( results, columns<span class="op">=</span>[<span class="st">"label"</span>, <span class="st">"FP rate"</span>, <span class="st">"TP rate"</span>] )</span>
<span id="cb102-8"><a href="#cb102-8" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>roc, </span>
<span id="cb102-9"><a href="#cb102-9" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"FP rate"</span>, y<span class="op">=</span><span class="st">"TP rate"</span>, </span>
<span id="cb102-10"><a href="#cb102-10" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"label"</span>, kind<span class="op">=</span><span class="st">"line"</span>, estimator<span class="op">=</span><span class="va">None</span></span>
<span id="cb102-11"><a href="#cb102-11" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="regression_files/figure-html/cell-67-output-1.png" width="526" height="468"></p>
</div>
</div>
<p>Based on the ROC curves, we could choose a high decision threshold to cut down on false positives without losing many true positives.</p>
</div>
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-regression-no-intercept" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.1 </strong></span>Suppose that the distinct plane points <span class="math inline">\((x_i,y_i)\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span> are to be fit using a linear function without intercept, <span class="math inline">\(\hat{f}(x)=\alpha x\)</span>. Use calculus to find a formula for the value of <span class="math inline">\(\alpha\)</span> that minimizes the sum of squared residuals, <span class="math display">\[ r = \sum_{i=1}^n (f(x_i)-y_i)^2. \]</span></p>
</div>
<div id="exr-regression-combination" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.2 </strong></span>Suppose that <span class="math inline">\(x_1=-2\)</span>, <span class="math inline">\(x_2=1\)</span>, and <span class="math inline">\(x_3=2\)</span>. Define <span class="math inline">\(\alpha\)</span> as in <a href="#exr-regression-no-intercept">Exercise&nbsp;<span>5.1</span></a>, and define the predicted values <span class="math inline">\(\hat{y}_k=\alpha x_k\)</span> for <span class="math inline">\(k=1,2,3\)</span>. Express each <span class="math inline">\(\hat{y}_k\)</span> as a combination of the three values <span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>, and <span class="math inline">\(y_3\)</span>, which remain arbitrary. (This is a special case of a general fact about linear regression: each prediction is a linear combination of the training values.)</p>
</div>
<div id="exr-regression-means" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.3 </strong></span>Using the formulas derived in <a href="#sec-regression-linear"><span>Section&nbsp;5.1</span></a>, show that the point <span class="math inline">\((\bar{x},\bar{y})\)</span> always lies on the linear regression line. (Hint: You only have to show that <span class="math inline">\(f(\bar{x}) = \bar{y}\)</span>. This can be done without first solving for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>, which is a bit tedious to write out.)</p>
</div>
<div id="exr-regression-two-features" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.4 </strong></span>Suppose that values <span class="math inline">\(y_i\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span> are to be fit to features <span class="math inline">\((u_i,v_i)\)</span> using a multilinear function <span class="math inline">\(f(u,v)=\alpha u + \beta v\)</span>. Define the sum of squared residuals <span class="math display">\[
r = \sum_{i=1}^n (f(u_i,v_i)-y_i)^2.
\]</span> Show that by holding <span class="math inline">\(\alpha\)</span> is constant and taking a derivative with respect to <span class="math inline">\(\beta\)</span>, and then holding <span class="math inline">\(\beta\)</span> constant and taking a derivative with respect to <span class="math inline">\(\alpha\)</span>, at the minimum residual we must have <span class="math display">\[
\begin{split}
\left(\sum u_i^2 \right) \alpha + \left(\sum u_i v_i \right) \beta &amp;= \sum u_i y_i, \\
\left(\sum u_i v_i \right) \alpha + \left(\sum v_i^2 \right) \beta &amp;= \sum v_i y_i.
\end{split}
\]</span></p>
</div>
<p>::::{#exr-regression-regular-no-intercept} Repeat <a href="#exr-regression-no-intercept">Exercise&nbsp;<span>5.1</span></a>, but using the regularized residual <span class="math display">\[
\tilde{r} = C \alpha^2 + \sum_{i=1}^n (f(x_i)-y_i)^2.
\]</span></p>
<div id="exr-regression-regular-two-features" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.5 </strong></span>Repeat <a href="#exr-regression-two-features">Exercise&nbsp;<span>5.4</span></a>, but using the regularized residual <span class="math display">\[
\tilde{r} = C (\alpha^2 + \beta^2) + \sum_{i=1}^n (f(u_i,v_i)-y_i)^2.
\]</span></p>
</div>
<div id="exr-regression-splits" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.6 </strong></span>Given the data set <span class="math inline">\((x_i,y_i)=\{(0,-1),(1,1),(2,3),(3,0),(4,3)\}\)</span>, find the MAD-based <span class="math inline">\(Q\)</span> score for the following hypothetical decision tree splits.</p>
<p><strong>(a)</strong> <span class="math inline">\(x \le 0.5\quad\)</span></p>
<p><strong>(b)</strong> <span class="math inline">\(x \le 1.5\quad\)</span></p>
<p><strong>(c)</strong> <span class="math inline">\(x \le 2.5\quad\)</span></p>
<p><strong>(d)</strong> <span class="math inline">\(x \le 3.5\)</span></p>
</div>
<div id="exr-regression-lattice-values" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.7 </strong></span>Here are values on an integer lattice.</p>
<p><img src="_media/griddata.png" class="img-fluid"></p>
<p>Let <span class="math inline">\(\hat{f}(x_1,x_2)\)</span> be a kNN regressor with <span class="math inline">\(k=4\)</span>, Euclidean metric, and mean averaging. Carefully sketch a one-dimensional plot of <span class="math inline">\(\hat{f}\)</span> along the given line.</p>
<p><strong>(a)</strong> <span class="math inline">\(\hat{f}(1.2,t)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
<p><strong>(b)</strong> <span class="math inline">\(\hat{f}(t,-0.75)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
<p><strong>(c)</strong> <span class="math inline">\(\hat{f}(t,1.6)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
<p><strong>(d)</strong> <span class="math inline">\(\hat{f}(-0.25,t)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
</div>
<div id="exr-regression-lattice-colors" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.8 </strong></span>Here are blue/orange labels on an integer lattice.</p>
<p><img src="_media/gridlabels.png" class="img-fluid"></p>
<p>Let <span class="math inline">\(\hat{f}(x_1,x_2)\)</span> be a kNN probabilistic classifier with <span class="math inline">\(k=4\)</span>, Euclidean metric, and mean averaging. Carefully sketch a one-dimensional plot of the probability of the blue class along the given line.</p>
<p><strong>(a)</strong> <span class="math inline">\(\hat{f}(1.2,t)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
<p><strong>(b)</strong> <span class="math inline">\(\hat{f}(t,-0.75)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
<p><strong>(c)</strong> <span class="math inline">\(\hat{f}(t,1.6)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
<p><strong>(d)</strong> <span class="math inline">\(\hat{f}(-0.25,t)\)</span> for <span class="math inline">\(2\le t \le 2\)</span></p>
</div>
<div id="exr-regression-cross-entropy" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.9 </strong></span>Here are some label values and probabilistic predicted categories for them.</p>
<p><span class="math display">\[
\begin{split}
    y: &amp;\quad [0,0,1,1] \\
    \hat{p}: &amp;\quad [\tfrac{1}{4},0,\tfrac{1}{2},1]
\end{split}
\]</span></p>
<p>Using base-2 logarithms, calculate the cross-entropy loss for these predictions.</p>
</div>
<div id="exr-regression-cross-entropy-optim" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.10 </strong></span>Let <span class="math inline">\(\bfx=[-1,0,1]\)</span> and <span class="math inline">\(\bfy=[0,1,0]\)</span>. This is to be fit to a probabilistic predictor <span class="math inline">\(\hat{p}(x) = \sigma(a x)\)</span> for parameter <span class="math inline">\(a\)</span>.</p>
<p><strong>(a)</strong> Show that the cross-entropy loss function <span class="math inline">\(L(a)\)</span>, using natural logarithms, satisfies</p>
<p><span class="math display">\[
L'(a) = \frac{e^a-1}{e^a+1}.
\]</span></p>
<p><strong>(b)</strong> Explain why part (a) implies that <span class="math inline">\(a=0\)</span> is the global minimizer of the loss <span class="math inline">\(L\)</span>.</p>
<p><strong>(c)</strong> Using the result of part (b), simplify the optimum predictor function <span class="math inline">\(\hat{p}\)</span>.</p>
</div>
<div id="exr-regression-logistic-lasso" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.11 </strong></span>Let <span class="math inline">\(\bfx=[-1,1]\)</span> and <span class="math inline">\(\bfy=[0,1]\)</span>. This is to be fit to a probabilistic predictor <span class="math inline">\(\hat{p}(x) = \sigma(a x)\)</span> for parameter <span class="math inline">\(a\)</span>. Without regularization, the best fit takes <span class="math inline">\(a\to\infty\)</span>, which makes the predictor become infinitely steep at <span class="math inline">\(x=0\)</span>. To combat this behavior, let <span class="math inline">\(L\)</span> be the cross-entropy loss function with LASSO penalty, i.e.,</p>
<p><span class="math display">\[
L(a) = \ln[1-\hat{p}(-1)] - \ln[\hat{p}(1)] + C |a|,
\]</span></p>
<p>for a positive regularization constant <span class="math inline">\(C\)</span>.</p>
<p><strong>(a)</strong> Show that <span class="math inline">\(L'\)</span> is never zero for <span class="math inline">\(a&lt;0\)</span>.</p>
<p><strong>(b)</strong> Show that if <span class="math inline">\(0&lt;C&lt;1\)</span>, then <span class="math inline">\(L'\)</span> has a zero at</p>
<p><span class="math display">\[
a=\ln\left(\frac{2}{C}-1\right).
\]</span></p>
<p>Assume that this value minimizes <span class="math inline">\(L\)</span>.</p>
<p><strong>(c)</strong> Show that the minimizer above is a decreasing function of <span class="math inline">\(C\)</span>. (Therefore, increasing <span class="math inline">\(C\)</span> makes the predictor less steep as a function of <span class="math inline">\(x\)</span>.)</p>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./selection.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./clustering.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Data Science 1
  </li>  
</ul>
    </div>   
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Toby Driscoll
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>



</body></html>