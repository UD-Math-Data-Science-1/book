<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.55">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>5&nbsp; Regression – Data Science 1</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./clustering.html" rel="next">
<link href="./selection.html" rel="prev">
<link href="./_media/logo_small.png" rel="icon" type="image/png">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/videojs/video.min.js"></script>
<link href="site_libs/quarto-contrib/videojs/video-js.css" rel="stylesheet">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script> 
MathJax = {
  chtml: {
    scale: 0.92,
  }
}
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./regression.html"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto"><div class="quarto-sidebar-header"><div class="sidebar-header-item">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="_media/logo_small.png" class="quarto-figure quarto-figure-center figure-img" height="120"></p>
</figure>
</div>
</div></div>
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">Data Science 1</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./examples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">List of examples</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./resources.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Resources</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./starting.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Getting started</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Representation of data</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./stats.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Descriptive statistics</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./classification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Classification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./selection.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./regression.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./clustering.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./networks.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Networks</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Review questions by section</span></a>
  </div>
</li>
    </ul>
    </div>
<div class="quarto-sidebar-footer"><div class="sidebar-footer-item">
<p>Copyright (c) 2024 by <a href="https://tobydriscoll.net">Toby Driscoll</a></p>
</div></div></nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-regression-linear" id="toc-sec-regression-linear" class="nav-link active" data-scroll-target="#sec-regression-linear"><span class="header-section-number">5.1</span> Linear regression</a>
  <ul class="collapse">
  <li><a href="#linear-algebra" id="toc-linear-algebra" class="nav-link" data-scroll-target="#linear-algebra"><span class="header-section-number">5.1.1</span> Linear algebra</a></li>
  <li><a href="#performance-metrics" id="toc-performance-metrics" class="nav-link" data-scroll-target="#performance-metrics"><span class="header-section-number">5.1.2</span> Performance metrics</a></li>
  </ul></li>
  <li><a href="#multilinear-regression" id="toc-multilinear-regression" class="nav-link" data-scroll-target="#multilinear-regression"><span class="header-section-number">5.2</span> Multilinear regression</a>
  <ul class="collapse">
  <li><a href="#polynomial-regression" id="toc-polynomial-regression" class="nav-link" data-scroll-target="#polynomial-regression"><span class="header-section-number">5.2.1</span> Polynomial regression</a></li>
  </ul></li>
  <li><a href="#regularization" id="toc-regularization" class="nav-link" data-scroll-target="#regularization"><span class="header-section-number">5.3</span> Regularization</a></li>
  <li><a href="#nonlinear-regression" id="toc-nonlinear-regression" class="nav-link" data-scroll-target="#nonlinear-regression"><span class="header-section-number">5.4</span> Nonlinear regression</a>
  <ul class="collapse">
  <li><a href="#nearest-neighbors" id="toc-nearest-neighbors" class="nav-link" data-scroll-target="#nearest-neighbors"><span class="header-section-number">5.4.1</span> Nearest neighbors</a></li>
  <li><a href="#decision-tree" id="toc-decision-tree" class="nav-link" data-scroll-target="#decision-tree"><span class="header-section-number">5.4.2</span> Decision tree</a></li>
  </ul></li>
  <li><a href="#logistic-regression" id="toc-logistic-regression" class="nav-link" data-scroll-target="#logistic-regression"><span class="header-section-number">5.5</span> Logistic regression</a>
  <ul class="collapse">
  <li><a href="#loss-function" id="toc-loss-function" class="nav-link" data-scroll-target="#loss-function"><span class="header-section-number">5.5.1</span> Loss function</a></li>
  <li><a href="#sec-regression-logistic-regular" id="toc-sec-regression-logistic-regular" class="nav-link" data-scroll-target="#sec-regression-logistic-regular"><span class="header-section-number">5.5.2</span> Regularization</a></li>
  <li><a href="#multiclass-classification" id="toc-multiclass-classification" class="nav-link" data-scroll-target="#multiclass-classification"><span class="header-section-number">5.5.3</span> Multiclass classification</a></li>
  </ul></li>
  <li><a href="#exercises" id="toc-exercises" class="nav-link" data-scroll-target="#exercises">Exercises</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Regression</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="hidden">
<p><span class="math display">\[
    \newcommand{\float}{\mathbb{F}}
    \newcommand{\real}{\mathbb{R}}
    \newcommand{\complex}{\mathbb{C}}
    \newcommand{\nat}{\mathbb{N}}
    \newcommand{\integer}{\mathbb{Z}}
    \newcommand{\bfa}{\mathbf{a}}
    \newcommand{\bfe}{\mathbf{e}}
    \newcommand{\bfh}{\mathbf{h}}
    \newcommand{\bfp}{\mathbf{p}}
    \newcommand{\bfq}{\mathbf{q}}
    \newcommand{\bfu}{\mathbf{u}}
    \newcommand{\bfv}{\mathbf{v}}
    \newcommand{\bfw}{\mathbf{w}}
    \newcommand{\bfx}{\mathbf{x}}
    \newcommand{\bfy}{\mathbf{y}}
    \newcommand{\bfz}{\mathbf{z}}
    \newcommand{\bfA}{\mathbf{A}}
    \newcommand{\bfW}{\mathbf{W}}
    \newcommand{\bfX}{\mathbf{X}}
    \newcommand{\bfzero}{\boldsymbol{0}}
    \newcommand{\bfmu}{\boldsymbol{\mu}}
    \newcommand{\TP}{\text{TP}}
    \newcommand{\TN}{\text{TN}}
    \newcommand{\FP}{\text{FP}}
    \newcommand{\FN}{\text{FN}}
    \newcommand{\rmn}[2]{\mathbb{R}^{#1 \times #2}}
    \newcommand{\dd}[2]{\frac{d #1}{d #2}}
    \newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
    \newcommand{\norm}[1]{\left\lVert \mathstrut #1 \right\rVert}
    \newcommand{\abs}[1]{\left\lvert \mathstrut #1 \right\rvert}
    \newcommand{\twonorm}[1]{\norm{#1}_2}
    \newcommand{\onenorm}[1]{\norm{#1}_1}
    \newcommand{\infnorm}[1]{\norm{#1}_\infty}
    \newcommand{\innerprod}[2]{\langle #1,#2 \rangle}
    \newcommand{\pr}[1]{^{(#1)}}
    \newcommand{\diag}{\operatorname{diag}}
    \newcommand{\sign}{\operatorname{sign}}
    \newcommand{\dist}{\operatorname{dist}}
    \newcommand{\simil}{\operatorname{sim}}
    \newcommand{\ee}{\times 10^}
    \newcommand{\floor}[1]{\lfloor#1\rfloor}
    \newcommand{\argmin}{\operatorname{argmin}}
    \newcommand{\E}[1]{\operatorname{\mathbb{E}}\left[\mathstrut #1\right]}
    \newcommand{\Cov}{\operatorname{Cov}}
    \newcommand{\logit}{\operatorname{logit}}
\]</span></p>
</div>
<div id="b9a9855d" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.random <span class="im">import</span> default_rng</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeClassifier</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsClassifier</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> datasets</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.utils <span class="im">import</span> shuffle</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, f1_score, balanced_accuracy_score</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> make_pipeline</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> roc_curve</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> BaggingClassifier</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> KFold, StratifiedKFold</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> cross_validate, validation_curve</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GridSearchCV</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="def-regression-regression" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.1</strong></span> <strong>Regression</strong> is the task of approximating the value of a dependent quantitative variable as a function of independent variables, sometimes called <em>predictors</em>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_ewiseksk&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_0uwb0w5v" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.0" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>Regression and classification are distinct but not altogether different. Abstractly, both are concerned with reproducing a function <span class="math inline">\(f\)</span> whose domain is feature space. In classification, the range of <span class="math inline">\(f\)</span> is a finite set of class labels, while in regression, the range is the real number line (or an interval in it). We can always take the output of a regression and round or bin it to get a finite set of classes; therefore, any regression method can also be used for classification. Likewise, most classification methods have a generalization to regression.</p>
<p>In addition to prediction tasks, some regression methods can be used to identify the relative significance of each feature and whether is has a direct or inverse relationship to the function value. Unimportant features can then be removed to help minimize overfitting.</p>
<section id="sec-regression-linear" class="level2 page-columns page-full" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="sec-regression-linear"><span class="header-section-number">5.1</span> Linear regression</h2>
<p>You have likely previously encountered the most basic form of regression: fitting a straight line to data points <span class="math inline">\((x_i,y_i)\)</span> in the <span class="math inline">\(xy\)</span>-plane. In <strong>linear regression</strong>, we have a one-dimensional feature <span class="math inline">\(x\)</span> and assume a relation</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_ewshhvaa&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_i21wni0i" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p><span id="eq-regression-linear"><span class="math display">\[
y \approx \hat{f}(x) = ax + b.
\tag{5.1}\]</span></span></p>
<p>We say that the model is <em>parameterized by</em> <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> in <a href="#eq-regression-linear" class="quarto-xref">Equation&nbsp;<span>5.1</span></a>, and we must specify how to use the data to select their values. We describe this process as <strong>fitting</strong> the model to the data, and it almost always means solving an optimization problem.</p>
<div id="def-regression-loss" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.2</strong></span> A <strong>loss function</strong> is a scalar function of a model’s parameters that measures how well a model fits the data. The loss function is minimized to choose the best values for the parameters.</p>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>We like to minimize losses in life, so that is a mnemonic to remember the role of the loss function.</p>
</div>
</div>
<p>The standard approach in linear regression is to minimize the sum of squared differences between the predictions and the true values:</p>
<p><span id="eq-regression-linear-loss"><span class="math display">\[
L(a,b) = \sum_{i=1}^n (\hat{f}(x_i)-y_i)^2 = \sum_{i=1}^n (a x_i + b - y_i)^2.
\tag{5.2}\]</span></span></p>
<p>This loss <span class="math inline">\(L\)</span> can be minimized using a little (multidimensional) calculus. Momentarily suppose that <span class="math inline">\(b\)</span> is held fixed and take a derivative with respect to <span class="math inline">\(a\)</span>:</p>
<p><span class="math display">\[
\pp{L}{a} = \sum_{i=1}^n 2x_i(a x_i + b - y_i) = 2 a \left(\sum_{i=1}^n x_i^2\right) + 2b\left(\sum_{i=1}^n x_i\right) - 2\sum_{i=1}^n x_i y_i.
\]</span></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The symbol <span class="math inline">\(\pp{}{}\)</span> is called a <strong>partial derivative</strong> and is defined just as described here: differentiate in one variable while all others are temporarily held constant.</p>
</div>
</div>
<p>Similarly, if we hold <span class="math inline">\(a\)</span> fixed and differentiate with respect to <span class="math inline">\(b\)</span>, then</p>
<p><span class="math display">\[
\pp{L}{b} = \sum_{i=1}^n 2(a x_i + b - y_i) = 2 a \left(\sum_{i=1}^n x_i\right) + 2bn - 2 \sum_{i=1}^n y_i.
\]</span></p>
<p>Setting both derivatives to zero creates a system of two linear equations to be solved for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>: <span id="eq-regression-linear1d"><span class="math display">\[
\begin{split}
    a \left(\sum_{i=1}^n x_i^2\right) + b\left(\sum_{i=1}^n x_i\right) &amp;= \sum_{i=1}^n x_i y_i, \\
    a \left(\sum_{i=1}^n x_i\right) + b n &amp;= \sum_{i=1}^n y_i.
\end{split}
\tag{5.3}\]</span></span></p>
<div id="exm-regression-linear" class="theorem example" data-chapter="5" type="✍️" data-description="Linear regression">
<p><span class="theorem-title"><strong>Example 5.1</strong></span> Suppose we want to find the linear regressor of the points <span class="math inline">\((-1,0)\)</span>, <span class="math inline">\((0,2)\)</span>, <span class="math inline">\((1,3)\)</span>. We need to calculate a few sums:</p>
<p><span class="math display">\[
\begin{split}
\sum_{i=1}^n x_i^2 = 1+0+1=2, \qquad &amp; \sum_{i=1}^n x_i = -1+0+1=0, \\
\sum_{i=1}^n x_iy_i = 0+0+3=3, \qquad &amp; \sum_{i=1}^n y_i = 0+2+3=5.
\end{split}
\]</span></p>
<p>Note that <span class="math inline">\(n=3\)</span>. Therefore we must solve</p>
<p><span class="math display">\[
\begin{split}
2a + 0b &amp;= 3, \\
0a + 3b &amp;= 5.
\end{split}
\]</span></p>
<p>The regression function is <span class="math inline">\(\hat{f}(x)=\tfrac{3}{2} x + \tfrac{5}{3}\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
(video example is different from the text) <br>
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_yhpg0xey&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_54wk2rgh" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
<p><br> <a href="_media/memes/slope-intercept.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1"><img src="_media/memes/slope-intercept.jpg" class="img-fluid"></a></p>
</div></div><section id="linear-algebra" class="level3 page-columns page-full" data-number="5.1.1">
<h3 data-number="5.1.1" class="anchored" data-anchor-id="linear-algebra"><span class="header-section-number">5.1.1</span> Linear algebra</h3>
<p>Before moving on, we want to examine a vector-oriented description of the process. First, a handy definition.</p>
<div id="def-regression-ones" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.3</strong></span> The <strong>ones vector</strong> <span class="math inline">\(\bfe\)</span> is a vector of length <span class="math inline">\(n\)</span> with all elements equal to 1:</p>
<p><span class="math display">\[
\bfe = [1,1,\ldots,1] \in \real^n,
\]</span></p>
<p>The dimension <span class="math inline">\(n\)</span> is usually clear from context.</p>
</div>
<p>Now we can rewrite the loss function in <a href="#eq-regression-linear-loss" class="quarto-xref">Equation&nbsp;<span>5.2</span></a> as a vector operation. If we define <span class="math inline">\(\bfx\)</span> as a vector of the <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\bfy\)</span> as a vector of the <span class="math inline">\(y_i\)</span>, then</p>
<p><span class="math display">\[
L(a,b) =  \twonorm{a\, \bfx + b \,\bfe - \bfy}^2,
\]</span></p>
<p>Minimizing <span class="math inline">\(L\)</span> over all values of <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> is called the <strong>least squares</strong> problem. (More specifically, this setup is called <em>simple least squares</em> or <em>ordinary least squares</em>.)</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_z3epf6f9&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_nfbft0cp" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.1.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>We can write out the equations <a href="#eq-regression-linear1d" class="quarto-xref">Equation&nbsp;<span>5.3</span></a> for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> using another important idea from linear algebra.</p>
<div id="def-regression-inner-product" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.4</strong></span> Given any <span class="math inline">\(d\)</span>-dimensional real-values vectors <span class="math inline">\(\bfu\)</span> and <span class="math inline">\(\bfv\)</span>, their <strong>inner product</strong> is <span id="eq-regression-inner-product"><span class="math display">\[
\bfu^T \bfv = \sum_{i=1}^d u_i v_i = u_1v_1 + u_2v_2 + \cdots + u_d v_d.
\tag{5.4}\]</span></span></p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><em>Inner product</em> is a term from linear algebra. In physics and vector calculus with <span class="math inline">\(d=2\)</span> or <span class="math inline">\(d=3\)</span>, the same thing is often called a <em>dot product</em>.</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The <span class="math inline">\({}^T\)</span> symbol is a <em>transpose</em> operation in linear algebra. We won’t need it as an independent concept, so we are just using it as notation in the inner product.</p>
</div>
</div>
<p>The vector inner product is defined only between two vectors of the same length (dimension). There is an important link between the inner product and the 2-norm: <span id="eq-regression-hilbert"><span class="math display">\[
\bfu^T \bfu = \sum_{i=1}^d u_i^2 = \twonorm{\bfu}^2.
\tag{5.5}\]</span></span></p>
<div id="exm-regression-dot" class="theorem example" data-chapter="5" type="✍️" data-description="Inner product">
<p><span class="theorem-title"><strong>Example 5.2</strong></span> Let <span class="math inline">\(\bfu = [1,-1,1,-2]\)</span> and <span class="math inline">\(\bfv = [5,3,-1,2]\)</span>. Then <span class="math display">\[
\bfu^T \bfv = (1)(5) + (-1)(3) + (1)(-1) + (-2)(2) = -3.
\]</span> We also have <span class="math display">\[
\twonorm{\bfu}^2 = \bfu^T \bfu = (1)^2 + (-1)^2 + (1)^2 + (-2)^2 = 7.
\]</span></p>
</div>
<p>The equations in <a href="#eq-regression-linear1d" class="quarto-xref">Equation&nbsp;<span>5.3</span></a> may now be written as <span id="eq-regression-linear1d-dot"><span class="math display">\[
\begin{split}
    a \left(\bfx^T \bfx\right) + b \left(\bfx^T\bfe\right) &amp;= \bfx^T\bfy, \\
    a \left(\bfe^T \bfx\right) + b \left(\bfe^T\bfe\right) &amp;= \bfe^T\bfy.
\end{split}
\tag{5.6}\]</span></span></p>
<p>We can write this as a single equation between two vectors: <span class="math display">\[
a
\begin{bmatrix}
\bfx^T \bfx \\ \bfe^T \bfx
\end{bmatrix}
+ b
\begin{bmatrix}
\bfx^T\bfe \\ \bfe^T\bfe
\end{bmatrix}
=
\begin{bmatrix}
\bfx^T\bfy \\  \bfe^T\bfy
\end{bmatrix}.
\]</span></p>
<p>In fact, the operation on the left-hand side is how we define the product of a matrix and a vector, and we can write<br>
<span class="math display">\[
\begin{bmatrix}
\bfx^T \bfx &amp; \bfx^T\bfe \\
\bfe^T \bfx &amp; \bfe^T\bfe
\end{bmatrix}
\cdot
\begin{bmatrix}
a \\ b
\end{bmatrix}
=
\begin{bmatrix}
\bfx^T\bfy \\  \bfe^T\bfy
\end{bmatrix}.
\]</span></p>
<p>This takes the form of the equation <span class="math inline">\(\bfA \bfw = \bfv\)</span>, where <span class="math inline">\(\bfA\)</span> is a known <span class="math inline">\(2\times 2\)</span> matrix, <span class="math inline">\(\bfv\)</span> is a known 2-vector, and <span class="math inline">\(\bfw\)</span> is the 2-vector of the unknowns <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. This equation is referred to as a <em>linear system</em> for the unknown vector. Linear systems and their solutions are the central topic of linear algebra. In the background, it is this linear system that is being solved when you perform a linear regression fit.</p>
</section>
<section id="performance-metrics" class="level3 page-columns page-full" data-number="5.1.2">
<h3 data-number="5.1.2" class="anchored" data-anchor-id="performance-metrics"><span class="header-section-number">5.1.2</span> Performance metrics</h3>
<p>We need ways to measure regression performance. Unlike with binary classification, in regression it’s not just a matter of right and wrong answers—the amount of wrongness matters, too.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>A quirk of linear regression is that it’s an older and broader idea than most of machine learning, and it’s often presented as though the training and testing sets are identical. We follow that convention for the definitions in this section. The same quantities can also be calculated for a set of labels and predictions obtained from a separate testing set, although a few of the properties stated here don’t apply in that case.</p>
</div>
</div>
<div id="def-regression-residual-error" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.5</strong></span> The <strong>residuals</strong> of the regression are <span id="eq-regression-residual"><span class="math display">\[
y_i - \hat{y}_i, \qquad i=1,\ldots,n,
\tag{5.7}\]</span></span></p>
<p>where the <span class="math inline">\(y_i\)</span> are the true labels and the <span class="math inline">\(\hat{y}_i\)</span> are the values predicted by the regressor. We can express them compactly as the <strong>residual vector</strong> <span class="math inline">\(\bfy-\hat{\bfy}\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_myxlnosi&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_y65cyatg" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.1.2" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>The terms <em>error</em> and <em>residual</em> are frequently used interchangeably and even inconsistently. I try to follow the most common practices here, even though the names can be confusing if you think about them too hard.</p>
</div>
</div>
<div id="def-regression-mse" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.6</strong></span> The <strong>mean squared error</strong> (MSE) is <span class="math display">\[
\text{MSE} = \frac{1}{n} \sum_{i=1}^n \, \left( y_i - \hat{y}_i \right)^2 = \frac{1}{n} \twonorm{\bfy - \hat{\bfy}}^2.
\]</span> The <strong>mean absolute error</strong> (MAE) is <span class="math display">\[
\text{MAE} = \frac{1}{n} \sum_{i=1}^n \abs{y_i - \hat{y}_i }= \frac{1}{n} \onenorm{\bfy - \hat{\bfy}}.
\]</span></p>
</div>
<p>The MSE is simply <span class="math inline">\(1/n\)</span> times the loss function <span class="math inline">\(L\)</span>. MAE is less sensitive than MSE to large outliers. Both quantities are dimensional and therefore depend on how the variables are scaled, but only the units of MAE are the same as of the data.</p>
<div id="exm-regression-mse-mae" class="theorem example" data-chapter="5" type="✍️" data-description="Mean squared and mean absolute error">
<p><span class="theorem-title"><strong>Example 5.3</strong></span> In <a href="#exm-regression-linear" class="quarto-xref">Example&nbsp;<span>5.13</span></a> the points <span class="math inline">\((-1,0)\)</span>, <span class="math inline">\((0,2)\)</span>, <span class="math inline">\((1,3)\)</span> were found to have the least-squares regressor <span class="math inline">\(\hat{f}(x)=\tfrac{3}{2} x + \tfrac{5}{3}\)</span>. Hence <span class="math display">\[
\begin{split}
y_1 &amp;= 0,\; \hat{y}_1 = \hat{f}(-1)=\tfrac{1}{6} \\
y_2 &amp;=2,\; \hat{y}_2=\hat{f}(0) = \tfrac{5}{3}, \\
y_3 &amp;=3; \; \hat{y}_3=\hat{f}(1)=\tfrac{19}{6}.
\end{split}
\]</span></p>
<p>This implies <span class="math display">\[
\begin{split}
\text{MSE} &amp;= \frac{1}{3} \left[ \left(0-\tfrac{1}{6}\right)^2 + \left(2-\tfrac{5}{3}\right)^2 + \left(3-\tfrac{19}{6}\right)^2 \right]= \frac{1}{18}, \\
\text{MAE} &amp;= \frac{1}{3} \left( \abs{0-\tfrac{1}{6}} + \abs{2-\tfrac{5}{3}} + \abs{3-\tfrac{19}{6}} \right) = \frac{2}{9}. \\
\end{split}
\]</span></p>
</div>
<div id="def-regression-cod" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.7</strong></span> The <strong>coefficient of determination</strong> (CoD) is defined as <span class="math display">\[
\text{CoD} = 1 - \frac{\displaystyle\sum_{i=1}^n \,\left(y_i - \hat{y}_i \right)^2}{\displaystyle\sum_{i=1}^n \, \left(y_i - \bar{y}\right)^2},
\]</span></p>
<p>where <span class="math inline">\(\bar{y}\)</span> is the sample mean of <span class="math inline">\(y_1,\ldots,y_n\)</span>.</p>
</div>
<p>Here are important things to know about the coefficient of determination.</p>
<div id="thm-regression-cod" class="theorem">
<p><span class="theorem-title"><strong>Theorem 5.1</strong></span> Given sample values <span class="math inline">\(y_1,\ldots,y_n\)</span> with mean <span class="math inline">\(\bar{y}\)</span> and the predictions <span class="math inline">\(\hat{y}_1,\ldots, \hat{y}_n\)</span>,</p>
<ol type="1">
<li>The CoD is dimensionless and therefore independent of scaling.</li>
<li>If <span class="math inline">\(\hat{y}_i=y_i\)</span> for all <span class="math inline">\(i\)</span> (i.e., perfect predictions), then <span class="math inline">\(\text{CoD}=1\)</span>.</li>
<li>If <span class="math inline">\(\hat{y}_i=\bar{y}\)</span> for all <span class="math inline">\(i\)</span> (i.e., always predict the sample mean), then <span class="math inline">\(\text{CoD}=0\)</span>.</li>
<li>If the <span class="math inline">\(\hat{y}_i\)</span> are found from a linear regression, then <span class="math inline">\(\text{CoD}\)</span> is the square of the Pearson correlation coefficient between <span class="math inline">\(\bfy\)</span> and <span class="math inline">\(\hat{\bfy}\)</span>.</li>
</ol>
</div>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>Typically the coefficient of determination is denoted <span class="math inline">\(R^2\)</span> because of the special case in part 4 of <a href="#thm-regression-cod" class="quarto-xref">Theorem&nbsp;<span>5.1</span></a>. However, if the regression method is different or the testing set is not equal to the training set, the CoD can actually be <em>negative</em>, hence writing it as the square of another number just makes no sense in general.</p>
</div>
</div>
<div id="exm-regression-cod" class="theorem example" data-chapter="5" type="✍️" data-description="Coefficient of determination">
<p><span class="theorem-title"><strong>Example 5.4</strong></span> Continuing with <a href="#exm-regression-linear" class="quarto-xref">Example&nbsp;<span>5.13</span></a> and <a href="#exm-regression-mse-mae" class="quarto-xref">Example&nbsp;<span>5.3</span></a>, we find that <span class="math inline">\(\bar{y}=(0+2+3)/3=\tfrac{5}{3}\)</span>, and</p>
<p><span class="math display">\[
\begin{split}
    \sum_{i=1}^n \left(y_i - \bar{y}\right)^2 &amp;= \left(0-\tfrac{5}{3}\right)^2 + \left(2-\tfrac{5}{3}\right)^2 + \left(3-\tfrac{5}{3}\right)^2 = \frac{14}{3}.
\end{split}
\]</span></p>
<p>This gives the coefficient of determination <span class="math display">\[
\begin{split}
\text{CoD} &amp;= 1 - \frac{\left(0-\tfrac{1}{6}\right)^2 + \left(2-\tfrac{5}{3}\right)^2 + \left(3-\tfrac{19}{6}\right)^2}{14/3} \\
&amp;= 1 - \frac{1/6}{14/3} = \frac{27}{28}.
\end{split}
\]</span></p>
<p>This is quite close to 1, indicating a good fit. Compare that to the arbitrary predictor <span class="math inline">\(\hat{f}(x)=x\)</span>, which has <span class="math inline">\(\hat{y}_1=-1\)</span>, <span class="math inline">\(\hat{y}_2=0\)</span>, <span class="math inline">\(\hat{y}_3=1\)</span>: <span class="math display">\[
\begin{split}
    text{CoD} &amp; = 1 - \frac{\left(y_i - \hat{y}_i \right)^2 = \left(0+1\right)^2 + \left(2-0\right)^2 + \left(3-1\right)^2}{14/3}  \\
    &amp;= 1 - \frac{9}{14/3} = -\frac{13}{14}.
\end{split}
\]</span></p>
<p>Since this result is negative, we would be better off always predicting <span class="math inline">\(5/3\)</span> rather than using <span class="math inline">\(\hat{f}(x)=x\)</span>.</p>
</div>
<div id="exm-regression-ice-linear" class="theorem example" data-chapter="5" type="💻" data-description="Linear regression for sea ice dataset">
<p><span class="theorem-title"><strong>Example 5.5</strong></span> We import data about the extent of sea ice in the Arctic circle, collected monthly since 1979:</p>
<div id="92a67879" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>ice <span class="op">=</span> pd.read_csv(<span class="st">"_datasets/sea-ice.csv"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Simplify column names:</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>ice.columns <span class="op">=</span> [s.strip() <span class="cf">for</span> s <span class="kw">in</span> ice.columns]   </span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>ice.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="2">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">year</th>
<th data-quarto-table-cell-role="th">mo</th>
<th data-quarto-table-cell-role="th">data-type</th>
<th data-quarto-table-cell-role="th">region</th>
<th data-quarto-table-cell-role="th">extent</th>
<th data-quarto-table-cell-role="th">area</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>1979</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>15.41</td>
<td>12.41</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>1980</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>14.86</td>
<td>11.94</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>1981</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>14.91</td>
<td>11.91</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>1982</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>15.18</td>
<td>12.19</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>1983</td>
<td>1</td>
<td>Goddard</td>
<td>N</td>
<td>14.94</td>
<td>12.01</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>A quick plot reveals something odd-looking:</p>
<div id="1fffbe76" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"mo"</span>, y<span class="op">=</span><span class="st">"extent"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-4-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Everything in the plot above is dominated by two large negative values. These probably represent missing data, so we make a new copy without those rows:</p>
<div id="6ea5443a" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>ice <span class="op">=</span> ice[ice[<span class="st">"extent"</span>] <span class="op">&gt;</span> <span class="dv">0</span>]</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"mo"</span>, y<span class="op">=</span><span class="st">"extent"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-5-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Each dot in the plot above represents one measurement. As you would expect, the extent of ice rises in the winter months and falls in summer:</p>
<div id="52a12ecf" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>bymonth <span class="op">=</span> ice.groupby(<span class="st">"mo"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>bymonth[<span class="st">"extent"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="5">
<pre><code>mo
1     14.214762
2     15.100233
3     15.256977
4     14.525581
5     13.117442
6     11.539767
7      9.097907
8      6.793256
9      5.993488
10     7.887907
11    10.458182
12    12.664419
Name: extent, dtype: float64</code></pre>
</div>
</div>
<p>While the effect of the seasonal variation somewhat cancels out over time when fitting a line, it’s preferable to remove this obvious trend before the fit takes place. To do that, we add a column that measures within each month group the relative change from the mean, <span class="math display">\[
\frac{x-\bar{x}}{\bar{x}}.
\]</span> This is done with a <code>transform</code> method applied to the grouped frame:</p>
<div id="9a90667f" class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>recenter <span class="op">=</span> <span class="kw">lambda</span> x: x<span class="op">/</span>x.mean() <span class="op">-</span> <span class="dv">1</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>ice[<span class="st">"detrended"</span>] <span class="op">=</span> bymonth[<span class="st">"extent"</span>].transform(recenter)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"mo"</span>, y<span class="op">=</span><span class="st">"detrended"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-7-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>An <code>lmplot</code> in seaborn shows the least squares line:</p>
<div id="abb16720" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>ice, x<span class="op">=</span><span class="st">"year"</span>, y<span class="op">=</span><span class="st">"detrended"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-8-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>However, we should be mindful of <a href="@sec-stats-simpson">Simpson’s paradox</a>. The previous plot showed considerably more variance within the warm months. How do these fits look for the data <em>within</em> each month? This is where a facet plot shines:</p>
<div id="c9b71d90" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>ice,</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"year"</span>, y<span class="op">=</span><span class="st">"detrended"</span>,</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>    col<span class="op">=</span><span class="st">"mo"</span>, col_wrap<span class="op">=</span><span class="dv">3</span>, height<span class="op">=</span><span class="dv">2</span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-9-output-1.png" width="565" height="755" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Thus, while the correlation is negative within each month, the effect size is clearly larger in the summer and early fall.</p>
<p>We can get numerical information about a regression line from a <code>LinearRegression()</code> learner in sklearn. We will focus on the data for August:</p>
<div id="b27b454e" class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>ice <span class="op">=</span> ice[ ice[<span class="st">"mo"</span>]<span class="op">==</span><span class="dv">8</span> ]</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> ice[ [<span class="st">"year"</span>] ]  </span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> ice[<span class="st">"detrended"</span>]</span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>lm.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="9">
<style>#sk-container-id-1 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-1 {
  color: var(--sklearn-color-text);
}

#sk-container-id-1 pre {
  padding: 0;
}

#sk-container-id-1 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-1 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-1 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-1 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-1 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-1 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-1 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-1 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-1 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-1 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-1 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-1 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-1 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-1 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-1 div.sk-label label.sk-toggleable__label,
#sk-container-id-1 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-1 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-1 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-1 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-1 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-1 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-1 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-1 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-1 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-1 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-1 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</a></style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LinearRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox" checked=""><label for="sk-estimator-id-1" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LinearRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LinearRegression.html">?<span>Documentation for LinearRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LinearRegression()</pre></div> </div></div></div></div>
</div>
</div>
<p>We can get the slope and <span class="math inline">\(y\)</span>-intercept of the regression line from the learner’s properties. (Calculated parameters tend to have underscores at the ends of their names in sklearn.)</p>
<div id="29650181" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>slope, intercept <span class="op">=</span> lm.coef_[<span class="dv">0</span>], lm.intercept_</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Slope is </span><span class="sc">{</span>slope<span class="sc">:.3g}</span><span class="ss"> and intercept is </span><span class="sc">{</span>intercept<span class="sc">:.3g}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Slope is -0.011 and intercept is 22.1</code></pre>
</div>
</div>
<p>The slope indicates average decrease over time.</p>
<p>Next, we assess the performance on the training set. Both the MSE and mean absolute error are small relative to dispersion within the values themselves:</p>
<div id="207db781" class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, mean_absolute_error</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>yhat <span class="op">=</span> lm.predict(X)</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>mse <span class="op">=</span> mean_squared_error(y, yhat)</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>mae <span class="op">=</span> mean_absolute_error(y, yhat)</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MSE: </span><span class="sc">{</span>mse<span class="sc">:.2e}</span><span class="ss">, compared to variance </span><span class="sc">{</span>y<span class="sc">.</span>var()<span class="sc">:.2e}</span><span class="ss">"</span>)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"MAE: </span><span class="sc">{</span>mae<span class="sc">:.2e}</span><span class="ss">, compared to standard deviation </span><span class="sc">{</span>y<span class="sc">.</span>std()<span class="sc">:.2e}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 4.01e-03, compared to variance 2.33e-02
MAE: 4.93e-02, compared to standard deviation 1.53e-01</code></pre>
</div>
</div>
<p>The <code>score</code> method of the regressor object computes the coefficient of determination:</p>
<div id="2287d11f" class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>CoD <span class="op">=</span> lm.score(X, y)</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"coefficient of determination: </span><span class="sc">{</span>CoD<span class="sc">:.3f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>coefficient of determination: 0.824</code></pre>
</div>
</div>
<p>A CoD value this close to 1 would usually be considered a sign of a good fit, although we have not tested for generalization to new data.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_kxpjms7o&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_sorg6uqg" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.5" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>Python objects can have <em>attributes</em>, which are values, and <em>methods</em>, which are functions. In scikit-learn the attribute names of models usually end with an underscore to help distinguish them, as in <code>lm.coef_</code> and <code>lm.intercept_</code>. These attributes can be accessed after fitting the model.</p>
</div>
</div>
</section>
</section>
<section id="multilinear-regression" class="level2 page-columns page-full" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="multilinear-regression"><span class="header-section-number">5.2</span> Multilinear regression</h2>
<p>We can extend linear regression to <span class="math inline">\(d\)</span> predictor features <span class="math inline">\(x_1,\ldots,x_d\)</span>:</p>
<p><span id="eq-regression-multilinear"><span class="math display">\[
y \approx \hat{f}(\bfx) = b + w_1 x_1 + w_2 x_2 + \cdots w_d x_d.
\tag{5.8}\]</span></span></p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_jfzjl1ac&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_cfv59s7b" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.2" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>First, observe that we can actually drop the intercept term <span class="math inline">\(b\)</span> from the discussion, because we could always define an additional constant feature <span class="math inline">\(x_0=1\)</span> and get the same effect in one higher dimension. So we will use the following.</p>
<div id="def-regression-multilinear" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.8</strong></span> <strong>Multilinear regression</strong> is the approximation <span class="math display">\[
y \approx \hat{f}(\bfx) = w_1 x_1 + w_2x_2 + \cdots w_d x_d = \bfw^T\bfx,
\]</span></p>
<p>for a constant vector <span class="math inline">\(\bfw\)</span> known as the <strong>weight vector</strong>.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Multilinear regression is also simply called <em>linear regression</em> most of the time. What we previously called linear regression is just a special case. The <code>LinearRegression</code> learner class does both types of fits. It has a keyword option <code>fit_intercept</code> that determines whether or not the <span class="math inline">\(b\)</span> term in <a href="#eq-regression-multilinear" class="quarto-xref">Equation&nbsp;<span>5.12</span></a> is used.</p>
</div>
</div>
<p>As before, we find the unknown weight vector <span class="math inline">\(\bfw\)</span> by minimizing a loss function. To create the least-squares loss function, we use <span class="math inline">\(\bfX_i\)</span> to denote the <span class="math inline">\(i\)</span>th row of an <span class="math inline">\(n\times d\)</span> feature matrix <span class="math inline">\(\bfX\)</span>. Then</p>
<p><span class="math display">\[
L(\bfw) = \sum_{i=1}^n (y_i - \hat{f}(\bfX_i))^2 = \sum_{i=1}^n (y_i - \bfX_i^T\bfw)^2.
\]</span></p>
<p>We encountered a matrix-vector product earlier. It turns out that the following definition is equivalent to that earlier one.</p>
<div id="def-regression-matvec" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.9</strong></span> Given an <span class="math inline">\(n\times d\)</span> matrix <span class="math inline">\(\bfX\)</span> with rows <span class="math inline">\(\bfX_1,\ldots,\bfX_n\)</span> and a <span class="math inline">\(d\)</span>-vector <span class="math inline">\(\bfw\)</span>, the product <span class="math inline">\(\bfX\bfw\)</span> is defined by <span class="math display">\[
\bfX \bfw =
\begin{bmatrix}
    \bfX_1^T\bfw \\ \bfX_2^T\bfw \\ \vdots \\ \bfX_n^T\bfw
\end{bmatrix}.
\]</span></p>
</div>
<div id="exm-regression-matvec" class="theorem example" data-chapter="5" type="✍️" data-description="Matrix times vector">
<p><span class="theorem-title"><strong>Example 5.6</strong></span> Suppose that <span class="math display">\[
\bfX = \begin{bmatrix}
3 &amp; -1 \\ 0 &amp; 2 \\ 1 &amp; 4
\end{bmatrix},
\qquad
\bfw = [5,-2].
\]</span></p>
<p>Then <span class="math inline">\(\bfX_1=[3,-1]\)</span>, <span class="math inline">\(\bfX_2=[0,2]\)</span>, <span class="math inline">\(\bfX_3=[1,4]\)</span>, and <span class="math display">\[
\bfX \bfw = \begin{bmatrix}
(3)(5)+(-1)(-2) \\ (0)(5) + (2)(-2) \\ (1)(5) + (4)(-2)
\end{bmatrix}
=
\begin{bmatrix}
17 \\ -4 \\ -3
\end{bmatrix}.
\]</span></p>
</div>
<p>We now have the compact expression</p>
<p><span id="eq-regression-multilinear-loss"><span class="math display">\[
L(\bfw) = \twonorm{\bfX \bfw- \bfy}^2.
\tag{5.9}\]</span></span></p>
<p>As in the <span class="math inline">\(d=1\)</span> case, minimizing the loss is equivalent to solving a linear system of equations known as the <em>normal equations</em> for the weight vector <span class="math inline">\(\bfw\)</span>. We do not present the details here.</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Be careful interpreting the magnitudes of regression coefficients (i.e., entries of the weight vector). These are sensitive to the units and scales of the features. For example, distances expressed in meters would have a coefficient that is 1000 times larger than the same distances expressed in kilometers. For quantitative comparisons, it helps to standardize the features first, which does not affect the quality of the fit.</p>
</div>
</div>
<div id="exm-regression-cars-multi" class="theorem example" data-chapter="5" type="💻" data-description="Multilinear regression for MPG dataset">
<p><span class="theorem-title"><strong>Example 5.7</strong></span> We return to the data set regarding the fuel efficiency of cars:</p>
<div id="f9315574" class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>cars <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>cars.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="13">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mpg</th>
<th data-quarto-table-cell-role="th">cylinders</th>
<th data-quarto-table-cell-role="th">displacement</th>
<th data-quarto-table-cell-role="th">horsepower</th>
<th data-quarto-table-cell-role="th">weight</th>
<th data-quarto-table-cell-role="th">acceleration</th>
<th data-quarto-table-cell-role="th">model_year</th>
<th data-quarto-table-cell-role="th">origin</th>
<th data-quarto-table-cell-role="th">name</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>18.0</td>
<td>8</td>
<td>307.0</td>
<td>130.0</td>
<td>3504</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>chevrolet chevelle malibu</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>15.0</td>
<td>8</td>
<td>350.0</td>
<td>165.0</td>
<td>3693</td>
<td>11.5</td>
<td>70</td>
<td>usa</td>
<td>buick skylark 320</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>18.0</td>
<td>8</td>
<td>318.0</td>
<td>150.0</td>
<td>3436</td>
<td>11.0</td>
<td>70</td>
<td>usa</td>
<td>plymouth satellite</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>16.0</td>
<td>8</td>
<td>304.0</td>
<td>150.0</td>
<td>3433</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>amc rebel sst</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>17.0</td>
<td>8</td>
<td>302.0</td>
<td>140.0</td>
<td>3449</td>
<td>10.5</td>
<td>70</td>
<td>usa</td>
<td>ford torino</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>In order to ease experimentation, we define a function that fits a given learner to the <em>mpg</em> variable using a given list of features from the data frame:</p>
<div id="826b7adf" class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fitcars(model, features):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> cars[features]</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        X, y,</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        test_size<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">302</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    model.fit(X_train, y_train)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    MSE <span class="op">=</span> mean_squared_error(y_test, model.predict(X_test))</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"MSE: </span><span class="sc">{</span>MSE<span class="sc">:.3f}</span><span class="ss">, compared to variance </span><span class="sc">{</span>y_test<span class="sc">.</span>var()<span class="sc">:.3f}</span><span class="ss">"</span>)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>When you run the same lines of code over and over with only slight changes, it’s advisable to put the repeated code into a function. It makes the overall code shorter, easier to understand, and less prone to silly errors.</p>
</div>
</div>
<p>First, we try using <em>horsepower</em> as the only feature in a linear regression to fit <em>mpg</em>:</p>
<div id="f85259c8" class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"horsepower"</span>]</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression( fit_intercept<span class="op">=</span><span class="va">True</span> )</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>fitcars(lm, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 26.354, compared to variance 56.474</code></pre>
</div>
</div>
<p>As we would expect, there is an inverse relationship between horsepower and vehicle efficiency:</p>
<div id="3aa6abbd" class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>lm.coef_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="16">
<pre><code>array([-0.1596552])</code></pre>
</div>
</div>
<p>Next, we add <em>displacement</em> to the regression:</p>
<div id="2e0a5130" class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"horsepower"</span>, <span class="st">"displacement"</span>]</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>fitcars(lm, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 19.683, compared to variance 56.474</code></pre>
</div>
</div>
<p>The error has decreased from the univariate case because we have a more capable model.</p>
<p>Finally, we try using 4 features as predictors. In order to help us compare the regression coefficients, we chain the model with a <code>StandardScaler</code> so that all columns are z-scores:</p>
<div id="985c248f" class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"horsepower"</span>, <span class="st">"displacement"</span>, <span class="st">"cylinders"</span>, <span class="st">"weight"</span>]</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), lm)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>fitcars(pipe, features)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MSE: 19.266, compared to variance 56.474</code></pre>
</div>
</div>
<p>We did not get much improvement in the fit this time. But by comparing the coefficients of the individual features, some interesting information emerges:</p>
<div id="92b6afb0" class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pd.Series(pipe[<span class="dv">1</span>].coef_, index<span class="op">=</span>features).sort_values()</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>weights</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="19">
<pre><code>weight         -4.847433
horsepower     -1.892329
cylinders      -0.870727
displacement    0.722049
dtype: float64</code></pre>
</div>
</div>
<p>We now have evidence that <em>weight</em> is the most significant negative factor for MPG, by a wide margin.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="_media/memes/multiple-regression.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><img src="_media/memes/multiple-regression.jpg" class="img-fluid"></a></p>
</div></div><p>In the next example we will see that we can create new features out of the ones that are initially given. Sometimes these new features add a lot to the quality of the regression.</p>
<div id="exm-regression-sales-multi" class="theorem example" data-chapter="5" type="💻" data-description="Multilinear regression for sales dataset">
<p><span class="theorem-title"><strong>Example 5.8</strong></span> Here we load data about advertising spending on different media in many markets:</p>
<div id="e109c032" class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>ads <span class="op">=</span> pd.read_csv(<span class="st">"_datasets/advertising.csv"</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>ads.head(<span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="20">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">TV</th>
<th data-quarto-table-cell-role="th">Radio</th>
<th data-quarto-table-cell-role="th">Newspaper</th>
<th data-quarto-table-cell-role="th">Sales</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>230.1</td>
<td>37.8</td>
<td>69.2</td>
<td>22.1</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>44.5</td>
<td>39.3</td>
<td>45.1</td>
<td>10.4</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>17.2</td>
<td>45.9</td>
<td>69.3</td>
<td>12.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>151.5</td>
<td>41.3</td>
<td>58.5</td>
<td>16.5</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>180.8</td>
<td>10.8</td>
<td>58.4</td>
<td>17.9</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">5</td>
<td>8.7</td>
<td>48.9</td>
<td>75.0</td>
<td>7.2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>Pairwise scatter plots yield some hints about what to expect from this dataset:</p>
<div id="021b4c16" class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>sns.pairplot(data<span class="op">=</span>ads, height<span class="op">=</span><span class="fl">1.5</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-22-output-1.png" width="569" height="568" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The last column, which shows relationships with <em>Sales</em>, is of greatest interest. From it we see that the clearest association between <em>Sales</em> and spending is with <em>TV</em>. So we first try a univariate linear fit of sales against TV spending alone:</p>
<div id="3f8b94dd" class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> ads[ [<span class="st">"TV"</span>] ]    <span class="co"># has to be a frame, so ["TV"] not "TV"</span></span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> ads[<span class="st">"Sales"</span>]</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb31-4"><a href="#cb31-4" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb31-5"><a href="#cb31-5" aria-hidden="true" tabindex="-1"></a>lm.fit(X, y)</span>
<span id="cb31-6"><a href="#cb31-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"CoD score:"</span>, <span class="ss">f"</span><span class="sc">{</span>lm<span class="sc">.</span>score(X, y)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb31-7"><a href="#cb31-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Regression weight:"</span>, lm.coef_)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CoD score: 0.8122
Regression weight: [0.05546477]</code></pre>
</div>
</div>
<p>The coefficient of determination is already quite good. Since we are going to do multiple fits with different features, we write a function that does the grunt work:</p>
<div id="6734cdce" class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> regress(lm, data, y, features):</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> data[features]</span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>    lm.fit(X, y)</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a>    CoD <span class="op">=</span> lm.score(X,y)</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"CoD score:"</span>, <span class="ss">f"</span><span class="sc">{</span>CoD<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">"Regression weights:"</span>)</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>( pd.Series(lm.coef_, index<span class="op">=</span>features) )</span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Next, we try folding in <em>Newspaper</em> as well:</p>
<div id="d0b7ae95" class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>regress(lm, ads, y, [<span class="st">"TV"</span>, <span class="st">"Newspaper"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CoD score: 0.82364
Regression weights:
TV           0.055091
Newspaper    0.026021
dtype: float64</code></pre>
</div>
</div>
<p>The additional feature had very little effect on the quality of fit. We go on to fit using all three features:</p>
<div id="8d9b7922" class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>regress(lm, ads, y, [<span class="st">"TV"</span>, <span class="st">"Newspaper"</span>, <span class="st">"Radio"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CoD score: 0.90259
Regression weights:
TV           0.054446
Newspaper    0.000336
Radio        0.107001
dtype: float64</code></pre>
</div>
</div>
<p>Judging by the weights of the model, it’s even clearer now that we can explain <em>Sales</em> very well without contributions from <em>Newspaper</em>. In order to reduce model variance, it would be reasonable to leave that column out. Doing so has a negligible effect:</p>
<div id="37de121e" class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>regress(lm, ads, y, [<span class="st">"TV"</span>, <span class="st">"Radio"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CoD score: 0.90259
Regression weights:
TV       0.054449
Radio    0.107175
dtype: float64</code></pre>
</div>
</div>
<p>While we have a very good CoD now, we can try to improve it. We can add an additional feature that is the product of <em>TV</em> and <em>Radio</em>, representing the possibility that these media reinforce one another’s effects:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>In order to modify a frame, it has to be an independent copy, not just a subset of another frame.</p>
</div>
</div>
<div id="a52c4896" class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> ads[ [<span class="st">"Radio"</span>, <span class="st">"TV"</span>] ].copy()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>X[<span class="st">"Radio*TV"</span>] <span class="op">=</span> X[<span class="st">"Radio"</span>]<span class="op">*</span>X[<span class="st">"TV"</span>]</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a>regress(lm, X, y, X.columns)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CoD score: 0.91404
Regression weights:
Radio       0.042270
TV          0.043578
Radio*TV    0.000443
dtype: float64</code></pre>
</div>
</div>
<p>We did see a small increase in the CoD score, and the combination of both types of spending does have a positive effect on <em>Sales</em>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_0otfbu23&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_xs33vwho" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.8" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>It’s not uncommon to introduce a product term as done in <a href="#exm-regression-sales-multi" class="quarto-xref">Example&nbsp;<span>5.8</span></a>, and more exotic choices are also possible. Keep in mind, though, that additional variables usually add variance to the model, even if they don’t seriously affect the bias.</p>
<p>Interpreting linear regression is a major topic in statistics. There are tests that can lend much more precision and rigor to the brief discussion in this section.</p>
<section id="polynomial-regression" class="level3 page-columns page-full" data-number="5.2.1">
<h3 data-number="5.2.1" class="anchored" data-anchor-id="polynomial-regression"><span class="header-section-number">5.2.1</span> Polynomial regression</h3>
<p>An important special case of <a href="#eq-regression-multilinear" class="quarto-xref">Equation&nbsp;<span>5.12</span></a> happens when all of the features <span class="math inline">\(x_i\)</span> are powers of a single variable <span class="math inline">\(t\)</span>:</p>
<p><span class="math display">\[
x_1 = t^0, \, x_2 = t^1, \ldots, x_d = t^{d-1}.
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_3228tsmj&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_sd10nlnm" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.2.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>This makes the regressive approximation into</p>
<p><span id="eq-regression-polynomial"><span class="math display">\[
y \approx w_1 + w_2 t + \cdots + w_d t^{d-1},
\tag{5.10}\]</span></span></p>
<p>which is a polynomial of degree <span class="math inline">\(d-1\)</span>. This allows representation of data that depends on <span class="math inline">\(t\)</span> in ways more complicated than a straight line. However, it can lead to overfitting if taken too far.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>It might seem odd to categorize polynomial fits under the heading “linear.” The <span class="math inline">\(y\)</span> in <a href="#eq-regression-polynomial" class="quarto-xref">Equation&nbsp;<span>5.10</span></a> has a nonlinear dependence as a function of <span class="math inline">\(t\)</span>. But the dependence on the weights <span class="math inline">\(\bfw\)</span> is linear, which means that the problem can be solved by linear algebra methods alone.</p>
</div>
</div>
<p>Rather than constructing polynomial features manually, it’s easier to use a pipeline.</p>
<div id="exm-regression-polynomial" class="theorem example" data-chapter="5" type="💻" data-description="Polynomial regression for the MPG dataset">
<p><span class="theorem-title"><strong>Example 5.9</strong></span> We return to the data set regarding the fuel efficiency of cars:</p>
<div id="433eb70a" class="cell" data-execution_count="29">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>cars <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a>cars.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="28">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">mpg</th>
<th data-quarto-table-cell-role="th">cylinders</th>
<th data-quarto-table-cell-role="th">displacement</th>
<th data-quarto-table-cell-role="th">horsepower</th>
<th data-quarto-table-cell-role="th">weight</th>
<th data-quarto-table-cell-role="th">acceleration</th>
<th data-quarto-table-cell-role="th">model_year</th>
<th data-quarto-table-cell-role="th">origin</th>
<th data-quarto-table-cell-role="th">name</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>18.0</td>
<td>8</td>
<td>307.0</td>
<td>130.0</td>
<td>3504</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>chevrolet chevelle malibu</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>15.0</td>
<td>8</td>
<td>350.0</td>
<td>165.0</td>
<td>3693</td>
<td>11.5</td>
<td>70</td>
<td>usa</td>
<td>buick skylark 320</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>18.0</td>
<td>8</td>
<td>318.0</td>
<td>150.0</td>
<td>3436</td>
<td>11.0</td>
<td>70</td>
<td>usa</td>
<td>plymouth satellite</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>16.0</td>
<td>8</td>
<td>304.0</td>
<td>150.0</td>
<td>3433</td>
<td>12.0</td>
<td>70</td>
<td>usa</td>
<td>amc rebel sst</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>17.0</td>
<td>8</td>
<td>302.0</td>
<td>140.0</td>
<td>3449</td>
<td>10.5</td>
<td>70</td>
<td>usa</td>
<td>ford torino</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>As we would expect, horsepower and miles per gallon are negatively correlated. However, the relationship is not well captured by a straight line:</p>
<div id="198c554d" class="cell" data-execution_count="30">
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>cars, x<span class="op">=</span><span class="st">"horsepower"</span>, y<span class="op">=</span><span class="st">"mpg"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-30-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Seaborn can show us how a cubic polynomial regression would look:</p>
<div id="55b595bb" class="cell" data-execution_count="31">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1" aria-hidden="true" tabindex="-1"></a>sns.lmplot(data<span class="op">=</span>cars, x<span class="op">=</span><span class="st">"horsepower"</span>, y<span class="op">=</span><span class="st">"mpg"</span>, order<span class="op">=</span><span class="dv">3</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-31-output-1.png" width="469" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The figure above suggests that the cubic regression produces a better fit—that is, lower bias over the full range of horsepower.</p>
<p>In order to obtain the cubic fit within sklearn, we use the <code>PolynomialFeatures</code> preprocessor in a pipeline. If the original predictor variable is <span class="math inline">\(t\)</span>, then the preprocessor will create features for <span class="math inline">\(1\)</span>, <span class="math inline">\(t\)</span>, <span class="math inline">\(t^2\)</span>, and <span class="math inline">\(t^3\)</span>. (Since the constant feature is added in by the preprocessor, we don’t need it again within the regression, so we set <code>fit_intercept=False</code> in <code>LinearRegression</code>.)</p>
<div id="46b573ec" class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb45-1"><a href="#cb45-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> PolynomialFeatures</span>
<span id="cb45-2"><a href="#cb45-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-3"><a href="#cb45-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars[ [<span class="st">"horsepower"</span>] ]</span>
<span id="cb45-4"><a href="#cb45-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb45-5"><a href="#cb45-5" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression( fit_intercept<span class="op">=</span><span class="va">False</span> )</span>
<span id="cb45-6"><a href="#cb45-6" aria-hidden="true" tabindex="-1"></a>cubic <span class="op">=</span> make_pipeline(PolynomialFeatures(degree<span class="op">=</span><span class="dv">3</span>), lm)</span>
<span id="cb45-7"><a href="#cb45-7" aria-hidden="true" tabindex="-1"></a>cubic.fit(X, y)</span>
<span id="cb45-8"><a href="#cb45-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb45-9"><a href="#cb45-9" aria-hidden="true" tabindex="-1"></a><span class="co"># Predict MPG at horsepower=200:</span></span>
<span id="cb45-10"><a href="#cb45-10" aria-hidden="true" tabindex="-1"></a>query <span class="op">=</span> pd.DataFrame([<span class="dv">200</span>], columns<span class="op">=</span>X.columns)</span>
<span id="cb45-11"><a href="#cb45-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"prediction at hp=200:"</span>, cubic.predict(query))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>prediction at hp=200: [12.90220247]</code></pre>
</div>
</div>
<p>Let’s compare the coefficients of determination for the linear and cubic fits:</p>
<div id="b79da458" class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>linscore <span class="op">=</span> LinearRegression().fit(X,y).score(X,y)</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CoD for linear fit: </span><span class="sc">{</span>linscore<span class="sc">:4f}</span><span class="ss">"</span>)</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"CoD for cubic fit: </span><span class="sc">{</span>cubic<span class="sc">.</span>score(X,y)<span class="sc">:4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>CoD for linear fit: 0.605948
CoD for cubic fit: 0.688214</code></pre>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_myfj2re4&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_41qyb4dt" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.9" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>If a cubic polynomial can fit better than a line, it seems reasonable that increasing the degree more will lead to even better fits. In fact, the training error can only go down, because a lower-degree polynomial case is a subset of a higher-degree case. But there is a major catch, in the form of overfitting.</p>
<div id="exm-regression-polynomial-high" class="theorem example" data-chapter="5" type="💻" data-description="Overfitting in polynomial regression">
<p><span class="theorem-title"><strong>Example 5.10</strong></span> Continuing with <a href="#exm-regression-polynomial" class="quarto-xref">Example&nbsp;<span>5.9</span></a>, we explore the effect of polynomial degree after splitting into training and test sets:</p>
<div id="4941bd94" class="cell" data-execution_count="34">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, </span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">302</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> deg <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1</span>,<span class="dv">9</span>):</span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    poly <span class="op">=</span> make_pipeline( </span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a>        StandardScaler(), </span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>        PolynomialFeatures(degree<span class="op">=</span>deg), </span>
<span id="cb49-14"><a href="#cb49-14" aria-hidden="true" tabindex="-1"></a>        lm </span>
<span id="cb49-15"><a href="#cb49-15" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb49-16"><a href="#cb49-16" aria-hidden="true" tabindex="-1"></a>    poly.fit(X_train, y_train)</span>
<span id="cb49-17"><a href="#cb49-17" aria-hidden="true" tabindex="-1"></a>    MSE_tr <span class="op">=</span> mean_squared_error(y_train, poly.predict(X_train))</span>
<span id="cb49-18"><a href="#cb49-18" aria-hidden="true" tabindex="-1"></a>    MSE_te <span class="op">=</span> mean_squared_error(y_test, poly.predict(X_test))</span>
<span id="cb49-19"><a href="#cb49-19" aria-hidden="true" tabindex="-1"></a>    results.append((deg, <span class="st">"train"</span>, MSE_tr))</span>
<span id="cb49-20"><a href="#cb49-20" aria-hidden="true" tabindex="-1"></a>    results.append((deg, <span class="st">"test"</span>, MSE_te))</span>
<span id="cb49-21"><a href="#cb49-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-22"><a href="#cb49-22" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> pd.DataFrame(results, columns<span class="op">=</span>[<span class="st">"degree"</span>,<span class="st">"measuring set"</span>,<span class="st">"MSE"</span>])</span>
<span id="cb49-23"><a href="#cb49-23" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>results,</span>
<span id="cb49-24"><a href="#cb49-24" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"degree"</span>, y<span class="op">=</span><span class="st">"MSE"</span>,</span>
<span id="cb49-25"><a href="#cb49-25" aria-hidden="true" tabindex="-1"></a>    kind<span class="op">=</span><span class="st">"line"</span>, hue<span class="op">=</span><span class="st">"measuring set"</span></span>
<span id="cb49-26"><a href="#cb49-26" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-34-output-1.png" width="570" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The results above demonstrate that increasing the polynomial degree eventually leads to overfitting. In fact, in this case it seems that we don’t want to go beyond the cubic fit. <!-- 

Finally, we can combine the use of multiple features and higher degree:

::: {#a36f191e .cell execution_count=35}
``` {.python .cell-code}
pipe = make_pipeline(
    StandardScaler(),
    PolynomialFeatures(degree=2),
    lm
    )
fitcars(pipe, features)
```

::: {.cell-output .cell-output-stdout}
```
MSE: 15.749, compared to variance 56.474
```
:::
:::


This is our best regression fit so far, by a mile. --></p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_jyjb5url&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_e3m6ce0a" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.10" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>Let’s summarize the situation:</p>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>Adding polynomial features improves the expressive power of a regression model, which can decrease bias but also increase overfitting.</p>
</div>
</div>
</section>
</section>
<section id="regularization" class="level2 page-columns page-full" data-number="5.3">
<h2 data-number="5.3" class="anchored" data-anchor-id="regularization"><span class="header-section-number">5.3</span> Regularization</h2>
<p>As a general term, <em>regularization</em> refers to modifying something that is difficult to compute accurately with something more tractable. For learning models, regularization is a useful way to combat overfitting.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_i48yjeqq&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_w8z85xgi" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.3" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>Suppose we had an <span class="math inline">\(\real^{n\times 4}\)</span> feature matrix in which the features are identical; that is, the predictor variables satisfy <span class="math inline">\(x_1=x_2=x_3=x_4\)</span>, and suppose the target <span class="math inline">\(y\)</span> also equals <span class="math inline">\(x_1\)</span>. Clearly, we get a perfect regression if we use</p>
<p><span class="math display">\[
y = 1x_1 + 0x_2 + 0x_3 + 0x_4.
\]</span></p>
<p>But an equally good regression is</p>
<p><span class="math display">\[
y = \frac{1}{4}x_1 + \frac{1}{4}x_2 + \frac{1}{4}x_3 + \frac{1}{4}x_4.
\]</span></p>
<p>For that matter, so is</p>
<p><span class="math display">\[
y = 1000x_1 - 500x_2 - 500x_3 + 1x_4.
\]</span></p>
<p>A problem with more than one valid solution is called <strong>ill-posed</strong>. If we made tiny changes to the predictor variables in this thought experiment, the problem would technically be well-posed, but there would be a wide range of solutions that were very nearly correct, in which case the problem is said to be <strong>ill-conditioned</strong>; for practical purposes, it remains just as difficult.</p>
<p>The poor conditioning can be regularized away by modifying the least-squares loss function to penalize complexity in the model, in the form of excessively large regression coefficients. There are two dominant variants for doing this.</p>
<div id="def-regression-regular" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.10</strong></span> <strong>Ridge regression</strong> minimizes the loss function</p>
<p><span id="eq-regression-ridge"><span class="math display">\[
L(\bfw) = \twonorm{ \bfX \bfw- \bfy }^2 + \alpha \twonorm{\bfw}^2,
\tag{5.11}\]</span></span></p>
<p>where <span class="math inline">\(\alpha\)</span> is a nonnegative hyperparameter. <strong>LASSO regression</strong> minimizes the loss function <span class="math display">\[
L(\bfw) = \twonorm{ \bfX \bfw- \bfy }^2 + \alpha \onenorm{\bfw},
\]</span></p>
<p>again for a nonnegative value of <span class="math inline">\(\alpha\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="_media/memes/penalty.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><img src="_media/memes/penalty.jpg" class="img-fluid"></a></p>
</div></div><p>The new terms in <a href="#eq-regression-ridge" class="quarto-xref">Equation&nbsp;<span>5.11</span></a> and <a href="#eq-regression-ridge" class="quarto-xref">Equation&nbsp;<span>5.11</span></a> based on norms of <span class="math inline">\(\bfw\)</span> are called <strong>penalty terms</strong>. As <span class="math inline">\(\alpha\to 0\)</span>, the penalty terms vanish and both types of regularization revert to the usual least-squares loss. But as <span class="math inline">\(\alpha \to \infty\)</span>, the penalty terms dominate and the optimization becomes increasingly concerned with prioritizing a small result for <span class="math inline">\(\bfw\)</span>. In this limit, the model pays less and less attention to the data, which can reduce overfitting.</p>
<div id="exm-regression-ridge-diabetes" class="theorem example" data-chapter="5" type="💻" data-description="Ridge regression for diabetes dataset">
<p><span class="theorem-title"><strong>Example 5.11</strong></span> We’ll apply ridge regression to data collected about the progression of diabetes:</p>
<div id="1d5d377c" class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1" aria-hidden="true" tabindex="-1"></a>diabetes <span class="op">=</span> datasets.load_diabetes(as_frame<span class="op">=</span><span class="va">True</span>)[<span class="st">"frame"</span>]</span>
<span id="cb50-2"><a href="#cb50-2" aria-hidden="true" tabindex="-1"></a>diabetes.head(<span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">age</th>
<th data-quarto-table-cell-role="th">sex</th>
<th data-quarto-table-cell-role="th">bmi</th>
<th data-quarto-table-cell-role="th">bp</th>
<th data-quarto-table-cell-role="th">s1</th>
<th data-quarto-table-cell-role="th">s2</th>
<th data-quarto-table-cell-role="th">s3</th>
<th data-quarto-table-cell-role="th">s4</th>
<th data-quarto-table-cell-role="th">s5</th>
<th data-quarto-table-cell-role="th">s6</th>
<th data-quarto-table-cell-role="th">target</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>0.038076</td>
<td>0.050680</td>
<td>0.061696</td>
<td>0.021872</td>
<td>-0.044223</td>
<td>-0.034821</td>
<td>-0.043401</td>
<td>-0.002592</td>
<td>0.019907</td>
<td>-0.017646</td>
<td>151.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>-0.001882</td>
<td>-0.044642</td>
<td>-0.051474</td>
<td>-0.026328</td>
<td>-0.008449</td>
<td>-0.019163</td>
<td>0.074412</td>
<td>-0.039493</td>
<td>-0.068332</td>
<td>-0.092204</td>
<td>75.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>0.085299</td>
<td>0.050680</td>
<td>0.044451</td>
<td>-0.005670</td>
<td>-0.045599</td>
<td>-0.034194</td>
<td>-0.032356</td>
<td>-0.002592</td>
<td>0.002861</td>
<td>-0.025930</td>
<td>141.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>-0.089063</td>
<td>-0.044642</td>
<td>-0.011595</td>
<td>-0.036656</td>
<td>0.012191</td>
<td>0.024991</td>
<td>-0.036038</td>
<td>0.034309</td>
<td>0.022688</td>
<td>-0.009362</td>
<td>206.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>0.005383</td>
<td>-0.044642</td>
<td>-0.036385</td>
<td>0.021872</td>
<td>0.003935</td>
<td>0.015596</td>
<td>0.008142</td>
<td>-0.002592</td>
<td>-0.031988</td>
<td>-0.046641</td>
<td>135.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">5</td>
<td>-0.092695</td>
<td>-0.044642</td>
<td>-0.040696</td>
<td>-0.019442</td>
<td>-0.068991</td>
<td>-0.079288</td>
<td>0.041277</td>
<td>-0.076395</td>
<td>-0.041176</td>
<td>-0.096346</td>
<td>97.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">6</td>
<td>-0.045472</td>
<td>0.050680</td>
<td>-0.047163</td>
<td>-0.015999</td>
<td>-0.040096</td>
<td>-0.024800</td>
<td>0.000779</td>
<td>-0.039493</td>
<td>-0.062917</td>
<td>-0.038357</td>
<td>138.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">7</td>
<td>0.063504</td>
<td>0.050680</td>
<td>-0.001895</td>
<td>0.066629</td>
<td>0.090620</td>
<td>0.108914</td>
<td>0.022869</td>
<td>0.017703</td>
<td>-0.035816</td>
<td>0.003064</td>
<td>63.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">8</td>
<td>0.041708</td>
<td>0.050680</td>
<td>0.061696</td>
<td>-0.040099</td>
<td>-0.013953</td>
<td>0.006202</td>
<td>-0.028674</td>
<td>-0.002592</td>
<td>-0.014960</td>
<td>0.011349</td>
<td>110.0</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">9</td>
<td>-0.070900</td>
<td>-0.044642</td>
<td>0.039062</td>
<td>-0.033213</td>
<td>-0.012577</td>
<td>-0.034508</td>
<td>-0.024993</td>
<td>-0.002592</td>
<td>0.067737</td>
<td>-0.013504</td>
<td>310.0</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>The features in this dataset were standardized, making it easy to compare the magnitudes of the regression coefficients.</p>
<p>First, we look at basic linear regression on all ten predictive features in the data:</p>
<div id="59017677" class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> diabetes.drop(<span class="st">"target"</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> diabetes[<span class="st">"target"</span>]</span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">2</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LinearRegression</span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>lm.fit(X_train, y_train)</span>
<span id="cb51-12"><a href="#cb51-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"linear model CoD score: </span><span class="sc">{</span>lm<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>linear model CoD score: 0.4399</code></pre>
</div>
</div>
<p>First, we find that ridge regression can improve the score a bit:</p>
<div id="0152190e" class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Ridge</span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>rr <span class="op">=</span> Ridge(alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb53-4"><a href="#cb53-4" aria-hidden="true" tabindex="-1"></a>rr.fit(X_train, y_train)</span>
<span id="cb53-5"><a href="#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"ridge CoD score: </span><span class="sc">{</span>rr<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>ridge CoD score: 0.4411</code></pre>
</div>
</div>
<p>Ridge regularization added a penalty for the 2-norm of the regression coefficients vector. Accordingly, the regularized solution has smaller coefficients:</p>
<div id="ca8aaf59" class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy.linalg <span class="im">import</span> norm</span>
<span id="cb55-2"><a href="#cb55-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"2-norm of unregularized coefficients: </span><span class="sc">{</span>norm(lm.coef_)<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb55-3"><a href="#cb55-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"2-norm of ridge coefficients: </span><span class="sc">{</span>norm(rr.coef_)<span class="sc">:.1f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2-norm of unregularized coefficients: 1525.2
2-norm of ridge coefficients: 605.9</code></pre>
</div>
</div>
<p>As we continue to increase the regularization parameter, the method becomes increasingly obsessed with keeping the coefficient vector small and pays ever less attention to the data:</p>
<div id="4566ee0d" class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> [<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">2</span>]:</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a>    rr <span class="op">=</span> Ridge(alpha<span class="op">=</span>alpha)    <span class="co"># more regularization</span></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a>    rr.fit(X_train, y_train)</span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"alpha = </span><span class="sc">{</span>alpha<span class="sc">:.2f}</span><span class="ss">"</span>)</span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"2-norm of coefficient vector: </span><span class="sc">{</span>norm(rr.coef_)<span class="sc">:.1f}</span><span class="ss">"</span>)</span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"ridge regression CoD score: </span><span class="sc">{</span>rr<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>alpha = 0.25
2-norm of coefficient vector: 711.7
ridge regression CoD score: 0.4527

alpha = 0.50
2-norm of coefficient vector: 605.9
ridge regression CoD score: 0.4411

alpha = 1.00
2-norm of coefficient vector: 480.8
ridge regression CoD score: 0.4078

alpha = 2.00
2-norm of coefficient vector: 353.5
ridge regression CoD score: 0.3478
</code></pre>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_cjwkntgp&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_hgnnthz7" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.11" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>While ridge regression is an easier function to minimize quickly, LASSO has an interesting advantage, as illustrated in this figure.</p>
<p><img src="_media/regularization.png" class="img-fluid"></p>
<p>LASSO tends to produce <em>sparse</em> results, meaning that some of the regression coefficients are zero or negligible. These zeros indicate predictor variables that have minor predictive value, which can be valuable information in itself. Moreover, these variables can often be removed from the regression to reduce variance without noticeably affecting the bias.</p>
<div id="exm-regression-lasso-diabetes" class="theorem example" data-chapter="5" type="💻" data-description="LASSO regression for diabetes dataset">
<p><span class="theorem-title"><strong>Example 5.12</strong></span> We continue with the diabetes data from <a href="#exm-regression-ridge-diabetes" class="quarto-xref">Example&nbsp;<span>5.11</span></a>, but this time, we try LASSO regularization. A validation curve suggests initial gains in the CoD score as the regularization parameter <span class="math inline">\(\alpha\)</span> is varied, followed by a decrease:</p>
<div id="5d650f9b" class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb59-1"><a href="#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> Lasso</span>
<span id="cb59-2"><a href="#cb59-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-3"><a href="#cb59-3" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">4</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb59-4"><a href="#cb59-4" aria-hidden="true" tabindex="-1"></a>alpha <span class="op">=</span> np.linspace(<span class="dv">0</span>, <span class="fl">0.1</span>, <span class="dv">60</span>)[<span class="dv">1</span>:]  <span class="co"># exclude alpha=0</span></span>
<span id="cb59-5"><a href="#cb59-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-6"><a href="#cb59-6" aria-hidden="true" tabindex="-1"></a>_,scores <span class="op">=</span> validation_curve(</span>
<span id="cb59-7"><a href="#cb59-7" aria-hidden="true" tabindex="-1"></a>    Lasso(),</span>
<span id="cb59-8"><a href="#cb59-8" aria-hidden="true" tabindex="-1"></a>    X_train, y_train,</span>
<span id="cb59-9"><a href="#cb59-9" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>kf,</span>
<span id="cb59-10"><a href="#cb59-10" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span>,</span>
<span id="cb59-11"><a href="#cb59-11" aria-hidden="true" tabindex="-1"></a>    param_name<span class="op">=</span><span class="st">"alpha"</span>, param_range<span class="op">=</span>alpha</span>
<span id="cb59-12"><a href="#cb59-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb59-13"><a href="#cb59-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb59-14"><a href="#cb59-14" aria-hidden="true" tabindex="-1"></a>sns.relplot(x<span class="op">=</span>alpha, y<span class="op">=</span>np.mean(scores, axis<span class="op">=</span><span class="dv">1</span>) )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-41-output-1.png" width="451" height="450" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Moreover, while ridge regression still used all of the features, LASSO put zero weight on three of them:</p>
<div id="56ceb7f5" class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1" aria-hidden="true" tabindex="-1"></a>lass <span class="op">=</span> Lasso(alpha<span class="op">=</span><span class="fl">0.05</span>)</span>
<span id="cb60-2"><a href="#cb60-2" aria-hidden="true" tabindex="-1"></a>lass.fit(X_train, y_train)</span>
<span id="cb60-3"><a href="#cb60-3" aria-hidden="true" tabindex="-1"></a>pd.DataFrame( {</span>
<span id="cb60-4"><a href="#cb60-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"feature"</span>: X.columns,</span>
<span id="cb60-5"><a href="#cb60-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"ridge"</span>: rr.coef_,</span>
<span id="cb60-6"><a href="#cb60-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"LASSO"</span>: lass.coef_</span>
<span id="cb60-7"><a href="#cb60-7" aria-hidden="true" tabindex="-1"></a>    } )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">feature</th>
<th data-quarto-table-cell-role="th">ridge</th>
<th data-quarto-table-cell-role="th">LASSO</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>age</td>
<td>43.113029</td>
<td>-0.000000</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>sex</td>
<td>-23.953301</td>
<td>-155.276227</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>bmi</td>
<td>199.535945</td>
<td>529.173009</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>bp</td>
<td>144.586873</td>
<td>313.419043</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>s1</td>
<td>25.977923</td>
<td>-132.507438</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">5</td>
<td>s2</td>
<td>2.751708</td>
<td>-0.000000</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">6</td>
<td>s3</td>
<td>-106.337626</td>
<td>-165.167100</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">7</td>
<td>s4</td>
<td>89.526889</td>
<td>0.000000</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">8</td>
<td>s5</td>
<td>185.660175</td>
<td>580.262391</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">9</td>
<td>s6</td>
<td>85.576399</td>
<td>30.557703</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<p>We can isolate the coefficients that were (within small rounding errors) zeroed out:</p>
<div id="6065ef6f" class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb61-1"><a href="#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Get the locations (indices) of the very small weights:</span></span>
<span id="cb61-2"><a href="#cb61-2" aria-hidden="true" tabindex="-1"></a>zeroed <span class="op">=</span> np.nonzero( np.<span class="bu">abs</span>(lass.coef_) <span class="op">&lt;</span> <span class="fl">1e-5</span> )</span>
<span id="cb61-3"><a href="#cb61-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Names of the corresponding columns:</span></span>
<span id="cb61-4"><a href="#cb61-4" aria-hidden="true" tabindex="-1"></a>dropped <span class="op">=</span> X.columns[zeroed].values</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Now we can drop these features from the dataset:</p>
<div id="a22289c2" class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1" aria-hidden="true" tabindex="-1"></a>X_train_reduced <span class="op">=</span> X_train.drop(dropped, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb62-2"><a href="#cb62-2" aria-hidden="true" tabindex="-1"></a>X_test_reduced <span class="op">=</span> X_test.drop(dropped, axis<span class="op">=</span><span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Returning to a fit with no regularization, we find that little is lost by using the reduced feature set:</p>
<div id="dd942584" class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb63-1"><a href="#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"original linear model score: </span><span class="sc">{</span>lm<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span>
<span id="cb63-2"><a href="#cb63-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb63-3"><a href="#cb63-3" aria-hidden="true" tabindex="-1"></a>lm.fit(X_train_reduced, y_train)</span>
<span id="cb63-4"><a href="#cb63-4" aria-hidden="true" tabindex="-1"></a>CoD <span class="op">=</span> lm.score(X_test_reduced, y_test)</span>
<span id="cb63-5"><a href="#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"reduced linear model score: </span><span class="sc">{</span>CoD<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>original linear model score: 0.4399
reduced linear model score: 0.4388</code></pre>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_3kg9kwjn&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_4fi9awbt" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.12" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div></section>
<section id="nonlinear-regression" class="level2 page-columns page-full" data-number="5.4">
<h2 data-number="5.4" class="anchored" data-anchor-id="nonlinear-regression"><span class="header-section-number">5.4</span> Nonlinear regression</h2>
<p>Multilinear regression limits the representation of the dataset to a function of the form <span id="eq-regression-multilinear"><span class="math display">\[
\hat{f}(\bfx) = \bfw^T \bfx.
\tag{5.12}\]</span></span></p>
<p>This is a <strong>linear</strong> function, meaning that two key properties are satisfied. For all possible vectors <span class="math inline">\(\bfu,\bfv\)</span> and numbers <span class="math inline">\(c\)</span>,</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_tgywzwbw&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_sodg9i2n" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.4" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><ol type="1">
<li><span class="math inline">\(\hat{f}(\bfu + \bfv) = \hat{f}(\bfu) + \hat{f}(\bfv)\)</span>,</li>
<li><span class="math inline">\(\hat{f}(c\bfu) = c \hat{f}(\bfu)\)</span>.</li>
</ol>
<p>These properties are the essence of what makes a function easy to manipulate, solve for, and analyze.</p>
<div id="exm-regression-linear" class="theorem example" data-chapter="5" type="✍️" data-description="Proving linearity">
<p><span class="theorem-title"><strong>Example 5.13</strong></span> In two dimensions, the function <span class="math inline">\(\hat{f}(\bfx) = x_1 - x_2\)</span> is linear. To prove this, first suppose that <span class="math inline">\(\bfu\)</span> and <span class="math inline">\(\bfv\)</span> are any 2-vectors. Then <span class="math display">\[
\begin{split}
\hat{f}(\bfu + \bfv) &amp;= \hat{f}\bigl( [u_1+v_1,u_2+v_2] \bigr) \\
    &amp;=  (u_1 + u_2) - (v_1 + v_2)  \\
    &amp;= (u_1-v_1) + (u_2-v_2) \\
    &amp;= \hat{f}(\bfu) + \hat{f}(\bfv).
\end{split}
\]</span> Now, suppose that <span class="math inline">\(\bfu\)</span> is a 2-vector and <span class="math inline">\(c\)</span> is any real number. Then <span class="math display">\[
\begin{split}
\hat{f}(c\bfu) &amp;= \hat{f}\bigl([cu_1, cu_2] \bigr) \\
    &amp;= cu_1 - cu_2 \\
    &amp;= c(u_1-u_2) \\
    &amp;= c\cdot\hat{f}(\bfu).
\end{split}
\]</span></p>
</div>
<div id="exm-regression-nonlinear" class="theorem example" data-chapter="5" type="✍️" data-description="Proving nonlinearity">
<p><span class="theorem-title"><strong>Example 5.14</strong></span> In two dimensions, the function <span class="math inline">\(\hat{f}(\bfx) = |x_1 + x_2|\)</span> is not linear. Suppose <span class="math inline">\(\bfu\)</span> is any 2-vector and <span class="math inline">\(c\)</span> is any real number. If we try to prove the second property of linearity, we would start with: <span class="math display">\[
\begin{split}
\hat{f}(c\bfu) &amp;= \hat{f}\bigl([cu_1, cu_2] \bigr) \\
    &amp;= |cu_1 + cu_2| \\
    &amp;= |c| \cdot |u_1 + u_2|.
\end{split}
\]</span></p>
<p>We are trying to show that this equals <span class="math inline">\(c \cdot \hat{f}(\bfu)\)</span>, which is <span class="math inline">\(c\cdot |u_1+u_2|\)</span>. However, it doesn’t look as though this is equivalent to the last line above in all cases. In fact, they must be different if <span class="math inline">\(c &lt; 0\)</span> and <span class="math inline">\(|u_1+u_2|\)</span> is nonzero.</p>
<p>To prove <em>nonlinearity</em>, we only have to find one counterexample where one of the properties of a linear function doesn’t hold. The attempt above suggests, for instance, <span class="math inline">\(\bfu=[1,1]\)</span> and the number <span class="math inline">\(c=-1\)</span>. Then <span class="math display">\[
\hat{f}(c \bfu) = | -1 - 1| = 2,
\]</span> but at the same time, <span class="math display">\[
c\cdot \hat{f}(\bfu) = -1 \cdot |1+1| = -2.
\]</span></p>
<p>Since these values are different, <span class="math inline">\(\hat{f}\)</span> can’t be linear.</p>
</div>
<p>For our regression function <a href="#eq-regression-multilinear" class="quarto-xref">Equation&nbsp;<span>5.12</span></a>, the linearity properties follow easily from how the inner product is defined. For example, <span class="math display">\[
\hat{f}(c\bfu) = (c\bfu)^T\bfw = \sum_{i=1}^d (cu_i) w_i = c \sum_{i=1}^d u_i w_i = c(\bfu^T\bfw) = c \hat{f}(\bfu).
\]</span></p>
<p>One major benefit of the linear approach is that the dependence of the weight vector <span class="math inline">\(\bfw\)</span> on the regressed data is also linear, which makes solving for it straightforward.</p>
<p>As the simplest type of multidimensional function, linear relationships are a good first resort. Furthermore, we can augment the features with powers in order to get polynomial relationships. However, that approach becomes infeasible for more than 2 or 3 dimensions, because the number of polynomial terms needed explodes. (While there is a way around this restriction known as the <em>kernel trick</em>, that’s beyond our mathematical scope here.)</p>
<p>Alternatively, we can resort to fully nonlinear regression methods. Two of them come from generalizations of our staple classifiers.</p>
<section id="nearest-neighbors" class="level3 page-columns page-full" data-number="5.4.1">
<h3 data-number="5.4.1" class="anchored" data-anchor-id="nearest-neighbors"><span class="header-section-number">5.4.1</span> Nearest neighbors</h3>
<p>To use kNN for regression, we find the <span class="math inline">\(k\)</span> nearest examples as with classification, but replace voting on classes with the mean or median of the neighboring values. A simple example confirms that the resulting approximation is not linear.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_gcf5bxmt&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_lp6ala2r" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.4.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div id="exm-regression-knn" class="theorem example" data-chapter="5" type="✍️" data-description="Nonlinearity of kNN regression">
<p><span class="theorem-title"><strong>Example 5.15</strong></span> Suppose we have just two samples with one-dimensional features: <span class="math inline">\(x_1=0\)</span> and <span class="math inline">\(x_2=2\)</span>, and let the corresponding sample values be <span class="math inline">\(y_1=0\)</span> and <span class="math inline">\(y_2=1\)</span>. Using kNN with <span class="math inline">\(k=1\)</span>, the resulting approximation <span class="math inline">\(\hat{f}(x)\)</span> is <span class="math display">\[
\hat{f}(x) =
\begin{cases}
    0, &amp; x &lt; 1, \\
    \tfrac{1}{2}, &amp; x=1, \\  
    1, &amp; x &gt; 1.
\end{cases}
\]</span></p>
<p>(Convince yourself that the result is the same whether the mean or the median is used.) Thus, for instance, <span class="math inline">\(\hat{f}(2 \cdot 0.6)=\hat{f}(1.2)=1\)</span>, while <span class="math inline">\(2\cdot \hat{f}(0.6) = 0\)</span>.</p>
</div>
<p>kNN regression can produce a function that conforms itself to the training data much more closely than a linear regressor does. This can both decrease bias and increase variance, especially for small values of <span class="math inline">\(k\)</span>. As illustrated in the following video, increasing <span class="math inline">\(k\)</span> flattens out the approximation, decreasing variance while increasing bias.</p>
<div class="quarto-video"><video id="video_shortcode_videojs_video1" width="600" class="video-js vjs-default-skin " controls="" preload="auto" data-setup="{}" title=""><source src="_media/knn_regression.mp4"></video></div>
<p>As with classification, we can choose the norm to use and whether to weight the neighbors equally or by inverse distance. As a reminder, it is usually advisable to work with z-scores for the features rather than raw data.</p>
<div id="exm-regression-knn-demo" class="theorem example" data-chapter="5" type="💻" data-description="kNN regression for MPG dataset">
<p><span class="theorem-title"><strong>Example 5.16</strong></span> We return again to the dataset of cars and their fuel efficiency. A linear regression on four quantitative features is only OK:</p>
<div id="f81f326b" class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb65-1"><a href="#cb65-1" aria-hidden="true" tabindex="-1"></a>cars <span class="op">=</span> sns.load_dataset(<span class="st">"mpg"</span>).dropna()</span>
<span id="cb65-2"><a href="#cb65-2" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [<span class="st">"displacement"</span>, <span class="st">"horsepower"</span>, <span class="st">"weight"</span>, <span class="st">"acceleration"</span>]</span>
<span id="cb65-3"><a href="#cb65-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars[features]</span>
<span id="cb65-4"><a href="#cb65-4" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb65-5"><a href="#cb65-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-6"><a href="#cb65-6" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb65-7"><a href="#cb65-7" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb65-8"><a href="#cb65-8" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">0</span></span>
<span id="cb65-9"><a href="#cb65-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb65-10"><a href="#cb65-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb65-11"><a href="#cb65-11" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> LinearRegression()</span>
<span id="cb65-12"><a href="#cb65-12" aria-hidden="true" tabindex="-1"></a>lm.fit(X_train, y_train)</span>
<span id="cb65-13"><a href="#cb65-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"linear model CoD: </span><span class="sc">{</span>lm<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>linear model CoD: 0.6928</code></pre>
</div>
</div>
<p>Next we try a kNN regressor, doing a grid search to find good hyperparameters:</p>
<div id="7c4f51d8" class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.neighbors <span class="im">import</span> KNeighborsRegressor</span>
<span id="cb67-2"><a href="#cb67-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-3"><a href="#cb67-3" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> KFold(n_splits<span class="op">=</span><span class="dv">6</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">3383</span>)</span>
<span id="cb67-4"><a href="#cb67-4" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> {</span>
<span id="cb67-5"><a href="#cb67-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"kneighborsregressor__n_neighbors"</span>: <span class="bu">range</span>(<span class="dv">2</span>, <span class="dv">25</span>),</span>
<span id="cb67-6"><a href="#cb67-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"kneighborsregressor__weights"</span>: [<span class="st">"uniform"</span>, <span class="st">"distance"</span>] </span>
<span id="cb67-7"><a href="#cb67-7" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb67-8"><a href="#cb67-8" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> make_pipeline( StandardScaler(), KNeighborsRegressor() )</span>
<span id="cb67-9"><a href="#cb67-9" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> GridSearchCV(</span>
<span id="cb67-10"><a href="#cb67-10" aria-hidden="true" tabindex="-1"></a>    knn, grid, </span>
<span id="cb67-11"><a href="#cb67-11" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>kf, </span>
<span id="cb67-12"><a href="#cb67-12" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb67-13"><a href="#cb67-13" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb67-14"><a href="#cb67-14" aria-hidden="true" tabindex="-1"></a>optim.fit(X_train, y_train)</span>
<span id="cb67-15"><a href="#cb67-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb67-16"><a href="#cb67-16" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"best kNN CoD: </span><span class="sc">{</span>optim<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>best kNN CoD: 0.7439</code></pre>
</div>
</div>
<p>As you can see above, we got some improvement over the linear regressor.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_vcdc7ddk&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_wri6tnzb" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.14" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
<p><br> <a href="_media/memes/not-a-classifier.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="_media/memes/not-a-classifier.jpg" class="img-fluid"></a></p>
</div></div></section>
<section id="decision-tree" class="level3 page-columns page-full" data-number="5.4.2">
<h3 data-number="5.4.2" class="anchored" data-anchor-id="decision-tree"><span class="header-section-number">5.4.2</span> Decision tree</h3>
<p>Recall that a decision tree recursively divides the examples into subsets. As with kNN regression, we can replace taking a classification vote over a leaf subset with taking a mean or median of the values in the leaf. As in classification, a decision tree regressor seeks the best possible split of samples along coordinate planes. A proposal to split into subsets <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> is assigned the weighted score <span class="math display">\[
Q = |S| H(S) + |T| H(T),
\]</span> where <span class="math inline">\(H\)</span> is an empirical error measure. The split location is chosen to minimize <span class="math inline">\(Q\)</span>. After the process is applied recursively, each leaf of the decision tree contains a subset of the training samples. There are two common variations:</p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_5zlo1g14&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_yzrae7bx" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.4.2" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><ol type="1">
<li>The mean of the leaf labels is the regression value, and <span class="math inline">\(H\)</span> is mean square error from the prediction.</li>
<li>The median of the leaf labels is the regression value, and <span class="math inline">\(H\)</span> is the mean absolute error from the prediction.</li>
</ol>
<div id="exm-regression-split" class="theorem example" data-chapter="5" type="✍️" data-description="Creating a decision tree regressor">
<p><span class="theorem-title"><strong>Example 5.17</strong></span> Suppose we are given the observations <span class="math inline">\(x_i=i\)</span>, <span class="math inline">\(i=1,\ldots,4\)</span>, where <span class="math inline">\(y_1=2\)</span>, <span class="math inline">\(y_2=-1\)</span>, <span class="math inline">\(y_3=1\)</span>, <span class="math inline">\(y_4=0\)</span>. Let’s find the decision tree regressor using medians.</p>
<p>The original value set has median <span class="math inline">\(\frac{1}{2}\)</span>, so its MAE is <span class="math display">\[
\frac{1}{4} \left( \abs{2-\tfrac{1}{2}} +  \abs{{-1}-\tfrac{1}{2}} + \abs{1-\tfrac{1}{2}} + \abs{0-\tfrac{1}{2}}  \right) = \frac{1}{4}\cdot \frac{8}{2} = 1.
\]</span> Thus, the <span class="math inline">\(Q\)</span>-score of the full set is 4.</p>
<p>There are three allowable ways to split the data, as it is ordered from left to right:</p>
<ul>
<li><span class="math inline">\(S=\{2\},\,T=\{-1,1,0\}\)</span>. Note that <span class="math inline">\(\abs{S}=1\)</span> and <span class="math inline">\(\abs{T}=3\)</span>, so <span class="math display">\[
\begin{split}
Q &amp;= 1\left[ \frac{1}{1} |2-2|  \right] +  3 \left[ \frac{1}{3}\bigl( | -1-0 | + |1-0| + |0-0|  \bigr)  \right]\\
&amp;=  0 + 2 = 2.
\end{split}
\]</span></li>
<li><span class="math inline">\(S=\{2,-1\},\, T=\{1,0\}\)</span> <span class="math display">\[
\begin{split}
  Q &amp;= 2\left[ \frac{1}{2}\bigl( \left| 2-\tfrac{1}{2} \right| + \left| -1-\tfrac{1}{2} \right| \bigr)  \right] +  2 \left[ \frac{1}{2}\bigl( \left|1-\tfrac{1}{2} \right| + \left|0-\tfrac{1}{2} \right|  \bigr)  \right]\\
  &amp;=  3 + 1 = 4.
\end{split}
\]</span></li>
<li><span class="math inline">\(S=\{2,-1,1\},\, T=\{0\}\)</span> <span class="math display">\[
\begin{split}
  Q &amp;= 3\left[ \frac{1}{3}\bigl( \left| 2-1 \right| + \left| -1-1 \right|+ |1-1| \bigr)  \right] +  1 \left[ \frac{1}{1} \left|0-0 \right|  \right]\\
  &amp;=  3 + 0 = 3.
\end{split}
\]</span></li>
</ul>
<p>Thus, the first split above produces the smallest Q, and it improves on the original set.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_pjw3mdbc&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_4aa35vvi" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.15" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div id="exm-regression-tree" class="theorem example" data-chapter="5" type="💻" data-description="Tree regression on a small dataset">
<p><span class="theorem-title"><strong>Example 5.18</strong></span> Here is some simple 2D data:</p>
<div id="b61bfad1" class="cell" data-execution_count="48">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> default_rng(<span class="dv">1</span>)</span>
<span id="cb69-2"><a href="#cb69-2" aria-hidden="true" tabindex="-1"></a>x1 <span class="op">=</span> rng.random((<span class="dv">10</span>,<span class="dv">2</span>))</span>
<span id="cb69-3"><a href="#cb69-3" aria-hidden="true" tabindex="-1"></a>x1[:,<span class="dv">0</span>] <span class="op">-=</span> <span class="fl">0.25</span></span>
<span id="cb69-4"><a href="#cb69-4" aria-hidden="true" tabindex="-1"></a>x2 <span class="op">=</span> rng.random((<span class="dv">10</span>,<span class="dv">2</span>))</span>
<span id="cb69-5"><a href="#cb69-5" aria-hidden="true" tabindex="-1"></a>x2[:,<span class="dv">0</span>] <span class="op">+=</span> <span class="fl">0.25</span></span>
<span id="cb69-6"><a href="#cb69-6" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> np.vstack((x1,x2))</span>
<span id="cb69-7"><a href="#cb69-7" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> np.exp( X[:,<span class="dv">0</span>]<span class="op">-</span><span class="dv">2</span><span class="op">*</span>X[:,<span class="dv">1</span>]<span class="op">**</span><span class="dv">2</span><span class="op">+</span>X[:,<span class="dv">0</span>]<span class="op">*</span>X[:,<span class="dv">1</span>] )</span>
<span id="cb69-8"><a href="#cb69-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb69-9"><a href="#cb69-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.DataFrame({<span class="st">"x₁"</span>: X[:,<span class="dv">0</span>], <span class="st">"x₂"</span>: X[:,<span class="dv">1</span>], <span class="st">"y"</span>: y})</span>
<span id="cb69-10"><a href="#cb69-10" aria-hidden="true" tabindex="-1"></a>sns.scatterplot(data<span class="op">=</span>df, x<span class="op">=</span><span class="st">"x₁"</span>, y<span class="op">=</span><span class="st">"x₂"</span>, hue<span class="op">=</span><span class="st">"y"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-48-output-1.png" width="589" height="429" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>The default in <code>sklearn</code> is to use means on leaves and MSE (called <code>squared_error</code> in sklearn) as the quality measure. Here is a shallow tree fitted to the data:</p>
<div id="8e969af1" class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb70"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb70-1"><a href="#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.tree <span class="im">import</span> DecisionTreeRegressor, plot_tree</span>
<span id="cb70-2"><a href="#cb70-2" aria-hidden="true" tabindex="-1"></a>dtree <span class="op">=</span> DecisionTreeRegressor(max_depth<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb70-3"><a href="#cb70-3" aria-hidden="true" tabindex="-1"></a>dtree.fit(X, y)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<style>#sk-container-id-2 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-2 {
  color: var(--sklearn-color-text);
}

#sk-container-id-2 pre {
  padding: 0;
}

#sk-container-id-2 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-2 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-2 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-2 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-2 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-2 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-2 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-2 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-2 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-2 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-2 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-2 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-2 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-2 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-2 div.sk-label label.sk-toggleable__label,
#sk-container-id-2 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-2 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-2 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-2 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-2 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-2 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-2 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-2 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-2 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-2 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-2 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-2 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</a></style><div id="sk-container-id-2" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>DecisionTreeRegressor(max_depth=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox" checked=""><label for="sk-estimator-id-2" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;DecisionTreeRegressor<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.tree.DecisionTreeRegressor.html">?<span>Documentation for DecisionTreeRegressor</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>DecisionTreeRegressor(max_depth=2)</pre></div> </div></div></div></div>
</div>
</div>
<div id="75da0baa" class="cell" data-execution_count="50">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib.pyplot <span class="im">import</span> figure</span>
<span id="cb71-2"><a href="#cb71-2" aria-hidden="true" tabindex="-1"></a>figure(figsize<span class="op">=</span>(<span class="dv">6</span>,<span class="dv">4</span>), dpi<span class="op">=</span><span class="dv">160</span>)</span>
<span id="cb71-3"><a href="#cb71-3" aria-hidden="true" tabindex="-1"></a>plot_tree(dtree,feature_names<span class="op">=</span>[<span class="st">"x₁"</span>,<span class="st">"x₂"</span>])<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-50-output-1.png" width="776" height="524" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>All of the original samples end up in one of the four leaves. We can find out the tree node number that each sample ends up at using <code>apply</code>:</p>
<div id="034c5601" class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb72"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb72-1"><a href="#cb72-1" aria-hidden="true" tabindex="-1"></a>leaf <span class="op">=</span> dtree.<span class="bu">apply</span>(X)</span>
<span id="cb72-2"><a href="#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(leaf)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[3 3 2 5 2 2 3 2 2 2 5 6 5 5 3 5 6 6 2 5]</code></pre>
</div>
</div>
<p>From the above we deduce that the leaves are the nodes numbered 2, 3, 5, and 6. With some pandas grouping, we can find out the mean value for the samples within each of these:</p>
<div id="e2e40894" class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1" aria-hidden="true" tabindex="-1"></a>leaves <span class="op">=</span> pd.DataFrame( {<span class="st">"y"</span>: y, <span class="st">"leaf"</span>: leaf} )</span>
<span id="cb74-2"><a href="#cb74-2" aria-hidden="true" tabindex="-1"></a>leaves.groupby(<span class="st">"leaf"</span>)[<span class="st">"y"</span>].mean()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>leaf
2    0.911328
3    0.270725
5    2.378427
6    1.003786
Name: y, dtype: float64</code></pre>
</div>
</div>
<p>All values of the regressor will be one of the four values above. This is exactly what is done internally by the <code>predict</code> method of the regressor:</p>
<div id="c8a7d675" class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>( dtree.predict(X) )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[0.27072468 0.27072468 0.91132782 2.37842709 0.91132782 0.91132782
 0.27072468 0.91132782 0.91132782 0.91132782 2.37842709 1.00378567
 2.37842709 2.37842709 0.27072468 2.37842709 1.00378567 1.00378567
 0.91132782 2.37842709]</code></pre>
</div>
</div>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_bpcrc2rs&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_nviq9get" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.16" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><div id="exm-regression-tree-demo" class="theorem example" data-chapter="5" type="💻" data-description="Random forest for regression">
<p><span class="theorem-title"><strong>Example 5.19</strong></span> Continuing with the data from <a href="#exm-regression-knn-demo" class="quarto-xref">Example&nbsp;<span>5.16</span></a>, we find that we can do even better with a random forest of decision tree regressors:</p>
<div id="4b281c6a" class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> cars[features]</span>
<span id="cb78-2"><a href="#cb78-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> cars[<span class="st">"mpg"</span>]</span>
<span id="cb78-3"><a href="#cb78-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-4"><a href="#cb78-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb78-5"><a href="#cb78-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb78-6"><a href="#cb78-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">302</span></span>
<span id="cb78-7"><a href="#cb78-7" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-8"><a href="#cb78-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-9"><a href="#cb78-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.ensemble <span class="im">import</span> RandomForestRegressor</span>
<span id="cb78-10"><a href="#cb78-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-11"><a href="#cb78-11" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> {</span>
<span id="cb78-12"><a href="#cb78-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_depth"</span>: <span class="bu">range</span>(<span class="dv">3</span>, <span class="dv">8</span>),</span>
<span id="cb78-13"><a href="#cb78-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">"max_samples"</span>: np.arange(<span class="fl">0.2</span>, <span class="fl">0.6</span>, <span class="fl">0.1</span>),</span>
<span id="cb78-14"><a href="#cb78-14" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb78-15"><a href="#cb78-15" aria-hidden="true" tabindex="-1"></a>knn <span class="op">=</span> RandomForestRegressor(n_estimators<span class="op">=</span><span class="dv">60</span>)</span>
<span id="cb78-16"><a href="#cb78-16" aria-hidden="true" tabindex="-1"></a>optim <span class="op">=</span> GridSearchCV(</span>
<span id="cb78-17"><a href="#cb78-17" aria-hidden="true" tabindex="-1"></a>    knn,</span>
<span id="cb78-18"><a href="#cb78-18" aria-hidden="true" tabindex="-1"></a>    grid, </span>
<span id="cb78-19"><a href="#cb78-19" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>kf, </span>
<span id="cb78-20"><a href="#cb78-20" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb78-21"><a href="#cb78-21" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb78-22"><a href="#cb78-22" aria-hidden="true" tabindex="-1"></a>optim.fit(X_train, y_train)</span>
<span id="cb78-23"><a href="#cb78-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb78-24"><a href="#cb78-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"best forest CoD: </span><span class="sc">{</span>optim<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.4f}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>best forest CoD: 0.7410</code></pre>
</div>
</div>
</div>
</section>
</section>
<section id="logistic-regression" class="level2 page-columns page-full" data-number="5.5">
<h2 data-number="5.5" class="anchored" data-anchor-id="logistic-regression"><span class="header-section-number">5.5</span> Logistic regression</h2>
<p>Sometimes a regressed value is subject to certain known bounds or other conditions. A major example is probability, which has to be between 0 and 1 (inclusive).</p>
<p>A linear regressor, <span class="math inline">\(\hat{f}(\bfx) = \bfw^T \bfx\)</span> for a constant vector <span class="math inline">\(\bfw\)</span>, typically ranges over all of <span class="math inline">\((-\infty,\infty)\)</span>. In order to get a result that must lie within <span class="math inline">\([0,1]\)</span>, we can transform its output.</p>
<div id="def-regression-logistic" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.11</strong></span> The <strong>logistic function</strong> is <span class="math display">\[
\sigma(x) = \frac{1}{1+e^{-x}},
\]</span></p>
<p>defined for all real values of <span class="math inline">\(x\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_8ioj8704&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_zgf52cij" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.5" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>The logistic function takes the form of a smoothed step increasing from 0 to 1:</p>
<div id="4ec33262" class="cell" data-execution_count="55">
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-55-output-1.png" width="493" height="324" class="figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Given samples of a probability variable <span class="math inline">\(p(\bfx)\)</span>, the regression task is to find a weight vector <span class="math inline">\(\bfw\)</span> so that <span class="math display">\[
p \approx \sigma(\bfx^T\bfw).
\]</span></p>
<p>The result is known as <strong>logistic regression</strong>. A common way to use logistic regression is for binary classification. Suppose we have training samples <span class="math inline">\((\bfx_i, y_i)\)</span>, <span class="math inline">\(i=1,\ldots,n\)</span>, where for each <span class="math inline">\(i\)</span> either <span class="math inline">\(y_i=0\)</span> or <span class="math inline">\(y_i=1\)</span>. The resulting approximation to <span class="math inline">\(p\)</span> at some query <span class="math inline">\(\bfx\)</span> can then be interpreted as the probability of observing a 1 at <span class="math inline">\(\bfx\)</span>.</p>
<p>In order to fully specify the regressor, we need to specify a loss function to be optimized.</p>
<section id="loss-function" class="level3 page-columns page-full" data-number="5.5.1">
<h3 data-number="5.5.1" class="anchored" data-anchor-id="loss-function"><span class="header-section-number">5.5.1</span> Loss function</h3>
<p>Defining <span class="math inline">\(\hat{p}_i = \sigma(\bfx_i^T\bfw)\)</span> at all the training points, a straightforward loss function would be <span class="math display">\[
\sum_{i=1}^n \left( \hat{p}_i - y_i \right)^2.
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_o2jgpf6u&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_whltv54a" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Section 5.5.1" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div><p>However, it’s more common to use a different kind of loss function.</p>
<div id="def-regression-cross-entropy" class="theorem definition">
<p><span class="theorem-title"><strong>Definition 5.12</strong></span> For a logistic model with predictions <span class="math inline">\(\hat{p}_i = \sigma(\bfx_i^T\bfw)\)</span>, the <strong>cross-entropy</strong> loss function is</p>
<p><span id="eq-regression-cross-entropy"><span class="math display">\[
L(\bfw) = -\sum_{i=1}^n \left[ y_i \log(\hat{p}_i) + (1-y_i) \log(1-\hat{p}_i) \right],
\tag{5.13}\]</span></span></p>
<p>where every label <span class="math inline">\(y_i\)</span> takes the value 0 or 1.</p>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The logarithms in <a href="#eq-regression-cross-entropy" class="quarto-xref">Equation&nbsp;<span>5.13</span></a> can be in any base, since that choice only affects <span class="math inline">\(L\)</span> by a constant factor.</p>
</div>
</div>
<p>Suppose we choose some <span class="math inline">\(i\)</span> for which <span class="math inline">\(y_i=0\)</span>. Then the contribution of the <span class="math inline">\(i\)</span>th term in the sum of <a href="#eq-regression-cross-entropy" class="quarto-xref">Equation&nbsp;<span>5.13</span></a> is</p>
<p><span class="math display">\[
-\log(1-\hat{p}_i),
\]</span> which becomes infinite as <span class="math inline">\(\hat{p}_i\to 1^-\)</span>. In other words, the model is penalized heavily for being almost completely wrong about sample <span class="math inline">\(i\)</span>. Similarly, if <span class="math inline">\(y_i=1\)</span>, the contribution is</p>
<p><span class="math display">\[
-\log(\hat{p}_i),
\]</span></p>
<p>which becomes infinite as <span class="math inline">\(\hat{p}_i\to 0^+\)</span>. This sensitivity to the worst-case values of every <span class="math inline">\(\hat{p}_i\)</span> usually makes cross-entropy more successful than least squares for logistic regression.</p>
<div id="exm-regression-cross-entropy" class="theorem example" data-chapter="5" type="✍️" data-description="Cross-entropy">
<p><span class="theorem-title"><strong>Example 5.20</strong></span> Suppose we have true labels <span class="math inline">\(\bfy=[0,0,0,1]\)</span> and predicted probabilities <span class="math inline">\(\hat{\mathbf{p}} = [0.1,0.4,0.2,0.3].\)</span> This gives the following terms in the cross-entropy sum: <span class="math display">\[
\begin{split}
\cancel{y_1 \log(\hat{p}_1)} + (1-y_1) \log(1-\hat{p}_1) &amp;= \log(0.9), \\
\cancel{y_2 \log(\hat{p}_2)} + (1-y_2) \log(1-\hat{p}_2) &amp;= \log(0.6), \\
\cancel{y_3 \log(\hat{p}_3)} + (1-y_3) \log(1-\hat{p}_3) &amp;= \log(0.8), \\
y_4 \log(\hat{p}_4) + \cancel{(1-y_4) \log(1-\hat{p}_4)} &amp;= \log(0.3).\\
\end{split}
\]</span></p>
<p>Hence the total cross-entropy is <span class="math display">\[
-\log\bigl[ (0.9)(0.6)(0.8)(0.3)\bigr] = -\log(0.1296).
\]</span></p>
<p>Using the natural log, this is about <span class="math inline">\(2.043\)</span>.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<p><a href="_media/memes/cross-entropy.jpeg" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><img src="_media/memes/cross-entropy.jpeg" class="img-fluid"></a></p>
</div></div><p>Logistic regression does have a major disadvantage compared to linear regression: the minimization of loss does <em>not</em> lead to a linear problem for the weight vector <span class="math inline">\(\bfw\)</span>. The difference in practice is usually not a concern, though.</p>
<p>Cross-entropy is the default loss function for a <code>LogisticRegression</code> in scikit-learn. In <a href="#exm-regression-spam" class="quarto-xref">Example&nbsp;<span>5.21</span></a>, we use logistic regression on a binary classification problem, where we interpret the regression output as the probability of being in class 1, as opposed to class 0.</p>
<div id="exm-regression-spam" class="theorem example" data-chapter="5" type="💻" data-description="Logistic regression as a spam filter">
<p><span class="theorem-title"><strong>Example 5.21</strong></span> We will try logistic regression for a simple spam filter. The data set is based on work and personal emails for one individual. The features are calculated word and character frequencies, as well as the appearance of capital letters.</p>
<div id="5a5a5aef" class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1" aria-hidden="true" tabindex="-1"></a>spam <span class="op">=</span> pd.read_csv(<span class="st">"_datasets/spambase.csv"</span>)</span>
<span id="cb80-2"><a href="#cb80-2" aria-hidden="true" tabindex="-1"></a>spam.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="55">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr>
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">word_freq_make</th>
<th data-quarto-table-cell-role="th">word_freq_address</th>
<th data-quarto-table-cell-role="th">word_freq_all</th>
<th data-quarto-table-cell-role="th">word_freq_3d</th>
<th data-quarto-table-cell-role="th">word_freq_our</th>
<th data-quarto-table-cell-role="th">word_freq_over</th>
<th data-quarto-table-cell-role="th">word_freq_remove</th>
<th data-quarto-table-cell-role="th">word_freq_internet</th>
<th data-quarto-table-cell-role="th">word_freq_order</th>
<th data-quarto-table-cell-role="th">word_freq_mail</th>
<th data-quarto-table-cell-role="th">...</th>
<th data-quarto-table-cell-role="th">char_freq_%3B</th>
<th data-quarto-table-cell-role="th">char_freq_%28</th>
<th data-quarto-table-cell-role="th">char_freq_%5B</th>
<th data-quarto-table-cell-role="th">char_freq_%21</th>
<th data-quarto-table-cell-role="th">char_freq_%24</th>
<th data-quarto-table-cell-role="th">char_freq_%23</th>
<th data-quarto-table-cell-role="th">capital_run_length_average</th>
<th data-quarto-table-cell-role="th">capital_run_length_longest</th>
<th data-quarto-table-cell-role="th">capital_run_length_total</th>
<th data-quarto-table-cell-role="th">class</th>
</tr>
</thead>
<tbody>
<tr>
<td data-quarto-table-cell-role="th">0</td>
<td>0.00</td>
<td>0.64</td>
<td>0.64</td>
<td>0.0</td>
<td>0.32</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>...</td>
<td>0.00</td>
<td>0.000</td>
<td>0.0</td>
<td>0.778</td>
<td>0.000</td>
<td>0.000</td>
<td>3.756</td>
<td>61</td>
<td>278</td>
<td>1</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">1</td>
<td>0.21</td>
<td>0.28</td>
<td>0.50</td>
<td>0.0</td>
<td>0.14</td>
<td>0.28</td>
<td>0.21</td>
<td>0.07</td>
<td>0.00</td>
<td>0.94</td>
<td>...</td>
<td>0.00</td>
<td>0.132</td>
<td>0.0</td>
<td>0.372</td>
<td>0.180</td>
<td>0.048</td>
<td>5.114</td>
<td>101</td>
<td>1028</td>
<td>1</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">2</td>
<td>0.06</td>
<td>0.00</td>
<td>0.71</td>
<td>0.0</td>
<td>1.23</td>
<td>0.19</td>
<td>0.19</td>
<td>0.12</td>
<td>0.64</td>
<td>0.25</td>
<td>...</td>
<td>0.01</td>
<td>0.143</td>
<td>0.0</td>
<td>0.276</td>
<td>0.184</td>
<td>0.010</td>
<td>9.821</td>
<td>485</td>
<td>2259</td>
<td>1</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">3</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.0</td>
<td>0.63</td>
<td>0.00</td>
<td>0.31</td>
<td>0.63</td>
<td>0.31</td>
<td>0.63</td>
<td>...</td>
<td>0.00</td>
<td>0.137</td>
<td>0.0</td>
<td>0.137</td>
<td>0.000</td>
<td>0.000</td>
<td>3.537</td>
<td>40</td>
<td>191</td>
<td>1</td>
</tr>
<tr>
<td data-quarto-table-cell-role="th">4</td>
<td>0.00</td>
<td>0.00</td>
<td>0.00</td>
<td>0.0</td>
<td>0.63</td>
<td>0.00</td>
<td>0.31</td>
<td>0.63</td>
<td>0.31</td>
<td>0.63</td>
<td>...</td>
<td>0.00</td>
<td>0.135</td>
<td>0.0</td>
<td>0.135</td>
<td>0.000</td>
<td>0.000</td>
<td>3.537</td>
<td>40</td>
<td>191</td>
<td>1</td>
</tr>
</tbody>
</table>

<p>5 rows × 58 columns</p>
</div>
</div>
</div>
<p>The labels in this case are 0 for “ham” (not spam) and 1 for “spam”. We create a feature matrix and label vector, and split into train/test sets:</p>
<div id="31677911" class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> spam.drop(<span class="st">"class"</span>, axis<span class="op">=</span><span class="st">"columns"</span>)</span>
<span id="cb81-2"><a href="#cb81-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> spam[<span class="st">"class"</span>]</span>
<span id="cb81-3"><a href="#cb81-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb81-4"><a href="#cb81-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb81-5"><a href="#cb81-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb81-6"><a href="#cb81-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.2</span>,</span>
<span id="cb81-7"><a href="#cb81-7" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">19716</span></span>
<span id="cb81-8"><a href="#cb81-8" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We fit a logistic regression just like any other learner. (The meaning of the argument <code>penalty</code> in the call below is explained in <a href="#sec-regression-logistic-regular" class="quarto-xref"><span>Section 5.5.2</span></a>.)</p>
<div id="da4ec637" class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.linear_model <span class="im">import</span> LogisticRegression</span>
<span id="cb82-2"><a href="#cb82-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb82-3"><a href="#cb82-3" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression(penalty<span class="op">=</span><span class="va">None</span>)</span>
<span id="cb82-4"><a href="#cb82-4" aria-hidden="true" tabindex="-1"></a>logreg.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="57">
<style>#sk-container-id-3 {
  /* Definition of color scheme common for light and dark mode */
  --sklearn-color-text: black;
  --sklearn-color-line: gray;
  /* Definition of color scheme for unfitted estimators */
  --sklearn-color-unfitted-level-0: #fff5e6;
  --sklearn-color-unfitted-level-1: #f6e4d2;
  --sklearn-color-unfitted-level-2: #ffe0b3;
  --sklearn-color-unfitted-level-3: chocolate;
  /* Definition of color scheme for fitted estimators */
  --sklearn-color-fitted-level-0: #f0f8ff;
  --sklearn-color-fitted-level-1: #d4ebff;
  --sklearn-color-fitted-level-2: #b3dbfd;
  --sklearn-color-fitted-level-3: cornflowerblue;

  /* Specific color for light theme */
  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));
  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));
  --sklearn-color-icon: #696969;

  @media (prefers-color-scheme: dark) {
    /* Redefinition of color scheme for dark theme */
    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));
    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));
    --sklearn-color-icon: #878787;
  }
}

#sk-container-id-3 {
  color: var(--sklearn-color-text);
}

#sk-container-id-3 pre {
  padding: 0;
}

#sk-container-id-3 input.sk-hidden--visually {
  border: 0;
  clip: rect(1px 1px 1px 1px);
  clip: rect(1px, 1px, 1px, 1px);
  height: 1px;
  margin: -1px;
  overflow: hidden;
  padding: 0;
  position: absolute;
  width: 1px;
}

#sk-container-id-3 div.sk-dashed-wrapped {
  border: 1px dashed var(--sklearn-color-line);
  margin: 0 0.4em 0.5em 0.4em;
  box-sizing: border-box;
  padding-bottom: 0.4em;
  background-color: var(--sklearn-color-background);
}

#sk-container-id-3 div.sk-container {
  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`
     but bootstrap.min.css set `[hidden] { display: none !important; }`
     so we also need the `!important` here to be able to override the
     default hidden behavior on the sphinx rendered scikit-learn.org.
     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */
  display: inline-block !important;
  position: relative;
}

#sk-container-id-3 div.sk-text-repr-fallback {
  display: none;
}

div.sk-parallel-item,
div.sk-serial,
div.sk-item {
  /* draw centered vertical line to link estimators */
  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));
  background-size: 2px 100%;
  background-repeat: no-repeat;
  background-position: center center;
}

/* Parallel-specific style estimator block */

#sk-container-id-3 div.sk-parallel-item::after {
  content: "";
  width: 100%;
  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);
  flex-grow: 1;
}

#sk-container-id-3 div.sk-parallel {
  display: flex;
  align-items: stretch;
  justify-content: center;
  background-color: var(--sklearn-color-background);
  position: relative;
}

#sk-container-id-3 div.sk-parallel-item {
  display: flex;
  flex-direction: column;
}

#sk-container-id-3 div.sk-parallel-item:first-child::after {
  align-self: flex-end;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:last-child::after {
  align-self: flex-start;
  width: 50%;
}

#sk-container-id-3 div.sk-parallel-item:only-child::after {
  width: 0;
}

/* Serial-specific style estimator block */

#sk-container-id-3 div.sk-serial {
  display: flex;
  flex-direction: column;
  align-items: center;
  background-color: var(--sklearn-color-background);
  padding-right: 1em;
  padding-left: 1em;
}


/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is
clickable and can be expanded/collapsed.
- Pipeline and ColumnTransformer use this feature and define the default style
- Estimators will overwrite some part of the style using the `sk-estimator` class
*/

/* Pipeline and ColumnTransformer style (default) */

#sk-container-id-3 div.sk-toggleable {
  /* Default theme specific background. It is overwritten whether we have a
  specific estimator or a Pipeline/ColumnTransformer */
  background-color: var(--sklearn-color-background);
}

/* Toggleable label */
#sk-container-id-3 label.sk-toggleable__label {
  cursor: pointer;
  display: block;
  width: 100%;
  margin-bottom: 0;
  padding: 0.5em;
  box-sizing: border-box;
  text-align: center;
}

#sk-container-id-3 label.sk-toggleable__label-arrow:before {
  /* Arrow on the left of the label */
  content: "▸";
  float: left;
  margin-right: 0.25em;
  color: var(--sklearn-color-icon);
}

#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {
  color: var(--sklearn-color-text);
}

/* Toggleable content - dropdown */

#sk-container-id-3 div.sk-toggleable__content {
  max-height: 0;
  max-width: 0;
  overflow: hidden;
  text-align: left;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content pre {
  margin: 0.2em;
  border-radius: 0.25em;
  color: var(--sklearn-color-text);
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-toggleable__content.fitted pre {
  /* unfitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {
  /* Expand drop-down */
  max-height: 200px;
  max-width: 100%;
  overflow: auto;
}

#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {
  content: "▾";
}

/* Pipeline/ColumnTransformer-specific style */

#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator-specific style */

/* Colorize estimator box */
#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

#sk-container-id-3 div.sk-label label.sk-toggleable__label,
#sk-container-id-3 div.sk-label label {
  /* The background is the default theme color */
  color: var(--sklearn-color-text-on-default-background);
}

/* On hover, darken the color of the background */
#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-unfitted-level-2);
}

/* Label box, darken color on hover, fitted */
#sk-container-id-3 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {
  color: var(--sklearn-color-text);
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Estimator label */

#sk-container-id-3 div.sk-label label {
  font-family: monospace;
  font-weight: bold;
  display: inline-block;
  line-height: 1.2em;
}

#sk-container-id-3 div.sk-label-container {
  text-align: center;
}

/* Estimator-specific */
#sk-container-id-3 div.sk-estimator {
  font-family: monospace;
  border: 1px dotted var(--sklearn-color-border-box);
  border-radius: 0.25em;
  box-sizing: border-box;
  margin-bottom: 0.5em;
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-0);
}

#sk-container-id-3 div.sk-estimator.fitted {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-0);
}

/* on hover */
#sk-container-id-3 div.sk-estimator:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-2);
}

#sk-container-id-3 div.sk-estimator.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-2);
}

/* Specification for estimator info (e.g. "i" and "?") */

/* Common style for "i" and "?" */

.sk-estimator-doc-link,
a:link.sk-estimator-doc-link,
a:visited.sk-estimator-doc-link {
  float: right;
  font-size: smaller;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1em;
  height: 1em;
  width: 1em;
  text-decoration: none !important;
  margin-left: 1ex;
  /* unfitted */
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
  color: var(--sklearn-color-unfitted-level-1);
}

.sk-estimator-doc-link.fitted,
a:link.sk-estimator-doc-link.fitted,
a:visited.sk-estimator-doc-link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
div.sk-estimator:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover,
div.sk-label-container:hover .sk-estimator-doc-link:hover,
.sk-estimator-doc-link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover,
div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,
.sk-estimator-doc-link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

/* Span, style for the box shown on hovering the info icon */
.sk-estimator-doc-link span {
  display: none;
  z-index: 9999;
  position: relative;
  font-weight: normal;
  right: .2ex;
  padding: .5ex;
  margin: .5ex;
  width: min-content;
  min-width: 20ex;
  max-width: 50ex;
  color: var(--sklearn-color-text);
  box-shadow: 2pt 2pt 4pt #999;
  /* unfitted */
  background: var(--sklearn-color-unfitted-level-0);
  border: .5pt solid var(--sklearn-color-unfitted-level-3);
}

.sk-estimator-doc-link.fitted span {
  /* fitted */
  background: var(--sklearn-color-fitted-level-0);
  border: var(--sklearn-color-fitted-level-3);
}

.sk-estimator-doc-link:hover span {
  display: block;
}

/* "?"-specific style due to the `<a>` HTML tag */

#sk-container-id-3 a.estimator_doc_link {
  float: right;
  font-size: 1rem;
  line-height: 1em;
  font-family: monospace;
  background-color: var(--sklearn-color-background);
  border-radius: 1rem;
  height: 1rem;
  width: 1rem;
  text-decoration: none;
  /* unfitted */
  color: var(--sklearn-color-unfitted-level-1);
  border: var(--sklearn-color-unfitted-level-1) 1pt solid;
}

#sk-container-id-3 a.estimator_doc_link.fitted {
  /* fitted */
  border: var(--sklearn-color-fitted-level-1) 1pt solid;
  color: var(--sklearn-color-fitted-level-1);
}

/* On hover */
#sk-container-id-3 a.estimator_doc_link:hover {
  /* unfitted */
  background-color: var(--sklearn-color-unfitted-level-3);
  color: var(--sklearn-color-background);
  text-decoration: none;
}

#sk-container-id-3 a.estimator_doc_link.fitted:hover {
  /* fitted */
  background-color: var(--sklearn-color-fitted-level-3);
}
</a></style><div id="sk-container-id-3" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>LogisticRegression(penalty=None)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item"><div class="sk-estimator fitted sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox" checked=""><label for="sk-estimator-id-3" class="sk-toggleable__label fitted sk-toggleable__label-arrow fitted">&nbsp;&nbsp;LogisticRegression<a class="sk-estimator-doc-link fitted" rel="noreferrer" target="_blank" href="https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html">?<span>Documentation for LogisticRegression</span></a><span class="sk-estimator-doc-link fitted">i<span>Fitted</span></span></label><div class="sk-toggleable__content fitted"><pre>LogisticRegression(penalty=None)</pre></div> </div></div></div></div>
</div>
</div>
<p>Despite the name, though, scikit-learn treats a <code>LogisticRegression</code> object like a classifier, and its preductions are true/false:</p>
<div id="a853cfd6" class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb83-1"><a href="#cb83-1" aria-hidden="true" tabindex="-1"></a>logreg.predict( X_test.iloc[:<span class="dv">5</span>,:] )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>array([1, 0, 0, 0, 0])</code></pre>
</div>
</div>
<p>The default scoring function is classification accuracy:</p>
<div id="10fb8d7d" class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb85-1"><a href="#cb85-1" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> logreg.score(X_test, y_test)</span>
<span id="cb85-2"><a href="#cb85-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"spam classification accuracy is </span><span class="sc">{</span>acc<span class="sc">:.2%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>spam classification accuracy is 93.81%</code></pre>
</div>
</div>
<p>The actual logistic outputs are the <em>probabilistic</em> predictions:</p>
<div id="07de8ba1" class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb87-1"><a href="#cb87-1" aria-hidden="true" tabindex="-1"></a>logreg.predict_proba( X_test.iloc[:<span class="dv">5</span>,:] )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>array([[4.90657586e-01, 5.09342414e-01],
       [9.84926888e-01, 1.50731119e-02],
       [5.41538534e-01, 4.58461466e-01],
       [7.45802020e-01, 2.54197980e-01],
       [1.00000000e+00, 6.21859861e-12]])</code></pre>
</div>
</div>
<p>From these, we could use ROC and AUC metrics as we did in <a href="classification.html#sec-class-probabilistic" class="quarto-xref"><span>Section 3.5</span></a>.</p>
<p>Let’s repeat the computation with a standardization pipeline, so that we can easily compare the magnitudes of the weights in the coefficient vector:</p>
<div id="07b6ea25" class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb89-1"><a href="#cb89-1" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), logreg)</span>
<span id="cb89-2"><a href="#cb89-2" aria-hidden="true" tabindex="-1"></a>pipe.fit(X_train, y_train)</span>
<span id="cb89-3"><a href="#cb89-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-4"><a href="#cb89-4" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> pd.Series( logreg.coef_[<span class="dv">0</span>], index<span class="op">=</span>X.columns) </span>
<span id="cb89-5"><a href="#cb89-5" aria-hidden="true" tabindex="-1"></a>weights <span class="op">=</span> weights.sort_values()</span>
<span id="cb89-6"><a href="#cb89-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb89-7"><a href="#cb89-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"most hammy features:"</span>)</span>
<span id="cb89-8"><a href="#cb89-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(weights[:<span class="dv">4</span>])</span>
<span id="cb89-9"><a href="#cb89-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb89-10"><a href="#cb89-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"most spammy features:"</span>)</span>
<span id="cb89-11"><a href="#cb89-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(weights[<span class="op">-</span><span class="dv">4</span>:])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>most hammy features:
word_freq_george    -30.387745
word_freq_cs        -12.485031
word_freq_hp         -3.590711
word_freq_meeting    -1.638417
dtype: float64

most spammy features:
word_freq_remove              0.862786
char_freq_%24                 1.111789
capital_run_length_longest    1.892760
word_freq_3d                  4.830594
dtype: float64</code></pre>
</div>
</div>
<div class="callout callout-style-default callout-caution callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Caution
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a <code>LinearRegression</code> and its regularized siblings, the <code>coef_</code> property of a trained learner is a vector of coefficients, while for <code>LogisticRegression</code>, it is a matrix with one row. The reason is that logistic regression is often applied to a multiclass problem, and there is one row for each version of the one-vs-rest fit.</p>
</div>
</div>
<p>We see above that the word “george” is a strong counter-indicator for spam; remember that this data set comes from an individual’s inbox. Its presence makes the inner product <span class="math inline">\(\bfx^T\bfw\)</span> more negative, which drives the logistic function closer to 0. Conversely, the presence of consecutive capital letters increases the inner product and pushes the probability of spam closer to 1.</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_d2b8yjqe&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_dun9p0b8" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.19" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div></section>
<section id="sec-regression-logistic-regular" class="level3 page-columns page-full" data-number="5.5.2">
<h3 data-number="5.5.2" class="anchored" data-anchor-id="sec-regression-logistic-regular"><span class="header-section-number">5.5.2</span> Regularization</h3>
<p>As with other forms of regression, the loss function may be regularized using the ridge or LASSO penalty. The standard formulation is</p>
<p><span id="eq-regression-logistic-regular"><span class="math display">\[
\widetilde{L}(\bfw) = C \, L(\bfw) + \norm{\bfw},
\tag{5.14}\]</span></span></p>
<p>where the vector norm is either the 1-norm (LASSO style) or 2-norm (ridge style). In either case, <span class="math inline">\(C\)</span> is a positive hyperparameter. For <span class="math inline">\(C\)</span> close to zero, the regression is mainly concerned with the penalty term, and as <span class="math inline">\(C\)</span> increases, the regression gradually pays more and more attention to the data, at the expense of the penalty term.</p>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The parameter <span class="math inline">\(C\)</span> functions like the inverse of the regularization parameter <span class="math inline">\(\alpha\)</span> we used in the linear regressor. It’s because of a different convention chosen historically.</p>
</div>
</div>
<div id="exm-regression-spam-regular" class="theorem example" data-chapter="5" type="💻" data-description="Regularization for the spam filter regressor">
<p><span class="theorem-title"><strong>Example 5.22</strong></span> We will continue <a href="#exm-regression-spam" class="quarto-xref">Example&nbsp;<span>5.21</span></a> with an exploration of regularization. When using norm-based regularization, it’s good practice to standardize the variables, so we will use a standardization pipeline:</p>
<p>In <a href="#exm-regression-spam" class="quarto-xref">Example&nbsp;<span>5.21</span></a>, we disabled regularization. The default call uses 2-norm regularization with <span class="math inline">\(C=1\)</span>, which, in this case, improves the accuracy a bit:</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>The use of the <code>solver</code> keyword argument in <code>LogisticRegression</code> is optional, but the default choice does not work with the LASSO-style penalty term.</p>
</div>
</div>
<div id="7ba28562" class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb91-1"><a href="#cb91-1" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>)</span>
<span id="cb91-2"><a href="#cb91-2" aria-hidden="true" tabindex="-1"></a>logreg.fit(X_train, y_train)</span>
<span id="cb91-3"><a href="#cb91-3" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> logreg.score(X_test, y_test)</span>
<span id="cb91-4"><a href="#cb91-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"accuracy with default regularization is </span><span class="sc">{</span>acc<span class="sc">:.2%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy with default regularization is 93.81%</code></pre>
</div>
</div>
<p>The regularization parameter in a <code>LogisticRegression</code> can be set using the <code>C=</code> keyword:</p>
<div id="a25deb51" class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb93-1"><a href="#cb93-1" aria-hidden="true" tabindex="-1"></a>logreg <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>, C<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb93-2"><a href="#cb93-2" aria-hidden="true" tabindex="-1"></a>logreg.fit(X_train, y_train)</span>
<span id="cb93-3"><a href="#cb93-3" aria-hidden="true" tabindex="-1"></a>acc <span class="op">=</span> logreg.score(X_test, y_test)</span>
<span id="cb93-4"><a href="#cb93-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"accuracy with C=0.5 is </span><span class="sc">{</span>acc<span class="sc">:.2%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy with C=0.5 is 93.92%</code></pre>
</div>
</div>
<p>A validation-based grid search is one way to look for the optimal regularization. Usually, we want the grid values of <span class="math inline">\(C\)</span> to be spaced exponentially, not equally. For example,</p>
<div id="ea145be0" class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb95-1"><a href="#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="dv">10</span> <span class="op">**</span> np.linspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">13</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>array([1.00000000e-02, 2.15443469e-02, 4.64158883e-02, 1.00000000e-01,
       2.15443469e-01, 4.64158883e-01, 1.00000000e+00, 2.15443469e+00,
       4.64158883e+00, 1.00000000e+01, 2.15443469e+01, 4.64158883e+01,
       1.00000000e+02])</code></pre>
</div>
</div>
<p>We use that strategy along with a search over both the 2-norm and the 1-norm for the regularization penalty term:</p>
<div id="78bc1d73" class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb97-1"><a href="#cb97-1" aria-hidden="true" tabindex="-1"></a>grid <span class="op">=</span> { <span class="st">"logisticregression__C"</span>: <span class="dv">10</span> <span class="op">**</span> np.linspace(<span class="op">-</span><span class="dv">1</span>, <span class="dv">4</span>, <span class="dv">40</span>), </span>
<span id="cb97-2"><a href="#cb97-2" aria-hidden="true" tabindex="-1"></a>         <span class="co"># ridge and LASSO cases:</span></span>
<span id="cb97-3"><a href="#cb97-3" aria-hidden="true" tabindex="-1"></a>         <span class="st">"logisticregression__penalty"</span>: [<span class="st">"l2"</span>, <span class="st">"l1"</span>]   </span>
<span id="cb97-4"><a href="#cb97-4" aria-hidden="true" tabindex="-1"></a>        }</span>
<span id="cb97-5"><a href="#cb97-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-6"><a href="#cb97-6" aria-hidden="true" tabindex="-1"></a>learner <span class="op">=</span> make_pipeline(</span>
<span id="cb97-7"><a href="#cb97-7" aria-hidden="true" tabindex="-1"></a>    StandardScaler(),</span>
<span id="cb97-8"><a href="#cb97-8" aria-hidden="true" tabindex="-1"></a>    LogisticRegression( solver<span class="op">=</span><span class="st">"liblinear"</span> )</span>
<span id="cb97-9"><a href="#cb97-9" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb97-10"><a href="#cb97-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-11"><a href="#cb97-11" aria-hidden="true" tabindex="-1"></a>kf <span class="op">=</span> StratifiedKFold(n_splits<span class="op">=</span><span class="dv">6</span>, shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">302</span>)</span>
<span id="cb97-12"><a href="#cb97-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-13"><a href="#cb97-13" aria-hidden="true" tabindex="-1"></a>search <span class="op">=</span> GridSearchCV(</span>
<span id="cb97-14"><a href="#cb97-14" aria-hidden="true" tabindex="-1"></a>    learner, grid, </span>
<span id="cb97-15"><a href="#cb97-15" aria-hidden="true" tabindex="-1"></a>    cv<span class="op">=</span>kf,</span>
<span id="cb97-16"><a href="#cb97-16" aria-hidden="true" tabindex="-1"></a>    n_jobs<span class="op">=-</span><span class="dv">1</span></span>
<span id="cb97-17"><a href="#cb97-17" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb97-18"><a href="#cb97-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-19"><a href="#cb97-19" aria-hidden="true" tabindex="-1"></a>search.fit(X_train, y_train)</span>
<span id="cb97-20"><a href="#cb97-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-21"><a href="#cb97-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Best parameters:"</span>)</span>
<span id="cb97-22"><a href="#cb97-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(search.best_params_)</span>
<span id="cb97-23"><a href="#cb97-23" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>()</span>
<span id="cb97-24"><a href="#cb97-24" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Best score is </span><span class="sc">{</span>search<span class="sc">.</span>best_score_<span class="sc">:.2%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Best parameters:
{'logisticregression__C': 1.0608183551394483, 'logisticregression__penalty': 'l1'}

Best score is 92.55%</code></pre>
</div>
</div>
<p>In this case, we failed to improve on the default regularization. (Recall that cross-validation means that each learner is trained on less data, so the grid metrics might not be completely accurate.)</p>
</div>

<div class="no-row-height column-margin column-container"><div class="">
<div style="max-width:304px">
<div style="position:relative;padding-bottom:75.986842105263%">
<iframe id="kaltura_player" src="https://cdnapisec.kaltura.com/p/2358381/sp/235838100/embedIframeJs/uiconf_id/43030021/partner_id/2358381?iframeembed=true&amp;playerId=kaltura_player&amp;entry_id=1_y9qykrqv&amp;flashvars[streamerType]=auto&amp;flashvars[localizationCode]=en&amp;flashvars[sideBarContainer.plugin]=true&amp;flashvars[sideBarContainer.position]=left&amp;flashvars[sideBarContainer.clickToClose]=true&amp;flashvars[chapters.plugin]=true&amp;flashvars[chapters.layout]=vertical&amp;flashvars[chapters.thumbnailRotator]=false&amp;flashvars[streamSelector.plugin]=true&amp;flashvars[EmbedPlayer.SpinnerTarget]=videoHolder&amp;flashvars[dualScreen.plugin]=true&amp;flashvars[hotspots.plugin]=1&amp;flashvars[Kaltura.addCrossoriginToIframe]=true&amp;&amp;wid=1_20nobqz7" width="304" height="231" allowfullscreen="" webkitallowfullscreen="" mozallowfullscreen="" allow="autoplay *; fullscreen *; encrypted-media *" sandbox="allow-downloads allow-forms allow-same-origin allow-scripts allow-top-navigation allow-pointer-lock allow-popups allow-modals allow-orientation-lock allow-popups-to-escape-sandbox allow-presentation allow-top-navigation-by-user-activation" frameborder="0" title="Example 5.20" style="position:absolute;top:0;left:0;width:100%;height:100%">
</iframe>
</div>
</div>
</div></div></section>
<section id="multiclass-classification" class="level3" data-number="5.5.3">
<h3 data-number="5.5.3" class="anchored" data-anchor-id="multiclass-classification"><span class="header-section-number">5.5.3</span> Multiclass classification</h3>
<p>When there are more than two unique labels possible in a classification, logistic regression can be extended through the one-vs-rest (OVR) paradigm we have used previously. Given <span class="math inline">\(K\)</span> classes, there are <span class="math inline">\(K\)</span> binary regressors fit for the outcomes “class 1/not class 1,” “class 2/not class 2,” and so on. These give <span class="math inline">\(K\)</span> different weight vectors, <span class="math inline">\(\bfw_1,\ldots,\bfw_K\)</span>.</p>
<p>For a query vector <span class="math inline">\(\bfx\)</span>, we can predict the probabilities for it being in each class:</p>
<p><span class="math display">\[
\hat{q}_{k}(\bfx) = \sigma(\bfx^T \bfw_k), \qquad k=1,\ldots,K.
\]</span></p>
<p>However, since the regressions were performed separately, there is no reason to think the <span class="math inline">\(q_k\)</span> are probabilities that sum to 1 over all the classes. So we must normalize them:</p>
<p><span class="math display">\[
\hat{p}_{k}(\bfx) = \frac{\hat{q}_{k}(\bfx)}{\sum_{k=1}^K \hat{q}_{k}(\bfx)}.
\]</span></p>
<p>Now we can interpret <span class="math inline">\(\hat{p}_{k}(\bfx)\)</span> as the probability of <span class="math inline">\(\bfx\)</span> belonging to class <span class="math inline">\(k\)</span>.</p>
<div id="exm-regression-logistic-ovr" class="theorem example" data-chapter="5" type="✍️" data-description="Multiclass logistic regression">
<p><span class="theorem-title"><strong>Example 5.23</strong></span> Suppose we have <span class="math inline">\(d=2\)</span> features and three classes, and that the three logistic regressions gave the weight vectors <span class="math display">\[
\bfw_1 = [-2,0], \qquad \bfw_2=[1,1], \qquad \bfw_3=[0,1].
\]</span></p>
<p>For the query <span class="math inline">\(\bfx=[1,0]\)</span>, we get the predictions <span class="math display">\[
\begin{split}
\hat{q}_1 &amp;= \sigma(\bfx^T\bfw_1) =  \sigma\bigl((1)(-2)+(0)(0)\bigr) \approx 0.11920, \\
\hat{q}_2 &amp;= \sigma(\bfx^T\bfw_2) = \sigma\bigl((1)(1)+(0)(1)\bigr) \approx 0.73106, \\
\hat{q}_3 &amp;= \sigma(\bfx^T\bfw_3) = \sigma\bigl((1)(0)+(0)(1)\bigr) = 0.5.
\end{split}
\]</span></p>
<p>So, if we ask, “How much does this <span class="math inline">\(\bfx\)</span> look like class 1?” the answer is, “11.9%”, for “How much like class 2?” it’s “73.1%”, and so on. Since there are three exclusive options for the class of <span class="math inline">\(\bfx\)</span>, the probabilities assigned to the classes are <span class="math display">\[
\begin{split}
\hat{p}_1 &amp;= \frac{\hat{q}_1}{\hat{q}_1 + \hat{q_2} + \hat{q_3}}  \approx 0.08828, \\
\hat{p}_2 &amp;= \frac{\hat{q}_2}{\hat{q}_1 + \hat{q_2} + \hat{q_3}}  \approx 0.54142, \\
\hat{p}_3 &amp;= \frac{\hat{q}_3}{\hat{q}_1 + \hat{q_2} + \hat{q_3}} \approx 0.37030.
\end{split}
\]</span></p>
<p>These probabilities, in exact arithmetic, add up to 100%.</p>
</div>
<p>The situation is now the same as for probabilistic classification in <a href="classification.html#sec-class-probabilistic" class="quarto-xref"><span>Section 3.5</span></a>. Over a testing set, we get a matrix of probabilities. Each of the rows gives the class probabilities at a single query point. We can simply select the most likely class at each test query, or use ROC and AUC to better understand the probabilistic results.</p>
<div id="exm-regression-logistic-gas" class="theorem example" data-chapter="5" type="💻" data-description="Multiclass logistic regression for forest cover dataset">
<p><span class="theorem-title"><strong>Example 5.24</strong></span> As a multiclass example, we return to the dataset for classifying forest cover:</p>
<div id="37d87e4b" class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb99"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb99-1"><a href="#cb99-1" aria-hidden="true" tabindex="-1"></a>forest <span class="op">=</span> datasets.fetch_covtype()</span>
<span id="cb99-2"><a href="#cb99-2" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> forest[<span class="st">"data"</span>][:<span class="dv">250000</span>,:<span class="dv">12</span>]   <span class="co"># 250,000 samples, 12 features</span></span>
<span id="cb99-3"><a href="#cb99-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> forest[<span class="st">"target"</span>][:<span class="dv">250000</span>]</span>
<span id="cb99-4"><a href="#cb99-4" aria-hidden="true" tabindex="-1"></a>X_train, X_test, y_train, y_test <span class="op">=</span> train_test_split(</span>
<span id="cb99-5"><a href="#cb99-5" aria-hidden="true" tabindex="-1"></a>    X, y,</span>
<span id="cb99-6"><a href="#cb99-6" aria-hidden="true" tabindex="-1"></a>    test_size<span class="op">=</span><span class="fl">0.15</span>, </span>
<span id="cb99-7"><a href="#cb99-7" aria-hidden="true" tabindex="-1"></a>    shuffle<span class="op">=</span><span class="va">True</span>, random_state<span class="op">=</span><span class="dv">302</span></span>
<span id="cb99-8"><a href="#cb99-8" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb99-9"><a href="#cb99-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb99-10"><a href="#cb99-10" aria-hidden="true" tabindex="-1"></a>logr <span class="op">=</span> LogisticRegression(solver<span class="op">=</span><span class="st">"liblinear"</span>)</span>
<span id="cb99-11"><a href="#cb99-11" aria-hidden="true" tabindex="-1"></a>pipe <span class="op">=</span> make_pipeline(StandardScaler(), logr)</span>
<span id="cb99-12"><a href="#cb99-12" aria-hidden="true" tabindex="-1"></a>pipe.fit(X_train, y_train)</span>
<span id="cb99-13"><a href="#cb99-13" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"accuracy score is </span><span class="sc">{</span>pipe<span class="sc">.</span>score(X_test, y_test)<span class="sc">:.2%}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>accuracy score is 72.74%</code></pre>
</div>
</div>
<p>We can now look at probabilistic predictions for each class:</p>
<div id="16ecc149" class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb101"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb101-1"><a href="#cb101-1" aria-hidden="true" tabindex="-1"></a>p_hat <span class="op">=</span> pipe.predict_proba(X_test)</span>
<span id="cb101-2"><a href="#cb101-2" aria-hidden="true" tabindex="-1"></a>p_hat[:<span class="dv">3</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>array([[3.27753115e-01, 6.71344154e-01, 8.83821740e-06, 1.47289531e-05,
        6.20692039e-04, 2.57492056e-04, 9.79562864e-07],
       [4.20545770e-01, 5.66374061e-01, 2.90422085e-06, 4.50982065e-07,
        1.40842869e-03, 8.15605577e-06, 1.16602291e-02],
       [2.40111415e-01, 7.59067843e-01, 1.21609701e-05, 1.42404726e-05,
        3.93333619e-04, 3.98538413e-04, 2.46805163e-06]])</code></pre>
</div>
</div>
<p>The ROC curves show that the performance is much worse for 3 of the classes than for the others:</p>
<div id="620b81e6" class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb103"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb103-1"><a href="#cb103-1" aria-hidden="true" tabindex="-1"></a>results <span class="op">=</span> []</span>
<span id="cb103-2"><a href="#cb103-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, label <span class="kw">in</span> <span class="bu">enumerate</span>(pipe.classes_):</span>
<span id="cb103-3"><a href="#cb103-3" aria-hidden="true" tabindex="-1"></a>    actual <span class="op">=</span> (y_test<span class="op">==</span>label)</span>
<span id="cb103-4"><a href="#cb103-4" aria-hidden="true" tabindex="-1"></a>    fp, tp, theta <span class="op">=</span> roc_curve(actual, p_hat[:,i])</span>
<span id="cb103-5"><a href="#cb103-5" aria-hidden="true" tabindex="-1"></a>    results.extend( [ (label,fp,tp) <span class="cf">for</span> fp,tp <span class="kw">in</span> <span class="bu">zip</span>(fp,tp) ] )</span>
<span id="cb103-6"><a href="#cb103-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb103-7"><a href="#cb103-7" aria-hidden="true" tabindex="-1"></a>roc <span class="op">=</span> pd.DataFrame( results, columns<span class="op">=</span>[<span class="st">"label"</span>, <span class="st">"FP rate"</span>, <span class="st">"TP rate"</span>] )</span>
<span id="cb103-8"><a href="#cb103-8" aria-hidden="true" tabindex="-1"></a>sns.relplot(data<span class="op">=</span>roc, </span>
<span id="cb103-9"><a href="#cb103-9" aria-hidden="true" tabindex="-1"></a>    x<span class="op">=</span><span class="st">"FP rate"</span>, y<span class="op">=</span><span class="st">"TP rate"</span>, </span>
<span id="cb103-10"><a href="#cb103-10" aria-hidden="true" tabindex="-1"></a>    hue<span class="op">=</span><span class="st">"label"</span>, kind<span class="op">=</span><span class="st">"line"</span>, estimator<span class="op">=</span><span class="va">None</span>,</span>
<span id="cb103-11"><a href="#cb103-11" aria-hidden="true" tabindex="-1"></a>    palette<span class="op">=</span><span class="st">"Dark2"</span></span>
<span id="cb103-12"><a href="#cb103-12" aria-hidden="true" tabindex="-1"></a>    )<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="regression_files/figure-html/cell-69-output-1.png" width="526" height="468" class="figure-img"></p>
</figure>
</div>
</div>
</div>
</div>
</section>
</section>
<section id="exercises" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="exercises">Exercises</h2>
<div id="exr-regression-no-intercept" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.1</strong></span> (5.1) Suppose that the distinct plane points <span class="math inline">\((x_i,y_i)\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span> are to be fit using a linear function without intercept, <span class="math inline">\(\hat{f}(x)=w x\)</span>. Use calculus to find a formula for the value of <span class="math inline">\(w\)</span> that minimizes the sum of squared residuals, <span class="math display">\[ r = \sum_{i=1}^n \bigl(y_i - \hat{f}(x_i)\bigr)^2. \]</span></p>
</div>
<!-- ::::{#exr-regression-combination}
Suppose that $x_1=-2$, $x_2=1$, and $x_3=2$. Define $\alpha$ as in @exr-regression-no-intercept, and define the predicted values $\hat{y}_k=w x_k$ for $k=1,2,3$. Express each $\hat{y}_k$ as a combination of the three values $y_1$, $y_2$, and $y_3$, which remain arbitrary. (This is a special case of a general fact about linear regression: each prediction is a linear combination of the training values.)
:::: -->
<div id="exr-regression-means" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.2</strong></span> (5.1) Using the formulas derived in <a href="#sec-regression-linear" class="quarto-xref"><span>Section 5.1</span></a>, show that the point <span class="math inline">\((\bar{x},\bar{y})\)</span> always lies on the linear regression line. (Hint: You only have to show that <span class="math inline">\(\hat{f}(\bar{x}) = \bar{y}\)</span>, which can be done without first solving for <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.)</p>
</div>
<div id="exr-regression-performance" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.3</strong></span> (5.1) Suppose that <span class="math display">\[
\begin{split}
    \bfx &amp;= [-2, 0, 1, 3] \\
    \bfy &amp;= [4, 1, 2, 0].
\end{split}
\]</span></p>
<p>Find the <strong>(a)</strong> MSE, <strong>(b)</strong> MAE, and <strong>(c)</strong> coefficient of determination on this set for the regression function <span class="math inline">\(\hat{f}(x)=1-x\)</span>.</p>
</div>
<div id="exr-regression-matvec" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.4</strong></span> (5.2) Suppose for <span class="math inline">\(d=3\)</span> features you have the <span class="math inline">\(n=4\)</span> sample vectors <span class="math display">\[
\bfx_1 = [1,0,1], \quad \bfx_2 = [-1,2,2],\quad \bfx_3=[3,-1,0], \quad \bfx_4 = [0,2,-2],
\]</span></p>
<p>and a multilinear regression computes the weight vector <span class="math inline">\(\bfw = [2,1,-1]\)</span>. Find <strong>(a)</strong> the matrix-vector product <span class="math inline">\(\bfX\bfw\)</span>, and <strong>(b)</strong> the predictions of the regressor on the sample vectors.</p>
</div>
<div id="exr-regression-two-features" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.5</strong></span> (5.2) Suppose that values <span class="math inline">\(y_i\)</span> for <span class="math inline">\(i=1,\ldots,n\)</span> are to be fit to 2D sample vectors using a multilinear regression function <span class="math inline">\(\hat{f}(\bfx)=w_1 x_1 + w_2 x_2\)</span>. Define the sum of squared residuals <span class="math display">\[
r = \sum_{i=1}^n \bigl(y_i - \hat{f}(\bfx_i)\bigr)^2.
\]</span></p>
<p>Show that by holding <span class="math inline">\(w_1\)</span> constant and taking a derivative with respect to <span class="math inline">\(w_2\)</span>, and then holding <span class="math inline">\(w_2\)</span> constant and taking a derivative with respect to <span class="math inline">\(w_1\)</span>, at the minimum residual we must have <span class="math display">\[
\begin{split}
\left(\sum X_{i,1}^2 \right) w_1 + \left(\sum X_{i,1} X_{i,2}  \right) w_2 &amp;= \sum X_{i,1}\, y_i, \\
\left(\sum X_{i,1} X_{i,2} \right) w_1 + \left(\sum X_{i,2}^2 \right) w_2 &amp;= \sum X_{i,2} \, y_i,
\end{split}
\]</span></p>
<p>where <span class="math inline">\(X_{i,1}\)</span> and <span class="math inline">\(X_{i,2}\)</span> are the entries in the <span class="math inline">\(i\)</span>th row of the feature matrix <span class="math inline">\(\bfX\)</span>. (In each case above the sum is from <span class="math inline">\(i=1\)</span> to <span class="math inline">\(i=n\)</span>.)</p>
</div>
<div id="exr-regression-ridge" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.6</strong></span> (5.3) If we fit the model <span class="math inline">\(\hat{f}(x)=w x\)</span> to the single data point <span class="math inline">\((2,6)\)</span>, then the ridge loss is <span class="math display">\[
r(w) = (2w-6)^2 + \alpha w^2,
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is a nonnegative constant. When <span class="math inline">\(\alpha = 0\)</span>, it’s clear that <span class="math inline">\(w=3\)</span> is the minimizer of <span class="math inline">\(r(w)\)</span>. Show that if <span class="math inline">\(\alpha&gt;0\)</span>, then <span class="math inline">\(r'(w)\)</span> is zero at a value of <span class="math inline">\(w\)</span> in the interval <span class="math inline">\((0,3)\)</span>. (This shows that the weight decreases in the presence of the regularization penalty.)</p>
</div>
<div id="exr-regression-lasso" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.7</strong></span> (5.3) If we fit the model <span class="math inline">\(\hat{f}(x)=w x\)</span> to the single data point <span class="math inline">\((2,6)\)</span>, then the LASSO loss is <span class="math display">\[
r(w) = (2w-6)^2 + \alpha |w|,
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is a nonnegative constant. When <span class="math inline">\(\alpha = 0\)</span>, it’s clear that <span class="math inline">\(w=3\)</span> is the minimizer of <span class="math inline">\(r(w)\)</span>. Below you will show that the minimizer is less than this if <span class="math inline">\(\alpha &gt; 0\)</span>.</p>
<p><strong>(a)</strong> Show that if <span class="math inline">\(w &lt; 0\)</span>, then <span class="math inline">\(r'(w)\)</span> cannot be zero. (Remember that for such <span class="math inline">\(w\)</span>, <span class="math inline">\(|w|=-w\)</span>.)</p>
<p><strong>(b)</strong> Show that if <span class="math inline">\(w&gt;0\)</span> and <span class="math inline">\(0&lt;\alpha &lt; 24\)</span>, then <span class="math inline">\(r'(w)\)</span> has a single root in the interval <span class="math inline">\((0,3)\)</span>.</p>
</div>
<!-- 
::::{#exr-regression-regular-two-features}
Repeat @exr-regression-two-features, but using the regularized residual 
$$ 
\tilde{r} = C (\alpha^2 + \beta^2) + \sum_{i=1}^n \bigl(\hat{f}(u_i,v_i)-y_i\bigr)^2. 
$$
:::: -->
<div id="exr-regression-linearity" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.8</strong></span> (5.4) For each function on two-dimensional vectors, either prove that it is linear or produce a counterexample that shows it cannot be linear.</p>
<p><strong>(a)</strong> <span class="math inline">\(\hat{f}(\bfx) = x_1 x_2\)</span></p>
<p><strong>(b)</strong> <span class="math inline">\(\hat{f}(\bfx) = x_2\)</span></p>
<p><strong>(c)</strong> <span class="math inline">\(\hat{f}(\bfx) = x_1 + x_2 + 1\)</span></p>
</div>
<div id="exr-regression-splits" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.9</strong></span> (5.4) Given the data set <span class="math inline">\((x_i,y_i)=\{(0,-1),(1,1),(2,3),(3,0),(4,3)\}\)</span>, find the MAE-based <span class="math inline">\(Q\)</span> score for the following hypothetical decision tree splits.</p>
<p><strong>(a)</strong> <span class="math inline">\(x \le 0.5, \qquad\)</span> <strong>(b)</strong> <span class="math inline">\(x \le 1.5, \qquad\)</span> <strong>(c)</strong> <span class="math inline">\(x \le 2.5,\qquad\)</span> <strong>(d)</strong> <span class="math inline">\(x \le 3.5\)</span>.</p>
</div>
<div id="exr-regression-lattice-values" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.10</strong></span> (5.4) Here are values (labels) on an integer lattice.</p>
<p><a href="_media/griddata.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><img src="_media/griddata.png" class="img-fluid" width="360"></a></p>
<p>Let <span class="math inline">\(\hat{f}(x_1,x_2)\)</span> be the kNN regressor using <span class="math inline">\(k=4\)</span>, Euclidean metric, and mean averaging. In each case below, a function <span class="math inline">\(g(t)\)</span> is defined from values of <span class="math inline">\(\hat{f}\)</span> along a vertical or horizontal line. Carefully sketch a plot of <span class="math inline">\(g(t)\)</span> for <span class="math inline">\(-2\le t \le 2\)</span>.</p>
<p><strong>(a)</strong> <span class="math inline">\(g(t) = \hat{f}(1.2,t)\)</span></p>
<p><strong>(b)</strong> <span class="math inline">\(g(t) = \hat{f}(t,-0.75)\)</span></p>
<p><strong>(c)</strong> <span class="math inline">\(g(t) = \hat{f}(t,1.6)\)</span></p>
<p><strong>(d)</strong> <span class="math inline">\(g(t) = \hat{f}(-0.25,t)\)</span></p>
</div>
<div id="exr-regression-cross-entropy" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.11</strong></span> (5.5) Here are some label values and probabilistic predictions by a logistic regressor:</p>
<p><span class="math display">\[
\begin{split}
    \bfy  &amp;= [0,0,1,1], \\
    \hat{\mathbf{p}} &amp;= [\tfrac{3}{4},0,1,\tfrac{1}{2}].
\end{split}
\]</span></p>
<p>Using base-2 logarithms, calculate the cross-entropy loss for these predictions.</p>
</div>
<div id="exr-regression-cross-entropy-optim" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.12</strong></span> (5.5) Let <span class="math inline">\(\bfx=[-1,0,1]\)</span> and <span class="math inline">\(\bfy=[0,1,0]\)</span>. This small dataset is fit to a probabilistic predictor <span class="math inline">\(\hat{p}(x) = \sigma(w x)\)</span> for weight <span class="math inline">\(w\)</span>.</p>
<p><strong>(a)</strong> Let <span class="math inline">\(L(w)\)</span> be the cross-entropy loss function using natural logarithms. Show that <span class="math display">\[
L'(w) = \frac{e^w-1}{e^w+1}.
\]</span></p>
<p><strong>(b)</strong> Explain why part (a) implies that <span class="math inline">\(w=0\)</span> is the global minimizer of the loss <span class="math inline">\(L\)</span>.</p>
<p><strong>(c)</strong> Using the result of part (b), simplify the optimum predictor function <span class="math inline">\(\hat{p}(x)\)</span>.</p>
</div>
<div id="exr-regression-logistic-lasso" class="theorem exercise">
<p><span class="theorem-title"><strong>Exercise 5.13</strong></span> (5.5) Let <span class="math inline">\(\bfx=[-1,1]\)</span> and <span class="math inline">\(\bfy=[0,1]\)</span>. This small dataset is fit to a probabilistic predictor <span class="math inline">\(\hat{p}(x) = \sigma(w x)\)</span> for weight <span class="math inline">\(w\)</span>. Without regularization, the best fit takes <span class="math inline">\(w\to\infty\)</span>, which makes the predictor become infinitely steep at <span class="math inline">\(x=0\)</span>. To combat this behavior, let <span class="math inline">\(L\)</span> be the cross-entropy loss function with LASSO penalty, i.e.,</p>
<p><span class="math display">\[
L(w) = \ln[1-\hat{p}(-1)] - \ln[\hat{p}(1)] + \alpha |w|,
\]</span></p>
<p>for a positive regularization constant <span class="math inline">\(\alpha\)</span>.</p>
<p><strong>(a)</strong> Show that <span class="math inline">\(L'\)</span> is never zero for <span class="math inline">\(w &lt; 0\)</span>.</p>
<p><strong>(b)</strong> Show that if <span class="math inline">\(0 &lt;\alpha &lt;1\)</span>, then <span class="math inline">\(L'\)</span> has a zero at</p>
<p><span class="math display">\[
w = \ln\left( \frac{2}{\alpha}-1 \right).
\]</span></p>
<p><strong>(c)</strong> Show that <span class="math inline">\(w\)</span> from part (b) is a decreasing function of <span class="math inline">\(\alpha\)</span>. (Therefore, increasing <span class="math inline">\(\alpha\)</span> makes the predictor less steep as a function of <span class="math inline">\(x\)</span>.)</p>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./selection.html" class="pagination-link" aria-label="Model selection">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Model selection</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./clustering.html" class="pagination-link" aria-label="Clustering">
        <span class="nav-page-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Clustering</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Data Science 1
  </li>  
</ul>
    </div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item">
 Toby Driscoll
  </li>  
</ul>
    </div>
  </div>
</footer>
<script>videojs(video_shortcode_videojs_video1);</script>
<script>var lightboxQuarto = GLightbox({"openEffect":"zoom","closeEffect":"zoom","selector":".lightbox","loop":false,"descPosition":"bottom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>